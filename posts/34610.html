<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="Pytorch学习: Pytorch Lightning, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>Pytorch学习: Pytorch Lightning | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:.6"></i> <span>时间轴</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li><li><a href="/markdown"><i class="fab fa-markdown" style="margin-top:-20px;zoom:.6"></i> <span>Markdown</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li><li><a href="/markdown" style="margin-left:75px"><i class="fa fab fa-markdown" style="position:absolute;left:50px"></i> <span>Markdown</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/2.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">Pytorch学习: Pytorch Lightning</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/%E7%BC%96%E7%A8%8B/"><span class="chip bg-color">编程</span> </a><a href="/tags/pytorch/"><span class="chip bg-color">pytorch</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2022-08-14</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2023-02-12</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 3.6k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 15 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><h1 id="Pytorch学习-Pytorch-Lightning"><a href="#Pytorch学习-Pytorch-Lightning" class="headerlink" title="Pytorch学习: Pytorch Lightning"></a>Pytorch学习: Pytorch Lightning</h1><p>Pytorch Lightning是在Pytorch基础上封装的框架, 号称”Pytorch里的Keras”, 如官网所述, 它具有灵活, 解耦, 易于复现, 自动化, 扩展性好等优点(实际上大多也是Keras的优点哈哈哈). 知乎上对Pytorch Lightning的议论比较多, 有些人认为Pytorch Lightning纯属过度封装, 但它事实上确实能解决一些Pytorch自身不好解决的问题. 最主要的其实是保证了<strong>代码复用</strong>, 节省时间.<br>和Huggingface出品的Trainer相比, 我感觉在大多数任务上, Pytorch Lightning要更加灵活一些.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>下面是一个官方给出的VAE在MNIST上的例子, 大概建立一下Pytorch Lightning的初印象:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> random_split
<span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> MNIST
<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> transforms
<span class="token keyword">import</span> pytorch_lightning <span class="token keyword">as</span> pl

<span class="token keyword">class</span> <span class="token class-name">LitAutoEncoder</span><span class="token punctuation">(</span>pl<span class="token punctuation">.</span>LightningModule<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
          nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">28</span> <span class="token operator">*</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
          nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
          nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
          nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
          nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
          nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">28</span> <span class="token operator">*</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        embedding <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> embedding

    <span class="token keyword">def</span> <span class="token function">configure_optimizers</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> optimizer

    <span class="token keyword">def</span> <span class="token function">training_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> train_batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x<span class="token punctuation">,</span> y <span class="token operator">=</span> train_batch
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        z <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    
        x_hat <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>z<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> F<span class="token punctuation">.</span>mse_loss<span class="token punctuation">(</span>x_hat<span class="token punctuation">,</span> x<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token string">'train_loss'</span><span class="token punctuation">,</span> loss<span class="token punctuation">)</span>
        <span class="token keyword">return</span> loss

    <span class="token keyword">def</span> <span class="token function">validation_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> val_batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x<span class="token punctuation">,</span> y <span class="token operator">=</span> val_batch
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        z <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x_hat <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>z<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> F<span class="token punctuation">.</span>mse_loss<span class="token punctuation">(</span>x_hat<span class="token punctuation">,</span> x<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> loss<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># data</span>
dataset <span class="token operator">=</span> MNIST<span class="token punctuation">(</span><span class="token string">''</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
mnist_train<span class="token punctuation">,</span> mnist_val <span class="token operator">=</span> random_split<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">55000</span><span class="token punctuation">,</span> <span class="token number">5000</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

train_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>mnist_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">)</span>
val_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>mnist_val<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># model</span>
model <span class="token operator">=</span> LitAutoEncoder<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># training</span>
trainer <span class="token operator">=</span> pl<span class="token punctuation">.</span>Trainer<span class="token punctuation">(</span>gpus<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> num_nodes<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> precision<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> limit_train_batches<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span>
trainer<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>model<span class="token punctuation">,</span> train_loader<span class="token punctuation">,</span> val_loader<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Pytorch Lightning最重要的两个API便是<code>LightningModule</code>和<code>Trainer</code>. <code>pl.LightningModule</code>和<code>nn.Module</code>有点像对吧? 都有<code>forward()</code>. 没错, “A <code>LightningModule</code> is still just a <code>torch.nn.Module</code>“.<br>从代码里可以看出, 在<code>pl.LightningModule</code>下重写了<code>training_step</code>, <code>validation_step</code>, 完成模型训练和验证的<strong>内部流程</strong>即可, 整个训练的逻辑已经被它封装好了, 无需重写.<br>同时, DataLoader使用的是Pytorch自己的DataLoader, 二者兼容. Pytorch Lightning有对DataLoader在逻辑上的进一步封装, 方便组织数据的加载逻辑. 但是我自己用的不是很习惯, 本文中就不提及了, 感兴趣的去<a href="https://pytorch-lightning.readthedocs.io/en/latest/data/datamodule.html" target="_blank" rel="noopener">这里</a>查阅.<br>通过<code>trainer.fit</code>就开始了模型的训练, 和Keras很像.<br>事实上, 整个<code>pl.LightningModule</code>只是组织了下列6种行为的逻辑:</p><ul><li><strong>Computations</strong> (init).</li><li><strong>Train Loop</strong> (training_step)</li><li><strong>Validation Loop</strong> (validation_step)</li><li><strong>Test Loop</strong> (test_step)</li><li><strong>Prediction Loop</strong> (predict_step)</li><li><strong>Optimizers and LR Schedulers</strong> (configure_optimizers)<br>记住, <strong>它并没有做进一步抽象</strong>, 只是简单的<strong>把逻辑组织在一起</strong>.</li></ul><h2 id="Initialization-amp-Forward"><a href="#Initialization-amp-Forward" class="headerlink" title="Initialization &amp; Forward"></a>Initialization &amp; Forward</h2><p><code>pl.LightningModule</code><strong>继承</strong>于<code>nn.Module</code>, 也就是说你call它的时候会默认调用它的<code>forward()</code>.</p><p>但是, <code>forward</code>的具体行为在Training和Validation甚至是Prediction的时候可能是不同的, 所以只能写模型自身的逻辑, 不要把Loss的计算也写进去, 也不要把<code>logits.argmax</code>写进去.<br>一般来说, <code>pl.LightningModule</code>的初始化和<code>forward</code>是这样写的:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">TaskModel</span><span class="token punctuation">(</span>pl<span class="token punctuation">.</span>LightningModule<span class="token punctuation">)</span><span class="token punctuation">:</span>  
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model<span class="token punctuation">)</span><span class="token punctuation">:</span>  
        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>  
        self<span class="token punctuation">.</span>model <span class="token operator">=</span> model  

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">**</span>inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>  
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>没错, 仅仅是将模型在<code>pl.LightningModule</code>初始化时作为参数传进来, 然后添加一个Hook… 就像<a href="https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#child-modules" target="_blank" rel="noopener">这样</a>. 强烈建议把模型本身和训练逻辑解耦, 将来改起来方便很多.</p><h2 id="Training-amp-Validation"><a href="#Training-amp-Validation" class="headerlink" title="Training &amp; Validation"></a>Training &amp; Validation</h2><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p><code>pl.LightningModule</code>组织的训练逻辑伪代码如下:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">fit_loop</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    on_train_epoch_start<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">for</span> batch <span class="token keyword">in</span> train_dataloader<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        on_train_batch_start<span class="token punctuation">(</span><span class="token punctuation">)</span>

        on_before_batch_transfer<span class="token punctuation">(</span><span class="token punctuation">)</span>
        transfer_batch_to_device<span class="token punctuation">(</span><span class="token punctuation">)</span>
        on_after_batch_transfer<span class="token punctuation">(</span><span class="token punctuation">)</span>

        training_step<span class="token punctuation">(</span><span class="token punctuation">)</span>

        on_before_zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer_zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

        on_before_backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        on_after_backward<span class="token punctuation">(</span><span class="token punctuation">)</span>

        on_before_optimizer_step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        configure_gradient_clipping<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer_step<span class="token punctuation">(</span><span class="token punctuation">)</span>

        on_train_batch_end<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> should_check_val<span class="token punctuation">:</span>
            val_loop<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment" spellcheck="true"># end training epoch</span>
    training_epoch_end<span class="token punctuation">(</span><span class="token punctuation">)</span>

    on_train_epoch_end<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>似乎很多对吧, 事实上我们只需要关注下面两个函数:</p><ol><li><code>training_step(self, batch, batch_idx)</code>.</li><li><code>training_epoch_end(self, training_step_outputs)</code>.</li></ol><p>其他的函数在项目规模不大的时候不会用到, 例如<code>on_train_epoch_end</code>, <code>on_train_batch_end</code>, 看起来比较美好, 但是实际上有些鸡肋, 因为合适的逻辑已经在<code>training_step</code>和<code>training_epoch_end</code>里搞定了, 而且它们不存在耦合问题.<br>我们使用Pytorch Lightning的目的就是为了快速搭建一套能跑的流程, 如果真的用到了再去查文档就好.</p><h4 id="training-step"><a href="#training-step" class="headerlink" title="training_step"></a>training_step</h4><p>batch就是<code>DataLoader</code>里返回的batch, 一般来说<code>training_step</code>里就是把batch解包, 然后计算loss. 例如:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">training_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
    x<span class="token punctuation">,</span> y <span class="token operator">=</span> batch
    y_hat <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
    <span class="token keyword">return</span> loss<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>返回值可以是loss, 也可以是一个<strong>字典</strong>, 如果你想在每个训练epoch结束的时候再计算点别的什么东西, 可以这样写:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">training_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
    x<span class="token punctuation">,</span> y <span class="token operator">=</span> batch
    y_hat <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
    preds <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    <span class="token keyword">return</span> <span class="token punctuation">{</span>
        <span class="token string">"loss"</span><span class="token punctuation">:</span> loss<span class="token punctuation">,</span> 
        <span class="token string">"other_stuff"</span><span class="token punctuation">:</span> preds<span class="token punctuation">,</span>
    <span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这样在<code>training_epoch_end</code>中可以取到<code>other_stuff</code>. 但是一定要保证里面有个<code>loss</code>, 这样才能保证整个batch正常工作.</p><h4 id="training-epoch-end"><a href="#training-epoch-end" class="headerlink" title="training_epoch_end"></a>training_epoch_end</h4><p>在每个epoch训练结束时调用<code>training_epoch_end</code>, 其参数<code>training_step_outputs</code>实际上是<strong>每个step返回的字典的一个列表</strong>.<br>例如:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">training_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
    x<span class="token punctuation">,</span> y <span class="token operator">=</span> batch
    y_hat <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
    preds <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    <span class="token keyword">return</span> <span class="token punctuation">{</span><span class="token string">"loss"</span><span class="token punctuation">:</span> loss<span class="token punctuation">,</span> <span class="token string">"other_stuff"</span><span class="token punctuation">:</span> preds<span class="token punctuation">}</span>


<span class="token keyword">def</span> <span class="token function">training_epoch_end</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> training_step_outputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    all_preds <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>training_step_outputs<span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>training_epoch_end</code>无返回值限制.<br>例子中的<code>preds</code>应该也是一个Tensor, 我们也可以在每个step结束时返回其他类型的值.</p><h4 id="log"><a href="#log" class="headerlink" title="log"></a>log</h4><p>在训练时一般都要把loss记录下来, 使用<code>self.log()</code>就可以把标量记录下来, 在其他地方也都可以随时使用. 例如:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">training_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
    x<span class="token punctuation">,</span> y <span class="token operator">=</span> batch
    y_hat <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span>

    <span class="token comment" spellcheck="true"># logs metrics for each training_step,</span>
    <span class="token comment" spellcheck="true"># and the average across the epoch, to the progress bar and logger</span>
    self<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token string">"train_loss"</span><span class="token punctuation">,</span> loss<span class="token punctuation">,</span> on_step<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> on_epoch<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> prog_bar<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> logger<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> loss<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>on_step</code>即一个step记录一次, 如果也同时<code>on_epoch</code>, 它会将整个epoch的loss加起来求个平均, 在上述代码里同时记录了<code>train_loss_step</code>和<code>train_loss_epoch</code>.<br>记录的值可以在<strong>Tensorboard</strong>里看到, 非常方便.</p><ul><li>如果有多个要记录的值, 可以把它们都放进一个字典里, 然后使用<code>self.log_dict(dict)</code>一并记录下来.</li><li>如果要记录的内容是图像, 语音等其他类型, 则需要调用<code>logger</code>来存储, 从<a href="https://pytorch-lightning.readthedocs.io/en/latest/visualize/experiment_managers.html" target="_blank" rel="noopener">这里</a>获取更多信息.</li></ul><h3 id="Validataion"><a href="#Validataion" class="headerlink" title="Validataion"></a>Validataion</h3><p>验证和被包含在训练逻辑中, 但流程几乎是一样的, 只是少了梯度优化的参与.<br><code>pl.LightningModule</code>组织的验证逻辑伪代码如下:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">val_loop</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    on_validation_model_eval<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># calls `model.eval()`</span>
    torch<span class="token punctuation">.</span>set_grad_enabled<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span>

    on_validation_start<span class="token punctuation">(</span><span class="token punctuation">)</span>
    on_validation_epoch_start<span class="token punctuation">(</span><span class="token punctuation">)</span>

    val_outs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> batch_idx<span class="token punctuation">,</span> batch <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>val_dataloader<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        on_validation_batch_start<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span>

        batch <span class="token operator">=</span> on_before_batch_transfer<span class="token punctuation">(</span>batch<span class="token punctuation">)</span>
        batch <span class="token operator">=</span> transfer_batch_to_device<span class="token punctuation">(</span>batch<span class="token punctuation">)</span>
        batch <span class="token operator">=</span> on_after_batch_transfer<span class="token punctuation">(</span>batch<span class="token punctuation">)</span>

        out <span class="token operator">=</span> validation_step<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span>

        on_validation_batch_end<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span>
        val_outs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

    validation_epoch_end<span class="token punctuation">(</span>val_outs<span class="token punctuation">)</span>

    on_validation_epoch_end<span class="token punctuation">(</span><span class="token punctuation">)</span>
    on_validation_end<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment" spellcheck="true"># set up for train</span>
    on_validation_model_train<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># calls `model.train()`</span>
    torch<span class="token punctuation">.</span>set_grad_enabled<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>与训练不同的是, 在验证开始前, <code>pl.LightningModule</code>会自动为我们启用<code>model.eval()</code>, 还会禁用梯度. 可以不必重复声明<code>torch.no_grad</code>, 如果不放心的话可以再包上一层.<br>我们同样只需要关注与训练过程相似的两个函数:</p><ol><li><code>validation_step(self, batch, batch_idx)</code>.</li><li><code>validation_epoch_end(self, validation_step_outputs)</code>.</li></ol><h4 id="validation-step"><a href="#validation-step" class="headerlink" title="validation_step"></a>validation_step</h4><p>与训练中的<code>training_step</code>相同. 直接贴出一个例子:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LitModel</span><span class="token punctuation">(</span>pl<span class="token punctuation">.</span>LightningModule<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">validation_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x<span class="token punctuation">,</span> y <span class="token operator">=</span> batch
        y_hat <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token string">"val_loss"</span><span class="token punctuation">,</span> loss<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="validation-epoch-end"><a href="#validation-epoch-end" class="headerlink" title="validation_epoch_end"></a>validation_epoch_end</h4><p>与训练中的<code>training_epoch_end</code>相同, 这里拿到的<code>validation_step_outputs</code>也是每个<code>validation_step</code>的返回值的一个字典的列表. 例如:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">validation_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
    x<span class="token punctuation">,</span> y <span class="token operator">=</span> batch
    y_hat <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
    pred <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    <span class="token keyword">return</span> pred


<span class="token keyword">def</span> <span class="token function">validation_epoch_end</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> validation_step_outputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    all_preds <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>validation_step_outputs<span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>validation_epoch_end</code>无返回值限制.</p><h2 id="Optimizer-amp-LR-Scheduler"><a href="#Optimizer-amp-LR-Scheduler" class="headerlink" title="Optimizer &amp; LR Scheduler"></a>Optimizer &amp; LR Scheduler</h2><p>在文章最开始的例子中, 我们重写了<code>configure_optimizers()</code>来为模型准备优化器. 大多数时候我们只需要一个optimizer和scheduler:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">configure_optimizers</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># or</span>
<span class="token keyword">def</span> <span class="token function">configure_optimizers</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    optimizer <span class="token operator">=</span> Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span>
    scheduler <span class="token operator">=</span> get_linear_schedule_with_warmup<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> self<span class="token punctuation">.</span>total_step<span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token punctuation">[</span>optimizer<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>scheduler<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果只有optimizer, 直接返回即可, 如果还有scheduler, 则需要把optimizer和scheduler分别套上一个list返回.<br>同时, 在<code>pl.LightningModule</code>内部使用<code>self.parameters()</code>可以获得所有的模型参数, 因为它继承了<code>nn.Module</code>.<br>再复杂一点, 也可以通过返回字典来控制optimizer和scheduler执行的间隔(<code>interval / frequency</code>):</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># example with step-based learning rate schedulers</span>
<span class="token keyword">def</span> <span class="token function">configure_optimizers</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    gen_opt <span class="token operator">=</span> Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>model_gen<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>
    dis_opt <span class="token operator">=</span> Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>model_disc<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.02</span><span class="token punctuation">)</span>
    gen_sched <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'scheduler'</span><span class="token punctuation">:</span> ExponentialLR<span class="token punctuation">(</span>gen_opt<span class="token punctuation">,</span> <span class="token number">0.99</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                 <span class="token string">'interval'</span><span class="token punctuation">:</span> <span class="token string">'step'</span><span class="token punctuation">}</span>  <span class="token comment" spellcheck="true"># called after each training step</span>
    dis_sched <span class="token operator">=</span> CosineAnnealing<span class="token punctuation">(</span>discriminator_opt<span class="token punctuation">,</span> T_max<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># called every epoch</span>
    <span class="token keyword">return</span> <span class="token punctuation">[</span>gen_opt<span class="token punctuation">,</span> dis_opt<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>gen_sched<span class="token punctuation">,</span> dis_sched<span class="token punctuation">]</span>

<span class="token comment" spellcheck="true"># example with optimizer frequencies</span>
<span class="token comment" spellcheck="true"># see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1</span>
<span class="token comment" spellcheck="true"># https://arxiv.org/abs/1704.00028</span>
<span class="token keyword">def</span> <span class="token function">configure_optimizers</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    gen_opt <span class="token operator">=</span> Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>model_gen<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>
    dis_opt <span class="token operator">=</span> Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>model_disc<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.02</span><span class="token punctuation">)</span>
    n_critic <span class="token operator">=</span> <span class="token number">5</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>
        <span class="token punctuation">{</span><span class="token string">'optimizer'</span><span class="token punctuation">:</span> dis_opt<span class="token punctuation">,</span> <span class="token string">'frequency'</span><span class="token punctuation">:</span> n_critic<span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token punctuation">{</span><span class="token string">'optimizer'</span><span class="token punctuation">:</span> gen_opt<span class="token punctuation">,</span> <span class="token string">'frequency'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">}</span>
    <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>注意, <code>transformers</code> 里面的Warm up LRScheduler往往是根据Step完成学习率调节的!!! 而Pytorch Lightning默认是每个Epoch调整一次学习率</strong> 这点非常重要!!!</p><p>所以在使用<code>transformers</code> 的Scheduler时, <strong>必须</strong>把Scheduler的执行间隔<code>interval</code> 设置为<code>step</code>, 放入字典中返回:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AdamW<span class="token punctuation">,</span> get_linear_schedule_with_warmup
<span class="token keyword">def</span> <span class="token function">configure_optimizers</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    optimizer <span class="token operator">=</span> Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span>
    total_steps <span class="token operator">=</span> self<span class="token punctuation">.</span>trainer<span class="token punctuation">.</span>estimated_stepping_batches
    warmup_steps <span class="token operator">=</span> warmup_ratio <span class="token operator">*</span> total_steps
    scheduler <span class="token operator">=</span> get_linear_schedule_with_warmup<span class="token punctuation">(</span>
        optimizer<span class="token punctuation">,</span> 
        num_warmup_steps<span class="token operator">=</span>warmup_steps<span class="token punctuation">,</span>  
        num_training_steps<span class="token operator">=</span>total_steps<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
    scheduler <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">"scheduler"</span><span class="token punctuation">:</span> scheduler<span class="token punctuation">,</span> <span class="token string">"interval"</span><span class="token punctuation">:</span> <span class="token string">"step"</span><span class="token punctuation">}</span>
    <span class="token keyword">return</span> <span class="token punctuation">[</span>optimizer<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>scheduler<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>示例中的<code>total_steps</code>, 可以直接通过Trainer的<code>estimated_stepping_batches</code> 属性拿到, 不用手动计算.</p><blockquote><p>此外, 一些特殊情况会用到多个优化器或者多个Scheduler, 首先参考<a href="https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#configure-optimizers" target="_blank" rel="noopener">这里</a> , 并在<code>training_step</code>中使用<code>optimizer_idx</code>来控制loss和optimizer的关联, 参考<a href="https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#training-step" target="_blank" rel="noopener">这里</a>.</p></blockquote><h2 id="Test-amp-Predict"><a href="#Test-amp-Predict" class="headerlink" title="Test &amp; Predict"></a>Test &amp; Predict</h2><p>PL应该是为了满足客制化而将Test和Predict区分开. 在我们跑实验而没有部署时, Test和Predict行为并没有什么区别, 但测试和真正Inference的时候的Predict还是不一样的, Predict没有标签.<br>和验证时相同, model.eval()和<code>torch.no_grad()</code>会自动在测试和预测时自动配上.<br>当Trainer调用<code>trainer.test()</code>时, 会调用<code>test_step()</code>, 它与<code>training_step</code>, <code>validation_step</code>类似, 一般重写<code>test_step</code>时只是一层对<code>validation_step</code>的封装.<br>在测试结束时, 我比较推荐在<code>test_step</code>返回batch级的预测结果, <code>test_epoch_end</code>一并<strong>保存实验结果</strong>, 这样封装一层比较有意义.<br>Predict仅有<code>predict_step</code>, 而没有<code>predict_epoch_end</code>.</p><h2 id="Trainer"><a href="#Trainer" class="headerlink" title="Trainer"></a>Trainer</h2><p><code>pl.LightningModule</code><strong>组织</strong>了逻辑, 而<code>pl.Trainer</code><strong>驱动</strong>了流程.<br>其拟合阶段伪代码如下:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">fit</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> global_rank <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># prepare data is called on GLOBAL_ZERO only</span>
        prepare_data<span class="token punctuation">(</span><span class="token punctuation">)</span>

    configure_callbacks<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">with</span> parallel<span class="token punctuation">(</span>devices<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># devices can be GPUs, TPUs, ...</span>
        train_on_device<span class="token punctuation">(</span>model<span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">train_on_device</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># called PER DEVICE</span>
    on_fit_start<span class="token punctuation">(</span><span class="token punctuation">)</span>
    setup<span class="token punctuation">(</span><span class="token string">"fit"</span><span class="token punctuation">)</span>
    configure_optimizers<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment" spellcheck="true"># the sanity check runs here</span>

    on_train_start<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> epochs<span class="token punctuation">:</span>
        fit_loop<span class="token punctuation">(</span><span class="token punctuation">)</span>
    on_train_end<span class="token punctuation">(</span><span class="token punctuation">)</span>

    on_fit_end<span class="token punctuation">(</span><span class="token punctuation">)</span>
    teardown<span class="token punctuation">(</span><span class="token string">"fit"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>一般我们这样使用Trainer来完成包含测试在内的整个流程:</p><pre class="line-numbers language-python"><code class="language-python">model <span class="token operator">=</span> MyLightningModule<span class="token punctuation">(</span><span class="token punctuation">)</span>

trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span><span class="token punctuation">)</span>
trainer<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>model<span class="token punctuation">,</span> train_dataloader<span class="token punctuation">,</span> val_dataloader<span class="token punctuation">)</span>
trainer<span class="token punctuation">.</span>test<span class="token punctuation">(</span>model<span class="token punctuation">,</span> test_dataloader<span class="token punctuation">,</span> ckpt<span class="token operator">=</span><span class="token string">"best"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>对Trainer的简单用法说明如下:</p><ul><li>使用<code>trainer.stage_name()</code>可以让模型执行相应的阶段(fit, validate, test, predict).<br>如果不主动调用<code>trainer.test()</code>, 则不会执行测试阶段.</li><li><code>trainer.validate()</code>, <code>trainer.predict()</code>, 可以分别让模型执行验证和预测阶段, 前者被包含在模型的训练过程中, 无需重复调用.</li><li>虽然官网有写<code>trainer.test()</code>, <code>trainer.predict()</code>会自动加载最好的模型检查点后再测试和预测, 但我实测的时候没有加载, 默认是使用最后一个epoch测试和预测. 而在设置<code>ckpt_path=&quot;best&quot;</code>才会加载最好的模型, 否则是以最后一个epoch的模型进行测试的. 该参数在<code>trainer.fit()</code>中附加也可以让模型从该检查点开始训练.</li></ul><h3 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h3><p>定义Trainer时有很多参数很好用, 在这里推荐一些.</p><ul><li><code>max_epochs</code>: <strong>最大Epoch</strong>, 肯定要设置.</li><li><code>default_root_dir</code>: 默认存储模型, 日志<strong>地址</strong>. 如果不设置, 每次跑实验时候都会多一个<code>version_x</code>文件夹, 看个人喜好和需求.</li><li><code>val_check_interval</code>: <strong>验证间隔</strong>, 计量单位是epoch, 如果有更高的验证频次需求, 也可以设置为小数, 即不到1个epoch验证一次.</li><li><code>gpus</code>: 使用的GPU数量. 在即将出现的2.0版本中会被<code>accelerator=gpu</code>, <code>device=x</code>取代.</li><li><code>precision</code>: 全精度 / 半精度训练.</li><li><code>accumulate_grad_batches</code>: <strong>梯度累加</strong>, 可以多个batch更新一次梯度, 以间接的近似大batch的效果. PS: 听说对比学习不能用.</li><li><code>gradient_clip_val</code>: <strong>梯度裁剪</strong>, 将梯度大小限制在该值内, 防止梯度过大崩掉.</li><li><code>num_sanity_val_steps</code>: 在执行训练前会先用几个batch的验证数据跑一下, 检查代码是否有问题, 设置为-1为全部, 0为不检测. 我一般设置为0.</li><li><code>callbacks</code>: <strong>回调函数</strong>, 接受值为回调函数的列表, 下小节会讲.</li></ul><h3 id="Callback"><a href="#Callback" class="headerlink" title="Callback"></a>Callback</h3><p>一般来说, <strong>早停</strong>和<strong>检查点</strong>是两个比较常用的Callback, 需要在Trainer定义时作为参数传入. 例如:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> pytorch_lightning <span class="token keyword">import</span> Trainer  
<span class="token keyword">from</span> pytorch_lightning<span class="token punctuation">.</span>callbacks <span class="token keyword">import</span> EarlyStopping<span class="token punctuation">,</span> ModelCheckpoint  

early_stopping <span class="token operator">=</span> EarlyStopping<span class="token punctuation">(</span><span class="token string">'val_loss'</span><span class="token punctuation">)</span>  
checkpoint <span class="token operator">=</span> ModelCheckpoint<span class="token punctuation">(</span>  
    save_weights_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>  
    save_on_train_epoch_end<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>  
    monitor<span class="token operator">=</span><span class="token string">"valid_f1"</span><span class="token punctuation">,</span>  
    mode<span class="token operator">=</span><span class="token string">"max"</span><span class="token punctuation">,</span>  
    save_top_k<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>  
<span class="token punctuation">)</span>

trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>callbacks<span class="token operator">=</span><span class="token punctuation">[</span>early_stopping<span class="token punctuation">,</span> checkpoint<span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>仅当<code>ModelCheckpoint</code>的<code>save_on_train_epoch_end</code>设置为False时才会在验证时保存, 否则设置为True时是在训练时保存, 默认为None.</p></blockquote><p>还有一个<code>PrintTableMetricsCallback</code>, 不用带参数, 会在每个epoch结束时打印表格, 不过我基本不用.</p><h3 id="Trainer-in-Python-scripts"><a href="#Trainer-in-Python-scripts" class="headerlink" title="Trainer in Python scripts"></a>Trainer in Python scripts</h3><p>通常情况下, 使用<code>ArgumentParser</code>能更灵活的跑实验. 可以对Trainer手动添加参数:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> argparse <span class="token keyword">import</span> ArgumentParser


<span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span>hparams<span class="token punctuation">)</span><span class="token punctuation">:</span>
    model <span class="token operator">=</span> LightningModule<span class="token punctuation">(</span><span class="token punctuation">)</span>
    trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>accelerator<span class="token operator">=</span>hparams<span class="token punctuation">.</span>accelerator<span class="token punctuation">,</span> devices<span class="token operator">=</span>hparams<span class="token punctuation">.</span>devices<span class="token punctuation">)</span>
    trainer<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>model<span class="token punctuation">)</span>


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    parser <span class="token operator">=</span> ArgumentParser<span class="token punctuation">(</span><span class="token punctuation">)</span>
    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--accelerator"</span><span class="token punctuation">,</span> default<span class="token operator">=</span>None<span class="token punctuation">)</span>
    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--devices"</span><span class="token punctuation">,</span> default<span class="token operator">=</span>None<span class="token punctuation">)</span>
    args <span class="token operator">=</span> parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span>

    main<span class="token punctuation">(</span>args<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果需要修改某些参数可以在命令行附带上:</p><pre class="line-numbers language-shell"><code class="language-shell">python main.py --accelerator 'gpu' --devices 2<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>但上面手动很麻烦, Trainer支持自动添加参数到里面:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> argparse <span class="token keyword">import</span> ArgumentParser


<span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
    model <span class="token operator">=</span> LightningModule<span class="token punctuation">(</span><span class="token punctuation">)</span>
    trainer <span class="token operator">=</span> Trainer<span class="token punctuation">.</span>from_argparse_args<span class="token punctuation">(</span>args<span class="token punctuation">)</span>
    trainer<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>model<span class="token punctuation">)</span>


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    parser <span class="token operator">=</span> ArgumentParser<span class="token punctuation">(</span><span class="token punctuation">)</span>
    parser <span class="token operator">=</span> Trainer<span class="token punctuation">.</span>add_argparse_args<span class="token punctuation">(</span>parser<span class="token punctuation">)</span>
    args <span class="token operator">=</span> parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span>

    main<span class="token punctuation">(</span>args<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>也可以走混合路线, 同时定义别的超参和Trainer的参数到parser里.</p><blockquote><p>其实都不如<strong>Hydra</strong>来的优雅, 见文末Recommended推荐的模板.</p></blockquote><h2 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h2><p>这一节写一些我自己使用过程中用到的一些很有用的小技巧.</p><ol><li><p>在<code>pl.LightningModule</code>的构造函数里面, 使用<code>self.save_hyperparameters()</code>可以将<code>pl.LightningModule</code>中所有的传入参数记录到yaml文件里, 非常方便于实验记录.</p></li><li><p><code>pl.seedeverything()</code>, 彻底告别自己写随机种子设置函数.</p></li><li><p>有的时候想把模型的预测结果和模型本身的权重保存到同一个目录下, 但是我不想自己按照规则去写路径, 而是和Trainer的设置同步, 该怎么办呢? 在<code>pl.LightningModule</code>里会添加<code>Trainer</code>的Hook, 调用<code>self.trainer</code>就能够获得它身上的属性. 例如我想把模型预测结果保存到日志目录下, 应该这么写:</p><pre class="line-numbers language-python"><code class="language-python">pred_save_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>self<span class="token punctuation">.</span>trainer<span class="token punctuation">.</span>log_dir<span class="token punctuation">,</span> <span class="token string">"prediction.json"</span><span class="token punctuation">)</span>  
your_save_function<span class="token punctuation">(</span>pred_save_path<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>其他需要的属性也是同理, 通过Hook可以轻松拿到Trainer身上的属性.</p></li><li><p>使用<code>pl.LightningModule.load_from_checkpoint(ckpt_path)</code>可以一条命令直接为TaskModel加载超参和模型权重.</p></li></ol><h2 id="Reference-amp-Recommended"><a href="#Reference-amp-Recommended" class="headerlink" title="Reference &amp; Recommended"></a>Reference &amp; Recommended</h2><ul><li>Pytorch Lightning官方文档: <a href="https://www.pytorchlightning.ai/tutorials" target="_blank" rel="noopener">PyTorch Lightning Tutorials</a>.</li><li>Pytorch Lightning API: <a href="https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html" target="_blank" rel="noopener">LightningModule — PyTorch Lightning 1.7.1 documentation</a>, <a href="https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html" target="_blank" rel="noopener">Trainer — PyTorch Lightning 1.7.1 documentation</a>.</li><li>Example of Transformer: <a href="https://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/text-transformers.html" target="_blank" rel="noopener">Finetune Transformers Models with PyTorch Lightning — PyTorch Lightning 1.8.0dev documentation</a>.</li><li>知乎的攻略帖: <a href="https://zhuanlan.zhihu.com/p/353985363" target="_blank" rel="noopener">Pytorch Lightning 完全攻略 - 知乎</a>.</li><li>一套传闻不错的PL模板(需要学习hydra): <a href="https://github.com/ashleve/lightning-hydra-template" target="_blank" rel="noopener">GitHub - ashleve/lightning-hydra-template: PyTorch Lightning + Hydra. A very user-friendly template for rapid and reproducible ML experimentation with best practices. ⚡🔥⚡</a>.</li><li>另一套简单很多的PL模板: <a href="https://github.com/KinWaiCheuk/pytorch_template" target="_blank" rel="noopener">GitHub - KinWaiCheuk/pytorch_template: Template that combines PyTorch Lightning and Hydra</a>.</li></ul></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">AnNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/34610.html">https://ADAning.github.io/posts/34610.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">AnNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/%E7%BC%96%E7%A8%8B/"><span class="chip bg-color">编程</span> </a><a href="/tags/pytorch/"><span class="chip bg-color">pytorch</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/4754.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/21.jpg" class="responsive-img" alt="OneRel: Joint Entity and Relation Extraction with One Module in One Step"> <span class="card-title">OneRel: Joint Entity and Relation Extraction with One Module in One Step</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: TPLinker: TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking. OneR</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2022-08-14 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/RTE/"><span class="chip bg-color">RTE</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/42381.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/16.jpg" class="responsive-img" alt="RFBFN: A Relation - First Blank Filling Network for Joint Relational Triple Extraction"> <span class="card-title">RFBFN: A Relation - First Blank Filling Network for Joint Relational Triple Extraction</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: SPN: SPN: Joint Entity and Relation Extraction with Set Prediction Networks. RFBFN: A Relation - First Blank</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2022-07-09 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/RTE/"><span class="chip bg-color">RTE</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">AnNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">358.1k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:695439722@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>