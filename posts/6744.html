<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="Transformer精讲, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>Transformer精讲 | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:.6"></i> <span>时间轴</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li><li><a href="/markdown"><i class="fab fa-markdown" style="margin-top:-20px;zoom:.6"></i> <span>Markdown</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li><li><a href="/markdown" style="margin-left:75px"><i class="fa fab fa-markdown" style="position:absolute;left:50px"></i> <span>Markdown</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/3.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">Transformer精讲</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/Transformer/"><span class="chip bg-color">Transformer</span> </a><a href="/tags/Attention/"><span class="chip bg-color">Attention</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2020-09-21</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2022-04-03</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 6.5k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 24 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p><strong>2020.10.05</strong>: 更新训练技巧.</p><p><strong>2020.09.27</strong>: 更新Masked Multi - Head Attention理解.</p><p><strong>2021.06.08</strong>: 更新Teacher Forcing.</p></blockquote><h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>Transformer (<del>擎天柱/变形金刚</del>), 是一个基于<strong>Attention和SeqSeq</strong>的模型, 完全摆脱了CNN和RNN, 整个模型单单只由自注意力和前馈神经网络组成. 该模型出自<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a>, 作者探究了Attention机制真正发挥的作用. Transformer在<strong>机器翻译</strong>等领域取得了革命性的成果, 并且由它衍生了很多在NLP方面的模型, 比如NLP现在通用的模型<a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">Bert</a>家族, 以后也会详细研究一下.</p><p>本文的图片大多数来自原论文和<a href="http://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">The Illustrated Transformer</a>这篇博客, 该文很清楚的解释了Transformer中原文的每一个细节, 图片简洁明了, 强烈推荐阅读.</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>在原文中, 作者先在介绍Transformer的细节前抛出了Transformer的大致结构:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer.jpg" style="zoom:80%"><p>如果抛去所有的细节不谈, 能够清晰的看出作者使用了Seq2Seq作为模型的基本结构, 左侧输入为<strong>Encoder</strong>部分, 右侧输出部分为<strong>Decoder</strong>, 在Decoder输出后, 用一个加以Softmax的神经层来分类.</p><p>在每个Encoder和Decoder之间, 还有Attention 相连接,</p><p>这种结构天生就非常适合机器翻译, 如果我们把分类结果与词进行转换, 从高阶的视角来看, 那么结果就是一个机器翻译的Pipeline:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer1.png" style="zoom:50%"><h2 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h2><p>不要忘记论文中给出的大致结构, 下面一步步剖析细节如何实现.</p><h3 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h3><p>对Encoder进行输入(Inputs)和对Decoder进行输入(Outputs)时用到了Embedding, Embedding这里不再多做解释了, 在DL中普遍用Embedding做词语向量化. 论文中使用到的$d_{model}=512$, 并且在Encoder和Decoder的Embedding<strong>共享</strong>相同参数.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer6.png" style="zoom:67%"><blockquote><p>值得一提的是, Transformer使用的词表并非是原始单词, 而是经过<strong>BPE(byte-pair encoding)</strong> 处理后的.</p></blockquote><h3 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h3><p>其实在大致模型结构中已经提到了, Transformer可以看做是一个许多Encoder组成的编码组件和一个许多Decoder组成的解码组件构成的.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer2.png" style="zoom:50%"><p>在论文中, 编码组件和解码组件的数量等同, 并且假设$N=6$, 即有6个编码器和6个解码器. 作者在后文中还尝试了取2, 4, 8, 但效果上来说没有6好.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer3.png" style="zoom:33%"><p>值得注意的是, 这里的Encoder和Decoder都采用了堆叠的方式, 并且对于堆叠的结构, 每次只接受一个单词的输入, 所以并不是每个Encoder的隐藏状态都会提供给每个Decoder, 而是<strong>Encoder堆叠后的输出统一提供给每个Decoder</strong>.</p><blockquote><p>在RNN + Seq2Seq执行机器翻译任务时, 每个RNN - Encoder接受的是<strong>不同单词</strong>的输入, 在Transformer中Encoder以堆叠的形式存在, 对应的是<strong>同一单词</strong>, 而Encoder - Decoder之间的Attention是为了调整对不同单词的信息权重, 所以并非每个Encoder和每个Decoder之间都有Attention, 不要搞混.</p></blockquote><h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p>每个Encoder由一个自注意力层(Self - Attention)和一个前馈神经网络层(FFN)组成, 后面会提到它们是如何实现的.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer4.png" style="zoom:67%"><p>我并不认为这张图做的很好, 因为在右侧的箭头是具有<strong>歧义性</strong>的, 容易让人认为每层的Encoder都与Decoder有连接, 实际上只有最后一个Encoder和所有的Decoder有连接.</p><p>如果将Embedding并添加位置编码后的输入向量设为$x_i$, 经过Self - Attention层的输出设为$z_i$, Encoder目前的向量流如下所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer7.png" style="zoom:50%"><p>当然, 输出$r_i$ 会流入下个Encoder, 当做输入:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer8.png" style="zoom:50%"><h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><p>每个Decoder除了有和Encoder相同的自注意力和前馈神经网络层, 还多了一个对Encoder的Attention.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer5.png" style="zoom:67%"><p>同样, 本图也具有歧义性.</p><h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self - Attention"></a>Self - Attention</h3><p>自注意力是Transformer中提出的一种新结构, 也是核心组件. 这个注意力并非Decoder对所有Encoder的隐藏状态的不同注意力, 而是<strong>自身对自身</strong>的注意力, 在做语义编码时使得<strong>单词在编码时能够根据上下文找到自己的真正含义</strong>. 自注意力将注意力采用<strong>Q-K-V模式</strong>, 即Query, Key, Value. 用不同的矩阵与Embedding输入$x_i$ 做矩阵乘, 就能分别得到对应的$q_i$, $k_i$, $v_i$, 而矩阵是随机初始化得来的, 之后通过学习调整参数. 至于QKV是定义的, 请参考<code>&lt;Seq2Seq和Attention&gt;</code>.</p><blockquote><p>为什么叫Self - Attention呢? 假设我们在执行机器翻译任务, 这个Attention不再是作用于我们给出的一种语言的输入Source和目标语言的输出Target, 而是作用于Source和Source内部, 即<strong>源语言的语义编码与原始输入Source之间</strong>的Attention, 这样能够获得单词在句子中更好的表示.</p></blockquote><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer9.png" style="zoom:67%"><blockquote><p>这里注意一下维度, 在论文中的Embedding维度$d_{model}=512$, 给出的Key维度和Value维度均为64, 即$d_k=d_v=d_{model}/h=64$, 那么对应QKV的矩阵$W_Q$, $W_K$, $W_V$ 大小应该都是$(512, 64)$.</p></blockquote><p>这样就能根据输入得到一个查询向量$q_i$, 一组键值对$&lt;k_i, v_i&gt;$.</p><p>有了QKV, 接下来需要按照Attention的流程计算$q_i$ 和$k_i$ 的Score, 根据论文中提到的<strong>缩放点积注意力</strong>(Scaled Dot-Product Attention):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/scaledotattention.jpg" style="zoom:67%"><p>先进行点积, 再进行缩放, 计算完$q_i$ 与句中所有单词的$k$ 的得分(这里采用点积得到)后, 再对Score除以$\sqrt{d_k}$, 完成缩放, 最后再通过Softmax得到Attention权重, 加权求和结果称为$z_i$.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer10.png" style="zoom:67%"><p>注意, 我们上面的讨论全部都是针对一个单词的, 但是在实际的运算中, 由于Encoder是线性Stack起来的, 所以其实Encoder的训练是可以并行的, 即<strong>多个单词做完Embedding后作为一个矩阵并行计算</strong>, 假设输入矩阵$X$, 通过$W_Q$, $W_K$, $W_V$ 计算后可以得到$Q$, $K$, $V$:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer11.png" style="zoom:50%"><p>综上, 将自注意力总结为:<br>$$<br>\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V<br>$$<br><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer12.png" style="zoom:50%"></p><p>这个公式其实在<code>&lt;Seq2Seq和Attention&gt;</code>一文中提到过, 但这里作者对Score使用了归一化, 即除以$\sqrt{d_k}$, $\sqrt{d_k}$ 为Key的维度. 这属于训练的一个Trick, 作者对此的解释如下:</p><blockquote><p>We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by $\frac{1}{\sqrt{d_k}}$.</p></blockquote><p>当$d_k$ 非常大时, 求得的内积可能会非常大, 如果不进行缩放, 不同的内积大小可能差异会非常大, Softmax在指数运算可能将梯度推到特别小, 导致梯度消失.</p><h3 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi - head Attention"></a>Multi - head Attention</h3><p>Multi - head Attention的思路和CNN中的多个<strong>卷积核</strong>起到的作用明显是一致的. 所谓”多头”, 放在<strong>卷积神经网络</strong>里就是卷积层多个卷积核的特征提取过程, 在这里就是进行多次注意力的提取, 就像多个卷积核一样, 多次<strong>不同的初始化矩阵</strong>经过训练可能会有多种<strong>不同的特征,</strong> 更有利于<strong>不同角度</strong>的特征抽取和信息提取.</p><p>论文中用多个头求出多组的数据堆叠, 也就是图中的$h$ 维:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/multiheadattention.jpg" style="zoom:50%"><p>这样就能得到多个不同的Attention结果:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer13.png" alt=""></p><p>论文中采用了8个头的注意力, 即$h=8$, 得到多个提取出来的特征:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer14.png" style="zoom:50%"><p>我们将所有Self - Attention提取的特征全部concat起来, 因为维度比较大, 所以要经过一个输出矩阵$W_O$, 对特征进行进一步提取, 直到大小和Encoder接收的输入相同:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer15.png" style="zoom:50%"><p>也就是说经过左侧的6个Encoder, 向量大小仍然不改变.</p><p>到现在总结一下流程:</p><ol><li>做Embedding和位置编码, 获得输入$X$.</li><li>通过多头获得多组对应的$Q$, $K$, $V$.</li><li>通过缩放点积注意力, 多个头分别加权求和求得$Z_i$.</li><li>将所有$Z_i$ 全部concat起来, 然后经过$W_O$ 的特征提取, 得到最终输出$Z$, 其大小与输入$X$ 是完全相同的.</li></ol><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer16.png" style="zoom:50%"><blockquote><p>图中的$R$ 代表除了最开始的Encoder, 其他Encoder都不需要Embedding, 用上一个Encoder的输出作为输入.</p></blockquote><p>对每个头对句子不同部分的注意力进行可视化, 能发现每个头的注意力都在不同的位置上, 作用确实类似于CNN的卷积核, 做到了不同的注意力表示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer17.png" style="zoom:67%"><h3 id="Encoder-Side"><a href="#Encoder-Side" class="headerlink" title="Encoder Side"></a>Encoder Side</h3><h4 id="Position-wise-Feed-Forward-neural-network"><a href="#Position-wise-Feed-Forward-neural-network" class="headerlink" title="Position-wise Feed Forward neural network"></a>Position-wise Feed Forward neural network</h4><p>前馈神经网络就是结构中提到的Feed Forward neural network. 当然不单单是一个全连接层, 这里还用到了<strong>ReLu</strong>作为激活函数, 并且加上了<strong>Layer Normalization</strong>. 只有一个隐藏层的神经网络也可以表示为2个<strong>一维</strong>的$1\times1$的卷积, 这二者是等价的:<br>$$<br>\operatorname{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}<br>$$</p><p>但这里值得说明的是, 在论文中有这样一段:</p><blockquote><p>In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position <strong>separately</strong> and <strong>identically</strong>. This consists of two linear transformations with a ReLU activation in between.</p></blockquote><p>对于并行计算的不同单词, 通过的FFN参数是<strong>共享</strong>的, 也可以看做不同单词先后通过同一个FFN, 如下图所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer20.png" style="zoom:50%"><p>这也就是为什么我前面要说FFN能够看做是<strong>一维卷积</strong>.</p><p>并且在经过每个Encoder后, 都<strong>不改变数据的大小</strong>.</p><h4 id="Layer-Norm"><a href="#Layer-Norm" class="headerlink" title="Layer Norm"></a>Layer Norm</h4><p>你应该接触过Batch Norm, Layer Norm也是一种类似于Batch Norm的归一化方式, 同样能起到加快收敛的作用, 在<strong>NLP任务</strong>中比较常用. Batch Norm中, 记录下多个Batch中每维Feature的均值和方差, 并进行放缩和平移, 即对<strong>不同样本的同一个通道特征</strong>进行归一化. 在Layer Norm中, 只是换了一个维度, 我们对<strong>同一个样本的不同特征</strong>进行归一化.<br>$$<br>\begin{aligned}<br>\mu_{j}&amp;=\frac{1}{m} \sum_{i=1}^{m} x_{i j} \\<br>\sigma_{j}^{2}&amp;=\frac{1}{m} \sum_{i=1}^{m}\left(x_{i j}-\mu_{j}\right)^{2}<br>\end{aligned}<br>$$<br>引入$\epsilon$ 做平滑, 使分母不为0:<br>$$<br>\text {LayerNorm}(x)=\frac{x_{i j}-\mu_{j}}{\sqrt{\sigma_{j}^{2}+\epsilon}}<br>$$<br>跟Batch Norm一样, 之后也进行平移和放缩:<br>$$<br>y = \text{LayerNorm}(x) \cdot \gamma + \beta<br>$$<br>下面这张图非常清晰:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/layernorm.png" style="zoom:50%"><blockquote><p>如果想了解它为什么和NLP领域比较契合, 详见下文:</p><ul><li><a href="https://zhuanlan.zhihu.com/p/54530247" target="_blank" rel="noopener">模型优化之Layer Normalization</a></li><li><a href="https://zhuanlan.zhihu.com/p/74516930" target="_blank" rel="noopener">NLP中 batch normalization与 layer normalization</a></li><li><a href="https://www.zhihu.com/question/395811291" target="_blank" rel="noopener">transformer 为什么使用 layer normalization, 而不是其他的归一化方法?</a></li><li>原论文<a href="https://arxiv.org/abs/1607.06450" target="_blank" rel="noopener">Layer Normalization</a></li></ul><p>大致原因是Batch Norm 对于Embedding后的数据进行归一化, 假设每个Batch是由多个Embedding组成的, 按照Batch方向对每个归一化, 就是对每个词的Embedding整体归一化. 这样做非常没有道理, 不符合NLP的规律, 它反而加强了不同词之间的相关性.</p><p>但如果按照Layer Norm, 按照Layer方向, 实际上是分别对每个Embedding后的词向量进行归一化, 这样每个词向量相对独立.</p><p>这主要还是CV和NLP的<strong>数据属性</strong>决定的. 在CV中, 不同样本之间的Channel信息是具有共性的(因为图像还是要用2D来表示), 这部分信息非常重要, 如果归一化会损失很多信息. 而NLP中, 数据是Embedding来的, 本来也没有包含位置信息, 反而不同词向量之间毫无相关性, 关注单词本身的归一化效果会更好.</p></blockquote><h4 id="Residual"><a href="#Residual" class="headerlink" title="Residual"></a>Residual</h4><p>残差连接从ResNet中提出也有一定年头了, 作为近些年使用频次比较高效果比较好的结构, 原理不再多赘述了, 在<code>&lt;卷积神经网络小结&gt;</code>和<code>&lt;卷积神经网络发展史&gt;</code>中都有对残差连接的详解.</p><p>在Encoder中残差连接伴随着Layer Norm, 每次经过一个子层都要做一次残差连接.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer21.png" style="zoom:50%"><p>在Decoder中也是同样的, 每次经过子层也都要做残差连接.</p><h3 id="Decoder-Side"><a href="#Decoder-Side" class="headerlink" title="Decoder Side"></a>Decoder Side</h3><p>当了解了Encoder的结构后, 结合起Decoder来看一下信息流. 假设只有两个Encoder和两个Decoder的堆叠, 那么信息的流动方向是这样的:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer22.png" style="zoom:50%"><p>确实只有最后的Encoder将输出传递给了Decoder的Encoder - Decoder Attention. Encoder的输出也是和Decoder唯一的交互数据, 其最终输出就是经过多个堆叠的Encoder计算得来的与Encoder输入大小相同的向量, 在交互的Encoder - Decoder Attention中会体现出来.</p><h4 id="Encoder-Decoder-Attention"><a href="#Encoder-Decoder-Attention" class="headerlink" title="Encoder - Decoder Attention"></a>Encoder - Decoder Attention</h4><p>Decoder中的Encoder - Decoder Attention和Encoder中的多头Self - Attention运算机制<strong>一致</strong>, 唯一不同的就是<strong>输入数据的来源</strong>. 其中, $Q$ 对应的输入来源于Decoder的<strong>Masked Self - Attention</strong>(即每个Decoder的第一层输出), 而$K$ 和$V$ 对应的输入来源于整个<strong>Encoder</strong>的最终输出(图中Encoder上方的蓝色向量). 根据这三个输入再结合Encoder - Decoder Attention层中的$W_Q$, $W_K$, $W_V$ 分别得到$Q$, $K$, $V$. 然后再根据缩放点积得到Attention Value.</p><p>对于多个经过Encoder的单词形成含有多个单词的矩阵, Decoder是在这一环节实现翻译时对不同单词的Attention的.</p><blockquote><p>注: 下面两张动图比较大, 挂在github上了, 如果没挂梯子可能无法正常显示,</p></blockquote><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/transformerdecode1.gif" style="zoom:50%"><h4 id="Outputs"><a href="#Outputs" class="headerlink" title="Outputs"></a>Outputs</h4><p>因为采用了Seq2Seq的架构, Decoder每过一个时间步不光接受Encoder的输出, 还接受了上一个Timestep的Decoder输入, 即论文中提到的”<strong>shifted right</strong>“.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/transformerdecode2.gif" style="zoom:50%"><h4 id="Masked-Multi-Head-Attention"><a href="#Masked-Multi-Head-Attention" class="headerlink" title="Masked Multi - Head Attention"></a>Masked Multi - Head Attention</h4><p>Mask是Transformer中一个关键点. Masked Multi - Head Attention 只出现在Decoder中. 到了Decoder, 可就不再像Encoder那样直接把数据拿过来并行训练了, 如果也像Encoder那样把所有输入的词向量全一股脑堆进去, Decoder做Self - Attention可以无视解码的时间跨度, <strong>获知全部的信息</strong>, 因此需要用Mask将当前预测的单词和之后的单词全都<strong>遮盖</strong>, 否则就没法训练了.</p><p>若仍然沿用传统Seq2Seq+RNN的思路, Decoder是一个<strong>顺序操作</strong>的结构, 我们代入一个场景来看看. 假设我们要执行<strong>机器翻译</strong>任务, 要将<code>我 是 大宁</code>翻译为<code>I am DaNing</code>, 假设所有参数与论文中提到的参数一样, batch size视为1. 根据前面已知的知识, Encoder堆叠后的输入和Embedding的大小是相同的, 在这里有三个词语, Embedding且通过Encoder后的编码大小为$(3, 512)$. 下面对Decoder进行训练:</p><ol><li>将起始符<code>&lt;start&gt;</code> 作为初始Decoder输入, 经过Decoder处理和分类得到输出<code>I</code>.</li><li>将<code>&lt;start&gt; I</code>作为Decoder输入, 经过Decoder处理和分类得到输出<code>am</code>.</li><li>将<code>&lt;start&gt; I am</code>作为Decoder输入, 经过Decoder处理和分类得到输出<code>DaNing</code>.</li><li>将<code>&lt;start&gt; I am DaNing</code>作为Decoder输入, 经过Decoder处理和分类得到结束符<code>&lt;end&gt;</code>.</li></ol><p>这种预测的方式也称为<strong>自回归</strong>.</p><p>参考将RNN更改为Self - Attention的Encoder思路, 对于这种依赖于前一个时间步预测结果的结构Decoder, 如果想做到<strong>并行</strong>训练, 需要将上面的过程转化为一个这样的矩阵直接作为Decoder的输入:<br>$$<br>\begin{bmatrix}<br>\text{&lt;start&gt;}&amp; &amp; &amp; \\<br>\text{&lt;start&gt;}&amp; \text{I} &amp; &amp; \\<br>\text{&lt;start&gt;}&amp; \text{I} &amp; \text{am} &amp; \\<br>\text{&lt;start&gt;}&amp; \text{I} &amp; \text{am} &amp; \text{DaNing}<br>\end{bmatrix}<br>$$<br>因为在<strong>训练时已知任务标签</strong>, 所以可以产生类似的效果. 这种方法被称为Teacher Forcing, 仅在训练阶段使用, 而不能使用在推断过程, 我会在下一节训练技巧中讲讲.</p><blockquote><p>图片取自<a href="https://wmathor.com/index.php/archives/1438/" target="_blank" rel="noopener">Transformer 详解</a>.</p></blockquote><p>在论文的图中, Mask操作顺序被放在$Q$ 和$K$ 计算并缩放后, Softmax计算前. 如果继续计算下去, 不做Mask, 与$V$ 相乘后得到Attention, 所有时间步信息全部都被泄露给Decoder, 必须用Mask将当前预测的单词信息和之后的单词信息全部遮住.</p><p>遮住的方法非常简单, 首先不能使用0进行遮盖, 因为Softmax中用零填充会产生错误, $e^0=1$. 所以必须要用$-\infty$来填充那些不能被看见的部分. 我们直接生成一个下三角全为0, 上三角全部为<strong>负无穷</strong>的矩阵, 与原数据相加就能完成遮盖的效果:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer24.png" style="zoom:50%"><p>做Softmax时, 所有的负无穷全变成了0, 不再干扰计算:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer25.png" style="zoom:67%"><p>其实Mask在对句子的<strong>无效部分填充</strong>时, 也是用同样方法将所有句子补齐, 无效部分用负无穷填充的.</p><blockquote><p>强调: Decoder仍然依赖与先前输出结果作为输入, 所以在正式使用时不能实现并行预测, 但在训练的时结果是已知的, 可以实现并行训练.</p></blockquote><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>最后来谈谈位置编码. 因为Transformer采用了纯粹的Attention结构, 不像RNN一样能够通过时间步来反映句子中单词的前后关系, 即不能得知<strong>位置信息.</strong> 要知道, 在NLP任务中, <strong>语序</strong>是一个相当重要的属性, 所以必须要通过某种方式让Transformer得知单词的位置, 作者通过<strong>位置编码</strong>在每次进入Encoder和Decoder前将位置信息写入. 这样来看, 与其叫位置编码, 不如叫<strong>位置嵌入</strong>. 位置编码可以直接与Embedding的向量相加:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer18.png" style="zoom:50%"><p>作者的做法非常有意思, 对不同的单词位置, 不同的Embedding维度, 它的编码都是<strong>唯一</strong>的, 应用正弦和余弦函数也方便Transformer学到位置的特征. 如果将当前单词位置记为$pos$, 而词向量的某个维度记为$i$, 那么位置编码的方法为:<br>$$<br>\begin{aligned}<br>P E_{(p o s, 2 i)} &amp;=\sin \left(p o s / 10000^{2 i / d_{\text {model }}}\right) \\<br>P E_{(p o s, 2 i+1)} &amp;=\cos \left(p o s / 10000^{2 i / d_{\text {model }}}\right)<br>\end{aligned}<br>$$<br>计算出来的结果应该是这样的:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer19.png" style="zoom:50%"><p>如果上面那组式子看起来有些乱, 写成这样或许好些:<br>$$<br>\begin{aligned}<br>P E_{(p o s, 2 i)} &amp;=\sin \left(\frac{p o s}{10000^\frac{2 i}{d_{\text {model }}}}\right) \\<br>P E_{(p o s, 2 i+1)} &amp;=\cos \left(\frac{p o s}{10000^\frac{2 i}{d_{\text {model }}}}\right)<br>\end{aligned}<br>$$<br>如果按照论文中的设定$d_{model}=512$, 由于奇偶数的计算方式是不同的, 所以$i \in[0, 255]$.</p><p>在这个式子中, 编码周期不受单词位置$pos$ 影响, 仅仅与模型开始设计的$d_{model}$ 和Embedding的不同维度$i$ 相关. 对于不同的$i$, $PE$ 的周期是$[2\pi, 10000\cdot2\pi]$.</p><p>这样看, 同一位置上的词语, 对于不同的Embedding维度, 都得到不同的编码, 并且随着$i$ 的增大, 位置编码的值的变化就越来越慢. 这种编码对于不同维度的Embedding来说是<strong>唯一</strong>的, 因此模型能够学习到关于Embedding的位置信息.</p><p>下面这个热图非常直观, 分开来看. 对于相同的Position的词语, 它不同维度的Embedding往往具有不同周期而交错的$\sin$ 和$\cos$ 组合, 而对于Embedding的同一个维度和不同Position的单词, 在Position上呈现出周期性.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/位置编码1.png" style="zoom:50%"><p>同样, 用折线图表示不同位置和编码后值的关系:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/位置编码2.png" style="zoom:67%"><p>起初, 我并不知道为什么这种方法Work. 在看过<a href="https://zhuanlan.zhihu.com/p/92017824" target="_blank" rel="noopener">浅谈 Transformer-based 模型中的位置表示</a>后, 感觉似乎有些道理. 作者意在利用正弦余弦的数学性质(周期性, 和角公式), 使得偏移了的一定position, 记为$k$ , 能够得到正弦余弦的<strong>不同线性组合</strong>(总感觉这种编码在通信的某个地方应该很常用, 只是DL第一次用而已).</p><p>三角函数性质:<br>$$<br>\left\{\begin{array}{l}<br>\sin (\alpha+\beta)=\sin \alpha \cos \beta+\cos \alpha \sin \beta \\<br>\cos (\alpha+\beta)=\cos \alpha \cos \beta-\sin \alpha \sin \beta<br>\end{array}\right.<br>$$<br>得到偏移后, 即$pos+k$ 的$PE$:<br>$$<br>\left\{\begin{array}{l}<br>PE(pos+k, 2i)=PE(pos, 2i) \times PE(k, 2i+1)+PE(pos, 2i+1) \times PE(k, 2i) \\<br>PE(pos+k, 2i+1)=PE(pos, 2i+1) \times PE(k, 2i+1)-PE(pos, 2i) \times PE(k, 2i)<br>\end{array}<br>\right.<br>$$</p><p>除了公式计算, 作者也实验了其他对于位置编码的方式, 比如通过训练得到, 但由于实际效果与计算得来相仿, 那么还不如通过公式计算直接得到位置编码.</p><h3 id="Final-Output"><a href="#Final-Output" class="headerlink" title="Final Output"></a>Final Output</h3><p>最终输出很简单, 根据Decoder的输出经过FC层和Softmax得到对应的单词.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer23.jpg" style="zoom:50%"><p>注意, Decoder的Embedding层和最后输出经过Softmax前的Linear层也是<strong>共享权重</strong>的.</p><h2 id="训练技巧"><a href="#训练技巧" class="headerlink" title="训练技巧"></a>训练技巧</h2><p>让Transformer跑的起来离不开下面说的这些训练技巧.</p><h3 id="WarmUp"><a href="#WarmUp" class="headerlink" title="WarmUp"></a>WarmUp</h3><p>在原文中, Optimizer使用Adam, 除了设置参数$\beta_1 = 0.9$, $\beta_2 = 0.98$, $\epsilon=10^{-9}$外, 还设置了Warmup:<br>$$<br>\text {lrate}=d_{\text {model }}^{-0.5} \cdot \min \left(\text {step}_{-} \text {num}^{-0.5}, \text {step_num} \cdot \text {warmup_steps}^{-1.5}\right)<br>$$<br>我按照step增长, 做出WarmUp对应的learning rate变化图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer26.jpg" style="zoom:25%"><p>在开始时学习率大, 然后逐渐变小. 对于越大的$d_{model}$ 来说, 初始的斜率越小. 论文中设置$\text{warmup_step}=4000$. 许多后续的模型也用到了WarmUp.</p><h3 id="Label-Smoothing"><a href="#Label-Smoothing" class="headerlink" title="Label Smoothing"></a>Label Smoothing</h3><p>在<code>&lt;卷积神经网络发展史&gt;</code>中提到过, 这里再重复一下. 标签平滑可以看做是一种添加到损失公式中的一种正规化组件, 因为独热编码后的标签只有0和1, 可能有些过于绝对, 标签平滑提供了一种手段来使其中的0也能分配到一些数值. 假设输入为$x$, 一共需要对$c$ 个类进行划分, 则标签平滑后的结果$y\prime$ 为:<br>$$<br>y\prime = (1 - \epsilon) \cdot x + \frac{\epsilon}{c}<br>$$<br>直接贴代码, Show you my code!</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># input_tensor is a tensor in pytorch</span>
<span class="token keyword">def</span> <span class="token function">label_smoothing</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> epsilon<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    classes <span class="token operator">=</span> input_tensor<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># compute the number of classes</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> epsilon<span class="token punctuation">)</span> <span class="token operator">*</span> input_tensor <span class="token operator">+</span> <span class="token punctuation">(</span>epsilon <span class="token operator">/</span> classes<span class="token punctuation">)</span> 

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'label_smoothing(a):'</span><span class="token punctuation">,</span> label_smoothing<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'label_smoothing(b):'</span><span class="token punctuation">,</span> label_smoothing<span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
label_smoothing(a): tensor([0.0333, 0.0333, 0.9333])
label_smoothing(b): tensor([0.0500, 0.9500])
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在论文中设置$\epsilon_{ls}=0.1$. 标签平滑可能会提高句子的困惑度, 因为添加了更多的不确定性, 但会提高准确率和BLEU.</p><h3 id="Residual-Dropout"><a href="#Residual-Dropout" class="headerlink" title="Residual Dropout"></a>Residual Dropout</h3><p>在进行残差连接时, 每个子层进行add和batch norm之前, 都添加了Dropout. 原论文中设置$P_{\text {drop}}=0.1$, 个人认为Dropout没有前两种技巧作用大.</p><h3 id="Teacher-Forcing"><a href="#Teacher-Forcing" class="headerlink" title="Teacher Forcing"></a>Teacher Forcing</h3><p>在模型的训练阶段, Decoder的所有正确输入是<strong>完全已知</strong>的, 如果自回归预测在某个时间步$t$ 解码出错, 则会导致$t$ 时刻后所有的预测结果都产生<strong>偏差</strong>.</p><p>Teacher Forcing通过将自回归模型解码过程中的所有输入<strong>强制修正</strong>为Ground Truth来避免了这个问题.</p><p>Teacher Forcing有诸多优点:</p><ul><li>解决了训练阶段自回归式模型的串行问题, 能够使得模型训练时<strong>并行</strong>.</li><li>避免了模型训练时预测<strong>一步错步步皆错</strong>的问题.</li><li>由于干涉了训练错误的情况, 加快了模型的<strong>收敛</strong>速度.</li></ul><p>但它也同时容易产生<strong>矫枉过正</strong>的问题:</p><ul><li><strong>Exposure Bias</strong>: 这是最为常见的问题. 在训练时因为受到干涉, 很容易产生<strong>训练推断不一致</strong>.</li><li><strong>Overcorrect</strong>: 有时候模型解码有自己的想法, 但因为Teacher Forcing的干涉, 导致生成的句子<strong>四不像</strong>.</li><li><strong>No diversity</strong>: Teacher Forcing对Ground Truth的约束是非常强的, 模型的<strong>多样性</strong>受到严重限制.</li></ul><blockquote><p>Teacher Forcing的衍生问题, 请阅读<a href="https://zhuanlan.zhihu.com/p/93030328" target="_blank" rel="noopener">关于Teacher Forcing 和Exposure Bias的碎碎念</a>.</p></blockquote><h2 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h2><blockquote><p>摘自<a href="https://www.nowcoder.com/discuss/258321" target="_blank" rel="noopener">牛客网: NLPer看过来, 一些关于Transformer的问题整理</a></p></blockquote><h3 id="Transformer相比于RNN-LSTM-有什么优势-为什么"><a href="#Transformer相比于RNN-LSTM-有什么优势-为什么" class="headerlink" title="Transformer相比于RNN/LSTM, 有什么优势? 为什么?"></a>Transformer相比于RNN/LSTM, 有什么优势? 为什么?</h3><ol><li>RNN系列的模型<strong>并行计算</strong>能力很差.</li><li>Transformer的<strong>特征抽取</strong>能力比RNN系列的模型要好(实验结论).</li></ol><h3 id="为什么说Transformer可以代替seq2seq"><a href="#为什么说Transformer可以代替seq2seq" class="headerlink" title="为什么说Transformer可以代替seq2seq?"></a>为什么说Transformer可以代替seq2seq?</h3><p>这里用代替这个词略显不妥当, seq2seq虽已老, 但始终还是有其用武之地, seq2seq最大的问题在于<strong>将Encoder端的所有信息压缩到一个固定长度的向量中</strong>, 并将其作为Decoder端首个隐藏状态的输入, 来预测Decoder端第一个单词(token)的隐藏状态. 在输入序列比较长的时候, 这样做显然会损失Encoder端的很多信息, 而且这样一股脑的把该固定向量送入Decoder端, Decoder端不能够关注到其想要关注的信息. 上述两点都是seq2seq模型的缺点, 后续论文对这两点有所改进, 如著名的<a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">Neural Machine Translation by Jointly Learning to Align and Translate</a>, 虽然确确实实对seq2seq模型有了实质性的改进, 但是由于主体模型仍然为RNN(LSTM)系列的模型, 因此模型的并行能力还是受限, 而transformer不但对seq2seq模型这两点缺点有了实质性的改进(多头交互式attention模块), 而且还引入了self-attention模块, 让源序列和目标序列首先”自关联”起来, 这样的话, 源序列和目标序列自身的embedding表示所蕴含的信息更加丰富, 而且后续的FFN层也增强了模型的表达能力(ACL 2018会议上有论文对Self-Attention和FFN等模块都有实验分析, 见论文: <a href="http://aclweb.org/anthology/P18-1167" target="_blank" rel="noopener">How Much Attention Do You Need?A Granular Analysis of Neural Machine Translation Architectures</a>), 并且Transformer并行计算的能力是远远超过seq2seq系列的模型, 因此我认为这是transformer优于seq2seq模型的地方.</p><h3 id="Transformer中句子的encoder表示是什么？如何加入词序信息的？"><a href="#Transformer中句子的encoder表示是什么？如何加入词序信息的？" class="headerlink" title="Transformer中句子的encoder表示是什么？如何加入词序信息的？"></a>Transformer中句子的encoder表示是什么？如何加入词序信息的？</h3><p>Transformer Encoder端得到的是整个输入序列的encoding表示, 其中最重要的是经过了self-attention模块, 让输入序列的表达更加丰富, 而加入词序信息是使用不同频率的正弦和余弦函数.</p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">AnNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/6744.html">https://ADAning.github.io/posts/6744.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">AnNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/Transformer/"><span class="chip bg-color">Transformer</span> </a><a href="/tags/Attention/"><span class="chip bg-color">Attention</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/36009.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/13.jpg" class="responsive-img" alt="指针网络家族"> <span class="card-title">指针网络家族</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文介绍了Pointer Network, CopyNet, Pointer-Generator Network以及Coverage机制在文本摘要与对话系统中的应用, 既可以作为知识点介绍, 也可以作为论文阅读笔记. 此外, 该部分内容为外</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-09-28 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/Attention/"><span class="chip bg-color">Attention</span> </a><a href="/tags/%E6%91%98%E8%A6%81%E7%94%9F%E6%88%90/"><span class="chip bg-color">摘要生成</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/38085.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/2.jpg" class="responsive-img" alt="卷积神经网络发展史"> <span class="card-title">卷积神经网络发展史</span></div></a><div class="card-content article-content"><div class="summary block-with-text">LeNetLeNet可以说是CNN的开山鼻祖之一了, 虽然它不是CNN的起点, 但是可以称为CNN兴起的标志. 它由图灵奖得主LeCun Yann在1998年的Gradient-based learning applied to docum</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-09-07 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/CNN/"><span class="chip bg-color">CNN</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">AnNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">340.9k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:695439722@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>