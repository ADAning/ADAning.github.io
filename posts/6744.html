<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="Transformer精讲, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>Transformer精讲 | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/3.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">Transformer精讲</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/Transformer/"><span class="chip bg-color">Transformer</span> </a><a href="/tags/Attention/"><span class="chip bg-color">Attention</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2020-09-21</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2025-02-10</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 7.3k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 28 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p><strong>2020.10.05</strong>: 更新训练技巧.</p><p><strong>2020.09.27</strong>: 更新Masked Multi - Head Attention理解.</p><p><strong>2021.06.08</strong>: 更新Teacher Forcing.</p><p><strong>2024.09.17</strong>: 更新了LN的描述.</p><p><strong>2025.02.10</strong>: 这篇博客已经好几年了, 甚至还是在我初学的时候写的. 在全文多处添加了一些描述, 并修正了一些错误.</p></blockquote><h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>Transformer (<del>擎天柱/变形金刚</del>), 是一个基于<strong>Attention和SeqSeq</strong>的模型, 完全摆脱了CNN和RNN, 整个模型单单只由自注意力和前馈神经网络组成. 该模型出自<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a>, 作者探究了Attention机制真正发挥的作用. Transformer在<strong>机器翻译</strong>等领域取得了革命性的成果, 并且由它衍生了很多在NLP方面的模型, 比如NLP现在通用的模型<a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">Bert</a>家族, 以后也会详细研究一下.</p><p>本文的图片大多数来自原论文和<a href="http://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">The Illustrated Transformer</a>这篇博客, 该文很清楚的解释了Transformer中原文的每一个细节, 图片简洁明了, 强烈推荐阅读.</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>在原文中, 作者先在介绍Transformer的细节前抛出了Transformer的大致结构:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer.jpg" style="zoom:80%"><p>如果抛去所有的细节不谈, 能够清晰的看出作者使用了Seq2Seq作为模型的基本结构, 左侧输入为<strong>Encoder</strong>部分, 右侧输出部分为<strong>Decoder</strong>, 在Decoder输出后, 用一个加以Softmax的神经层来分类.</p><p>在每个Encoder和Decoder之间, 还有Attention 相连接,</p><p>这种结构天生就非常适合机器翻译, 如果我们把分类结果与词进行转换, 从高阶的视角来看, 那么结果就是一个机器翻译的Pipeline:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer1.png" style="zoom:50%"><h2 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h2><p>不要忘记论文中给出的大致结构, 下面一步步剖析细节如何实现.</p><h3 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h3><p>在NLP中, 常将单词词编码为Embedding, 即将每个单词通过查表的方式映射成向量, 以此输入到模型中. 在DL中普遍用<strong>Word Embedding做词语向量化</strong>.</p><p>对Encoder进行输入(Inputs)和对Decoder进行输入(Outputs)时用到了Embedding, 论文中使用到的$d_{model}=512$, 并且在Encoder和Decoder的Embedding<strong>共享</strong>相同参数.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer6.png" style="zoom:67%"><blockquote><p>在NLP中, 直接以整个单词作为最小单位(Token)记录它们的Embedding的话, 会极大地增加词表的大小, 导致单词内部的细信息不能被重复利用, 如英文中的词根, 词缀等许多都含有共性. 所以Transformer使用的词表并非是原始单词, 而是经过<strong>BPE(byte-pair encoding)</strong> 处理后的”子词”(Sub-word), <strong>但为了方便理解</strong>, <strong>本文中还是以”单词”作为Token的描述</strong>.</p></blockquote><h3 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h3><p>其实在大致模型结构中已经提到了, Transformer可以看做是一个许多Encoder组成的编码组件和一个许多Decoder组成的解码组件构成的.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer2.png" style="zoom:50%"><p>在论文中, 编码组件和解码组件的数量等同, 并且假设$N=6$, 即有6个编码器和6个解码器. 作者在后文中还尝试了取2, 4, 8, 但效果上来说没有6好.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer3.png" style="zoom:33%"><p>Encoder作用就是用来<strong>编码整个输入的序列</strong>, Decoder的作用是<strong>对Encoder的序列编码进行解码</strong>.</p><p>因此, 只需要将<strong>最后一层Encoder的输出统一提供给每个Decoder</strong>, 并不需要将每个Encoder的隐藏状态都提供给每个Decoder.</p><h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p>每个Encoder由一个自注意力层(Self - Attention)和一个前馈神经网络层(FFN)组成, 后面会提到它们是如何实现的.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer4.png" style="zoom:67%"><p>我并不认为这张图做的很好, 因为在右侧的箭头是具有<strong>歧义性</strong>的, 容易让人认为每层的Encoder都与Decoder有连接, 实际上只有最后一个Encoder和所有的Decoder有连接.</p><p>如果将Embedding并添加位置编码后的输入向量设为$x_i$, 经过Self - Attention层的输出设为$z_i$, Encoder目前的向量流如下所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer7.png" style="zoom:50%"><p>当然, 输出$r_i$ 会流入下个Encoder, 当做输入:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer8.png" style="zoom:50%"><h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><p>每个Decoder除了有和Encoder相同的自注意力和前馈神经网络层, 还多了一个对Encoder的Encoder-Decoder Attention, 用来对Encoder编码的序列信息分配不同的权重.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer5.png" style="zoom:67%"><p>同样, 本图也具有歧义性.</p><h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self - Attention"></a>Self - Attention</h3><p>自注意力是Transformer中提出的一种新结构, 也是核心组件. 这个注意力并非Decoder对所有Encoder的隐藏状态的不同注意力, 而是<strong>自身对自身</strong>的注意力, 在做语义编码时使得<strong>单词在编码时能够根据上下文找到自己的真正含义</strong>. 自注意力将注意力采用<strong>Q-K-V模式</strong>, 即Query, Key, Value. 用不同的矩阵与Embedding输入$x_i$ 做矩阵乘, 就能分别得到对应的$q_i$, $k_i$, $v_i$, 而矩阵是随机初始化得来的, 之后通过学习调整参数. 至于QKV是定义的, 请参考<code>&lt;Seq2Seq和Attention&gt;</code>.</p><blockquote><p>为什么叫Self - Attention呢? 假设我们在执行机器翻译任务, 这个Attention不再是作用于我们给出的一种语言的输入Source和目标语言的输出Target, 而是作用于Source和Source内部, 即<strong>源语言的语义编码与原始输入Source之间</strong>的Attention, 这样能够获得单词在句子中更好的表示, 也称为”QKV”同源. 如果不好理解这个概念, 结合后文中提到的Cross - Attention就会明白.</p></blockquote><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer9.png" style="zoom:67%"><blockquote><p>这里注意一下维度, 在论文中的Embedding维度$d_{model}=512$, 给出的Key维度和Value维度均为64, 即$d_k=d_v=d_{model}/h=64$, 那么对应QKV的矩阵$W_Q$, $W_K$, $W_V$ 大小应该都是$(512, 64)$.</p></blockquote><p>这样就能根据输入得到一个查询向量$q_i$, 一组键值对$&lt;k_i, v_i&gt;$.</p><p>有了QKV, 接下来需要按照Attention的流程计算$q_i$ 和$k_i$ 的Score, 根据论文中提到的<strong>缩放点积注意力</strong>(Scaled Dot-Product Attention):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/scaledotattention.jpg" style="zoom:67%"><p>先进行点积, 再进行缩放, 计算完$q_i$ 与句中所有单词的$k$ 的得分(这里采用点积得到)后, 再对Score除以$\sqrt{d_k}$, 完成缩放, 最后再通过Softmax得到Attention权重, 加权求和结果称为$z_i$.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer10.png" style="zoom:67%"><p>注意, 我们上面的讨论全部都是针对一个单词的, 但是在实际的运算中, 由于Encoder是线性Stack起来的, 所以其实Encoder的训练是可以并行的, 即<strong>多个单词做完Embedding后作为一个矩阵并行计算</strong>, 假设输入矩阵$X$, 通过$W_Q$, $W_K$, $W_V$ 计算后可以得到$Q$, $K$, $V$:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer11.png" style="zoom:50%"><p>综上, 将自注意力总结为:<br>$$<br>\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V<br>$$<br><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer12.png" style="zoom:50%"></p><p>这个公式其实在<code>&lt;Seq2Seq和Attention&gt;</code>一文中提到过, 但这里作者对Score使用了归一化, 即除以$\sqrt{d_k}$, $\sqrt{d_k}$ 为Key的维度. 这属于训练的一个Trick, 作者对此的解释如下:</p><blockquote><p>We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by $\frac{1}{\sqrt{d_k}}$.</p></blockquote><p>当$d_k$ 非常大时, 求得的内积可能会非常大, 如果不进行缩放, 不同的内积大小可能差异会非常大, Softmax在指数运算可能将梯度推到特别小, 导致梯度消失.</p><h3 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi - head Attention"></a>Multi - head Attention</h3><p>Multi - head Attention的思路和CNN中的多个<strong>卷积核</strong>起到的作用明显是一致的. 所谓”多头”, 放在<strong>卷积神经网络</strong>里就是卷积层多个卷积核的特征提取过程, 在这里就是进行多次注意力的提取, 就像多个卷积核一样, 多次<strong>不同的初始化矩阵</strong>经过训练可能会有多种<strong>不同的特征,</strong> 更有利于<strong>不同角度</strong>的特征抽取和信息提取.</p><p>论文中用多个头求出多组的数据堆叠, 也就是图中的$h$ 维:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/multiheadattention.jpg" style="zoom:50%"><p>这样就能得到多个不同的Attention结果:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer13.png" alt=""></p><p>论文中采用了8个头的注意力, 即$h=8$, 得到多个提取出来的特征:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer14.png" style="zoom:50%"><p>我们将所有Self - Attention提取的特征全部concat起来, 因为维度比较大, 所以要经过一个输出矩阵$W_O$, 对特征进行进一步压缩, 直到大小和Encoder接收的输入相同:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer15.png" style="zoom:50%"><p>也就是说经过左侧的6个Encoder, 向量大小仍然不改变.</p><p>到现在总结一下Word Embedding以及Encoder的流程:</p><ol><li>做Embedding和位置编码, 获得输入$X$.</li><li>通过多头获得多组对应的$Q$, $K$, $V$.</li><li>通过缩放点积注意力, 多个头分别加权求和求得$Z_i$.</li><li>将所有$Z_i$ 全部concat起来, 然后经过$W_O$ 的特征提取, 得到最终输出$Z$, 其大小与输入$X$ 是完全相同的.</li></ol><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer16.png" style="zoom:50%"><blockquote><p>图中的$R$ 代表除了最开始的Encoder, 其他Encoder都不需要Embedding, 用上一个Encoder的输出作为输入.</p></blockquote><p>对每个头对句子不同部分的注意力进行可视化, 能发现每个头的注意力都在不同的位置上, 作用确实类似于CNN的卷积核, 做到了不同的注意力表示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer17.png" style="zoom:67%"><h3 id="Encoder-Side"><a href="#Encoder-Side" class="headerlink" title="Encoder Side"></a>Encoder Side</h3><h4 id="Position-wise-Feed-Forward-neural-network"><a href="#Position-wise-Feed-Forward-neural-network" class="headerlink" title="Position-wise Feed Forward neural network"></a>Position-wise Feed Forward neural network</h4><p>前馈神经网络就是结构中提到的Feed Forward neural network. 当然不单单是一个全连接层, 这里还用到了<strong>ReLu</strong>作为激活函数, 并且加上了<strong>Layer Normalization</strong>:<br>$$<br>\operatorname{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}<br>$$</p><p>但这里值得说明的是, 在论文中有这样一段:</p><blockquote><p>In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position <strong>separately</strong> and <strong>identically</strong>. This consists of two linear transformations with a ReLU activation in between.</p></blockquote><p>所以FFN本身也可以表示为2个<strong>一维</strong>的$1\times1$的卷积, 这二者是<strong>等价</strong>的.</p><p>对于并行计算的不同单词, 通过的FFN参数是<strong>共享</strong>的, 也可以看做<strong>不同单词先后通过同一个FFN</strong>, 如下图所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer20.png" style="zoom:50%"><p>这也就是为什么我前面要说FFN能够看做是<strong>一维卷积</strong>, 因为它们对每个单词独立的运算.</p><p>并且在经过每个Encoder Layer后, 都<strong>不改变数据的大小</strong>.</p><h4 id="Layer-Norm"><a href="#Layer-Norm" class="headerlink" title="Layer Norm"></a>Layer Norm</h4><p>你应该接触过Batch Norm, Layer Norm也是一种类似于Batch Norm的归一化方式, 同样能起到加快收敛的作用, 在<strong>NLP任务</strong>中比较常用.</p><p>Batch Norm中, 记录下多个Batch中每个Channel在该Batch内和该Batch内所有特征图上计算得到的均值和方差, 最后再进行放缩和平移, 达到<strong>每个通道的特征是相互独立</strong>的. 这么说可能有点绕, 例如, 对于输入大小为$(N, C, H, W)$ 的图像, 通过BN可以得到大小为$C$ 的均值以及方差, 即大小为$C$ 的均值方差由同个Batch维$N$ 以及特征图大小$H \times W$ 同时求得:<br>$$<br>\begin{aligned}<br>\mu_c &amp;= \frac{1}{NHW} \sum_{i=1}^N\sum_{j=1}^H\sum_{k=1}^W x_{i,c,j,k} \\<br>\sigma^2_c &amp;= \frac{1}{NHW} \sum_{i=1}^N\sum_{j=1}^H\sum_{k=1}^W (x_{i,c,j,k} - \mu_c)^2<br>\end{aligned}<br>$$<br><strong>Layer Norm是针对整个样本的归一化</strong>. 在Layer Norm中, 我们对<strong>同一个样本内不同特征</strong>进行归一化. 同样的, 对于输入大小为$(N, C, H, W)$ 的图像, LN计算的是所有Channel $C$ 和所有特征图$H \times W$ 尺度下大小为$N$ 的均值和方差:<br>$$<br>\begin{aligned}<br>\mu_n &amp;= \frac{1}{CHW} \sum_{i=1}^C\sum_{j=1}^H\sum_{k=1}^W x_{n,c,j,k} \\<br>\sigma^2_n &amp;= \frac{1}{CHW} \sum_{i=1}^C\sum_{j=1}^H\sum_{k=1}^W (x_{n,i,j,k} - \mu_n)^2<br>\end{aligned}<br>$$<br>下面这张图非常清晰:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/layernorm.png" style="zoom:50%"><p>在CV中, 整个样本是$(N, C, H, W)$ 的整张图片. 有趣的是, <strong>在NLP中</strong>, <strong>Layer Norm却指的是对每个Token表示做的归一化</strong>. 与其说它是Layer Norm, 不如说它其实是<strong>Instance Norm</strong>.</p><blockquote><p>详见: <a href="https://zhuanlan.zhihu.com/p/557696834" target="_blank" rel="noopener">震惊！BERT用LayerNorm的可能不是你认为的那个Layer Norm？</a>.</p></blockquote><p>LN与Batch无关, 故对于Hidden Size为$m$ 的句子, NLP任务中的Layer Norm实际上是:<br>$$<br>\begin{aligned}<br>\mu_{j}&amp;=\frac{1}{m} \sum_{i=1}^{m} x_{i j} \\<br>\sigma_{j}^{2}&amp;=\frac{1}{m} \sum_{i=1}^{m}\left(x_{i j}-\mu_{j}\right)^{2}<br>\end{aligned}<br>$$<br>引入$\epsilon$ 做平滑, 使分母不为0:<br>$$<br>\text {LayerNorm}(x)=\frac{x_{i j}-\mu_{j}}{\sqrt{\sigma_{j}^{2}+\epsilon}}<br>$$<br>跟Batch Norm一样, 之后也进行平移($\beta$)和放缩($\gamma$)两个可学习参数, 用于增强模型的表达能力:<br>$$<br>y = \text{LayerNorm}(x) \cdot \gamma + \beta<br>$$</p><blockquote><p>如果想了解它为什么和NLP领域比较契合, 详见下文:</p><ul><li><a href="https://zhuanlan.zhihu.com/p/54530247" target="_blank" rel="noopener">模型优化之Layer Normalization</a></li><li><a href="https://zhuanlan.zhihu.com/p/74516930" target="_blank" rel="noopener">NLP中 batch normalization与 layer normalization</a></li><li><a href="https://www.zhihu.com/question/395811291" target="_blank" rel="noopener">transformer 为什么使用 layer normalization, 而不是其他的归一化方法?</a></li><li>原论文<a href="https://arxiv.org/abs/1607.06450" target="_blank" rel="noopener">Layer Normalization</a></li></ul><p>大致原因是Batch Norm 对于Embedding后的数据进行归一化, 假设每个Batch是由多个Embedding组成的, 按照Batch方向对每个归一化, 就是对每个词的Embedding整体归一化. 这样做非常没有道理, 不符合NLP的规律, 它反而加强了不同词之间的相关性.</p><p>但如果按照Layer Norm, 按照Layer方向, 实际上是分别对每个Embedding后的词向量进行归一化, 这样每个词向量相对独立.</p><p>这主要还是CV和NLP的<strong>数据属性</strong>决定的. 在CV中, 不同样本之间的Channel信息是具有共性的(因为图像还是要用2D来表示), 这部分信息非常重要, 如果归一化会损失很多信息. 而NLP中, 数据是Embedding来的, 本来也没有包含位置信息, 反而不同词向量之间毫无相关性, 关注单词本身的归一化效果会更好.</p></blockquote><h4 id="Residual"><a href="#Residual" class="headerlink" title="Residual"></a>Residual</h4><p>残差连接从ResNet中提出也有一定年头了, 作为近些年使用频次比较高效果比较好的结构, 原理不再多赘述了, 在<code>&lt;卷积神经网络小结&gt;</code>和<code>&lt;卷积神经网络发展史&gt;</code>中都有对残差连接的详解.</p><p>在Encoder中残差连接伴随着Layer Norm, 每次经过一个子层都要做一次残差连接.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer21.png" style="zoom:50%"><p>在Decoder中也是同样的, 每次经过子层也都要做残差连接.</p><h3 id="Decoder-Side"><a href="#Decoder-Side" class="headerlink" title="Decoder Side"></a>Decoder Side</h3><p>当了解了Encoder的结构后, 结合起Decoder来看一下信息流.</p><p>假设只有两个Encoder和两个Decoder的堆叠, 那么信息的流动方向是这样的:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer22.png" style="zoom:50%"><p>确实只有最后的Encoder将输出传递给了Decoder的Encoder - Decoder Attention. Encoder的输出也是和Decoder唯一的交互途径, 它们通过Encoder - Decoder Attention进行交互(Cross - Attention). 整个Decoder的最终输出就是经过多个堆叠的Encoder计算得来的与Encoder输入大小相同的向量.</p><h4 id="Encoder-Decoder-Attention"><a href="#Encoder-Decoder-Attention" class="headerlink" title="Encoder - Decoder Attention"></a>Encoder - Decoder Attention</h4><p>Decoder中的Encoder - Decoder Attention和Encoder中的多头Self - Attention运算机制<strong>一致</strong>, 唯一不同的就是<strong>输入数据的来源</strong>.</p><p>其中, $Q$ 对应的输入来源于Decoder的<strong>Masked Self - Attention</strong>(即每个Decoder的Self - Attention层的输出), 而$K$ 和$V$ 对应的输入来源于整个<strong>Encoder</strong>的最终输出(图中Encoder上方的蓝色向量), 因此也称为”QKV”<strong>不同源</strong>. 根据这三个输入再结合Encoder - Decoder Attention层中的$W_Q$, $W_K$, $W_V$ 分别得到$Q$, $K$, $V$. 然后再根据缩放点积得到Attention Value.</p><p>对于多个经过Encoder的单词形成含有多个单词的矩阵, Decoder是在这一环节实现翻译时对不同单词的Attention的.</p><blockquote><p>注: 下面两张动图比较大, 挂在github上了, 如果没挂梯子可能无法正常显示,</p></blockquote><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/transformerdecode1.gif" style="zoom:50%"><h4 id="Outputs"><a href="#Outputs" class="headerlink" title="Outputs"></a>Outputs</h4><p>因为采用了Seq2Seq的架构, Decoder每过一个时间步不光接受Encoder的输出, 还要重新以上一个Timestep的Decoder输出作为输入, 即论文中提到的”<strong>shifted right</strong>“. 这种依赖上一时刻的输出作为输入的方式也被称为<strong>自回归范式</strong>.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/transformerdecode2.gif" style="zoom:50%"><h4 id="Masked-Multi-Head-Attention"><a href="#Masked-Multi-Head-Attention" class="headerlink" title="Masked Multi - Head Attention"></a>Masked Multi - Head Attention</h4><p>Mask是Transformer中一个关键点. Masked Multi - Head Attention 只出现在Decoder中. 到了Decoder, 可就不再像Encoder那样直接把数据拿过来并行训练了, 如果也像Encoder那样把所有输入的词向量全一股脑堆进去, Decoder做Self - Attention可以无视解码的时间跨度, <strong>获知全部的信息</strong>, 因此需要用Mask将当前预测的单词和之后的单词全都<strong>遮盖</strong>, 否则就没法训练了.</p><p>若仍然沿用传统Seq2Seq+RNN的思路, Decoder是一个<strong>顺序操作</strong>的结构, 我们代入一个场景来看看. 假设我们要执行<strong>机器翻译</strong>任务, 要将<code>我 是 大宁</code>翻译为<code>I am DaNing</code>, 假设所有参数与论文中提到的参数一样, batch size视为1. 根据前面已知的知识, Encoder堆叠后的输入和Embedding的大小是相同的, 在这里有三个词语, Embedding且通过Encoder后的编码大小为$(3, 512)$. 下面对Decoder进行训练:</p><ol><li>将起始符<code>&lt;start&gt;</code> 作为初始Decoder输入, 经过Decoder处理和分类得到输出<code>I</code>.</li><li>将<code>&lt;start&gt; I</code>作为Decoder输入, 经过Decoder处理和分类得到输出<code>am</code>.</li><li>将<code>&lt;start&gt; I am</code>作为Decoder输入, 经过Decoder处理和分类得到输出<code>DaNing</code>.</li><li>将<code>&lt;start&gt; I am DaNing</code>作为Decoder输入, 经过Decoder处理和分类得到结束符<code>&lt;end&gt;</code>.</li></ol><p>上面提到过, 这种依赖上一时刻的输出作为输入的方式也被称为<strong>自回归</strong>.</p><p>参考将RNN更改为Self - Attention的Encoder思路, 对于这种依赖于前一个时间步预测结果的结构Decoder, 如果想做到<strong>并行</strong>训练, 需要将上面的过程转化为一个这样的矩阵直接作为Decoder的输入:<br>$$<br>\begin{bmatrix}<br>\text{&lt;start&gt;}&amp; &amp; &amp; \\<br>\text{&lt;start&gt;}&amp; \text{I} &amp; &amp; \\<br>\text{&lt;start&gt;}&amp; \text{I} &amp; \text{am} &amp; \\<br>\text{&lt;start&gt;}&amp; \text{I} &amp; \text{am} &amp; \text{DaNing}<br>\end{bmatrix}<br>$$<br>因为在<strong>训练时已知任务标签</strong>(即已知翻译的双语平行语料), 所以可以不管Decoder每时间步预测的是什么, 直接将目标语言的语料作为Decoder的整个输入序列.</p><p>这种方法被称为<strong>Teacher Forcing</strong>, 仅在训练阶段使用, 而不能使用在推断过程, 我会在下一节训练技巧中讲讲.</p><p>并行训练虽然快了, 但是必须做一些小的处理, 使得Transformer的并行训练与训练好后的推理阶段是<strong>一致</strong>的, 例如对当前时间步后的输入打Mask.</p><blockquote><p>图片取自<a href="https://wmathor.com/index.php/archives/1438/" target="_blank" rel="noopener">Transformer 详解</a>.</p></blockquote><p>在论文的图中, Mask操作顺序被放在$Q$ 和$K$ 计算并缩放后, Softmax计算前. 如果继续计算下去, 不做Mask, 与$V$ 相乘后得到Attention, 所有时间步信息全部都被泄露给Decoder, 必须用Mask将当前预测的单词信息和之后的单词信息全部遮住.</p><p>遮住的方法非常简单, 首先不能使用0进行遮盖, 因为Softmax中用零填充会产生错误, $e^0=1$. 所以必须要用$-\infty$来填充那些不能被看见的部分. 我们直接生成一个下三角全为0, 上三角全部为<strong>负无穷</strong>的矩阵, 与原数据相加就能完成遮盖的效果:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer24.png" style="zoom:50%"><p>做Softmax时, 所有的负无穷全变成了0, 不再干扰计算:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer25.png" style="zoom:67%"><p>其实Mask在对句子的<strong>无效部分填充</strong>时, 也是用同样方法将所有句子补齐, 无效部分用负无穷填充的.</p><blockquote><p>强调: Decoder仍然依赖与先前输出结果作为输入, 所以在正式使用时不能实现并行预测, 但在训练的时结果是已知的, 可以实现并行训练.</p></blockquote><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>最后来谈谈位置编码. 因为Transformer采用了纯粹的Attention结构, 不像RNN一样能够通过时间步来反映句子中单词的前后关系, 即不能得知单词的<strong>位置信息.</strong> 这使得相同的单词被输入到模型时, Transformer并不能区分, 因为它们的Word Embedding是完全一样的. 所以必须让模型知道位置信息保证位于不同位置的相同单词是<strong>可区分的</strong>, 这点非常重要. 而且, 在NLP任务中, <strong>语序</strong>是一个相当重要的属性, 位于文档前面的单词和位于文档后面的单词可能会有不同的含义或比重.</p><p>作者通过<strong>位置编码</strong>在每次进入Encoder和Decoder前将位置信息写入. 这样来看, 与其叫位置编码, 不如叫<strong>位置嵌入</strong>. 位置编码可以直接与Embedding的向量相加:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer18.png" style="zoom:50%"><p>作者的做法非常有意思, 对不同的单词位置, 不同的Embedding维度, 它的编码都是<strong>唯一</strong>的, 应用正弦和余弦函数也方便Transformer学到位置的特征. 如果将当前单词位置记为$pos$, 而词向量的某个维度记为$i$, 那么位置编码的方法为:<br>$$<br>\begin{aligned}<br>P E_{(p o s, 2 i)} &amp;=\sin \left(p o s / 10000^{2 i / d_{\text {model }}}\right) \\<br>P E_{(p o s, 2 i+1)} &amp;=\cos \left(p o s / 10000^{2 i / d_{\text {model }}}\right)<br>\end{aligned}<br>$$<br>计算出来的结果应该是这样的:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer19.png" style="zoom:50%"><p>如果上面那组式子看起来有些乱, 写成这样或许好些:<br>$$<br>\begin{aligned}<br>P E_{(p o s, 2 i)} &amp;=\sin \left(\frac{p o s}{10000^\frac{2 i}{d_{\text {model }}}}\right) \\<br>P E_{(p o s, 2 i+1)} &amp;=\cos \left(\frac{p o s}{10000^\frac{2 i}{d_{\text {model }}}}\right)<br>\end{aligned}<br>$$<br>如果按照论文中的设定$d_{model}=512$, 由于奇偶数的计算方式是不同的, 所以$i \in[0, 255]$.</p><p>在这个式子中, 编码周期不受单词位置$pos$ 影响, 仅仅与模型开始设计的$d_{model}$ 和Embedding的不同维度$i$ 相关. 对于不同的$i$, $PE$ 的周期是$[2\pi, 10000\cdot2\pi]$.</p><p>这样看, 同一位置上的词语, 对于不同的Embedding维度, 都得到不同的编码, 并且随着$i$ 的增大, 位置编码的值的变化就越来越慢. 这种编码对于不同维度的Embedding来说是<strong>唯一</strong>的, 因此模型能够学习到关于Embedding的位置信息.</p><p>下面这个热图非常直观, 分开来看. 对于相同的Position的词语, 它不同维度的Embedding往往具有不同周期而交错的$\sin$ 和$\cos$ 组合, 而对于Embedding的同一个维度和不同Position的单词, 在Position上呈现出周期性.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/位置编码1.png" style="zoom:50%"><p>同样, 用折线图表示不同位置和编码后值的关系:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/位置编码2.png" style="zoom:67%"><p>起初, 我并不知道为什么这种方法Work. 在看过<a href="https://zhuanlan.zhihu.com/p/92017824" target="_blank" rel="noopener">浅谈 Transformer-based 模型中的位置表示</a>后, 感觉似乎有些道理. 作者意在利用正弦余弦的数学性质(周期性, 和角公式), 使得偏移了的一定position, 记为$k$ , 能够得到正弦余弦的<strong>不同线性组合</strong>(总感觉这种编码在通信的某个地方应该很常用, 只是DL第一次用而已).</p><p>三角函数性质:<br>$$<br>\left\{\begin{array}{l}<br>\sin (\alpha+\beta)=\sin \alpha \cos \beta+\cos \alpha \sin \beta \\<br>\cos (\alpha+\beta)=\cos \alpha \cos \beta-\sin \alpha \sin \beta<br>\end{array}\right.<br>$$<br>得到偏移后, 即$pos+k$ 的$PE$:<br>$$<br>\left\{\begin{array}{l}<br>PE(pos+k, 2i)=PE(pos, 2i) \times PE(k, 2i+1)+PE(pos, 2i+1) \times PE(k, 2i) \\<br>PE(pos+k, 2i+1)=PE(pos, 2i+1) \times PE(k, 2i+1)-PE(pos, 2i) \times PE(k, 2i)<br>\end{array}<br>\right.<br>$$</p><p>除了公式计算, 作者也实验了其他对于位置编码的方式, 比如通过训练得到, 但由于实际效果与计算得来相仿, 那么还不如通过公式计算直接得到位置编码.</p><h3 id="Final-Output"><a href="#Final-Output" class="headerlink" title="Final Output"></a>Final Output</h3><p>最终输出很简单, 根据Decoder的输出经过FC层和Softmax得到对应的单词.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer23.jpg" style="zoom:50%"><p>注意, Decoder的Embedding层和最后输出经过Softmax前的Linear层也是<strong>共享权重</strong>的.</p><h2 id="训练技巧"><a href="#训练技巧" class="headerlink" title="训练技巧"></a>训练技巧</h2><p>让Transformer跑的起来离不开下面说的这些训练技巧.</p><h3 id="WarmUp"><a href="#WarmUp" class="headerlink" title="WarmUp"></a>WarmUp</h3><p>在原文中, Optimizer使用Adam, 除了设置参数$\beta_1 = 0.9$, $\beta_2 = 0.98$, $\epsilon=10^{-9}$外, 还设置了Warmup:<br>$$<br>\text {lrate}=d_{\text {model }}^{-0.5} \cdot \min \left(\text {step}_{-} \text {num}^{-0.5}, \text {step_num} \cdot \text {warmup_steps}^{-1.5}\right)<br>$$<br>我按照step增长, 做出WarmUp对应的learning rate变化图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer26.jpg" style="zoom:25%"><p>在开始时学习率大, 然后逐渐变小. 对于越大的$d_{model}$ 来说, 初始的斜率越小. 论文中设置$\text{warmup_step}=4000$. 许多后续的模型也用到了WarmUp.</p><h3 id="Label-Smoothing"><a href="#Label-Smoothing" class="headerlink" title="Label Smoothing"></a>Label Smoothing</h3><p>在<code>&lt;卷积神经网络发展史&gt;</code>中提到过, 这里再重复一下. 标签平滑可以看做是一种添加到损失公式中的一种正规化组件, 因为独热编码后的标签只有0和1, 可能有些过于绝对, 标签平滑提供了一种手段来使其中的0也能分配到一些数值. 假设输入为$x$, 一共需要对$c$ 个类进行划分, 则标签平滑后的结果$y\prime$ 为:<br>$$<br>y\prime = (1 - \epsilon) \cdot x + \frac{\epsilon}{c}<br>$$<br>直接贴代码, Show you my code!</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># input_tensor is a tensor in pytorch</span>
<span class="token keyword">def</span> <span class="token function">label_smoothing</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> epsilon<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    classes <span class="token operator">=</span> input_tensor<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># compute the number of classes</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> epsilon<span class="token punctuation">)</span> <span class="token operator">*</span> input_tensor <span class="token operator">+</span> <span class="token punctuation">(</span>epsilon <span class="token operator">/</span> classes<span class="token punctuation">)</span> 

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'label_smoothing(a):'</span><span class="token punctuation">,</span> label_smoothing<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'label_smoothing(b):'</span><span class="token punctuation">,</span> label_smoothing<span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
label_smoothing(a): tensor([0.0333, 0.0333, 0.9333])
label_smoothing(b): tensor([0.0500, 0.9500])
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在论文中设置$\epsilon_{ls}=0.1$. 标签平滑可能会提高句子的困惑度, 因为添加了更多的不确定性, 但会提高准确率和BLEU.</p><h3 id="Residual-Dropout"><a href="#Residual-Dropout" class="headerlink" title="Residual Dropout"></a>Residual Dropout</h3><p>在进行残差连接时, 每个子层进行add和batch norm之前, 都添加了Dropout. 原论文中设置$P_{\text {drop}}=0.1$, 个人认为Dropout没有前两种技巧作用大.</p><h3 id="Teacher-Forcing"><a href="#Teacher-Forcing" class="headerlink" title="Teacher Forcing"></a>Teacher Forcing</h3><p>在模型的训练阶段, Decoder的所有正确输入是<strong>完全已知</strong>的, 如果自回归预测在某个时间步$t$ 解码出错, 则会导致$t$ 时刻后所有的预测结果都产生<strong>偏差</strong>.</p><p>Teacher Forcing通过将自回归模型解码过程中的所有输入<strong>强制修正</strong>为Ground Truth来避免了这个问题.</p><p>Teacher Forcing有诸多优点:</p><ul><li>解决了训练阶段自回归式模型的串行问题, 能够使得模型训练时<strong>并行</strong>.</li><li>避免了模型训练时预测<strong>一步错步步皆错</strong>的问题.</li><li>由于干涉了训练错误的情况, 加快了模型的<strong>收敛</strong>速度.</li></ul><p>但它也同时容易产生<strong>矫枉过正</strong>的问题:</p><ul><li><strong>Exposure Bias</strong>: 这是最为常见的问题. 在训练时因为受到干涉, 很容易产生<strong>训练推断不一致</strong>.</li><li><strong>Overcorrect</strong>: 有时候模型解码有自己的想法, 但因为Teacher Forcing的干涉, 导致生成的句子<strong>四不像</strong>.</li><li><strong>No diversity</strong>: Teacher Forcing对Ground Truth的约束是非常强的, 模型的<strong>多样性</strong>受到严重限制.</li></ul><blockquote><p>Teacher Forcing的衍生问题, 请阅读<a href="https://zhuanlan.zhihu.com/p/93030328" target="_blank" rel="noopener">关于Teacher Forcing 和Exposure Bias的碎碎念</a>.</p></blockquote><h2 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h2><blockquote><p>摘自<a href="https://www.nowcoder.com/discuss/258321" target="_blank" rel="noopener">牛客网: NLPer看过来, 一些关于Transformer的问题整理</a>.</p><p>不过以目前的眼光来看, 这些问题都已经过时了. 现在的Seq2Seq的概念是一种范式, 而不是以前的RNN-Attention.</p></blockquote><h3 id="Transformer相比于RNN-LSTM-有什么优势-为什么"><a href="#Transformer相比于RNN-LSTM-有什么优势-为什么" class="headerlink" title="Transformer相比于RNN/LSTM, 有什么优势? 为什么?"></a>Transformer相比于RNN/LSTM, 有什么优势? 为什么?</h3><ol><li>RNN系列的模型<strong>并行计算</strong>能力很差.</li><li>Transformer的<strong>特征抽取</strong>能力比RNN系列的模型要好(实验结论).</li></ol><h3 id="为什么说Transformer可以代替RNN-seq2seq"><a href="#为什么说Transformer可以代替RNN-seq2seq" class="headerlink" title="为什么说Transformer可以代替RNN-seq2seq?"></a>为什么说Transformer可以代替RNN-seq2seq?</h3><p>这里用代替这个词略显不妥当, RNN-seq2seq虽已老, 但始终还是有其用武之地, seq2seq最大的问题在于<strong>将Encoder端的所有信息压缩到一个固定长度的向量中</strong>, 并将其作为Decoder端首个隐藏状态的输入, 来预测Decoder端第一个单词(token)的隐藏状态. 在输入序列比较长的时候, 这样做显然会损失Encoder端的很多信息, 而且这样一股脑的把该固定向量送入Decoder端, Decoder端不能够关注到其想要关注的信息.</p><p>上述两点都是RNN-seq2seq模型的缺点, 后续论文对这两点有所改进, 如著名的<a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">Neural Machine Translation by Jointly Learning to Align and Translate</a>, 虽然确确实实对RNN-seq2seq模型有了实质性的改进, 但是由于主体模型仍然为RNN(LSTM)系列的模型, 因此模型的并行能力还是受限, 而transformer不但对RNN-seq2seq模型这两点缺点有了实质性的改进(多头交互式attention模块), 而且还引入了self-attention模块, 让源序列和目标序列首先”自关联”起来, 这样的话, 源序列和目标序列自身的embedding表示所蕴含的信息更加丰富, 而且后续的FFN层也增强了模型的表达能力(ACL 2018会议上有论文对Self-Attention和FFN等模块都有实验分析, 见论文: <a href="http://aclweb.org/anthology/P18-1167" target="_blank" rel="noopener">How Much Attention Do You Need?A Granular Analysis of Neural Machine Translation Architectures</a>), 并且Transformer并行计算的能力是远远超过seq2seq系列的模型, 因此我认为这是transformer优于RNN-seq2seq模型的地方.</p><h3 id="Transformer中句子的encoder表示是什么？如何加入词序信息的？"><a href="#Transformer中句子的encoder表示是什么？如何加入词序信息的？" class="headerlink" title="Transformer中句子的encoder表示是什么？如何加入词序信息的？"></a>Transformer中句子的encoder表示是什么？如何加入词序信息的？</h3><p>Transformer Encoder端得到的是整个输入序列的encoding表示, 其中最重要的是经过了self-attention模块, 让输入序列的表达更加丰富, 而加入词序信息是使用不同频率的正弦和余弦函数.</p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/6744.html">https://ADAning.github.io/posts/6744.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/Transformer/"><span class="chip bg-color">Transformer</span> </a><a href="/tags/Attention/"><span class="chip bg-color">Attention</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/36009.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/13.jpg" class="responsive-img" alt="指针网络家族"> <span class="card-title">指针网络家族</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文介绍了Pointer Network, CopyNet, Pointer-Generator Network以及Coverage机制在文本摘要与对话系统中的应用, 既可以作为知识点介绍, 也可以作为论文阅读笔记. 此外, 该部分内容为外</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-09-28 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/Attention/"><span class="chip bg-color">Attention</span> </a><a href="/tags/%E6%91%98%E8%A6%81%E7%94%9F%E6%88%90/"><span class="chip bg-color">摘要生成</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/38085.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/2.jpg" class="responsive-img" alt="卷积神经网络发展史"> <span class="card-title">卷积神经网络发展史</span></div></a><div class="card-content article-content"><div class="summary block-with-text">LeNetLeNet可以说是CNN的开山鼻祖之一了, 虽然它不是CNN的起点, 但是可以称为CNN兴起的标志. 它由图灵奖得主LeCun Yann在1998年的Gradient-based learning applied to docum</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-09-07 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/CNN/"><span class="chip bg-color">CNN</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">425.9k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:andaning@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>