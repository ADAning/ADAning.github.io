<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="知识蒸馏: Distilling the Knowledge in a Neural Network, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>知识蒸馏: Distilling the Knowledge in a Neural Network | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:.6"></i> <span>时间轴</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li><li><a href="/markdown"><i class="fab fa-markdown" style="margin-top:-20px;zoom:.6"></i> <span>Markdown</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li><li><a href="/markdown" style="margin-left:75px"><i class="fa fab fa-markdown" style="position:absolute;left:50px"></i> <span>Markdown</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/11.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">知识蒸馏: Distilling the Knowledge in a Neural Network</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/KD/"><span class="chip bg-color">KD</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2021-07-03</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2022-06-23</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 2.8k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 10 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><h1 id="Distilling-the-Knowledge-in-a-Neural-Network"><a href="#Distilling-the-Knowledge-in-a-Neural-Network" class="headerlink" title="Distilling the Knowledge in a Neural Network"></a>Distilling the Knowledge in a Neural Network</h1><p>本文是论文<a href="https://arxiv.org/abs/1503.02531" target="_blank" rel="noopener">Distilling the Knowledge in a Neural Network</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>现有机器学习中, 任何算法都可以用Ensemble的方法来提升性能, 但这样做会花费昂贵的计算资源, 并且不利于部署到真实场景中.</p><p>作者尝试提出一种<strong>把大模型知识尽可能的压缩进单个小模型</strong>的方法.</p><h2 id="Distillation"><a href="#Distillation" class="headerlink" title="Distillation"></a>Distillation</h2><p>人们希望得到的模型并不是在单一的数据集上拟合完美, 而是要求模型具有强大的<strong>泛化能力</strong>. 在某个问题的某个具体数据及上, 通常训练出的模型与真实问题会存在偏差, 存在一点点过拟合.</p><p>那么对于一个有能力的大模型, 就有希望直接利用大模型的知识, 训练一个具有更强泛化能力的小模型, 让小模型直接学习大模型的泛化能力.</p><blockquote><p>从泛化能力的角度来考虑, 知识蒸馏非常像一种<strong>正则化</strong>手段.</p></blockquote><p>训练时经常所采用的标签是独热编码, Softmax会刻意放大Logits之间的差距. 这使得模型输出的类别概率在某一类是非常大的(文中也称为<strong>Hard Target</strong>), 其他类别的概率都非常小.</p><p>但不同类别之间的<strong>相对概率</strong>仍然很重要, 例如猫的图片可能与狗有一定相似, 它一定比和苹果的相似性要低. 这种类别概率差异仍然可能存在着一些隐含的知识, 但它会被Softmax所抹除掉, 所以需要一些手段把这种知识传授给小模型.</p><p>一种可以尝试的方法是把大模型(<strong>Teacher</strong>)的预测结果和大模型的知识作为小模型(<strong>Student</strong>)的Target, 即将处理过后的大模型Logits作为Label或Label的一部分训练小模型.</p><p>既然是Softmax抹除了不同类别之间的差异, 那么可以对Softmax改动, 弱化其对隐含知识的影响.</p><p>假设神经网络在没有经过Softmax前的<strong>Logits</strong>记为$z_i$, 我们可以添加”<strong>温度</strong>“$T$ 来弱化影响, 记结果为$q_i$:</p><p>$$<br>q_{i}=\frac{\exp \left(z_{i}\right)}{\sum_{j} \exp \left(z_{j}\right)} \quad \rightarrow \quad q_{i}=\frac{\exp \left(z_{i} / T\right)}{\sum_{j} \exp \left(z_{j} / T\right)}<br>$$<br>$T=1$ 时, 就是正常的Softmax. 当$T &gt; 1$ 时, 原来的Softmax将变得<strong>更加软化</strong>, 不同类别之间的差距, 不再显得类别间的差距那么绝对. Teacher中的不同类别之间的暗含知识得到一定保留. 因为标签变得软化了, 所以熵更大, 也保存了更多的信息.</p><p>如果这个式子不够直观体现出它的作用, 我做出了不同$T$ 对Target $q_i$ 的影响变化曲线图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kd6.png" style="zoom:80%"><p>随着$T$ 的增大, 生成的Soft Target之间的差距会越来越小, 变得<strong>Softer</strong>.</p><p>Student将使用Hard Target和Soft Target共同训练自己, Teacher软化后的知识将作为损失函数的一部分调节Student的参数:<br>$$<br>\mathcal{L} = \mathcal{L}_{hard} + \lambda \mathcal{L}_{soft}<br>$$</p><p>$\lambda$ 为超参, 用于调节Teacher Soft Target的影响占比. 具体来说, Student应以常温$T=1$ 用Hard Target训练自己, 即损失的第一项. 同时, Teacher和Student的蒸馏时应对Softmax加以高温$T$, 即损失的第二项, 蒸馏过程的示意图如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kd7.png" style="zoom:15%"><blockquote><p>该图出自<a href="https://nni.readthedocs.io/en/stable/sharings/kd_example.html" target="_blank" rel="noopener">Knowledge Distillation on NNI</a>.</p></blockquote><blockquote><p>在训练时, 必须保证Teacher和Student的温度<strong>一致</strong>, 当训练完成后, Student预测不再使用$T$, 或者说训练完成后的推断设置$T=1$.</p><p>同时, 由于温度$T$ 的影响, 梯度均缩小了$T^2$ 倍(详见下一小节最后), 所以在设置$\lambda$ 时, 需要让其尽可能大一些, 或者乘$T^2$ 倍, 才能保证两种损失的贡献度相同.</p></blockquote><h3 id="Matching-Logits-is-a-Special-Case-of-Distillation"><a href="#Matching-Logits-is-a-Special-Case-of-Distillation" class="headerlink" title="Matching Logits is a Special Case of Distillation"></a>Matching Logits is a Special Case of Distillation</h3><p>作者下面证明了直接让Student学Teacher的Logits只是蒸馏的一种特殊情况.</p><blockquote><p>下面涉及到更详细的推导过程在末尾引文连接已附上.</p></blockquote><p>假设我们处理的问题所采用的损失函数是交叉熵$C$, 梯度为$\frac{\partial C}{\partial z_i}$, Teacher模型的Logits为$v_i$, 以及其对应的概率为$p_i$, 则有:</p><p>$$<br>\frac{\partial C}{\partial z_{i}}=\frac{1}{T}\left(q_{i}-p_{i}\right)=\frac{1}{T}\left(\frac{e^{z_{i} / T}}{\sum_{j} e^{z_{j} / T}}-\frac{e^{v_{i} / T}}{\sum_{j} e^{v_{j} / T}}\right)<br>$$</p><p>当$T$ 相较于Logits充分大的时候, 可以使用泰勒展开, 有$e^{x/T}\approx1+x/T$:</p><p>$$<br>\frac{\partial C}{\partial z_{i}} \approx \frac{1}{T}\left(\frac{1+z_{i} / T}{N+\sum_{j} z_{j} / T}-\frac{1+v_{i} / T}{N+\sum_{j} v_{j} / T}\right)<br>$$</p><p>当对Logits做了零均值假设后, 有$\sum_jz_j=\sum_jv_j=0$, 结合上式有:</p><p>$$<br>\frac{\partial C}{\partial z_{i}} \approx \frac{1}{N T^{2}}\left(z_{i}-v_{i}\right)<br>$$</p><p>因此, 在较高的温度$T$ 设置下, 蒸馏等价于最小化$\frac{1}{2}(z_i - v_i)^2$, 也就是直接把Teacher和Student的Logits匹配, 所以匹配Logits是一种蒸馏的特殊情况.</p><p>当温度较低时, 对负样本的关注就比较少, 可能滤去关键信息, 但实际上这<strong>有利有弊</strong>. 有些负样本的Logits应该是非常小的负值, 这种极小的负值在高温时的作用会被放大, 作为强大的噪声影响Student. 在低温时, 这种噪声将被滤去.</p><p>所以温度的选取一般依赖于经验, 不要太高也不要太低.</p><blockquote><p>分母上有$T^2$, 所以在知识蒸馏时, $\mathcal{L}_{soft}$ 的影响被缩小了$T^2$, 所以需要在设置损失项时平衡回来.</p></blockquote><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="MNIST"><a href="#MNIST" class="headerlink" title="MNIST"></a>MNIST</h3><p>作者将知识蒸馏应用于小规模数据集MNIST, Teacher训练了一个两隐层的DNN并使用Dropout和Weight Constraints, Student网络也是两隐层的DNN但神经元个数比Teacher少, 不使用正则化手段.</p><p>作者尝试了几种不同的小模型设置, 在合适的温度下取得了与Teacher相近的表现.</p><h3 id="Speech-Recognition"><a href="#Speech-Recognition" class="headerlink" title="Speech Recognition"></a>Speech Recognition</h3><p>语音识别中, 当时比较流行的做法是用HMM, 并按照如下目标优化模型参数$\theta$:<br>$$<br>\boldsymbol{\theta}=\arg \max _{\boldsymbol{\theta}^{\prime}} P\left(h_{t} \mid \mathbf{s}_{t} ; \boldsymbol{\theta}^{\prime}\right)<br>$$<br>其中$\mathbf{s_t}$ 为$t$ 时刻的结果, $h_t$ 为$t$ 时刻的HMM隐态.</p><p>结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kd1.jpg" style="zoom:50%"><p>蒸馏一个小模型出来后的结果比单独Train一个大模型的效果要好.</p><h3 id="JFT"><a href="#JFT" class="headerlink" title="JFT"></a>JFT</h3><p>JFT是一个比前面二者大得多的图像分类数据集, 这个数据集有1亿张图片, 15000个类别.</p><p>作者训练了一个通用模型和若干个专家模型, 作为没有使用知识蒸馏时的Baseline.</p><p>对若干种通用模型经常<strong>易混淆</strong>的类做一个聚类(也有可能有些类不被归纳进专家模型, 这就需要通用模型自己处理), 记为$S^m$, 作为多个JFT的子集, 将不同子集的数据交给不同的专家模型$m$ 预测, 下面是作者展示出的子集示例:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kd2.jpg" style="zoom:50%"><p>对于输入的图片$\mathbf{x}$, 得到分类结果需要两步:</p><ol><li><strong>粗分</strong>: 对于每个测试数据, 由通用模型得到$n$ 个最有可能的类别, 记这些类为$k$.</li><li><strong>细分</strong>: 按照$k$, $S^m$ 的非空交集找到相应的专家模型$A_k$ , 让专家模型进行预测.</li></ol><p><strong>专家模型极易过拟合</strong>. 为了防止过拟合, 专家模型所采用的一半数据来自指定类别, 剩下一半来自全数据集, 其他类别被全部设置为一个单独的”Dustbin”类.</p><p>然后最小化所有类别的概率分布$\mathbf{q}$ 和通用模型, 专家模型得到的概率分布的KL散度.</p><blockquote><p><strong>KL散度</strong>(也称为<strong>相对熵</strong>)常用于度量两个分布之间的差异性, 假设$P$ 为样本真实分布, $Q$ 为模型预测的分布, 根据KL散度有:<br>$$<br>D_{\mathrm{KL}}(P \| Q)=\mathbb{E}_{\mathrm{x} \sim P}\left[\log \frac{P(x)}{Q(x)}\right]=\mathbb{E}_{\mathrm{x} \sim P}[\log P(x)-\log Q(x)]<br>$$<br>当$P, Q$ 越接近时, $D_{\mathrm{KL}}(P \| Q)$ 就越小, 当$P, Q$ 分布完全相同时, $D_{\mathrm{KL}}(P \| Q)$ 为0.</p><p>KL散度还有两个性质:</p><ol><li>非负: KL散度是非负的.</li><li>不对称: 通常情况下, $D_{\mathrm{KL}}(P \| Q) \neq D_{\mathrm{KL}}(Q \| P)$, KL散度并不是真正意义上的距离.</li></ol></blockquote><p>记通用模型得到的概率分布为$\mathbf{p}^g$, 专家模型得到的概率分布为$\mathbf{p}^m$, 总体分布$\mathbf{q}$ 和模型预测得到概率分布的KL散度计算方式如下:</p><p>$$<br>K L\left(\mathbf{p}^{g}, \mathbf{q}\right)+\sum_{m \in A_{k}} K L\left(\mathbf{p}^{m}, \mathbf{q}\right)<br>$$</p><p>总体来说, 最好的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kd3.jpg" style="zoom:50%"><p>当时JFT的Baseline是CNN.</p><p>逐渐增大专家模型的数量, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kd4.jpg" style="zoom:50%"><p>随专家数量提升, 相对提升逐渐增大.</p><p>前面说过, 专家模型极易过拟合, 如果使用3%的数据的Hard Target训练专家模型, 它更有可能过拟合, 并且在附加早停的情况下非常早就停止了.</p><p>但如果使用知识蒸馏, 把Hard Target用Soft Target代替, 仅用3%的数据训练专家模型, 不但不会很早早停, 而且还能保留专家模型的泛化能力, 效果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kd5.jpg" style="zoom:50%"><p>使用Soft Target效果要好于同样使用3%的数据的Baseline的训练效果, 并且与使用全部数据的Baseline效果相近.</p><h2 id="Recommended"><a href="#Recommended" class="headerlink" title="Recommended"></a>Recommended</h2><ul><li>论文原文: <a href="https://arxiv.org/abs/1503.02531" target="_blank" rel="noopener">Distilling the Knowledge in a Neural Network</a></li><li>比较全面的文章: <a href="https://zhuanlan.zhihu.com/p/102038521" target="_blank" rel="noopener">【经典简读】知识蒸馏(Knowledge Distillation) 经典之作</a></li><li>BERT相关:<a href="https://zhuanlan.zhihu.com/p/71986772" target="_blank" rel="noopener">深度神经网络模型蒸馏Distillation</a></li><li>很详细的视频(需翻墙): <a href="https://www.youtube.com/watch?v=xKPvt3GQ1j8&ab_channel=peakqi" target="_blank" rel="noopener">Hinton：distilling knowledge in a neural network</a></li><li>数学推导(推荐看看): <a href="https://zhuanlan.zhihu.com/p/385374430" target="_blank" rel="noopener">Distilling the Knowledge in a Neural Network 理论推导</a></li></ul><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>知识蒸馏是一种将大模型的隐含知识通过某种手段提取出来, 提炼传授给小模型的<strong>模型压缩</strong>方法.</p><p>该论文发表自2014年, 当时深度学习的模型还没有发展到像现在这样的超大规模, Hinton能提出这种具有工程意义并且值得挖掘的新方向相当有远见.</p><blockquote><p>蒸馏为何有效, 人们还没有彻底摸清其中的作用原理.</p><p>甚至单个模型的<strong>自蒸馏</strong>也是有效的… 这点非常诡异, 为什么模型单单依靠样本本身却无法达到自蒸馏后的效果? 样本之间隐含的差异居然需要自己产生的产物重新喂给自己才能吸收(反刍)?</p><p>读完本论文后, 自然会产生进一步的想法. 直接把Logits蒸给小模型效果如何? 能蒸Logits为什么不直接蒸Feature呢? 要是蒸Feature也不够直接的话把参数蒸给小模型是不是也可以? 这些想法确实都可以, 或多或少都有效果.</p></blockquote></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/39586.html">https://ADAning.github.io/posts/39586.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/KD/"><span class="chip bg-color">KD</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/53598.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/22.jpg" class="responsive-img" alt="Introduction: Variational Auto - Encoder"> <span class="card-title">Introduction: Variational Auto - Encoder</span></div></a><div class="card-content article-content"><div class="summary block-with-text">Introduction: Variational Auto - Encoder变分自动编码器(VAE, Variational Auto - Encoder)是一种基于自编码器结构的深度生成模型. 本文对VAE更深层次的数学原理没有探讨,</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2021-07-09 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/VAE/"><span class="chip bg-color">VAE</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/60711.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/3.jpg" class="responsive-img" alt="ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"> <span class="card-title">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: BERT: 详见ELMo, GPT, BERT. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations本文是论文AL</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2021-06-29 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/BERT/"><span class="chip bg-color">BERT</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">367.8k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:695439722@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>