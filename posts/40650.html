<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="DDIM: Denoising Diffusion Implicit Models, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>DDIM: Denoising Diffusion Implicit Models | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/6.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">DDIM: Denoising Diffusion Implicit Models</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/Diffusion/"><span class="chip bg-color">Diffusion</span> </a><a href="/tags/DDIM/"><span class="chip bg-color">DDIM</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2025-03-21</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2025-03-20</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 7.2k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 34 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p>本文前置知识:</p><ul><li>DDPM: <a href="https://adaning.github.io/posts/58818.html">DDPM: Denoising Diffusion Probabilistic Model</a>.</li></ul></blockquote><h1 id="Denoising-Diffusion-Implicit-Models"><a href="#Denoising-Diffusion-Implicit-Models" class="headerlink" title="Denoising Diffusion Implicit Models"></a>Denoising Diffusion Implicit Models</h1><ul><li>论文: <a href="https://openreview.net/forum?id=St1giarCHLP" target="_blank" rel="noopener">Denoising Diffusion Implicit Models</a>, ICLR 2021.</li></ul><p>在<strong>DDPM</strong> 中我们提到, 其Forward Process和Generative(Reverse) Process都是在<strong>一阶马尔科夫链</strong>下定义的, 如下图左:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ddim1.png" style="zoom:50%"><p>但是依赖马尔科夫链的DDPM生成速度非常慢, 有没有什么办法来加速采样, 实现更高效率的生成呢? 要是能在这个基础上和DDPM的Forward Process兼容, 只改变Generative Process, 以此达到复用训练好的DDPM模型权重的目的就更好了.</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>先来简单的回顾一下<strong>DDPM</strong>, 并且需要注意DDIM中的定义与DDPM中存在一些差异.</p><p>对于数据分布$q(\boldsymbol{x}_0)$, 生成类模型希望用可学习的分布$p_\theta(\boldsymbol{x}_0)$ 来近似$q(\boldsymbol{x}_0)$. DDPM将其建模为一个联合概率分布:</p><p>$$<br>p_\theta\left(\boldsymbol{x}_0\right)=\int p_\theta\left(\boldsymbol{x}_{0: T}\right) \mathrm{d} \boldsymbol{x}_{1: T}, \quad \text { where } \quad p_\theta\left(\boldsymbol{x}_{0: T}\right):=p_\theta\left(\boldsymbol{x}_T\right) \prod_{t=1}^T p_\theta^{(t)}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t\right)<br>$$</p><p>其中$\boldsymbol{x}_1, \dots, \boldsymbol{x}_T$ 为隐变量.</p><p>DDPM中将Forward Process $q\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right)$ 定义为一个高斯过程的<strong>马尔科夫链</strong>:</p><p>$$<br>q\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right):=\prod_{t=1}^T q\left(\boldsymbol{x}_t \mid \boldsymbol{x}_{t-1}\right), \text { where } q\left(\boldsymbol{x}_t \mid \boldsymbol{x}_{t-1}\right):=\mathcal{N}\left(\sqrt{\frac{\alpha_t}{\alpha_{t-1}}} \boldsymbol{x}_{t-1},\left(1-\frac{\alpha_t}{\alpha_{t-1}}\right) \boldsymbol{I}\right)<br>$$</p><blockquote><p>DDIM中的$\alpha_t$ 对应的是DDPM论文中的$\bar\alpha_t$.</p></blockquote><p>其中, $\alpha_{1:T} \in (0, 1]^T$ 为逐渐减小的序列. 此时的$\boldsymbol{x}_t$ 具有一阶马尔科夫性, 即其分布受到$\boldsymbol{x}_{t-1}$ 影响.</p><p>与Forward Process相对应的<strong>Reverse Process</strong>就为 $q(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)$, 需要用模型$p_\theta(\boldsymbol{x}_{0:T})$ 来近似, $p_\theta(\boldsymbol{x}_{0:T})$ 倒推的过程称为<strong>Generative Process</strong>.</p><blockquote><p>如果你看过DDPM的论文, 这里的叫法可能会有些迷惑.<br>DDPM中将Generative Process $p_\theta\left(\boldsymbol{x}_{0: T}\right)$ 称为Reverse Process.<br>DDIM中更明确, 将Generative Process $p_\theta\left(\boldsymbol{x}_{0: T}\right)$ 和Reverse Process $q(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t)$ 区分开, $q$ 指的是推理分布.</p></blockquote><p>根据一阶马尔科夫性, 发现$q\left(\boldsymbol{x}_t \mid \boldsymbol{x}_0\right)$ 可以直接得到:</p><p>$$<br>q\left(\boldsymbol{x}_t \mid \boldsymbol{x}_0\right):=\int q\left(\boldsymbol{x}_{1: t} \mid \boldsymbol{x}_0\right) \mathrm{d} \boldsymbol{x}_{1:(t-1)}=\mathcal{N}\left(\boldsymbol{x}_t ; \sqrt{\alpha_t} \boldsymbol{x}_0,\left(1-\alpha_t\right) \boldsymbol{I}\right)<br>$$</p><p>因此可以用$\boldsymbol{x}_0$ 和噪声$\epsilon$ 的线性组合来表示$\boldsymbol{x}_t$:</p><p>$$<br>\boldsymbol{x}_t=\sqrt{\alpha_t} \boldsymbol{x}_0+\sqrt{1-\alpha_t} \epsilon, \quad \text { where } \quad \epsilon \sim \mathcal{N}(\mathbf{0}, \boldsymbol{I})<br>$$</p><p>$\alpha_T$ 接近于0, 且对于所有的$\boldsymbol{x}_0$, 都能使$q(\boldsymbol{x}_T \mid \boldsymbol{x}_0)$ 收敛于标准正态分布.</p><p>DDPM中用如下目标函数优化去噪网络参数$\theta$:</p><p>$$<br>L_\gamma\left(\epsilon_\theta\right):=\sum_{t=1}^T \gamma_t \mathbb{E}_{\boldsymbol{x}_0 \sim q\left(\boldsymbol{x}_0\right), \epsilon_t \sim \mathcal{N}(\mathbf{0}, \boldsymbol{I})}\left[\left||\epsilon_\theta^{(t)}\left(\sqrt{\alpha_t} \boldsymbol{x}_0+\sqrt{1-\alpha_t} \epsilon_t\right)-\epsilon_t\right||_2^2\right]<br>$$</p><p>其中, $\epsilon_\theta := \set{\epsilon_\theta^{(t)}}^{T}_{t=1}$ 为$T$ 个函数的集合, $\gamma:=[\gamma_1, \dots, \gamma_T]$ 为取决于$\alpha_{1:T}$ 的正系数.</p><h2 id="Variational-Inference-for-Non-Markovian-Forward-Processes"><a href="#Variational-Inference-for-Non-Markovian-Forward-Processes" class="headerlink" title="Variational Inference for Non-Markovian Forward Processes"></a>Variational Inference for Non-Markovian Forward Processes</h2><p>通过对DDPM的Training Object $L_\gamma$ 的观察, 发现$L_\gamma$ 实际上只取决于边缘分布$q(\boldsymbol{x}_{t}\mid\boldsymbol{x}_0)$, 而不是取决于联合分布$q(\boldsymbol{x}_{1:T} \mid \boldsymbol{x}_0)$. 因此可以考虑将$q(\boldsymbol{x}_{t}\mid\boldsymbol{x}_0)$ 推导过程中的遵循马尔科夫假设的$q(\boldsymbol{x}_{t}\mid\boldsymbol{x}_{t-1})$ 剥离, 建立一个非马尔科夫Forward Process, 从而得到一个新的Reverse Process与Generative Process.</p><blockquote><p>因此这个观察也给我们一个启发, 只要任何满足DDPM条件, 且能保证模型具有等价目标函数的Forward Process设计都是可行的.</p></blockquote><h3 id="Non-Markovian-Forward-Processes"><a href="#Non-Markovian-Forward-Processes" class="headerlink" title="Non-Markovian Forward Processes"></a>Non-Markovian Forward Processes</h3><p>如果不再考虑马尔科夫性, 可以将Forward Process $q_\sigma\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right)$ 重写为一组由<strong>方差</strong>$\sigma$ 为索引的一系列分布:</p><p>$$<br>\begin{equation}<br>q_\sigma\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right):=q_\sigma\left(\boldsymbol{x}_T \mid \boldsymbol{x}_0\right) \prod_{t=2}^T q_\sigma\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0\right)<br>\end{equation}<br>$$</p><p>由于DDPM的定义, 所以需要满足$t &gt; 1$ 时, $q_\sigma (\boldsymbol{x}_T\mid\boldsymbol{x}_0)=\mathcal{N}\left(\sqrt{\alpha_T}\boldsymbol{x}_0, (1-\alpha_T)\boldsymbol{I}\right)$, 即$q_\sigma\left(\boldsymbol{x}_T\mid\boldsymbol{x}_0\right)$ 已知.</p><p>回忆一下DDPM, 在DDPM中出现的$q_\sigma\left(\boldsymbol{x}_{t-1}\mid\boldsymbol{x}_t, \boldsymbol{x}_0\right)$ 实际上被我们用一阶马尔科夫链替换成$q_\sigma\left(\boldsymbol{x}_{t-1}\mid\boldsymbol{x}_t\right)$. 若想<strong>摆脱对马尔科夫链的依赖</strong>, 这里肯定就不能再把它替换成$q_\sigma\left(\boldsymbol{x}_{t-1}\mid\boldsymbol{x}_t\right)$ 了, 需要直接定义分布$q_\sigma\left(\boldsymbol{x}_{t-1}\mid\boldsymbol{x}_t, \boldsymbol{x}_0\right)$:</p><p>$$<br>\begin{equation}<br>q_\sigma\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0\right)=\mathcal{N}\left(\sqrt{\alpha_{t-1}} \boldsymbol{x}_0+\sqrt{1-\alpha_{t-1}-\sigma_t^2} \cdot \frac{\boldsymbol{x}_t-\sqrt{\alpha_t} \boldsymbol{x}_0}{\sqrt{1-\alpha_t}}, \sigma_t^2 \boldsymbol{I}\right)<br>\end{equation}<br>$$</p><p>当然, $q_\sigma\left(\boldsymbol{x}_{t-1}\mid\boldsymbol{x}_t, \boldsymbol{x}_0\right)$ 肯定不能乱定义. 它不能违背老祖宗(DDPM)的规矩, 必须得保证对于任意的时间步$t$, 都满足$q_\sigma\left(\boldsymbol{x}_t\mid\boldsymbol{x}_0\right)=\mathcal{N}\left(\sqrt{\alpha_t} \boldsymbol{x}_0, (1 - \alpha_t\right)\boldsymbol{I})$. 这样可以保证DDIM的Forward Process和DDPM的Forward Process<strong>兼容</strong>.</p><p>有了$q_\sigma\left(\boldsymbol{x}_{t-1}\mid\boldsymbol{x}_t, \boldsymbol{x}_0\right)$, Forward Process就可以进一步的用贝叶斯法则得到:</p><p>$$<br>q_\sigma\left(\boldsymbol{x}_t \mid \boldsymbol{x}_{t-1}, \boldsymbol{x}_0\right)=\frac{q_\sigma\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0\right) q_\sigma\left(\boldsymbol{x}_t \mid \boldsymbol{x}_0\right)}{q_\sigma\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_0\right)}<br>$$</p><p>它仍然是一个高斯分布. 但是与DDPM的Forward Process不同, 这里的Forward Process不再是一个马尔科夫链, 因为每个$\boldsymbol{x}_t$ 不光依赖于$\boldsymbol{x}_{t-1}$, 还依赖于$\boldsymbol{x}_0$.</p><p>$\sigma$ 决定了Forward Process的<strong>随机程度</strong>. 当$\sigma \rightarrow \boldsymbol{0}$ 时, 有一极端情况, 即已知$\boldsymbol{x}_0, \boldsymbol{x}_{t}$ 时, 所有的$\boldsymbol{x}_{t-1}$ 都是确定的.</p><h4 id="Proof"><a href="#Proof" class="headerlink" title="Proof"></a>Proof</h4><p>分布$q_\sigma\left(\boldsymbol{x}_{t-1}\mid\boldsymbol{x}_t, \boldsymbol{x}_0\right)$ 的形式是怎么得到的呢?</p><blockquote><p>这部分是证明, 从正反两个方向, 不感兴趣的可以跳过, 不过还是推荐看下.</p></blockquote><h5 id="Reverse-Proof"><a href="#Reverse-Proof" class="headerlink" title="Reverse Proof"></a>Reverse Proof</h5><p>逆向证明仅需证明作者构造的$q_\sigma\left(\boldsymbol{x}_{t-1}\mid\boldsymbol{x}_t, \boldsymbol{x}_0\right)$ 能使得DDPM的Forward Procees的设计$q_\sigma\left(\boldsymbol{x}_t\mid\boldsymbol{x}_0\right)=\mathcal{N}\left(\sqrt{\alpha_t} \boldsymbol{x}_0, (1 - \alpha_t\right)\boldsymbol{I})$ 对于所有的时间$t \leq T$ 都能被满足.</p><p>显然只需要检验中间过程是否有$q_\sigma\left(\boldsymbol{x}_{t-1}\mid\boldsymbol{x}_0\right)=\mathcal{N}\left(\sqrt{\alpha_{t-1}} \boldsymbol{x}_0, (1 - \alpha_{t-1}\right)\boldsymbol{I})$.</p><p>根据边缘分布, 用$\boldsymbol{x}_{t}$ 的积分计算$q_\sigma\left(\boldsymbol{x}_{t-1}\mid\boldsymbol{x}_0\right)$:</p><p>$$<br>q_\sigma\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_0\right):=\int_{\boldsymbol{x}_t} q_\sigma\left(\boldsymbol{x}_t \mid \boldsymbol{x}_0\right) q_\sigma\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0\right) \mathrm{d} \boldsymbol{x}_t<br>$$</p><p>这个积分是二者的卷积运算, 而且$q_\sigma\left(\boldsymbol{x}_{t-1}\mid\boldsymbol{x}_0\right)$ 是一个高斯分布, 设其为$\mathcal{N}(\mu_{t-1}, \Sigma_{t-1})$, 将条件$q_\sigma\left(\boldsymbol{x}_t\mid\boldsymbol{x}_0\right)=\mathcal{N}\left(\sqrt{\alpha_t} \boldsymbol{x}_0, (1 - \alpha_t\right)\boldsymbol{I})$ 代入$q_\sigma\left(\boldsymbol{x}_{t-1}\mid\boldsymbol{x}_t, \boldsymbol{x}_0\right)$, 可得:</p><p>$$<br>\begin{aligned}<br>\mu_{t-1} &amp;= \sqrt{\alpha_{t-1}} \boldsymbol{x}_0+\sqrt{1-\alpha_{t-1}-\sigma_t^2} \cdot \frac{\sqrt{\alpha_t} \boldsymbol{x}_0-\sqrt{\alpha_t} \boldsymbol{x}_0}{\sqrt{1-\alpha_t}} \\\<br>&amp;= \sqrt{\alpha_{t-1}}\boldsymbol{x}_0 \\<br>\Sigma_{t-1} &amp;= \sigma_t^2\boldsymbol{I} + \frac{1-\alpha_{t-1}-\sigma_t^2}{1-\alpha_t}(1-\alpha_t)\boldsymbol{I} \\\<br>&amp;=(1-\alpha_{t-1})\boldsymbol{I}<br>\end{aligned}<br>$$</p><p>方差$\Sigma_{t-1}$ 的求解用到了方差分解公式.</p><p>所以有$q_\sigma\left(\boldsymbol{x}_{t-1}\mid\boldsymbol{x}_0\right)=\mathcal{N}\left(\sqrt{\alpha_{t-1}} \boldsymbol{x}_0, (1 - \alpha_{t-1}\right)\boldsymbol{I})$. 因此按照递推, 对于所有的$t \leq T$, 都能够满足$q_\sigma\left(\boldsymbol{x}_t\mid\boldsymbol{x}_0\right)=\mathcal{N}\left(\sqrt{\alpha_t} \boldsymbol{x}_0, (1 - \alpha_t\right)\boldsymbol{I})$.</p><h5 id="Forward-Proof"><a href="#Forward-Proof" class="headerlink" title="Forward Proof"></a>Forward Proof</h5><p>当然, 一个更合理的视角是用<strong>待定系数法</strong>正着推, 思路来自<a href="https://spaces.ac.cn/archives/9181" target="_blank" rel="noopener">生成扩散模型漫谈（四）：DDIM = 高观点DDPM - 科学空间|Scientific Spaces</a>.</p><p>在DDPM中可知, $q_\sigma\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0\right)$ 本身就是一个正态分布, 因此我们可以更一般的假设$q_\sigma\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0\right) \sim \mathcal{N}(k\boldsymbol{x}_0 + m\boldsymbol{x}_{t}, \sigma^2)$, 便有:</p><p>$$<br>\boldsymbol{x}_{t-1} = k\boldsymbol{x}_0 + m\boldsymbol{x}_t + \sigma\epsilon, \quad \text { where } \quad \epsilon \sim \mathcal{N}(\mathbf{0}, \boldsymbol{I})<br>$$</p><p>在DDPM中我们的假设是:</p><p>$$<br>\boldsymbol{x}_t=\sqrt{\alpha_t} \boldsymbol{x}_0+\sqrt{1-\alpha_t} \epsilon^\prime, \quad \text { where } \quad \epsilon^\prime \sim \mathcal{N}(\mathbf{0}, \boldsymbol{I})<br>$$</p><p>$\boldsymbol{x}_{t-1}$ 中有$\boldsymbol{x}_t$, 将$\boldsymbol{x}_t$ 代入$\boldsymbol{x}_{t-1}$, 得:</p><p>$$<br>\begin{aligned}<br>\boldsymbol{x}_{t-1} &amp;= k\boldsymbol{x}_0 + m\boldsymbol{x}_t + \sigma\epsilon \\<br>&amp;=k\boldsymbol{x}_0 + m(\sqrt{\alpha_t} \boldsymbol{x}_0+\sqrt{1-\alpha_t} \epsilon^\prime) + \sigma\epsilon \\<br>&amp;=(k+m\sqrt{\alpha_{t}}) \boldsymbol{x}_0 + m\sqrt{1-\alpha_t}\epsilon^\prime + \sigma\epsilon<br>\end{aligned}<br>$$</p><p>此处的$\epsilon, \epsilon^\prime$ 互相独立, 来自于两个不同的分布, 两个正态分布的和的方差为二者方差之和.</p><p>由于$\boldsymbol{x}_{t-1}=\sqrt{\alpha_{t-1}} \boldsymbol{x}_0+\sqrt{1-\alpha_{t-1}} \epsilon$, 所以由待定系数法有:</p><p>$$<br>\begin{aligned}<br>k+m\sqrt{\alpha}&amp;=\sqrt{\alpha_{t-1}} \\<br>m^2(1-\alpha_t) + \sigma^2 &amp;= 1-\alpha_{t-1}<br>\end{aligned}<br>$$</p><p>可求得:</p><p>$$<br>\begin{aligned}<br>m &amp;= \sqrt{\frac{1-\alpha_{t-1}-\sigma^2}{1-\alpha_t}} \\<br>k &amp;= \sqrt{\alpha_{t-1}} - \sqrt{\alpha_t} \cdot \sqrt{\frac{1-\alpha_{t-1}-\sigma^2}{1-\alpha_t}}<br>\end{aligned}<br>$$</p><p>这样将$m, k$ 代回到$\boldsymbol{x}_{t-1}$ 中, 就得到了$q_\sigma\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0\right)$, 得证.</p><h3 id="Generative-Process"><a href="#Generative-Process" class="headerlink" title="Generative Process"></a>Generative Process</h3><p>接下来定义Generative Process $p_\theta(\boldsymbol{x}_{0:T})$, 这个过程中的每一步$p_\theta^{(t)}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t\right)$ 都需要使用刚才定义的$q_\sigma\left(\boldsymbol{x}_{t-1}\mid\boldsymbol{x}_t, \boldsymbol{x}_0\right)$. 从直觉上来说, 对于一个Noisy Observation $\boldsymbol{x}_t$, 首先需要对$t$ 时刻的$\boldsymbol{x}_0$ 预测, 然后使用$q_\sigma\left(\boldsymbol{x}_{t-1}\mid \boldsymbol{x}_t, \boldsymbol{x}_0\right)$ 来得到$\boldsymbol{x}_{t-1}$.</p><p>DDPM Forward Process中的条件为$\boldsymbol{x}_0 \sim q(\boldsymbol{x}_0)$, $\epsilon \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{I})$, $\boldsymbol{x}_t=\sqrt{\alpha_t} \boldsymbol{x}_0 + \sqrt{1- \alpha_t} \epsilon$. 在Generative Process中, 想要使用$q_\sigma\left(\boldsymbol{x}_{t-1}\mid \boldsymbol{x}_t, \boldsymbol{x}_0\right)$, 肯定不能依赖于推理时的最终目标$\boldsymbol{x}_0$ 对吧, 进而需要在已知$\boldsymbol{x}_t$ 的情况下用$f_\theta^{(t)}$ 将$\boldsymbol{x}_0$ 估计出来.</p><p>将 $\boldsymbol{x}_t=\sqrt{\alpha_t} \boldsymbol{x}_0 + \sqrt{1- \alpha_t} \epsilon$ 变形, 得到$\boldsymbol{x}_0 = \left(\boldsymbol{x}_t - \sqrt{1 - \alpha_{t}} \cdot \epsilon \right) / \sqrt{\alpha_t}$, 其中的$\epsilon$ 可以由模型$\epsilon^{(t)}_\theta(\boldsymbol{x}_t)$ 预测得到. 因此在$\boldsymbol{x}_t$ 处对$\boldsymbol{x}_0$ 的估计 $f_\theta^{(t)}\left(\boldsymbol{x}_t\right)$ 可以写成:</p><p>$$<br>\begin{equation}<br>f_\theta^{(t)}\left(\boldsymbol{x}_t\right):=\left(\boldsymbol{x}_t-\sqrt{1-\alpha_t} \cdot \epsilon_\theta^{(t)}\left(\boldsymbol{x}_t\right)\right) / \sqrt{\alpha_t}<br>\end{equation}<br>$$</p><p>接着就可以定义Generative Process $p_\theta(\boldsymbol{x}_{0:T})$, 同时将$q_\sigma\left(\boldsymbol{x}_{t-1}\mid \boldsymbol{x}_t, \boldsymbol{x}_0\right)$ 中的 $\boldsymbol{x}_0$ 替换为$f_\theta^{(t)}\left(\boldsymbol{x}_t\right)$:</p><p>$$<br>\begin{aligned}<br>p_\theta^{(t)}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t\right)&amp;= \begin{cases}\mathcal{N}\left(f_\theta^{(1)}\left(\boldsymbol{x}_1\right), \sigma_1^2 \boldsymbol{I}\right) &amp; \text { if } t=1 \\\ \\<br>q_\sigma\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, f_\theta^{(t)}\left(\boldsymbol{x}_t\right)\right) &amp; \text { otherwise }\end{cases} \\\ \\<br>p_\theta(\boldsymbol{x}_T)&amp;=\mathcal{N}(\boldsymbol{0}, \boldsymbol{I}) \\\<br>\end{aligned}<br>$$</p><h2 id="Sampling-from-Generalized-Generative-Processes"><a href="#Sampling-from-Generalized-Generative-Processes" class="headerlink" title="Sampling from Generalized Generative Processes"></a>Sampling from Generalized Generative Processes</h2><p>前面说过, DDPM中的训练目标$L_1$ 只取决于$q(\boldsymbol{x}_{t}\mid\boldsymbol{x}_0)$. 由于上面定义的$q_\sigma\left(\boldsymbol{x}_{t-1}\mid\boldsymbol{x}_t, \boldsymbol{x}_0\right)$ 满足$q_\sigma\left(\boldsymbol{x}_t\mid\boldsymbol{x}_0\right)=\mathcal{N}\left(\sqrt{\alpha_t} \boldsymbol{x}_0, (1 - \alpha_t\right)\boldsymbol{I})$ 的假设, 所以DDPM和DDIM的$q_\sigma(\boldsymbol{x}_{t}\mid\boldsymbol{x}_0)$ 是相同的, 因此DDPM和DDIM的目标函数就是<strong>等价</strong>的(偷个懒这里不证了, 感兴趣的去Reference里找资料看).</p><p>所以, 实际上在DDPM用$L_1$ 作为目标函数的同时, <strong>不光训练了基于马尔科夫链的Forward Process对应的Generative Process</strong>, <strong>还顺手训练了我们引入$\sigma$ 参数化的非马尔科夫的Forward Process对应的Generative Process</strong>. 如此以来就不用重新训DDPM, 这么一看DDPM真是个大善人.</p><h3 id="Denoising-Diffusion-Implicit-Models-1"><a href="#Denoising-Diffusion-Implicit-Models-1" class="headerlink" title="Denoising Diffusion Implicit Models"></a>Denoising Diffusion Implicit Models</h3><p>写出Generative Process $p_\theta(\boldsymbol{x}_{1:T})$ 中基于非马尔科夫推导的$q_\sigma\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, f_\theta^{(t)}\left(\boldsymbol{x}_t\right)\right)$, 得到通过$\boldsymbol{x}_t$ 得到$\boldsymbol{x}_{t-1}$ 的最终表达式:</p><p>$$<br>\boldsymbol{x}_{t-1}=\sqrt{\alpha_{t-1}} \underbrace{\left(\frac{\boldsymbol{x}_t-\sqrt{1-\alpha_t} \epsilon_\theta^{(t)}\left(\boldsymbol{x}_t\right)}{\sqrt{\alpha_t}}\right)}_{\text {“predicted } \boldsymbol{x}_0 \text { “ }}+\underbrace{\sqrt{1-\alpha_{t-1}-\sigma_t^2} \cdot \epsilon_\theta^{(t)}\left(\boldsymbol{x}_t\right)}_{\text {“direction pointing to } \boldsymbol{x}_t \text { “ }}+\underbrace{\sigma_t \epsilon_t}_{\text {random noise }}<br>$$</p><p>其中定义$\alpha_0 := 1$, $\epsilon_t \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{I})$ 为与$\boldsymbol{x}_t$ 无关的标准正态分布噪声. 不同的$\sigma$ 可以决定不同的Generative Process. $\epsilon_\theta$ 是由DDPM训练时候得到的, 所以在推理时采样可以不用对模型重新训练, 调整$\sigma$ 即可.</p><p>这个式子由三项组成, 逐一观察:</p><ul><li>前面一项: 就是$f_\theta^{(t)}\left(\boldsymbol{x}_t\right)$ 前面乘了个$\sqrt{\alpha_{t-1}}$, 整体作用是对$\boldsymbol{x}_0$ 的一个估计值(Predicted $\boldsymbol{x}_0$). 明显的, 在Generative Process的起始阶段($t$ 较大时), 由于$\boldsymbol{x}_{t}$ 相距$\boldsymbol{x}_0$ 还比较远, $\boldsymbol{x}_t$ 已经被大量噪声污染, 在$t$ 时刻预测的$\boldsymbol{x}_0$ 很有可能不太准, 所以这个过程需要多次迭代来实现对$\boldsymbol{x}_0$ 相对准确的估计.</li><li>中间一项: $\epsilon_\theta^{(t)}$ 是在$t$ 时刻模型预测出的噪声. 对于$\boldsymbol{x}_{t-1}$ 来说, $\boldsymbol{x}_t$ 是对$\boldsymbol{x}_{t-1}$ 添加噪声得到的, 所以这一项指示了$\boldsymbol{x}_t$ 的方向(Direction Pointing to $\boldsymbol{x}_t$). 上面提到过, 对$\hat{\boldsymbol{x}}_{0}$ 的估计并不够准确, 因此这一项可以看成是在$\hat{\boldsymbol{x}}_{0}$ 的基础上引入对$\boldsymbol{x}_{t}$ 方向的修正, 使得Generative Process满足对Forward Process中$\boldsymbol{x}_{t-1} \rightarrow \boldsymbol{x}_t$ 条件的保证, 防止得到的$\boldsymbol{x}_{t-1}$ 偏离$\boldsymbol{x}_{t-1} \rightarrow \boldsymbol{x}_t$ 的路径. 其中的$\sqrt{1 - \alpha_{t-1} -\sigma_t^2}$ 是对$\epsilon_\theta^{(t)}$ 的修正系数. $\epsilon_\theta^{(t)}$ 与$\sigma_t$ 负相关.</li><li>最后一项: 为了保证$\boldsymbol{x}_{t-1}$ 为概率分布而做的重采样, 即在$t$ 时刻添加的随机噪声$\sigma_t \epsilon_t$. 它是与$\sigma_t$ 正相关的, 总的噪声$\epsilon_\theta^{(t)} + \epsilon_t$ 恰好可以在后两项得到平衡.</li></ul><p>当然, $\sigma_t$ 是可以任意选取的, 有两种特殊情况:</p><ul><li>当有$\sigma_t = \sqrt{(1-\alpha_{t-1}) / (1-\alpha_t)} \sqrt{1-\alpha_t / \alpha_{t-1}}$ 时, Forward Process是马尔科夫链, Generative Process恰好是<strong>DDPM</strong>.</li><li>当有$\sigma_t=0$ 时, 在给定$\boldsymbol{x}_{t-1}, \boldsymbol{x}_{0}$ 情况下, 除去$t=1$ 外, Forward Process会变成一个<strong>完全确定性</strong>的过程, 随机噪声$\epsilon_t$ 也就为0了. 第二项的修正系数恰好为$\sqrt{1-\alpha_{t-1}}$, 有$\boldsymbol{x}_{t-1} = \sqrt{\alpha_{t-1}}f_\theta^{(t)}\left(\boldsymbol{x}_t\right) + \sqrt{1-\alpha_{t-1}}\epsilon^{(t)}_\theta(\boldsymbol{x}_t)$.</li></ul><p>虽然$\sigma_t$ 可以任意取, 但对于DDIM, 一般指$\sigma_t=0$ 的情况, 生成的结果不再具有随机性. 所以说它是一种具有DDPM训练目标的<strong>隐式概率模型</strong>, 因而被称为<strong>DDIM</strong>(<strong>D</strong>enoising <strong>D</strong>iffusion <strong>I</strong>mplicit <strong>M</strong>odel).</p><h3 id="Accelerated-Generation-Processes"><a href="#Accelerated-Generation-Processes" class="headerlink" title="Accelerated Generation Processes"></a>Accelerated Generation Processes</h3><p>正上文所述, DDPM的Training Object $L_1$ 与Forward Process的形式无关, 当$q_\sigma\left(\boldsymbol{x}_t\mid\boldsymbol{x}_0\right)$ 固定时就是确定的. 所以只要满足$q_\sigma\left(\boldsymbol{x}_t\mid\boldsymbol{x}_0\right)=\mathcal{N}\left(\sqrt{\alpha_t} \boldsymbol{x}_0, (1 - \alpha_t\right)\boldsymbol{I})$ 即可, 怎么取Forward Process都没问题.</p><p>分析一下, DDIM的两个优势:</p><ol><li>有了定义在非马尔科夫链上的$q_\sigma\left(\boldsymbol{x}_{t-1}\mid\boldsymbol{x}_t, \boldsymbol{x}_0\right)$, 这就使得$\boldsymbol{x}_{t-1}$ 可以依赖对$\boldsymbol{x}_0$ 的估计, 而不单取决于$\boldsymbol{x}_{t}$. 不过注意, 在DDPM中, $t-1, t$ 都来自于连续的时间序列$t \in [1, \dots, T]$, 所以$t-1, t$ 都是连续的. 但在DDIM中由于<strong>非马尔科夫性</strong>, 这里的$t-1, t$ <strong>不再是一个连续序列中的两个时间步</strong>. 觉得别扭可以将它们换个字母, 比如在一个新的不连续子序列$s \in [1, \dots, s_{i-1}, s_{i}, \dots, T]$ 中, $s_{i-1}$ 可以对应DDPM里的$t=900$, $s_i$ 可以对应$t=800$.</li><li>$\sigma_t=0$ 的确定性采样带来了一条<strong>确定性轨迹</strong>, 消除了随机性, 这个性质使我们在已知$\boldsymbol{x}_t$ 的情况下可以估计到更远的地方, 并具有更高的准确性, 即<strong>允许我们跳过更多的步骤采样</strong>.</li></ol><p>综上, 甚至可以考虑定义长度远小于$T$ 的Forward Process来<strong>加速采样过程</strong>.</p><p>例如, 可以考虑定义在一个$\boldsymbol{x}_{1:T}$ 的子集$\set{\boldsymbol{x}_{\tau_1}, \dots, \boldsymbol{x}_{\tau_S}}$ 上的Forward Process, $\tau$ 为长度为$S$ 的<strong>递增子序</strong>列$[1, \dots, T]=[\tau_1, \dots, \tau_S]$. 该Forward Process需要满足:</p><p>$$<br>q\left(\boldsymbol{x}_{\tau_i}\mid\boldsymbol{x}_0\right)=\mathcal{N}\left(\sqrt{\alpha_{\tau_i}} \boldsymbol{x}_0, (1 - \alpha_{\tau_i}\right)\boldsymbol{I})<br>$$</p><p>相对应的, 此时的Generative Process是定义在这个子序列$\tau$ 的逆序列$\text{reversed}(\tau)$ 上的采样过程. 此时的Generative Process中$\boldsymbol{x}_{\tau_{i-1}}$ 为:</p><p>$$<br>\boldsymbol{x}_{\tau_{i-1}}=\sqrt{\alpha_{\tau_{i-1}}}\left(\frac{\boldsymbol{x}_{\tau_i}-\sqrt{1-\alpha_{\tau_i}} \epsilon_\theta^{\left(\tau_i\right)}\left(\boldsymbol{x}_{\tau_i}\right)}{\sqrt{\alpha_{\tau_i}}}\right)+\sqrt{1-\alpha_{\tau_{i-1}}-\sigma_{\tau_i}^2} \cdot \epsilon_\theta^{\left(\tau_i\right)}\left(\boldsymbol{x}_{\tau_i}\right)+\sigma_{\tau_i} \epsilon<br>$$</p><p>由于子序列长度$S$ 远远小于原序列长度$T$, 所以推理速度可以实现显著提升, 即”<strong>跳步采样</strong>“. 例如$\tau=[1, 3]$ 时有:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ddim2.png" style="zoom:67%"><h4 id="Proof-1"><a href="#Proof-1" class="headerlink" title="Proof"></a>Proof</h4><blockquote><p>这里直接抄的论文.</p></blockquote><p>由于$L_1$ 与Forward Process的形式无关, 所以在加速过程中, 可将Forward Process分解为:</p><p>$$<br>q_{\sigma, \tau}\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right)=q_{\sigma, \tau}\left(\boldsymbol{x}_{\tau_S} \mid \boldsymbol{x}_0\right) \prod_{i=1}^S q_{\sigma, \tau}\left(\boldsymbol{x}_{\tau_{i-1}} \mid \boldsymbol{x}_{\tau_i}, \boldsymbol{x}_0\right) \prod_{t \in \bar{\tau}} q_{\sigma, \tau}\left(\boldsymbol{x}_t \mid \boldsymbol{x}_0\right)<br>$$</p><p>其中$\tau$ 为长度为$S$ 的子序列$[1, \dots, T]$, 且$\tau_S =T$, 同时令$\bar{\tau} := \set{1, \dots, T} \backslash \tau$ 为$\tau$ 的补集. 第一项与第二项来自子序列$\tau$, 第三项来自于补集$\bar{\tau}$.</p><p>接着, 仿照在推导非马尔科夫的Forward Process, 给出定义:</p><p>$$<br>\begin{gathered}<br>q_{\sigma, \tau}\left(\boldsymbol{x}_t \mid \boldsymbol{x}_0\right)=\mathcal{N}\left(\sqrt{\alpha_t} \boldsymbol{x}_0,\left(1-\alpha_t\right) \boldsymbol{I}\right) \quad \forall t \in \bar{\tau} \cup\{T\} \\<br>q_{\sigma, \tau}\left(\boldsymbol{x}_{\tau_{i-1}} \mid \boldsymbol{x}_{\tau_i}, \boldsymbol{x}_0\right)=\mathcal{N}\left(\sqrt{\alpha_{\tau_{i-1}}} \boldsymbol{x}_0+\sqrt{1-\alpha_{\tau_{i-1}}-\sigma_{\tau_i}^2} \cdot \frac{\boldsymbol{x}_{\tau_i}-\sqrt{\alpha_{\tau_i}} \boldsymbol{x}_0}{\sqrt{1-\alpha_{\tau_i}}}, \sigma_{\tau_i}^2 \boldsymbol{I}\right) \quad\forall i \in[S]<br>\end{gathered}<br>$$</p><p>使得子序列$\tau$ 对应的Forward Process满足条件:</p><p>$$<br>q\left(\boldsymbol{x}_{\tau_i}\mid\boldsymbol{x}_0\right)=\mathcal{N}\left(\sqrt{\alpha_{\tau_i}} \boldsymbol{x}_0, (1 - \alpha_{\tau_i}\right)\boldsymbol{I}) \quad\forall i \in[S]<br>$$</p><p>则对应的Generative Process定义为:</p><p>$$<br>p_\theta\left(\boldsymbol{x}_{0: T}\right):=\underbrace{p_\theta\left(\boldsymbol{x}_T\right) \prod_{i=1}^S p_\theta^{\left(\tau_i\right)}\left(\boldsymbol{x}_{\tau_{i-1}} \mid \boldsymbol{x}_{\tau_i}\right)}_{\text {use to produce samples }} \times \underbrace{\prod_{t \in \bar{\tau}} p_\theta^{(t)}\left(\boldsymbol{x}_0 \mid \boldsymbol{x}_t\right)}_{\text {in variational objective }}<br>$$</p><p>发现仅部分模型用于生成样本. 条件分布为:</p><p>$$<br>\begin{gathered}<br>p_\theta^{\left(\tau_i\right)}\left(\boldsymbol{x}_{\tau_{i-1}} \mid \boldsymbol{x}_{\tau_i}\right)=q_{\sigma, \tau}\left(\boldsymbol{x}_{\tau_{i-1}} \mid \boldsymbol{x}_{\tau_i}, f_\theta^{\left(\tau_i\right)}\left(\boldsymbol{x}_{\tau_{i-1}}\right)\right) \quad \text { if } i \in[S], i&gt;1 \\<br>p_\theta^{(t)}\left(\boldsymbol{x}_0 \mid \boldsymbol{x}_t\right)=\mathcal{N}\left(f_\theta^{(t)}\left(\boldsymbol{x}_t\right), \sigma_t^2 \boldsymbol{I}\right) \quad \text { otherwise }<br>\end{gathered}<br>$$</p><p>因此采用子序列$\tau$ 进行Generative Process来得到$\boldsymbol{x}_0$ 是可行的.</p><h3 id="Relevance-to-Neural-ODEs"><a href="#Relevance-to-Neural-ODEs" class="headerlink" title="Relevance to Neural ODEs"></a>Relevance to Neural ODEs</h3><p>在$\sigma_t=0$ 时, DDIM的生成是确定性的, 可以将DDIM重写为一个Neural ODE:</p><p>$$<br>\begin{equation}<br>\frac{\boldsymbol{x}_{t-\Delta t}}{\sqrt{\alpha_{t-\Delta t}}}=\frac{\boldsymbol{x}_t}{\sqrt{\alpha_t}}+\left(\sqrt{\frac{1-\alpha_{t-\Delta t}}{\alpha_{t-\Delta t}}}-\sqrt{\frac{1-\alpha_t}{\alpha_t}}\right) \epsilon_\theta^{(t)}\left(\boldsymbol{x}_t\right)<br>\end{equation}<br>$$</p><blockquote><p>这里拿$t-1$ 推广到$t - \Delta t$, $\alpha_t$ 推广到$\alpha_{t - \Delta t}$, 且令$\sigma=0$, 带进去推一下就能得到了, 过程略.</p></blockquote><p>记$\sigma = \frac{\sqrt{1-\alpha}}{\sqrt{\alpha}}, \bar{\boldsymbol{x}} = \frac{\boldsymbol{x}}{\sqrt{\alpha}}$. 在连续条件下, $\sigma, \boldsymbol{x}$ 都为$t$ 的函数, 上式重写为:</p><p>$$<br>\bar{\boldsymbol{x}}(t-\Delta t)=\bar{\boldsymbol{x}}(t)+(\sigma(t-\Delta t)-\sigma(t)) \cdot \epsilon_\theta^{(t)}(\boldsymbol{x}(t))<br>$$</p><p>构造差分形式:</p><p>$$<br>\bar{\boldsymbol{x}}(t) - \bar{\boldsymbol{x}}(t-\Delta t)=(\sigma(t) - \sigma(t-\Delta t) \cdot \epsilon_\theta^{(t)}(\boldsymbol{x}(t))<br>$$</p><p>当$\Delta t \rightarrow 0$ 时, 有差分转换为微分:</p><p>$$<br>\mathrm{d} \bar{\boldsymbol{x}}(t)=\epsilon_\theta^{(t)}(\boldsymbol{x}_t)\mathrm{d} \sigma(t)<br>$$</p><p>将$\boldsymbol{x}_{t}$ 用$\bar{\boldsymbol{x}}(t), \sigma(t)$ 表示, 有:</p><p>$$<br>\begin{gathered}<br>\boldsymbol{x}_t = \sqrt{\alpha} \cdot \bar{\boldsymbol{x}}(t), \quad \sqrt{\alpha} = \frac{1}{\sqrt{\sigma^2(t) + 1}} \\\<br>\Downarrow \\<br>\boldsymbol{x}_{t} = \frac{\bar{\boldsymbol{x}}(t)}{\sqrt{\sigma^2(t)+1}}<br>\end{gathered}<br>$$</p><p>代入微分方程得:</p><p>$$<br>\mathrm{d} \bar{\boldsymbol{x}}(t)=\epsilon_\theta^{(t)}\left(\frac{\bar{\boldsymbol{x}}(t)}{\sqrt{\sigma^2+1}}\right) \mathrm{d} \sigma(t)<br>$$</p><p>因此, DDIM的迭代去噪过程可以被写成是一个<strong>Neural ODE</strong>. 它对应着一个<strong>向量场</strong>, 这意味着在这个场中任意粒子的运动轨迹都是<strong>确定</strong>与<strong>可逆</strong>的.</p><p>初始条件为$\boldsymbol{x}(T) \sim \mathcal{N}(0, \sigma(T))$, 其中$\sigma(T)$ 是一个非常大的值, 这对应着$\alpha \approx 0$ 的情况. 这意味着只要离散步足够多, 离散近似连续, 在这个Neural ODE中是可以按沿着Generative Process的逆向轨迹将$\boldsymbol{x}_0$ 转换为$\boldsymbol{x}_T$ 的, 在某些需要Latent Variable的下游任务中这一性质可能是有用的.</p><blockquote><p>不过注意哈, DDIM的跳步步长肯定是不能选的太大的. 一方面是步长太大对$\boldsymbol{x}_{0}$的估计会不准, 多步错误累加会特别严重, 另外一方面是, 如果<strong>步长选的太大</strong>, <strong>DDIM就不能看成ODE了</strong>, 从而导致不再可逆.</p></blockquote><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的模型参数设置和实验设置请参考原论文.</p><p><strong>在实验部分都没有对DDPM的模型重新训练</strong>, 直接将训练好的DDPM复用, 原DDPM模型设置$T=1000$.</p><p>作者将DDIM中的标准差$\sigma$ 设定为与DDPM兼容的形式, 对DDPM前面乘上一个可控系数$\eta \in \mathbb{R}_{\geq 0}$:</p><p>$$<br>\begin{equation}<br>\sigma_{\tau_i}(\eta)=\eta \sqrt{\frac{1-\alpha_{\tau_{i-1}}}{1-\alpha_{\tau_i}}}\sqrt{1-\frac{\alpha_{\tau_i}}{\alpha_{\tau_{i-1}}}}<br>\end{equation}<br>$$</p><p>当$\eta=0$ 时为代表<strong>确定性</strong>的DDIM, $\eta=1$ 时为代表<strong>随机性</strong>的DDPM.</p><p>作者也考虑了随机噪声标准差大于$\sigma(1)$ 的情况, 用$\hat{\sigma}: \hat{\sigma}_{\tau_i} = \sqrt{1-\alpha_{\tau_i} / \alpha_{\tau_{i-1}}}$ 来表示.</p><p>考虑到不同的子序列$\tau$ 的取法可能会对结果产生影响. 对于希望的推理步长$\text{dim}(\tau) &lt; T$, 作者也采用了两种不同的$\tau$ 的采样方式:</p><ul><li><strong>Linear</strong>: 按照$\tau_i = \lfloor ci \rfloor$选择时间步, 在CIFAR10上采用.</li><li><strong>Quadratic</strong>: 按照$\tau_i = \lfloor ci^2 \rfloor$选择时间步, 在其余数据集上采用.</li></ul><p>$c$ 为常数, 能够使得$\tau_{-1}$ 与$T$ 接近.</p><h3 id="Sample-Quality-and-Efficiency"><a href="#Sample-Quality-and-Efficiency" class="headerlink" title="Sample Quality and Efficiency"></a>Sample Quality and Efficiency</h3><p>DDIM($\eta=0$)和DDPM($\eta=1$)在CIFAR10(Linear)和CelebA(Quadratic)上的FID如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ddim3.png" style="zoom:45%"><p>图中的$\text{dim}(\tau)$ 代采样步数$S$.</p><p>从表结果知, 对比DDIM和DDPM, DDIM在Step较小的时候($S \leq 100$)就可以取得比DDPM要好的多的效果. 不管是对Linear还是Quadratic, 结论基本不会变, 这基本上说明DDPM训练的时候每个Timestep Denoising也是因均匀采样而充分训练的, 进而再次证明了从中采样的子序列$\tau$ 是有效的.</p><p>但当时间步足够大($T=1000$)时, 反而较大噪声标准差的DDPM($\hat{\sigma}$)能取得更好的效果.</p><p>从图结果知, 当采样步数相对少的时候DDIM具有很高的样本质量. DDPM($\eta=1, \hat{\sigma}$)在$\text{dim}(\tau)=10$ 时受到噪声的影响要大得多, 样本质量很低.</p><p>DDIM的采样步数少, 自然而然采样成本也就压下来了:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ddim4.png" style="zoom:50%"><p>在100Steps的时候效果就已经很不错了, 采样那么多步也就没什么必要.</p><h3 id="Sample-Consistency-in-DDIMs"><a href="#Sample-Consistency-in-DDIMs" class="headerlink" title="Sample Consistency in DDIMs"></a>Sample Consistency in DDIMs</h3><p>理论上的DDIM的生成过程是确定性的, 因为$\boldsymbol{x}_0$ 仅依赖于$\boldsymbol{x}_T$. 对于DDIM来说, 不同的$\text{dim}(\tau)$ 的选择意味着不同的生成轨迹. DDIM在相同的$\boldsymbol{x}_T$ 下, 不同轨迹$\tau$ 生成的图像几乎是一致的:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ddim5.png" style="zoom:50%"><p>而且20步的结果基本上和1000步的相差不太多, 推理步长更大只会带来更多细节上的增益. 因此作者认为$\boldsymbol{x}_T$ 实际上已经可以作为一个信息丰富的Latent Encoding, 更多的细节被内化在模型参数$\theta$ 中.</p><h3 id="Interpolation-in-Deterministic-Generative-Processes"><a href="#Interpolation-in-Deterministic-Generative-Processes" class="headerlink" title="Interpolation in Deterministic Generative Processes"></a>Interpolation in Deterministic Generative Processes</h3><p>如果$\sigma_t=0$ 的DDIM是一个能将某个随机噪声向量确定性的转化为某个样本的模型, 它实际上就与GAN几乎一样了. 所以结合GAN等模型的经验, 作者好奇是否对其进行插值会有类似GAN中出现的语义插值效果.</p><p>具体的, 将两个随机噪声$\boldsymbol{x}_{T}^{(0)}, \boldsymbol{x}_{T}^{(1)}$ 进行<strong>球面线性插值</strong>:</p><p>$$<br>\boldsymbol{x}_T^{(\alpha)} = \frac{\sin{\left((1-\alpha)\theta\right)}}{\sin{(\theta)}} \boldsymbol{x}_{T}^{(0)} + \frac{\sin{(\alpha\theta)}}{\sin{(\theta)}} \boldsymbol{x}_{T}^{(1)}<br>$$</p><p>其中$\theta = \arccos{(\frac{(\boldsymbol{x}_T^{(0)})^\top\boldsymbol{x}_T^{(1)}}{||\boldsymbol{x}_T^{(0)} || \cdot || \boldsymbol{x}_T^{(1)} ||})}$ .</p><p>然后就可以将得到的$\boldsymbol{x}_T^{(\alpha)}$ 用DDIM得到$\boldsymbol{x}_0$. 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ddim6.png" style="zoom:45%"><p>由于去除了DDPM中的随机生成过程, DDIM中可以在Latent中做语义插值. 而DDPM却不能做到这一点.</p><h3 id="Reconstruction-from-Latent-Space"><a href="#Reconstruction-from-Latent-Space" class="headerlink" title="Reconstruction from Latent Space"></a>Reconstruction from Latent Space</h3><p>前面说过, 当离散采样步数足够大的时候 ,DDIM可以被写成是一个Neural ODE, 轨迹是可逆的. 这就意味着可以用一个$\boldsymbol{x}_0$ 确定的得到一个$\boldsymbol{x}_{T}$. 作者在CIFAR10上进行了给定$\boldsymbol{x}_0$ 对$\boldsymbol{x}_T$ 的重建实验:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ddim7.png" style="zoom:50%"><p>在步数足够多的时候, 重建误差是比较小的, 也就证明了此时轨迹是相对可逆的.</p><h2 id="Recommended-Reference"><a href="#Recommended-Reference" class="headerlink" title="Recommended / Reference"></a>Recommended / Reference</h2><ul><li>比较推荐去看这个视频, 包含推导, 也比较易懂:<ul><li><a href="https://www.bilibili.com/video/BV1v2pUeLE2v/" target="_blank" rel="noopener">【AI知识分享】保姆级扩散模型DDIM基本原理深度解析_哔哩哔哩_bilibili</a>.</li></ul></li><li>一些优秀的博客:<ul><li><a href="https://spaces.ac.cn/archives/9181" target="_blank" rel="noopener">生成扩散模型漫谈（四）：DDIM = 高观点DDPM - 科学空间|Scientific Spaces</a>.</li><li><a href="https://www.zhangzhenhu.com/aigc/ddim.html" target="_blank" rel="noopener">3. 去噪扩散隐式模型（Denoising Diffusion Implicit Models,DDIM） — 张振虎的博客 张振虎 文档</a>.</li></ul></li><li>知乎上的一些文章:<ul><li><a href="https://zhuanlan.zhihu.com/p/580106763" target="_blank" rel="noopener">Diffusion Model （扩散模型）解读系列二：(DDIM) denoising diffusion implicit models - 曾天真的文章 - 知乎</a>.</li><li><a href="https://zhuanlan.zhihu.com/p/639540034" target="_blank" rel="noopener">diffusion model（二）—— DDIM（将diffusion model 提速50倍！） - 莫叶何竹的文章 - 知乎</a>.</li><li><a href="https://zhuanlan.zhihu.com/p/666552214" target="_blank" rel="noopener">一文带你看懂DDPM和DDIM（含原理简易推导，pytorch代码） - Deja vu的文章 - 知乎</a>.</li></ul></li><li>Huggingface diffusers库的代码实现, 注释写的很全:<ul><li><a href="https://github.com/huggingface/diffusers/blob/ae14612673dd2e71ab55003c9b19c5498a8a21af/src/diffusers/schedulers/scheduling_ddim.py#L131" target="_blank" rel="noopener">diffusers/src/diffusers/schedulers/scheduling_ddim.py at ae14612673dd2e71ab55003c9b19c5498a8a21af · huggingface/diffusers · GitHub</a>.</li></ul></li></ul><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>DDIM构建了<strong>非马尔科夫</strong>的Forward Process与配套的Reverse Process / Generative Process, 并且其优化目标与DDPM完全一致(我在文中偷了个懒没有证), 因此可以<strong>直接复用训好的DDPM权重</strong>.</p><p>DDIM可以做到一些DDPM做不到的事情, 比如<strong>加速推理过程</strong>和<strong>确定性采样</strong>等. 如果只靠DDPM的资源消耗上确实很难绷. 如果DDPM $T=1000$, 那么DDIM就可以在维持图像生成质量的条件下只用20步生成, 达到50倍的加速效果. 这就更别说在建模更复杂任务时的效率了(比如出高清大图等).</p><p>现在的DDIM已经作为一项<strong>常见</strong>且<strong>实惠</strong>的技术内置在各类Diffusion Model中, 算是给DDPM续了一把火.</p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/40650.html">https://ADAning.github.io/posts/40650.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/Diffusion/"><span class="chip bg-color">Diffusion</span> </a><a href="/tags/DDIM/"><span class="chip bg-color">DDIM</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="far fa-dot-circle"></i>&nbsp;本篇</div><div class="card"><a href="/posts/40650.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/6.jpg" class="responsive-img" alt="DDIM: Denoising Diffusion Implicit Models"> <span class="card-title">DDIM: Denoising Diffusion Implicit Models</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: DDPM: DDPM: Denoising Diffusion Probabilistic Model. Denoising Diffusion Implicit Models 论文: Denoising Diffu</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2025-03-21 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/Diffusion/"><span class="chip bg-color">Diffusion</span> </a><a href="/tags/DDIM/"><span class="chip bg-color">DDIM</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/50765.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/3.jpg" class="responsive-img" alt="RoPE / RoFormer: Enhanced Transformer with Rotary Position Embedding"> <span class="card-title">RoPE / RoFormer: Enhanced Transformer with Rotary Position Embedding</span></div></a><div class="card-content article-content"><div class="summary block-with-text">RoPE / RoFormer: Enhanced Transformer with Rotary Position Embedding本文是论文 RoFormer: Enhanced Transformer with Rotary Pos</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2025-02-12 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/RoPE/"><span class="chip bg-color">RoPE</span> </a><a href="/tags/LLM/"><span class="chip bg-color">LLM</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">413.5k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:andaning@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>