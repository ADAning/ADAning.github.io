<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and Multi-Level Style Control, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and Multi-Level Style Control | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/8.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and Multi-Level Style Control</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/Audio/"><span class="chip bg-color">Audio</span> </a><a href="/tags/SVS/"><span class="chip bg-color">SVS</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2025-07-01</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2025-07-01</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 2.2k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 9 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><h1 id="TCSinger-Zero-Shot-Singing-Voice-Synthesis-with-Style-Transfer-and-Multi-Level-Style-Control"><a href="#TCSinger-Zero-Shot-Singing-Voice-Synthesis-with-Style-Transfer-and-Multi-Level-Style-Control" class="headerlink" title="TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and Multi-Level Style Control"></a>TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and Multi-Level Style Control</h1><ul><li>论文: <a href="https://aclanthology.org/2024.emnlp-main.117/" target="_blank" rel="noopener">TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and Multi-Level Style Control</a>, EMNLP 2024, Zhou Zhao组.</li><li>代码: <a href="https://github.com/AaronZ345/TCSinger" target="_blank" rel="noopener">GitHub - AaronZ345/TCSinger: PyTorch Implementation of TCSinger(EMNLP 2024): Zero-Shot Singing Voice Synthesis with Style Transfer and Multi-Level Style Control</a>.</li></ul><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>传统歌声合成(<strong>S</strong>inging <strong>V</strong>oice <strong>S</strong>ynthesis)依赖可见歌手的训练数据, 难以处理<strong>Zero-shot</strong>(即<strong>完全未见过的歌手</strong>). 现有方法在<strong>风格建模</strong>(如演唱方法, 情感, 节奏, 技巧, 发音)和多层次控制上存在不足, 导致生成结果缺乏风格细节.</p><p>作者提出TCSinger来解决上述问题. TCSinger是首个支持跨语言的Speech / Singing Style Transfer的Zero-Shot SVS模型, 并且具有Multi-Level Style Control的能力.</p><h2 id="TCSinger"><a href="#TCSinger" class="headerlink" title="TCSinger"></a>TCSinger</h2><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>在TCSinger中, 首先作者将<strong>Content</strong>, <strong>Style</strong>(Singing Method, Emotion, Rhythm, Technique, Pronunciation), <strong>Timbre</strong>三部分做了解耦, 用不同的结构来建模它们:</p><ul><li><strong>Content</strong>: Phoneme Encoder, Note Encoder对Music Score编码, 得到Content Embedding.</li><li><strong>Style</strong>: 采用Clustering Vector Quantization(CVQ)对Style离散化处理.</li><li><strong>Timbre</strong>: 对于同一歌手的不同Audio Prompt, 用Timbre Encoder得到Timbre Vector.</li></ul><p>对上述信息分别编码后, 一股脑输入到一个语言模型<strong>S</strong>tyle and <strong>D</strong>uration <strong>L</strong>anguage <strong>M</strong>odel(<strong>S&amp;D-LM</strong>)中, 预测出目标歌声的Style Embedding和Phoneme Duration.</p><p>之后, 采用<strong>Pitch Diffusion Predictor</strong>来预测F0, 并用<strong>Style Adaptive Decoder</strong>解码得到目标歌声的梅尔谱.</p><p>上述过程对应的Overview如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tcsinger1.png" style="zoom:50%"><blockquote><p>图中LR为Length Regulator.</p></blockquote><h3 id="Clustering-Style-Encoder"><a href="#Clustering-Style-Encoder" class="headerlink" title="Clustering Style Encoder"></a>Clustering Style Encoder</h3><p>Clustering Style Encoder整体比较简单, 主要将梅尔谱转换为离散的Style Representation, 结构如下所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tcsinger2.png" style="zoom:67%"><p>将<strong>梅尔谱</strong>作为VQ Module来获得一个离散的Style Representation.</p><blockquote><p>CVQ出自论文<a href="https://arxiv.org/abs/2307.15139" target="_blank" rel="noopener">Online Clustered Codebook</a>, 其主要思想是选取<strong>Anchor</strong>控制利用率不高的Codebook Entry进行<strong>在线</strong>更新, 从而提高Codebook Entry的利用率. 在本文中只起到VQ的作用, 只是更加稳定与紧凑, 因此不多叙述.</p></blockquote><h3 id="Style-Adaptive-Decoder"><a href="#Style-Adaptive-Decoder" class="headerlink" title="Style Adaptive Decoder"></a>Style Adaptive Decoder</h3><p>使用VQ来获取风格信息是<strong>有损</strong>的, 而且相似的风格很有可能被编码进相同的Codebook Indices, 引入了一种新的Mel-Style Adaptive Normalization, 这样即使用VQ, 也能合成多样风格的歌声.</p><p>Style-Adaptive Decoder是一个<a href="https://dl.acm.org/doi/abs/10.1145/3503161.3547855" target="_blank" rel="noopener">8步的Diffusion Model</a>, FFT作为Denoiser. Decoder每层都会应用<strong>Mel-Style Adaptive Normalization</strong>, 如下图所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tcsinger3.png" style="zoom:67%"><p>设Decoder第$i$ 层的输出为$m^i$, Mel-Style Adaptive Normalization定义如下:</p><p>$$<br>m^i=\phi_\gamma(s) \frac{m^{i-1}-\mu\left(m^{i-1}\right)}{\sigma\left(m^{i-1}\right)}+\phi_\beta(s)<br>$$</p><p>其中, $\phi_\gamma(\cdot), \phi_\beta(\cdot)$ 分别为两个用于将Style Embedding $s$ 缩放平移的可学仿射变换. 即使是相似的风格, 在进行Scale &amp; Shift后, Decoder被鼓励输出更加多样化的音频.</p><p>在训练Decoder的时候同时采用MAE和SSIM作为Loss.</p><h3 id="S-amp-D-LM"><a href="#S-amp-D-LM" class="headerlink" title="S&amp;D-LM"></a>S&amp;D-LM</h3><p>这里比较有意思, 作者这里认为Singing Style经常表现出<strong>局部</strong>与<strong>长期</strong>的依赖, 并且它们随时间变化, 与Content关系不太密切. 所以这里作者用了<strong>Language Model</strong>来做建模, 即<strong>S</strong>tyle and <strong>D</strong>uration <strong>L</strong>anguage <strong>M</strong>odel(S&amp;D-LM).</p><p>S&amp;D-LM可以实现两个功能, Zero-Shot <strong>Style Transfer</strong>与Multi-Level <strong>Style Control</strong>, 它们预期的输出都是Target Style $\tilde{s}$ 与Target Phoneme Duration $\tilde{d}$, 只是输入不同.</p><h4 id="Style-Transfer"><a href="#Style-Transfer" class="headerlink" title="Style Transfer"></a>Style Transfer</h4><p>对于给定的Target Lyrics $\tilde{l}$, Target Notes $\tilde{n}$, 以及与Audio Prompt对应的Mel-Spec $m$, Lyrics $l$, Notes $n$, TCSinger的目标是要根据Audio Prompt合成含有Target Timbre $\tilde{t}$ 和Target Style $\tilde{s}$ 的歌声Mel-Spec $\tilde{m}$.</p><p>所以, 首先用不同的Encoder来提取不同的表示, Style $s$, Timbre $t$, Content $c$, Target Content $\tilde{c}$:</p><p>$$<br>\begin{aligned}<br>&amp; s=E_{\text {style }}(m), t=E_{\text {timbre }}(m), \\<br>&amp; c=E_{\text {content }}(l, n), \tilde{c}=E_{\text {content }}(\tilde{l}, \tilde{n}),<br>\end{aligned}<br>$$</p><p>其中$E$ 代表每种属性的Encoder. Target Timbre $\tilde{t}$ 与Source Timbre $t$ 几乎是一样的, 所以只剩下Target Style $\tilde{s}$ 没有解决了, 作者希望用Language Model将$\tilde{s}$ 预测出来.</p><p>S&amp;D-LM为<strong>Decoder-Only的Transformer</strong>, 则进行Style Transfer时, 对Target Style $\tilde{s}$ 和Target Phoneme Duration $\tilde{d}$ 的自回归的预测过程形式化为:</p><p>$$<br>\begin{aligned}<br>p(\tilde{s}, \tilde{d} \mid s, d, c, \tilde{t}, \tilde{c} ; \theta)=<br>\prod_{t=0}^T p\left(\tilde{s}_t, \tilde{p}_t \mid \tilde{s}_{&lt;t}, \tilde{d}_{&lt;t}, s, d, c, \tilde{t}, \tilde{c} ; \theta\right)<br>\end{aligned}<br>$$</p><p>其中$\theta$ 为S&amp;D-LM的参数.</p><p>最后, 令$P$ 为Pitch Diffusion Predictor, $D$ 为Style Adaptive Decoder, 可以得到Target F0和Target Mel-Spec $\tilde{m}$:</p><p>$$<br>\begin{aligned}<br>&amp; F 0=P(\tilde{s}, \tilde{d}, \tilde{t}, \tilde{c}) \\<br>&amp; \tilde{m}=D(\tilde{s}, \tilde{d}, \tilde{t}, \tilde{c}, F 0)<br>\end{aligned}<br>$$</p><h4 id="Style-Control"><a href="#Style-Control" class="headerlink" title="Style Control"></a>Style Control</h4><p>由于S&amp;D-LM是一个<strong>Language Model</strong>, 所以可以用Text Prompt来表示Audio Prompt中的$s$ 和$d$对应的风格.</p><p>当执行Style Control的时候, 用Text Encoder对<strong>Global Style Prompt</strong>(Singing Method, Emotion) $s$ 和<strong>Phoneme-Level Prompt</strong>(Technique for each Phoneme) $d$ 的对应的Text Prompt $tp$ 进行编码, 然后与$c$ 和$\tilde{c}$ 拼接起来作为S&amp;D-LM的输入, 因此Style Control的预测过程形式化变为:</p><p>$$<br>\begin{aligned}<br>p(\tilde{s}, \tilde{d} \mid tp, c, \tilde{t}, \tilde{c} ; \theta)=<br>\prod_{t=0}^T p\left(\tilde{s}_t, \tilde{p}_t \mid tp, c, \tilde{t}, \tilde{c} ; \theta\right)<br>\end{aligned}<br>$$</p><p>训练期间, 对Style Information用CE, Phoneme Duration用MSE.</p><blockquote><p>具体的, 对于Global Style Embedding为Emotion(Happy / Sad), Singing Method(Bel Canto / Pop), Phoneme-Level Style Embedding为Techniques(Mixed Voice, Falsetto, Breathy, Vibrato, Glissando, Pharyngeal).</p></blockquote><p>由于S&amp;D-LM需要实现两项功能, 所以在训练的时候是以概率$p$ (文中没写, 代码似乎一半一半)采样决定要进行Style Transfer还是Style Control, 在训练的时候采用Teacher-Forcing:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tcsinger4.png" style="zoom:67%"><h3 id="Training-and-Inference-Procedures"><a href="#Training-and-Inference-Procedures" class="headerlink" title="Training and Inference Procedures"></a>Training and Inference Procedures</h3><h4 id="Training-Procedures"><a href="#Training-Procedures" class="headerlink" title="Training Procedures"></a>Training Procedures</h4><p>Loss由五个部分组成:</p><ol><li><strong>CVQ Loss</strong>: 用CVQ Loss $\mathcal{L}_{CVQ}$ 优化整个Clustering Style Encoder.</li><li><strong>Pitch Reconstruction Loss</strong>: 用于训练Pitch Diffusion Predictor的两个Loss, Gaussian Diffusion Loss $\mathcal{L}_{gdiff}$, Multinomial Diffusion Loss $\mathcal{L}_{mdiff}$. 非重点不展开说, 详见附录.</li><li><strong>Mel Reconstruction Loss</strong>: 用于训练Style Adaptive Decoder的Loss, MAE $\mathcal{L}_{mae}$ 和SSIM $\mathcal{L}_{ssim}$.</li><li><strong>Duration Prediction Loss</strong>: 用于训练S&amp;D-LM预测Duration的Loss, MSE $\mathcal{L}_{dur}$, <strong>对数尺度</strong>下的Prediction和GT的MSE.</li><li><strong>Style Prediction Loss</strong>: 用于训练S&amp;D-LM预测Style的Loss, CE Loss $\mathcal{L}_{style}$.</li></ol><blockquote><p>需要满足的约束比较多, 所以数据相对来说稍大一些会好点.</p></blockquote><h4 id="Inference-with-Style-Transfer-Style-Control"><a href="#Inference-with-Style-Transfer-Style-Control" class="headerlink" title="Inference with Style Transfer / Style Control"></a>Inference with Style Transfer / Style Control</h4><p>TCSinger的Inference过程:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tcsinger5.png" style="zoom:67%"><p>和训练模式一样, 只不过AR的时候没有GT了. Style Transfer就给Style Embedding和Phoneme Duration, Style Control就给Text Embedding.</p><blockquote><p>在进行Speech-to-Singing(STS)任务的时候, 使用Speech Audio Prompt, 其余步骤不变.</p></blockquote><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><blockquote><p>详细的实验设置和超参数设置请参照原论文.</p></blockquote><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>作者共使用累计294小时的语音和歌声开源数据:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tcsinger6.png" style="zoom:50%"><blockquote><p><a href="https://arxiv.org/abs/2409.13832" target="_blank" rel="noopener">GTSinger</a>, <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/2de60892dd329683ec21877a4e7c3091-Abstract-Datasets_and_Benchmarks.html" target="_blank" rel="noopener">M4Singer</a>, <a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475437" target="_blank" rel="noopener">OpenSinger</a>, <a href="https://arxiv.org/abs/2202.13277" target="_blank" rel="noopener">PopBuTFy</a>这几个数据全是他们组的…</p></blockquote><p>在进行评估的时候随机选择40位没见过的歌手作为测试集来评估TCSinger的Zero-Shot能力.</p><h3 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h3><p>计算资源为4卡3090Ti, 用Adam在Main SVS Model上训练了300k Steps, S&amp;D-LM 100k Steps. Vocoder为预训练的<a href="https://adaning.github.io/posts/43190.html">HiFi-GAN</a>.</p><p>评估指标:</p><ul><li>Subjective Metrics: MOS(Mean Opinion Score), CMOS(Comparative Mean Opinion Score), MOS-S(Singer Similarity), MOS-C(Style Controllability).</li><li>Objective Metrics: COS(Singer Cosine Similarity), MCD(Mean Cepstral Distortion), FFE(F0 Frame Error).</li></ul><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><h4 id="Zero-Shot-Style-Transfer"><a href="#Zero-Shot-Style-Transfer" class="headerlink" title="Zero-Shot Style Transfer"></a>Zero-Shot Style Transfer</h4><p>在Zero-Shot Style Transfer中, TCSinger表现挺不错的, MOS-Q, MOS-S都比较高.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tcsinger7.png" style="zoom:50%"><p>从生成的梅尔谱中看TCSinger合成效果也是相对不错的:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tcsinger8.png" style="zoom:50%"><p>这么一看<a href="https://ojs.aaai.org/index.php/AAAI/article/view/29932" target="_blank" rel="noopener">StyleSinger</a>其实也还不错. 还能观察到SVS模型生成的结果和TTS有明显差距.</p><h4 id="Multi-Level-Style-Control"><a href="#Multi-Level-Style-Control" class="headerlink" title="Multi-Level Style Control"></a>Multi-Level Style Control</h4><p>Multi-Level Style Control中TCSinger也是效果比较好:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tcsinger9.png" style="zoom:50%"><h4 id="Cross-Lingual-Style-Transfer"><a href="#Cross-Lingual-Style-Transfer" class="headerlink" title="Cross-Lingual Style Transfer"></a>Cross-Lingual Style Transfer</h4><p>Cross-Lingual Style Transfer结果如下, 表现出较好的跨语言合成能力:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tcsinger10.png" style="zoom:50%"><h4 id="Speech-to-Singing-Style-Transfer"><a href="#Speech-to-Singing-Style-Transfer" class="headerlink" title="Speech-to-Singing Style Transfer"></a>Speech-to-Singing Style Transfer</h4><p>Speech-to-Singing Style Transfer上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tcsinger11.png" style="zoom:50%"><p>看起来TCSinger在Cross-Lingual下也有比较好的表现, 尤其是在STS这种相对较难的任务下也能优于其他方法.</p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>消融实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tcsinger12.png" style="zoom:50%"><p>SAD为Style Adaptive Decoder, DM为S&amp;D-LM里的Duration Model.</p><p>如果把CVQ换成普通的VQ Module(<code>w/o CVQ</code>), 掉点居然是三个里面最多的, 可能也是因为TCSinger里都是由离散化的Style在驱动的, 看来一个稳定紧凑的离散表示确实有利于TCSinger.</p><p><code>w/o DM</code> 为令S&amp;D-LM仅预测Style, Duration采用<a href="https://arxiv.org/abs/2006.04558" target="_blank" rel="noopener">FastSpeech 2</a>直接预测, 仍然掉点. 也算验证了Style Modeling中的Style和Duration之间的耦合性, 二者联合建模会收益更大.</p><blockquote><p>不过不知道为什么CMOS-Q掉的并不是很多? 反而在CMOS-S中下降相对明显.</p><p>此外, 附录中还有一些消融实验, 感兴趣自行查阅.</p></blockquote><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>TCSinger从<strong>Style Modeling</strong>入手, 可以通过<strong>Style Transfer</strong>和<strong>Style Control</strong>在<strong>Zero-Shot SVS</strong>上合成高质量歌声.</p><p>乍一看, 整篇文章很多东西都直接引用其他工作, 乍一看觉得是缝合怪.</p><p>但是再一想, 其实TCSinger的本质和缝不缝都没太大关系… Encoder是VQ, S&amp;D-LM也是VQ(AR Generation), 这不全是VQ吗? 还有Zero-Shot? 在5202年的今天来看, 这明摆了就是在SVS任务上为<strong>离散化生成路线(Speech-LLM)</strong>在小规模数据和模型上进行尝试啊…</p><p>TCSinger也有续作, <a href="https://arxiv.org/abs/2505.14910" target="_blank" rel="noopener">TCSinger 2</a>感觉是一个完全不同的画风, 先挖坑再说.</p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/32823.html">https://ADAning.github.io/posts/32823.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/Audio/"><span class="chip bg-color">Audio</span> </a><a href="/tags/SVS/"><span class="chip bg-color">SVS</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/46935.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/14.jpg" class="responsive-img" alt="TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis"> <span class="card-title">TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: Flow Matching: Flow Matching: Flow Matching for Generative Modeling. TCSinger: TCSinger: Zero-Shot Singing Voi</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2025-08-07 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/Audio/"><span class="chip bg-color">Audio</span> </a><a href="/tags/SVS/"><span class="chip bg-color">SVS</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/19143.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/20.jpg" class="responsive-img" alt="Flow Matching: Flow Matching for Generative Modeling"> <span class="card-title">Flow Matching: Flow Matching for Generative Modeling</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: Rectified Flow: ReFlow: Flow Straight and Fast-Learning to Generate and Transfer Data with Rectified Flow. F</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2025-05-29 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/Diffusion/"><span class="chip bg-color">Diffusion</span> </a><a href="/tags/Flow/"><span class="chip bg-color">Flow</span> </a><a href="/tags/Flow-Matching/"><span class="chip bg-color">Flow Matching</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">435.3k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:andaning@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>