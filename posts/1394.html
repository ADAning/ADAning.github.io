<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="BART和mBART, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>BART和mBART | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:.6"></i> <span>时间轴</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li><li><a href="/markdown"><i class="fab fa-markdown" style="margin-top:-20px;zoom:.6"></i> <span>Markdown</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li><li><a href="/markdown" style="margin-left:75px"><i class="fa fab fa-markdown" style="position:absolute;left:50px"></i> <span>Markdown</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/5.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">BART和mBART</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2021-04-26</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2022-03-27</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 3k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 10 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p>本文前置知识:</p><ul><li><p>Transformer: 详见<a href="https://adaning.github.io/posts/6744.html">Transformer精讲</a>.</p></li><li><p>BERT, GPT: 详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>.</p></li></ul></blockquote><h1 id="BART和mBART"><a href="#BART和mBART" class="headerlink" title="BART和mBART"></a>BART和mBART</h1><p>本文是如下论文的阅读笔记和个人理解:</p><ul><li><a href="https://arxiv.org/abs/1910.13461" target="_blank" rel="noopener">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a></li><li><a href="http://arxiv.org/abs/2001.08210" target="_blank" rel="noopener">Multilingual Denoising Pre-training for Neural Machine Translation</a></li></ul><h2 id="BART"><a href="#BART" class="headerlink" title="BART"></a>BART</h2><h3 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h3><p>Transformer浑身都是宝, 相较于直接延续Transformer的Seq2Seq架构, 更广为使用的是把<strong>Encoder</strong>和<strong>Decoder</strong>的单独堆叠, 分别对应着只使用Encoder的BERT和只使用Decoder的GPT. 如果只是单纯的使用其中的某一部分, 就会造成两个鸿沟:</p><ul><li><p><strong>BERT</strong>: 具备<strong>双向语言理解</strong>能力的却不具备做<strong>生成任务</strong>的能力.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bart1.jpg" style="zoom:50%"></li><li><p><strong>GPT</strong>: 拥有<strong>自回归</strong>特性的却不能更好的从<strong>双向理解</strong>语言.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bart2.jpg" style="zoom:50%"><p>因此, 作者希望将二者融合, 让模型保留双向语言理解能力的同时, 也能解决生成任务.</p></li></ul><h3 id="BART-Model"><a href="#BART-Model" class="headerlink" title="BART Model"></a>BART Model</h3><p><strong>BART</strong>(<strong>B</strong>idirectional and <strong>A</strong>uto - <strong>R</strong>egressive <strong>T</strong>ransformers)的结构非常的简单, 既然只使用Encoder或者只使用Decoder不能让鱼和熊掌兼得, 那就重新把它们组装回来:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bart3.jpg" style="zoom:50%"><p>左侧为BERT(Transformer Encoder), 右侧为GPT(Transformer Decoder).</p><p>所以BART采用的其实还是<strong>标准Transformer</strong>结构, 如果把Encoder和Decoder组装回来, 就又成了标准Transformer:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer.jpg" style="zoom:50%"><p>但是, BART所采用的<strong>输入数据</strong>和<strong>训练目标</strong>和Transformer<strong>完全不一样</strong>, 换句话说, 作者希望BART所做的事情和Transformer是完全不一样的, 这也是BART与Transformer的最大区别.</p><p>作者指出, BART和BERT有两点最大的 不同:</p><ol><li>除了对Encoder的堆叠外, 还使用了Decoder, 并添加了Encoder和Decoder之间的<strong>Cross Attention</strong>(其实就是结构和标准Transformer一样).</li><li>BERT在预测时加了额外的<strong>FFN</strong>, 而BART没使用FFN. 整体来说, 包含了Decoder的BART只是比同级别的BERT多了<strong>10%</strong>的参数.</li></ol><h3 id="Pre-Training-BART"><a href="#Pre-Training-BART" class="headerlink" title="Pre - Training BART"></a>Pre - Training BART</h3><p>BART使用的是类似BERT的<strong>Denoising AutoEncoder</strong>的形式来训练的, 即模型需要对被添加噪声的数据<strong>去噪</strong>, <strong>恢复</strong>出原始数据.</p><blockquote><p>我猜测, 之所以BART名字是仿照BERT, 而不是仿照Transformer最大原因, 是因为BERT和BART都是去噪自编码器, 而Transformer不是.</p></blockquote><p>BART允许对原始数据做<strong>任意形式</strong>的噪声干扰, 作者提出了五种可行的添加噪声的方式:</p><ul><li><strong>Token Masking</strong>: 与BERT打<code>[Mask]</code>的策略完全相同.</li><li><strong>Token Deletion</strong>: 直接随机删除某些Token.</li><li><strong>Text Infilling</strong>: 同时选中多个连续的Token, 仅替换成一个<code>[Mask]</code>, 或者在原始数据中随机插入Mask Token(即使没有数据缺失). 模型不知道<code>[Mask]</code>对应的是多少个Token, 也不知道<code>[Mask]</code>是否有效. 这就要求模型有强大的学习能力.</li><li><strong>Sentence Permutation</strong>: 将一个文档中的句子之间的顺序打乱.</li><li><strong>Document Rotation</strong>: 从文档中随机选定一个Token作为整个文档的起始Token, 对文档Rotation. 该方法令模型能识别文档的起始Token.</li></ul><p>上述五种方式的示例如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bart4.jpg" style="zoom:33%"><blockquote><p>即使有Decoder, 也不需要让Encoder和Decoder对应的部分对齐. 在极端情况下, 所有的原始信息都丢失了, 此时BART相当于Language Model. BART添加噪声的方式不单包含了Token Masking, 还包含了更为复杂的噪声, 它们也可以互相组合. 想从这些噪声中按照原语序恢复出句子还是非常困难的, 因为它们包含了缺失, 乱序的情况.</p></blockquote><p>在输入加噪的数据后, BART先通过Encoder双向编码, 再通过Decoder用自回归极大似然解码, 恢复出原始序列.</p><h3 id="Fine-Tuning-BART"><a href="#Fine-Tuning-BART" class="headerlink" title="Fine - Tuning BART"></a>Fine - Tuning BART</h3><p>BART在预训练后, 需要对下游任务微调. 对于不同的任务, BART有不同的使用方法:</p><ul><li><p><strong>Sequence Classification</strong>: 对序列分类任务, Encoder和Decoder以相同的数据输入, 利用Decoder的最后一次输出代表整个句子的表示, 类似于BERT中的<code>[CLS]</code>. 只不过BART是有Decoder的, 所以需要让Decoder的输出作为整个句子的表示.</p></li><li><p><strong>Token Classification</strong>: 对于Token分类任务, Encoder和Decoder也以相同的数据输入, BART也将Decoder中所对应Token的输出作为分类的隐藏状态.</p></li><li><p><strong>Sequence Generation</strong>: 对于序列生成任务, BART直接适应生成任务, 对Encoder和Decoder微调即可. 该类任务包括文本摘要, 问答等.</p></li><li><p><strong>Machine Translation</strong>: 机器翻译任务比较特殊, 因为它的任务输入和输出是两种不同的语言.</p><p>结合先前在机器翻译上的研究, 额外添加一个专门用于外语映射的Encoder(例如其他语言映射到英语)将有助于模型性能的提升. 所以BART需要训练一个新的Encoder来将源语言与目标语言语义空间对齐, 来替代掉BART原来的Word Embedding, 在完成对齐后, BART将把源语言转换为目标语言, 与Transformer保持一致.</p><p>训练分为了两个阶段:</p><ol><li>一阶段中, 对BART的大多数参数冻结, 只更新随机初始化的源语言Encoder, BART的位置编码, BART Encoder的第一层Self - Attention投影矩阵.</li><li>二阶段中, 对整个BART(包含后来添加的Encoder)中的所有参数做少次迭代.</li></ol></li></ul><p>对于上述四种任务的处理概括为下图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bart5.jpg" style="zoom:45%"><p>左侧为处理分类问题的示意图, 右侧为处理机器翻译的示意图.</p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>详细的实验设置请参照原论文.</p><h4 id="Comparsion-Pre-Training-Objectives"><a href="#Comparsion-Pre-Training-Objectives" class="headerlink" title="Comparsion Pre - Training Objectives"></a>Comparsion Pre - Training Objectives</h4><p>作者做了各不同预训练目标的模型的效果对比, 这些模型并不是原始论文中的模型, 而是作者或多或少调整过的. 其中所使用的模型分别类似于:</p><ul><li>Language Model: GPT.</li><li>Permuted Language Model: XLNet.</li><li>Masked Language Mode: BERT.</li><li>Multitask Masked Language Model: UniLM.</li><li>Masked Seq - to - Seq: MASS.</li></ul><p>实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bart6.jpg" style="zoom:50%"><p>从实验中看到, 使用Text Infilling的效果非常好, 只使用Document Rotation和Sentence Shuffling的效果比较差. 并且, 预训练的有效性高度取决于任务, 自回归式的模型有利于解决生成类任务.</p><h4 id="Discriminative-Tasks"><a href="#Discriminative-Tasks" class="headerlink" title="Discriminative Tasks"></a>Discriminative Tasks</h4><p>各类Large模型在SQuAD和GLUE上的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bart7.jpg" style="zoom:50%"><p>RoBERTa和BART的表现相似, 但是BART能够在不牺牲性能的情况下将任务扩展到生成任务上, 这对BART来说是一个独天得厚的优势, 因为扩展只带来了将近10%的参数量增长.</p><h4 id="Generation-Tasks"><a href="#Generation-Tasks" class="headerlink" title="Generation Tasks"></a>Generation Tasks</h4><h5 id="Summarization"><a href="#Summarization" class="headerlink" title="Summarization"></a>Summarization</h5><p>在摘要任务上的实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bart8.jpg" style="zoom:50%"><h5 id="Dialogue"><a href="#Dialogue" class="headerlink" title="Dialogue"></a>Dialogue</h5><p>在对话任务上的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bart9.jpg" style="zoom:50%"><h5 id="Abstractive-QA"><a href="#Abstractive-QA" class="headerlink" title="Abstractive QA"></a>Abstractive QA</h5><p>抽象问答数据集上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bart10.jpg" style="zoom:50%"><h4 id="Machine-Translation"><a href="#Machine-Translation" class="headerlink" title="Machine Translation"></a>Machine Translation</h4><p>在机器翻译任务上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bart11.jpg" style="zoom:50%"><p>Baseline是标准Transformer. Fixed BART和Tuned BART分别代表单向翻译和使用<strong>反向翻译</strong>的BART. 没有反向翻译的BART效果不太好, 可能涉及到过拟合, 使用反向翻译后应该增强了泛化能力, 效果得到了改善.</p><blockquote><p>反向翻译指的是, 将目标语言输入模型得到源语言的预测结果, 其中预测错误的内容将视为噪声, 将该预测结果再重新输入新的模型, 让它生成目标语言, 这是一种常见的<strong>文本数据增强</strong>方式.</p></blockquote><h2 id="mBART"><a href="#mBART" class="headerlink" title="mBART"></a>mBART</h2><blockquote><p>但我不是MT方向的, 所以实验部分我只挑着看了比较关键的部分. 之所以把mBART也写在这篇文章中, 是因为mBART只是作为BART的多语言版本, 没有本质上的结构变化, 作者展示了一种基于BART处理机器翻译问题的方法, 比较有趣, 在本文中作为扩展内容.</p></blockquote><p><strong>mBART</strong>(<strong>M</strong>ultilingual <strong>B</strong>idirectional and <strong>A</strong>uto - <strong>R</strong>egressive <strong>T</strong>ransformers)是BART的<strong>多语言</strong>版本, 用于处理不同语言之间的<strong>机器翻译</strong>问题.</p><h3 id="mBART-Model"><a href="#mBART-Model" class="headerlink" title="mBART Model"></a>mBART Model</h3><p>mBART仍然是分为<strong>Pre - Training</strong>和<strong>Fine - Tuning</strong>两个阶段. Pre - Training将使用多个语种的语料作为预训练数据.</p><h4 id="Pre-Training-mBART"><a href="#Pre-Training-mBART" class="headerlink" title="Pre - Training mBART"></a>Pre - Training mBART</h4><p>预训练阶段延续了BART的做法, 仍然采用<strong>Denoising AutoEncoder</strong>的方法令mBART的每条数据在<strong>单语言</strong>内训练. 目标是将单语言文本加噪干扰后再恢复回来. 作者采取了两种BART中的加噪方式:</p><ul><li><strong>Sentence Permutation</strong>: 打乱句子和句子之间的顺序.</li><li><strong>Word - Span masking</strong>: 连续的Mask掉一些内容, 并且只用一个<code>[Mask]</code>替换.</li></ul><p>除此外, mBART的初衷是多语言模型, 必须将语种的信息加入. 在文本输入结束后, 在句子末尾处需要加上句子结尾标识<code>&lt;\s&gt;</code>和对应语言的标识<code>[LID]</code>.</p><h4 id="Fine-Tuning-mBART"><a href="#Fine-Tuning-mBART" class="headerlink" title="Fine - Tuning mBART"></a>Fine - Tuning mBART</h4><p>微调阶段才针对机器翻译任务训练. 用<code>[LID]</code>替换Decoder原来的第一个输入<code>[Start]</code>, 表明要翻译成哪个语种.</p><p>Sentence Level MT和Document Level MT的主要区别就在于文本的长度不同, Document Level MT更困难一些.</p><p>上述方法概括为下图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mbart1.jpg" style="zoom:67%"><p>左侧为预训练阶段, 每条数据使用单语言文本并加噪, mBART将其恢复为原来的单语言文本. 右侧为微调阶段, 针对某两种语言之间做微调, 输入为源语言, 期望输出为目标语言, Decoder的首次输入为目标语言的<code>&lt;LID&gt;</code>.</p><h3 id="Language-Transfer"><a href="#Language-Transfer" class="headerlink" title="Language Transfer"></a>Language Transfer</h3><p>作者额外提出了一种新的语言迁移的无监督机器翻译方式.</p><p>常见的的反向翻译示意图如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mbart2.jpg" style="zoom:50%"><p>先用预训练权重初始化翻译模型, 然后对目标语言到源语言的翻译模型做训练, 生成源语言的文本作为扩充数据, 再将之前的平行语料和新生成源语言的文本共同作为训练数据, 训练源语言到目标语言的模型.</p><p>而作者提出的语言迁移方法如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mbart3.jpg" style="zoom:50%"><p>直接在预训练阶段就注入多语言的平行语料, 使得模型能学习到不同语种之间潜在的共性, 在Fine Tuning后, 直接就能应对相似语种的翻译任务.</p><h3 id="Experiments-1"><a href="#Experiments-1" class="headerlink" title="Experiments"></a>Experiments</h3><p>详细的实验设置请参照原论文.</p><p>文中mBART25代表用25种语言预训练出的mBART. 文章中实验所涉及到的语言代码分别如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mbart4.jpg" style="zoom:50%"><p>在低资源数据集上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mbart5.jpg" style="zoom:50%"><p>Random代表不对每种翻译任务使用预训练, 直接在该任务上训练. mBART25效果要好.</p><p>在高资源数据集上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mbart6.jpg" style="zoom:50%"><p>结果显示, 当数据集大小增加后, 不做预训练的效果反而是要好一点, mBART和Random表现相近.</p><p>作者尝试了不同语种到英文的Fine Tuning, 并使用不同的Testting Language观察不同Fine Tuning Language对最终结果的影响:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mbart7.jpg" style="zoom:33%"><p>灰色的部分为同一个语系中的语言.</p><p>这个表格意味着:</p><ul><li>主对角线上Fine Tuning时所采用的语料为<code>X-EN</code>, Testing时测试的任务为<code>X-X</code>.</li><li>同一X轴上代表使用的Fine Tuning Language相同(<code>X-EN</code>), 但采用了不同的Testing Language.</li><li>同一Y轴上代表采用了不同的Fine Tuning Language, 但使用的Testing Language相同(<code>X-X</code>).</li></ul><p>除去翻译最好的是自己的语言外, 次优的一般都是同一语系下的其他语言. 确实说明了语言之间存在一定的共性.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>BART算是BERT和GPT的集大成者, 它的结构和标准Transformer一致, 但与Transformer不同点在于<strong>数据输入</strong>和<strong>训练目标</strong>, 以<strong>自回归式去噪自动编码器</strong>的形式存在.</p><p>因为使用了Transformer Decoder, 使得BART具有了BERT不具备的处理生成任务的能力, 实验结果表明, 没有损失性能, 也没有添加过多的参数.</p><p>作者尝试了包括Masking在内的多种<strong>噪声</strong>的添加方式, 这些噪声的干扰非常强大, 强制要求模型也要有与之匹配的预测能力.</p><p>mBART作为BART多语言版本, 给出了一种基于BART的多语言机器翻译上的处理思路, 也揭示了机器翻译中同一语系下不同语种之间的一些潜在共性.</p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/1394.html">https://ADAning.github.io/posts/1394.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/51848.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/14.jpg" class="responsive-img" alt="StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding"> <span class="card-title">StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: BERT: 详见ELMo, GPT, BERT. StructBERT: Incorporating Language Structures into Pre-training for Deep Language U</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2021-05-04 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/60222.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/1.jpg" class="responsive-img" alt="GAKE: Graph Aware Knowledge Embedding"> <span class="card-title">GAKE: Graph Aware Knowledge Embedding</span></div></a><div class="card-content article-content"><div class="summary block-with-text">GAKE: Graph Aware Knowledge Embedding本文是论文GAKE: Graph Aware Knowledge Embedding的阅读笔记和个人理解. Basic Idea在现有的KGE方法中, 都是基于三元组</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2021-04-18 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/" class="post-category">知识图谱</a></span></div></div><div class="card-action article-tags"><a href="/tags/KGE/"><span class="chip bg-color">KGE</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">367.7k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:695439722@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>