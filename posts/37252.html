<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="Two are Better than One: Joint Entity and Relation Extraction with Table - Sequence Encoders, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>Two are Better than One: Joint Entity and Relation Extraction with Table - Sequence Encoders | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/7.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">Two are Better than One: Joint Entity and Relation Extraction with Table - Sequence Encoders</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/ERE/"><span class="chip bg-color">ERE</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2021-11-16</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2024-07-28</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 3.3k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 13 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><h1 id="Two-are-Better-than-One-Joint-Entity-and-Relation-Extraction-with-Table-Sequence-Encoders"><a href="#Two-are-Better-than-One-Joint-Entity-and-Relation-Extraction-with-Table-Sequence-Encoders" class="headerlink" title="Two are Better than One: Joint Entity and Relation Extraction with Table - Sequence Encoders"></a>Two are Better than One: Joint Entity and Relation Extraction with Table - Sequence Encoders</h1><p>本文是论文<a href="https://arxiv.org/abs/2010.03851" target="_blank" rel="noopener">Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders</a>的阅读笔记和个人理解. 论文来自<strong>EMNLP 2020</strong>. 本文为RTE问题中, 探讨NER和RE任务间关系的系列三部曲中的第一篇.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>NER和RE是两个NLP里的基础任务, 最近<strong>联合学习</strong>的算法尝试把NER和RE<strong>同时解决</strong>, 很多算法都尝试用<strong>填表式</strong>一次性完成. 但是现有的算法大多采用的是单个Encoder, 把NER和RE放在同一个Encoder提供的空间捕获信息.</p><p>作者举了一个非常简单的例子:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse1.jpg" style="zoom:50%"><p>上图中, NER和RE在一张表中被一次性解决:</p><ul><li>在主对角线中, 实体被BIO标注法标出, NER被完成.</li><li>在对角线两侧, 不同的颜色代表不同的关系, 而方向标明了该实体是Subject还是Object, RE被完成.</li></ul><p>作者认为, 这样的设计是不符合联合学习的设计思想的, 必然会<strong>限制</strong>联合算法的效果:</p><ul><li>存在<strong>特征困惑</strong>问题, 即<strong>One Feature for Two Task</strong>, NER和RE本来是两个不同的任务, 单个Encoder会在学习过程中产生困惑, 不能学习到同时解决NER和RE任务的特征.</li><li>没有充分使用<strong>表结构</strong>信息, 大多数的Joint Extraction论文确实构造了表结构, 但是却最后把Table Filling问题转化为一个Sequence Labeling问题去解决.</li></ul><p>基于上述问题, 作者尝试在模型底层构建两个单独学习特征的Encoder, 分别学习NER和RE的特征.</p><h2 id="TSE"><a href="#TSE" class="headerlink" title="TSE"></a>TSE</h2><p><strong>TSE</strong>(<strong>T</strong>able - <strong>S</strong>equence <strong>E</strong>ncoders)是我自己给模型起的名字. 相较于后续其他论文起的Table - Sequence, 我还是认为简写更好.</p><p>作者眼中的NER和RE任务形式为:</p><ul><li><strong>NER</strong>: NER被建模为一个序列标注问题, 其Label $\boldsymbol{y}^{\text {NER}}$ 为标准的BIO标注.</li><li><strong>RE</strong>: RE被建模为一个填表问题, 对于给出的句子$\boldsymbol{x}=\left[x_{i}\right]_{1 \leq i \leq N}$, 在标注表中标注出$\boldsymbol{y}^{\mathrm{RE}}=\left[y_{i, j}^{\mathrm{RE}}\right]_{1 \leq i, j \leq N}$. 实体$x_{i^{b}}, \ldots, x_{i^{e}}$ 与实体$x_{j^{b}}, . ., x_{j^{e}}$ 之间的关系定义为$y_{i, j}^{\mathrm{RE}}=\overrightarrow{r}$, 当两实体调换位置时, 记为$y_{j, i}^{\mathrm{RE}}=\overleftarrow{r}$. $\perp$ 代表不存在关系.</li></ul><p>TSE主要由三个部分组成: 基础表示Text Embedder, 表表示Table Encoder, 序列表示Sequence Encoder:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse2.jpg" style="zoom:50%"><h3 id="Text-Embedder"><a href="#Text-Embedder" class="headerlink" title="Text Embedder"></a>Text Embedder</h3><p>句子中含有$N$ 个单词$\boldsymbol{x}=\left[x_{i}\right]_{1 \leq i \leq N}$, 句子中的Token表示分别由三类Embedding组成, 分别是<strong>Word Embedding</strong>, LSTM提取的<strong>Character Embedding</strong>, 以及BERT抽取的上下文相关的<strong>Contextualized Word Embedding</strong>.</p><p>句子级别的Token表示分别为$\boldsymbol{x}^w$, $\boldsymbol{x}^c$, $\boldsymbol{x}^l$. 句子表示$\boldsymbol{S}_0$如下:</p><p>$$<br>\boldsymbol{S}_{0}=\operatorname{Linear}\left(\left[\boldsymbol{x}^{c} ; \boldsymbol{x}^{w} ; \boldsymbol{x}^{\ell}\right]\right)<br>$$</p><p>即将三个Embedding拼接后直接变换, $\boldsymbol{S}_0 \in \mathbb{R}^{N \times H}$.</p><h3 id="Table-Encoder"><a href="#Table-Encoder" class="headerlink" title="Table Encoder"></a>Table Encoder</h3><p>Table Encoder专门学习<strong>表</strong>表示.</p><p>首先, 由Sequence表示构造表结构, 刚开始表是<strong>表内上下文无关</strong>的, 从第$l-1$ 层Sequence Encoder得到的Sequence表示中获取表第$i$ 行第$j$ 列所需的信息, 简单的用$\operatorname{Linear}$ 展开成一个表:</p><p>$$<br>X_{l, i, j}=\operatorname{ReLU}\left(\operatorname{Linear}\left(\left[S_{l-1, i} ; S_{l-1, j}\right]\right)\right)<br>$$</p><p>第$l$ 层Table Encoder有$\boldsymbol{X}_l \in \mathbb{R} ^{N \times N \times H}$.</p><p>接着, 采用<strong>多维RNN</strong>(MD - RNN)去迭代的融合<strong>表间</strong>信息, 也就是<strong>表内上下文相关</strong>的信息:</p><p>$$<br>T_{l, i, j}=\operatorname{GRU}\left(X_{l, i, j}, T_{l-1, i, j}, T_{l, i-1, j}, T_{l, i, j-1}\right)<br>$$</p><p>$\operatorname{GRU}$ 跨越了三个维度, 除了表的行$i$, 列$j$, 还有跨越不同Table Encoder层的$l$.</p><p>第$l$ 个Table Encoder的表中的第$i$ 行, 第$j$ 列的信息$T_{l, i, j}$ 由多个GRU捕获, 每个GRU可以捕获到不同信息:</p><p>$$<br>\begin{aligned}<br>&amp;T_{l, i, j}^{(a)}=\operatorname{GRU}^{(a)}\left(X_{l, i, j}, T_{l-1, i, j}^{(a)}, T_{l, i-1, j}^{(a)}, T_{l, i, j-1}^{(a)}\right) \\<br>&amp;T_{l, i, j}^{(c)}=\operatorname{GRU}^{(c)}\left(X_{l, i, j}, T_{l-1, i, j}^{(c)}, T_{l, i+1, j}^{(c)}, T_{l, i, j+1}^{(c)}\right) \\<br>&amp;T_{l, i, j}=\left[T_{l, i, j}^{(a)} ; T_{l, i, j}^{(c)}\right]<br>\end{aligned}<br>$$</p><p>在表结构中, RNN可以选择递归<strong>方向</strong>, 对于式子中出现的$a, b, c, d$ 分别代表作者所尝试的方向, 示意图如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse4.jpg" style="zoom:50%"><p>作者尝试了a, c组合和a, b, c, d组合两种设置, 后来发现前后二者差别不大, 所以采用简单的前者.</p><blockquote><p>个人猜测应该是b, d<strong>信息冗余</strong>. 从a触发, 代表表行和列的顺向, 从c出发代表表行和列的逆向. 而b代表行的顺向, 列的逆向, d代表行的逆向, 列的顺向.</p><p>在我们所习惯的双向RNN中扩展到表结构上, 就是a, c的组合, 无需再重复添加b, d.</p></blockquote><h3 id="Sequence-Encoder"><a href="#Sequence-Encoder" class="headerlink" title="Sequence Encoder"></a>Sequence Encoder</h3><p>Sequence Encoder专门学习<strong>序列</strong>表示.</p><p>作者认为单纯的缩放点积注意力在本模型中不够好, 于是提出了Table - Guided Attention, 用表信息指导Sequence中的Attention.</p><p>对于给定的$\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}$, 一般的注意力可被下述式子概括:<br>$$<br>f\left(Q_{i}, K_{j}\right)=U \cdot g\left(Q_{i}, K_{j}\right)<br>$$</p><p>$f$ 能返回$Q_i$ 下$V_i$ 的权重. 其中$U$ 为可学习参数, $g$ 为计算$Q, K$ 间相关性的函数.</p><p>如下图所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse5.jpg" style="zoom:50%"><p>在本模型中, 恰好$\boldsymbol{Q}= \boldsymbol{K} =\boldsymbol{V}=\boldsymbol{S}_{l-1}$, 而$\boldsymbol{T}_l$ 是由$\boldsymbol{S}_{l-1}$ 得到的, 可以视为$T_{l, i, j} = g(S_{l-1, i}, S_{l-1, j}) = g(Q_i, K_j)$. 因此有:</p><p>$$<br>f\left(Q_{i}, K_{j}\right)=U \cdot T_{l, i, j}<br>$$</p><p>作者认为这种Attention有三个好处:</p><ol><li>不用算$g(\cdot)$, 直接从Table Encoder里拿$\boldsymbol{T}_l$ 就行.</li><li>因为$\boldsymbol{T}_l$ 是表内上下文相关的, 所以更能捕捉词间不同相关性.</li><li>使得Table Encoder能参与到Sequence Encoder的学习过程中, 从而提升两个Encoder间的交互性.</li></ol><p>其余的内容和Self - Attention一样:<br>$$<br>\begin{aligned}<br>\tilde{\boldsymbol{S}}_{l} &amp;=\operatorname{LayerNorm}\left(\boldsymbol{S}_{l-1}+\operatorname{SelfAttn}\left(\boldsymbol{S}_{l-1}\right)\right) \\<br>\boldsymbol{S}_{l} &amp;=\operatorname{LayerNorm}\left(\tilde{\boldsymbol{S}}_{l}+\operatorname{FFNN}\left(\tilde{\boldsymbol{S}}_{l}\right)\right)<br>\end{aligned}<br>$$</p><p>我再回来看一眼Table Encoder和Sequence Encoder之间<strong>迭代提升</strong>的过程:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse2.jpg" style="zoom:50%"><blockquote><p>注意, 在这个图左侧是有虚线的! 它的作用将在下一小节说明.</p></blockquote><h3 id="Exploit-Pre-trained-Attention-Weights"><a href="#Exploit-Pre-trained-Attention-Weights" class="headerlink" title="Exploit Pre - trained Attention Weights"></a>Exploit Pre - trained Attention Weights</h3><p>作者认为, BERT的潜力没有被充分发掘, 因为BERT能很好的把握住<strong>词和词</strong>之间的关系, 所以<strong>Attention Weights</strong>最好被<strong>显式</strong>的加进来, 预训练模型的潜力就被更进一步的挖掘了.</p><p>作者把所有层的所有头的Attention Weights全部堆叠起来, 记为$\boldsymbol{T}^{l} \in \mathbb{R}^{N \times N \times (L^\ell \times A^\ell)}$, $L^{\ell}$ 为Transformer堆叠的层数, $A^\ell$ 为注意力头数.</p><p>接着只要把前面Sequence Encoder公式替换即可:<br>$$<br>\displaylines{X_{l, i, j}=\operatorname{ReLU}\left(\operatorname{Linear}\left(\left[S_{l-1, i} ; S_{l-1, j}\right]\right)\right)<br>\\<br>\downarrow<br>\\<br>X_{l, i, j}=\operatorname{ReLU}\left(\operatorname{Linear}\left(\left[S_{l-1, i} ; S_{l-1, j} ; T_{i, j}^{\ell}\right]\right)\right)}<br>$$</p><p>在每层生成Table表示的时候, 就必须考虑第$i$ 个词和第$j$ 个词之间的Attention Score $T_{i, j}^\ell$.</p><p>其余部分完全不用变, 就可以直接把预训练的BERT拿过来用.</p><p>再来回顾一下模型的整个流程:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse3.jpg" style="zoom:50%"><ol><li>由Sequence Encoding和预训练模型的Attention Score一起得到最初的Table Encoding.</li><li>Table Encoding内部用MD - RNN做表间和跨层的交互.</li><li>Table信息通过Table - Guided Attention反作用于Sequence Encoder的训练.</li></ol><p>总体来看设计还是挺精妙的.</p><h3 id="Training-and-Evaluation"><a href="#Training-and-Evaluation" class="headerlink" title="Training and Evaluation"></a>Training and Evaluation</h3><p>因为作者假定NER为序列标注问题, RE为填表问题, 所以直接拿Sequence的结果$\boldsymbol{S}_L$ 来标注NER, 填表$\boldsymbol{T}_L$ 的结果来标注RE:</p><p>$$<br>\begin{aligned}<br>P_{\theta}\left(\boldsymbol{Y}^{\mathrm{NER}}\right) &amp;=\operatorname{softmax}\left(\operatorname{Linear}\left(\boldsymbol{S}_{L}\right)\right) \\<br>P_{\theta}\left(\boldsymbol{Y}^{\mathrm{RE}}\right) &amp;=\operatorname{softmax}\left(\operatorname{Linear}\left(\boldsymbol{T}_{L}\right)\right)<br>\end{aligned}<br>$$</p><p>这二者都是分类问题, 直接用交叉熵优化:</p><p>$$<br>\begin{aligned}<br>\mathcal{L}^{\mathrm{NER}} &amp;=\sum_{i \in[1, N]}-\log P_{\theta}\left(Y_{i}^{\mathrm{NER}}=y_{i}^{\mathrm{NER}}\right) \\<br>\mathcal{L}^{\mathrm{RE}} &amp;=\sum_{i, j \in[1, N] ; i \neq j}-\log P_{\theta}\left(Y_{i, j}^{\mathrm{RE}}=y_{i, j}^{\mathrm{RE}}\right)<br>\end{aligned}<br>$$</p><p>最终目标就是联合优化二者, 即最小化$\mathcal{L}^{\text{NER}} + \mathcal{L}^{\text{RE}}$.</p><p>在推断时, 还是把NER和RE作为两个任务<strong>分别处理</strong>.</p><p>对于NER任务, 简单的有:<br>$$<br>\underset{e}{\operatorname{argmax}} P_{\theta}\left(Y_{i}^{\mathrm{NER}}=e\right)<br>$$<br>对于RE任务, 需要区分开关系所对应的Subject和Object. 对于NER抽取到的两实体的Span$\left(i^{b}, i^{e}\right), \left(j^{b}, j^{e}\right)$, 关系由下式得来:<br>$$<br>\underset{\vec{r}}{\operatorname{argmax}} \sum_{i \in\left[i^{b}, i^{e}\right], j \in\left[j^{b}, j^{e}\right]} P_{\theta}\left(Y_{i, j}^{\mathrm{RE}}=\overrightarrow{r}\right)+P_{\theta}\left(Y_{j, i}^{\mathrm{RE}}=\overleftarrow{r}\right)<br>$$</p><p>$\perp$ 代表不存在关系, 它是不存在方向的.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文. BTW, 附录里面还有作者很多小想法和实验结果, 建议阅读.</p><p>作者采用了ACE04, ACE05, CoNLL04, ADE这几个数据集.</p><p>下文中, 作者所采用的指标为F1 Score, NER仅当实体类型和边界均匹配时才算正确, RE当实体边界和关系类型匹配时才算正确.</p><h3 id="Comparison-with-Other-Models"><a href="#Comparison-with-Other-Models" class="headerlink" title="Comparison with Other Models"></a>Comparison with Other Models</h3><p>作者将本文方法与诸多模型做对比, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse7.jpg" style="zoom:67%"> <img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse8.jpg" style="zoom:67%"><p>RE + 代表实体边界, 以及实体类型, 关系类型, 三者均匹配时才算正确, 比传统的RE任务要更加严格, 还包含了<strong>实体类型</strong>.</p><p>模型在上述数据集上均达到SOTA.</p><h3 id="Comparison-of-Pre-trained-Models"><a href="#Comparison-of-Pre-trained-Models" class="headerlink" title="Comparison of Pre - trained Models"></a>Comparison of Pre - trained Models</h3><p>作者通过比较不同预训练之间的差距, 来间接凸显自己方法的有效性. 在ACE05上的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse9.jpg" style="zoom:67%"><p>$+\boldsymbol{x}^\ell$ 代表使用了上下文相关的Word Embedding, $+\boldsymbol{T}^\ell$ 代表使用了Attention Weights.</p><p>即使是使用了不是真正上下文相关的ELMo, 性能依然能够与一些最好的模型媲美. 引入预训训练后, 性能进一步上涨, 在引入Attention Weights后涨幅比较大.</p><p>这证明了利用Attention Weight的重要性, 对NER和RE都有增益.</p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><h4 id="Bidirectional-Interaction"><a href="#Bidirectional-Interaction" class="headerlink" title="Bidirectional Interaction"></a>Bidirectional Interaction</h4><p>作者研究了NER和RE这两个任务之间的交互对性能的影响:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse10.jpg" style="zoom:67%"><p>RE(gold) 代表在做RE时候直接使用Golden Entity.</p><p>结论如下:</p><ul><li>不使用NER Loss或者不使用RE Loss, 都会降低性能, NER和RE彼此间确实存在关联, 且有相互促进作用.</li><li>不使用Table Encoder或不使用Sequence Encoder都会对性能产生负面影响, 如果切断两个Encoder之间的交互, 效果会更差.</li><li>如果直接把Table Representation的主对角线用来做NER任务, 也就是把NER和RE统一在一个空间中解决, 效果也不错, 但达不到作者的设置. 作者认为这是因为NER和RE仍然存在潜在差异, 不能直接用RE任务的Feature来做NER. 在该基础上, 继续去掉Sequence Encoder, 效果会变差, 这表明了Sequence信息对NER的作用.</li></ul><h4 id="Encoding-Layers"><a href="#Encoding-Layers" class="headerlink" title="Encoding Layers"></a>Encoding Layers</h4><p>作者探究了Encoder层数对性能的影响, ACE05上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse11.jpg" style="zoom:67%"><p>堆到3层的时候性能就没有明显提升了, 如果每层的参数共享, 层数越多越好.</p><h4 id="Settings-of-MD-RNN"><a href="#Settings-of-MD-RNN" class="headerlink" title="Settings of MD - RNN"></a>Settings of MD - RNN</h4><p>作者还对不同MD - RNN设定做了探究, 在ACE05上部分结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse12.jpg" style="zoom:67%"><p>就如在前文中提到的, 四方向的效果并不比双向好太多. 且融入跨层信息后对RE提升比较大.</p><h3 id="Attention-Visualization"><a href="#Attention-Visualization" class="headerlink" title="Attention Visualization"></a>Attention Visualization</h3><p>作者还模型做了Attention可视化:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse13.jpg" style="zoom:67%"><p>相较于ALBERT下的Attention, 本文提出的Table - Guided Attention要更像人类的Ground Truth一些, 可视化一定程度上说明了改进的有效性.</p><h3 id="Probing-Intermediate-States"><a href="#Probing-Intermediate-States" class="headerlink" title="Probing Intermediate States"></a>Probing Intermediate States</h3><p>在ACE05中作者挑了一个Case, 然后把训练好的Linear层放在每层Encoder后面检测结果:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse6.jpg" style="zoom:50%"><p>从下到上层数依次变深, 能看到模型逐渐对结果做<strong>修正</strong>, 作者认为更多地允许层间交互有助于捕捉困难实体和关系.</p><blockquote><p>Probing在这里起作用的原因是作者引入了跨Encoder层信息.</p></blockquote><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>在当时, 填表类的联合抽取模型已经有几年没有被提起, 此篇论文算是重新将填表式的方法抬上了桌面.</p><p>作者注意到了联合模型中存在的任务<strong>特征学习冲突</strong>问题, 并设计了一种<strong>两个独立Encoder</strong>的<strong>迭代</strong>式<strong>交互提升</strong>模型. 实验做的非常全面, 还用到了Probing等手段, 最后得到了”<strong>Two are better than One</strong>“的结论.</p><blockquote><p>非常巧合的是, 本文同期与另一篇论述Pipeline模型不一定弱于Joint模型的文章<a href="https://arxiv.org/abs/2010.12812" target="_blank" rel="noopener">A Frustratingly Easy Approach for Entity and Relation Extraction</a>得到了相似的结论, 这篇文章打算下次更新, 算作三部曲中的第二部.</p></blockquote></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/37252.html">https://ADAning.github.io/posts/37252.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/ERE/"><span class="chip bg-color">ERE</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/22256.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/1.jpg" class="responsive-img" alt="PURE: A Frustratingly Easy Approach for Entity and Relation Extraction"> <span class="card-title">PURE: A Frustratingly Easy Approach for Entity and Relation Extraction</span></div></a><div class="card-content article-content"><div class="summary block-with-text">A Frustratingly Easy Approach for Entity and Relation Extraction本文是论文A Frustratingly Easy Approach for Entity and Relati</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2021-12-01 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/ERE/"><span class="chip bg-color">ERE</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/49694.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/17.jpg" class="responsive-img" alt="TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking"> <span class="card-title">TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking</span></div></a><div class="card-content article-content"><div class="summary block-with-text">TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking本文是论文TPLinker: Single-stage</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2021-10-22 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/RTE/"><span class="chip bg-color">RTE</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">390.5k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:andaning@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>