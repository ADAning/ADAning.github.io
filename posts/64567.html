<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="Multimodal Large Language Model 总结, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>Multimodal Large Language Model 总结 | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/10.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">Multimodal Large Language Model 总结</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/MLLM/"><span class="chip bg-color">MLLM</span> </a><a href="/tags/MM/"><span class="chip bg-color">MM</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2024-07-03</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2025-05-19</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 7.3k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 28 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p>本文前置知识: <a href="https://adaning.github.io/posts/44986.html">Vision &amp; Language Pretrained Model 总结</a>.</p><p><strong>2025.05.06</strong>: 应评论区要求, 更新了Qwen-VL系列(Qwen-VL, Qwen2-VL, Qwen2.5-VL).</p></blockquote><h1 id="Multimodal-Large-Language-Model-总结"><a href="#Multimodal-Large-Language-Model-总结" class="headerlink" title="Multimodal Large Language Model 总结"></a>Multimodal Large Language Model 总结</h1><p>最近MLLM的进展实在是太快了, 必须得赶紧写一篇博客出来了… 再不写这些知识就要过期了…</p><p>所以, 本文只是以<strong>总结</strong>的形式梳理了近期比较有代表性的MLLM, 推荐有基础后再阅读.</p><h2 id="Revolution-of-Visual-Language-Adapter"><a href="#Revolution-of-Visual-Language-Adapter" class="headerlink" title="Revolution of Visual-Language Adapter"></a>Revolution of Visual-Language Adapter</h2><p>目前的MLLM基本组成有三部分, <strong>Visual Backbone</strong>, <strong>V-L Adapter</strong>, <strong>LLM</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结1.png" style="zoom:50%"><blockquote><p>该图出自<a href="https://arxiv.org/abs/2402.12451" target="_blank" rel="noopener">The Revolution of Multimodal Large Language Models: A Survey</a>.</p></blockquote><p>之前的MLLM基本在LLM内部没有什么变化, Visual Encoder基本也用的CLIP的Vision Encoder, 主要区别在于Adapter上.</p><h3 id="Flamingo"><a href="#Flamingo" class="headerlink" title="Flamingo"></a>Flamingo</h3><p>Flamingo代表了在LLM主干中加入Cross Attention从而用视觉增强文本表示的一派.</p><ul><li>论文: <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html" target="_blank" rel="noopener">Flamingo: a Visual Language Model for Few-Shot Learning</a>.</li></ul><p>Flamingo将视觉信息融入LLM的方式是在LM Block的主干上<strong>串行</strong>的加入一个用Cross Attention增强文本表示的模块, 从而让文本表示中能融入视觉信息:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结2.png" style="zoom:67%"><blockquote><p>预训练LM为Chinchilla 1.4 / 7 / 70B.</p></blockquote><p>作者在每个LM Block前面加上了一个Gated Cross - Attention Block. 结构如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结3.png" style="zoom:67%"><p>以Language为Query, Vision input为Key和Value, 并用Tanh和残差做一下过滤, 决定视觉增强的文本表示流通率的门控系数为全0初始化, 跟LoRA有点类似.</p><p>其中<strong>Receiver Resampler</strong>是用类似<a href="https://adaning.github.io/posts/44986.html#toc-heading-6">BLIP</a>和<a href="https://adaning.github.io/posts/44986.html#toc-heading-7">CoCa</a>的<strong>Query和Cross-Attention</strong>汲取有效的视觉信息:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结4.png" style="zoom:50%"><p>Resampler中Cross Attention的Key和Value<strong>是Visual Representation和Query Representation的拼接</strong>.</p><p>比较有趣的是作者提到了Flamingo对<strong>交错图文</strong>(Interleaved Image Text)的数据的处理方法:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm%E6%80%BB%E7%BB%935.png" alt=""></p><p>在一系列文本和一系列图像构成的图文交错数据中, 每个文本块中Token在Cross - Attention中只能对对应的Visual Token做Attention(深蓝色), 而无法对其他Visual Token做Attention(浅蓝).</p><blockquote><p>在作者的实验中, 将作者构建的交错图文数据集去掉后, 模型效果下降非常恐怖.</p></blockquote><h3 id="BLIP-2-InstructBLIP"><a href="#BLIP-2-InstructBLIP" class="headerlink" title="BLIP-2 / InstructBLIP"></a>BLIP-2 / InstructBLIP</h3><p>BLIP-2开创了以VL对齐的Q - Former抽取视觉信息送给LLM的先河.</p><h4 id="BLIP-2"><a href="#BLIP-2" class="headerlink" title="BLIP-2"></a>BLIP-2</h4><blockquote><p>BLIP-2我们在<a href="https://adaning.github.io/posts/44986.html#toc-heading-9">VLP总结</a>里面其实已经讲过了, 在这里只是简单的把它粘过来, 以保证内容完整性.</p></blockquote><ul><li>论文: <a href="https://proceedings.mlr.press/v202/li23q.html" target="_blank" rel="noopener">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a>.</li></ul><p>出发点: 当前大规模模型在预训练期间的高额计算消耗太大, 数据也用的特别多.</p><p>作者引入一个lightweight Querying Transformer (Q - Former)来完成Visual &amp; Language模态的桥接过程:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结6.png" style="zoom:75%"><p>作者把Q - Former的训练拆分为两个阶段:</p><ul><li>首阶段: 让Q - Former从Freeze Image Encoder中学习VL表示.</li><li>次阶段: 从Freeze LLM中学习VL表示.</li></ul><p>Q - Former结构和首阶段预训练如下:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm%E6%80%BB%E7%BB%937.png" alt=""></p><p>Q - Former实际上由<strong>双塔</strong>的两个Transformer组成, 分别被称为Image Transformer和Text Transformer. 结构上类似于<strong>BLIP</strong>中的Image - Grounded Text Encoder和Text Encoder.</p><p>Image Transformer的SA和Text Transformer的SA参数是共享的(这点和<strong>VLMo</strong>出奇的一致). Learnable Query从Image Transformer给入, 通过CA来从Frozen Image Encoder中获取视觉信号.</p><blockquote><p>作者还在文中补充了一个小细节, 实际上的Visual Key Value采用的是Image Encoder的倒数第二层输出, 而不是最后一层, 效果会稍微好一点, 这点与大家使用Stable Diffusion的时候取CLIP的倒数第二层输出有点类似.</p></blockquote><p>所以从结构上来看, 首阶段的训练目标是希望Query能够学到从Image Encoder中抽取对Text最有用的内容. 再看训练任务也是这样, 设计了三种:</p><ul><li><strong>ITC</strong>(Image - Text Contrastive Learning): 虽然说是老生常谈的Loss, 但因为Query经过Trm以后得到的表示有多个, 所以作者计算了多个Query与Text Transformer<code>[CLS]</code>的余弦相似度, 选择相似度最大的作为正样本. 为了避免<strong>信息泄露</strong>, 在做ITC的时候要保证Q和T之间是互相不可见的(最右侧Mask).</li><li><strong>ITG</strong>(Image - grounded Text Generation): 使得Q对T完全可见, T单独用causal Mask, 然后生成图文匹配的文本段. 这就要求Query必须覆盖Image的全部信息, 且Query抽取出的信息必须是有效的(中间Mask). <code>[CLS]</code>也被换成<code>[DEC]</code>.</li><li><strong>ITM</strong>(Image - Text Matching): ITM也是常见Loss, Q必须拥有两个模态的信息才能一起判断图文是否匹配, 作者对所有Query都计算ITM Loss, 最后取平均作为Logits, 同时也使用Hard Negative.</li></ul><p>次阶段预训练, 直接用Q - Former完成图生文:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm%E6%80%BB%E7%BB%938.png" alt=""></p><p>由于在首阶段中Q - Former已经完成了Query从Image Encoder中抽取关键信息的学习, 这也就使得Visual Signal可以被Query以Soft Visual Prompt的形式传递给LLM. 所以Q - Former中的Text Transformer变得不再必要, 可以被<strong>丢弃</strong>. Query表示还需要过一层Linear Project和大模型输入维度对齐.</p><blockquote><p>如果不要首阶段直接硬学的话, 由于没有Text Transformer打辅助, 所以想要让Q - Former学到从Image中抽取出更多有关文本的信息会更难. 但文本模态在Q - Former首阶段训练中起到的实际上是一个Grounding的作用, 根据Language来让Learnable Query抽取更多有用的信息.</p></blockquote><h4 id="InstructBLIP"><a href="#InstructBLIP" class="headerlink" title="InstructBLIP"></a>InstructBLIP</h4><ul><li>论文: <a href="https://openreview.net/forum?id=vvoWPYqZJA" target="_blank" rel="noopener">InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning</a>.</li></ul><p>延续<strong>BLIP-2</strong>的Q - Former, 在Q - Former中添加了Instruct, 从而使得Q - Former能完成Instruction-aware Visual Feature Extraction, 从而将Visual Feature从静态的变为<strong>动态</strong>的, 能够做到<strong>instruction following</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结9.png" style="zoom:50%"><p>其余细节基本一致.</p><p>直接使用BLIP-2里面已经对齐好的Q - Former作为初始化(看做是预训练), 然后直接在Q - Former和LLM侧继续进行指令微调, 使Q - Former能够理解指令, 并完成指令引导的视觉特征抽取. 指令构造来自人工手动.</p><blockquote><p>与之类似的还有同样为BLIP系列续作的X-InstructBLIP, 但审稿人似乎认为这种方法并没有具备很大的贡献, 以及实验不够充分缺乏与当前的MLLM对比, 于是在ICLR 24被拒稿了.</p></blockquote><h3 id="LLaVA系列"><a href="#LLaVA系列" class="headerlink" title="LLaVA系列"></a>LLaVA系列</h3><p>LLaVA代表了整个使用MLP为Adapter的一派.</p><h4 id="LLaVA"><a href="#LLaVA" class="headerlink" title="LLaVA"></a>LLaVA</h4><ul><li>论文: <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/6dcf277ea32ce3288914faf369fe6de0-Abstract-Conference.html" target="_blank" rel="noopener">Visual Instruction Tuning</a>.</li></ul><p>与<strong>BLIP-2</strong>的Q - Former不同, LLaVA抛弃了沉重的Visual Extractor设计:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结10.png" style="zoom:50%"><p>用预训练的<a href="https://adaning.github.io/posts/44986.html#toc-heading-3">CLIP</a>抽取的Visual Feature作为Vision Signal, 再用一次<strong>Linear Projection</strong>后送到LLM里面.</p><blockquote><p>在LLM中, Visual Token仍然是<strong>Autoregressive Encoding</strong>的.</p></blockquote><p>LLaVA训练的时候遵循多轮对话:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结11.png" style="zoom:50%"><p>在第一轮对话的时候把图像信息附加进去即可.</p><p>作者设计了两阶段微调, 让LLM能适配Visual Input:</p><ul><li><strong>Stage 1</strong>: Pre - training for Feature Alignment, 只调Linear Projection的参数, 使Visual Feature和LLM Embedding Space对齐.</li><li><strong>Stage 2</strong>: Fine - tuning End-to-End, 让Linear Projection和LLM一起调.</li></ul><p>比较有意思的是, LLaVA的指令数据集是用LLM(ChatGPT / GPT4)生成的, 通过把图像中的信息以自然语言描述出来从而传递给更高阶的LLM, 让LLM生成指令数据.</p><p>比如直接把图中物体的Caption和BBox都传进去, 然后让LLM生成三种类型的问题:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结12.png" style="zoom:50%"><p>LLaVA的表现超过了BLIP-2.</p><h4 id="LLaVA-1-5"><a href="#LLaVA-1-5" class="headerlink" title="LLaVA 1.5"></a>LLaVA 1.5</h4><ul><li>论文: <a href="https://arxiv.org/abs/2310.03744v1" target="_blank" rel="noopener">Improved Baselines with Visual Instruction Tuning (v1)</a>.</li></ul><p>LLaVA 1.5是LLaVA的改进版本:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结13.png" style="zoom:67%"><p>主要做了如下改动:</p><ul><li>限制了LLM的输出格式, 让LLaVA直接以简短的方式回答, 有利于VQA任务.</li><li>从一层Linear Project变成了两层MLP.</li><li>加入了学术方面的数据集, 用于解锁LLaVA对视觉区域细粒度理解能力.</li><li>提高了图像分辨率, 并加入了额外数据源.</li></ul><h4 id="LLaVA-NeXT-LLaVA-1-5-HD"><a href="#LLaVA-NeXT-LLaVA-1-5-HD" class="headerlink" title="LLaVA-NeXT(LLaVA-1.5-HD)"></a>LLaVA-NeXT(LLaVA-1.5-HD)</h4><ul><li>论文: <a href="https://arxiv.org/abs/2310.03744v2" target="_blank" rel="noopener">Improved Baselines with Visual Instruction Tuning (v2)</a>.</li></ul><p>增强了推理, OCR和World Knowledge.</p><p>通过动态分辨率输入, 支持”任意”分辨率大小的图像作为输入(实际训练阶段最大支持4倍):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结14.png" style="zoom:67%"><p>把大图切分为小Patch, 分别用Visual Encoder编码, 再把一个下采样的图编码, 作为<strong>全局信息</strong>, 全拼接后一起送给LLM.</p><p>特性如下:</p><ul><li>支持动态分辨率输入.</li><li>采用更强的用户指令数据, 加入更多跟文档以及图片理解的数据.</li><li>增大LLM backbone的Scale.</li></ul><h3 id="Qwen-VL系列"><a href="#Qwen-VL系列" class="headerlink" title="Qwen-VL系列"></a>Qwen-VL系列</h3><h4 id="Qwen-VL"><a href="#Qwen-VL" class="headerlink" title="Qwen-VL"></a>Qwen-VL</h4><ul><li>论文: <a href="https://arxiv.org/abs/2308.12966" target="_blank" rel="noopener">Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond</a>.</li><li>代码: <a href="https://github.com/QwenLM/Qwen-VL" target="_blank" rel="noopener">GitHub - QwenLM/Qwen-VL: The official repo of Qwen-VL (通义千问-VL) chat &amp; pretrained large vision language model proposed by Alibaba Cloud.</a>.</li></ul><h5 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h5><p>Qwen-VL是一个9.6B的MLLM. Vision Encoder(OpenCLIP的ViT-bigG初始化)占1.9B, LLM(Qwen-7B初始化)占7.7B, VL-Adapter占0.08B, 其设计遵循<strong>BLIP-2</strong>的Q-Former的形式.</p><h5 id="Inputs-and-Outputs"><a href="#Inputs-and-Outputs" class="headerlink" title="Inputs and Outputs"></a>Inputs and Outputs</h5><ul><li><strong>Image Input</strong>: 由Visual Encoder和Adapter处理的固定长度的Patch Embeddings, 用<code>&lt;img&gt;</code>, <code>&lt;/img&gt;</code> 进行包裹.</li><li><strong>Bounding Box Input &amp; Output</strong>: 进行Grounding的时候, 需要用到Bounding Box. Bounding Box会被Normalize到<code>[0, 1000)</code> 之间, 并且用<code>&lt;box&gt; (X_topleft, Y_topleft), (X_bottomright, Y_bottomright) &lt;/box&gt;</code>来描述. 如果是Reference Grounding, 则将Reference Sentence 用<code>&lt;ref&gt;</code>, <code>&lt;/ref&gt;</code> 包裹.</li></ul><p>结合各类具体任务, 格式如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结25.png" style="zoom:50%"><h5 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h5><p>Qwen-VL经历了三个阶段的训练:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结26.png" style="zoom:33%"><ol><li><strong>Pre-training</strong>: 在低分辨率($224 \times 224$)的图像上用ViT和Adapter, QwenLM是frozen的. 做Image-Guided Text Generation, 做Vision-Language初步对齐, 用了50亿图文对, 清洗后14亿, 其中有77.3%的英语数据, 22.7的中文数据.</li><li><strong>Multi-task Pre-training</strong>: 经历过第一阶段的预训练后, 解冻所有组件. 输入图像也换成高分辨率($448 \times 448$)的. 把Caption, VQA, Grounding, OCR, Pure-text Autoregression等任务串起来放到一起训. Caption和OCR的数据占比比较大, 其次是Grounding. 不难看出这块主要是为了对齐, 而且OCR一定程度加强了MLLM对图中文字的利用能力.</li><li><strong>Supervised Fine-tuning</strong>: 冻住ViT, 解冻QwenLM和Adapter. 提高Qwen-VL的Instruction Following和Dialog Performance.</li></ol><h5 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h5><p>Qwen-VL当时也探索了高分辨率图像输入和Global / Window Attention对MLLM的影响, 以目前的视角来看是当今的研究重点之一, 而且也为Qwen-VL系列的后续工作打下了基础.</p><p>Training Loss的话肯定是越高分辨率的图像输入效果越好, 但是Window Attention不如Global Attention:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结27.png" style="zoom:50%"><p>训练速度上其实Window Attention仅在$896 \times 896$下有优势, $448 \times 448$设置下区别不大:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结28.png" style="zoom:67%"><h4 id="Qwen2-VL"><a href="#Qwen2-VL" class="headerlink" title="Qwen2-VL"></a>Qwen2-VL</h4><ul><li>论文: <a href="https://arxiv.org/abs/2409.12191" target="_blank" rel="noopener">Qwen2-VL: Enhancing Vision-Language Model’s Perception of the World at Any Resolution</a>.</li><li>代码: <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py" target="_blank" rel="noopener">transformers/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py at main · huggingface/transformers · GitHub</a>.</li></ul><h5 id="Model-Architecture-1"><a href="#Model-Architecture-1" class="headerlink" title="Model Architecture"></a>Model Architecture</h5><p><strong>Qwen-VL</strong>的续作, 在<strong>Qwen-VL</strong>的基础上添加了一些特性. Visual Encoder换成了Apple的<a href="https://arxiv.org/abs/2309.17425" target="_blank" rel="noopener">DFN ViT</a>, 去掉了Cross-Attention, LLM换成Qwen2.</p><ol><li><p><strong>Naive Dynamic Resolution</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结29.png" style="zoom:33%"> Qwen2-VL提供了一种朴素的动态分辨率支持, 将ViT中的绝对位置编码改为2D-RoPE, 从而支持任意分辨率图像作为输入. 并且为了减少推理资源消耗, 还在后面加了个MLP, 将每$2 \times 2$ 的Patch Token Merge成一个. 所以用`patchsize=14`的ViT提取$224 \times 224$图像, 加上`<|vision_start|>`, `<|vision_end|>`两个用于标识Visual的Special Token, 一共应有66个Token. 这部分想知道具体怎么实现的建议看下代码.</li><li><p><strong>Multimodal Rotary Position Embedding (M-RoPE)</strong>:</p></li></ol><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结30.png" style="zoom:33%"><p>视频是3D-RoPE(Temporal, Height, Width), 图像是2D-RoPE(Height, Width), 文本是1D-RoPE(Text Position), 然后把它们拼接在一起. 这里的视频Temporal维是视频输入帧的序号.<br>实际上这是苏神的博客中已经提过的一种将<strong>RoPE</strong>扩展到Multimodal的一种解法, 把图像的2D-RoPE和文本的1D-RoPE结合起来, 已附链接, 感兴趣的可以看一下:</p><ul><li><a href="https://kexue.fm/archives/10040" target="_blank" rel="noopener">Transformer升级之路：17、多模态位置编码的简单思考 - 科学空间|Scientific Spaces</a>.</li><li><a href="https://kexue.fm/archives/10352" target="_blank" rel="noopener">“闭门造车”之多模态思路浅谈（三）：位置编码 - 科学空间|Scientific Spaces</a>.</li></ul><ol start="3"><li><strong>Unified Image and Video Understanding</strong>: Qwen2-VL采用了图像和视频混合训练的方式, 来加强对图像和视频信息的理解能力. 在用视频训练时, 每秒采样2帧, 并用深度为2的Conv 3D来处理视频帧, 也就是<strong>将视频的两帧的同位置Patch整合为单个Patch Token</strong>. 为了兼容长视频理解, 限制视频Token数最大为16384. 图像可以向视频侧对齐兼容Conv 3D, 将一张图片复制一次, 当做视频的两帧来处理.</li></ol><h5 id="Training-1"><a href="#Training-1" class="headerlink" title="Training"></a>Training</h5><p>训练和<strong>Qwen-VL</strong>是一样的, 也是分为渐进式的三个阶段:</p><ol><li>第一阶段只训练Visual Encoder.</li><li>第二阶段解冻所有的参数.</li><li>第三阶段只训练LLM.</li></ol><p><strong>Data Format</strong>:</p><ul><li><strong>Dialogue Data</strong>: 使用了ChatML的格式. 图像用<code>&lt;|vision_start|&gt;</code>, <code>&lt;|vision_end|&gt;</code>包裹, 不同角色(User / Assistant)的回复在<code>&lt;|im_start|&gt;</code>, <code>&lt;|im_entd|&gt;</code>中:<img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结31.png" style="zoom:50%"></li><li><strong>Visual Grounding</strong>: 和<strong>Qwen-VL</strong>也是一样的, Normalize到<code>[0, 1000)</code> 之间, 然后用<code>&lt;|box_start|&gt; (X_topleft, Y_topleft), (X_bottomright, Y_bottomright) &lt;|box_end|&gt;</code>来描述. 如果有Reference就用<code>&lt;|object_ref_start&gt;</code>, <code>&lt;|object_ref_end|&gt;</code> 包裹:<img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结32.png" style="zoom:50%"></li><li><strong>Visual Agent</strong>: Qwen2-VL将能力扩展到Visual Agent. 使得MLLM能够使用Tools与环境交互, 并依赖返回结果迭代:<img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结33.png" style="zoom:50%"></li></ul><h5 id="Experiments-1"><a href="#Experiments-1" class="headerlink" title="Experiments"></a>Experiments</h5><p>实验部分测试了M-RoPE在视频任务上的良好外推性:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结34.png" style="zoom:50%"><h4 id="Qwen2-5-VL"><a href="#Qwen2-5-VL" class="headerlink" title="Qwen2.5-VL"></a>Qwen2.5-VL</h4><ul><li>论文: <a href="https://arxiv.org/abs/2502.13923" target="_blank" rel="noopener">Qwen2.5-VL Technical Report</a>.</li><li>代码: <a href="https://github.com/QwenLM/Qwen2.5-VL" target="_blank" rel="noopener">GitHub - QwenLM/Qwen2.5-VL: Qwen2.5-VL is the multimodal large language model series developed by Qwen team, Alibaba Cloud</a>.</li></ul><p>能明显感觉到, Qwen2.5-VL自从最近出来了以后, 受关注度就比较高.</p><h5 id="Model-Architecture-2"><a href="#Model-Architecture-2" class="headerlink" title="Model Architecture"></a>Model Architecture</h5><p>整体结构如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结35.png" style="zoom:50%"><p>还是老三样, LLM(Qwen2.5), Visual Encoder(一个重新训练的ViT), MLP-based VL Merger(两层Linear, 并对$2 \times 2$ 的Patch聚合).</p><ol><li><p><strong>Fast and Efficient Vision Encoder</strong>:</p><p>作为<strong>Qwen2-VL</strong>的升级版, Qwen2.5-VL从Visual Encoder上融入了和<a href="https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper" target="_blank" rel="noopener">Swin Transformer</a>同款的Window Attention, 这其实就是Qwen-VL里面实验部分的回旋镖.</p><p>Window Attention由于不做全交互, 所以在计算效率上具有一些优势. Qwen2.5-VL在Visual Encoder中只有四层仍然采用Global Attention, 其余各层采用最大窗口为$112 \times 112$(对应$8 \times 8$个Patch)的Window Attention. 所以Visual Encoder是M层Window Attention + 1一层Global Attention交替组成的:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结36.png" style="zoom:50%"> 此外, Qwen2.5-VL重新训了一个ViT, 并将其中做了一些与LLM结构的对齐设计, 比如Layer Norm替换为RMS Norm, 激活函数换SwiGLU. 训练包含一些常规的阶段, CLIP Pre-Training, Vision-Language Alignment, Finetune. 为了增强Visual Encoder的鲁棒性, 在训练期间图像会进行随机宽高比采样, 从而适应不同分辨率图像作为输入.</li><li><p><strong>Native Dynamic Resolution and Frame Rate</strong>:</p><p>与之前的Qwen-VL, Qwen2-VL不同的是, Qwen2.5-VL不再对图像 / 视频中的坐标进行重新缩放, 而是直接采用像素坐标作为输入, 不再限制模型对原生像素位置的学习能力.</p><p>与图像动态采样宽高比类似的, 对于视频输入, Qwen2.5-VL采用了动态帧率训练和绝对时间编码, 通过调整M-RoPE来增强模型对不同FPS的适应能力.</p></li><li><p><strong>Multimodal Rotary Position Embedding Aligned to Absolute Time</strong>:</p><p>在Qwen2-VL中, M-RoPE是建立在<strong>视频输入帧序号</strong>之上的, 虽然这样可以建模输入视频帧的相对位置(事件的发生前后), 但是对于视频中的绝对时间信息就没有塞进时间位置编码里.</p><p>在Qwen2.5-VL中, 不像一些其他直接使用时间戳文本作为输入的对齐方式, 而是将视频帧序号与绝对时间对齐:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结37.png" style="zoom:50%"><p>例如, 图中的2FPS, 1秒2帧, 8秒就是16帧, 对应的M-RoPE ID就是0-15. 由于还有一个Conv 3D, 会将两帧聚合为一个Token, 所以最后就只需要8帧的M-RoPE IDs. 其他FPS同理.</p><p>这里也可以理解为, 视频的3D M-RoPE中的Temporal维不再采用输入帧的序号, 而是将绝对时间映射为M-RoPE ID, 算是一个小细节上的改动吧.</p><blockquote><p>这部分改动应该是要配合动态FPS训练, 因为在动态FPS下没有绝对时间信息的问题会更加显著.</p></blockquote></li></ol><h5 id="Pre-Training"><a href="#Pre-Training" class="headerlink" title="Pre-Training"></a>Pre-Training</h5><ul><li><p><strong>Pre-Training Data</strong>:</p><p>Qwen2.5-VL在数据方面也做了不少工作, 从<strong>Qwen2-VL</strong>的1.2 trillion Token(这一数字在Qwen2-VL论文里似乎写的是1.4…)扩展到4 trillion Token. 里面除了网上爬下来的清洗数据, 还有一部分合成数据.</p><p>数据中包含多种形式: Image caption, Interleaved Image-Text data, OCR data, Visual Knowledge, Landmark, Multi-modalAcademic Questions, Localization data, Document Parsing data, Video Descriptions, Video Localization, Agent-based Interaction data.</p><ul><li><strong>Interleaved Image-Text Data</strong>: 由于交错图文对数据质量很差, 噪声很多(比如图文不相关等), 但是对MLLM又必不可少. Qwen团队使用了一套清洗交错图文对数据的方式. 先进行标准数据清理, 然后用一个<strong>内部模型</strong>分别从四阶段对数据进行打分: <strong>纯文本质量</strong>, <strong>图文相关性</strong>, <strong>图文互补性</strong>, <strong>信息密度平衡</strong>来评估.</li><li><strong>Grounding Data with Absolute Position Coordinates</strong>: 前面提到过, Qwen2.5-VL不再对Bounding Box进行标准化. 这部分除了公开数据集还引入了合成数据, 例如用<strong>Grounding DINO</strong>, <strong>SAM</strong>之类的模型直接打标.</li><li><strong>Document Omni-Parsing Data</strong>: 也是合成的. Qwen2.5-VL合成了一大批文档数据.</li><li><strong>OCR Data</strong>: 除了多语言的数据集成, 还引入了一部分合成数据. 尤其是对于图表类数据, 用<code>matplotlib</code>, <code>seaborn</code>, <code>plotly</code>等一些可视化库合成了100w样本. 还有一些是内部数据.</li><li><strong>Video Data</strong>: 在训练过程中采用动态FPS. 对于时长超过半小时的视频, 用多帧合成的方式来获得长视频的描述. 对于Video Grounding, 采用HMSF的格式统一时间戳格式.</li><li><strong>Agent Data</strong>: 感知方面的数据是通过一些收集到的页面, APP截图来合成按钮的位置, 截图描述等. 决策方面数据是从开源数据中合成的带注释的多步轨迹. 统一了移动端和桌面端的Action. 然后用人工过滤和标注了一些推理步骤.</li></ul></li></ul><p>Qwen2.5-VL的训练过程: 先只训ViT, 然后解冻所有参数做Multimodal Pre-Training, 最后再针对长上下文做Pre-Training. 采用的数据分别如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结38.png" style="zoom:50%"><h5 id="Post-training"><a href="#Post-training" class="headerlink" title="Post-training"></a>Post-training</h5><p>Qwen2.5-VL先后进行了SFT和DPO.</p><ul><li><p><strong>Data Filtering Pipeline</strong>:</p><p>为提升SFT数据质量, 进行两阶段Pipeline过滤:</p><ol><li><strong>Domain-Specific Categorization</strong>: 用另外一个Qwen2-VL衍生出的模型对QA问题进行层次化分类, 分为30个细粒度子类.</li><li><strong>Domain-Tailored Filtering</strong>: 结合Rule-based Filtering和Model-based Filtering对文档处理, OCR, Visual Grounding进行分类别的过滤. 详细看原论文吧.</li></ol></li><li><p><strong>Rejection Sampling for Enhanced Reasoning</strong>:</p><p>这部分作者没有写的很细. 主要是对一些会对模型产生误导或者让模型产生错误模式的数据被过滤掉.</p><ul><li>对于一些推理数据集, Qwen2.5-VL使用中间训练得到的Checkpoint对这部分推理数据进行对比评估, 只保留模型输出与Ground Truth相同的样本作为训练数据.</li><li>然后去掉了导致模型出现一些混合代码, 过长或者重复模式的数据.</li><li>最后作者针对只依赖单一模态的问题做了Rule-based和Model-based两种方法过滤, 但是文章里没有细提.</li></ul></li></ul><p>最后, 在进行SFT和DPO时, ViT都Frozen. SFT时在纯文本, 图文对, 视频上进行Finetune. DPO采用图文对和纯文本对齐.</p><h2 id="Rethinking-Design-of-MLLM"><a href="#Rethinking-Design-of-MLLM" class="headerlink" title="Rethinking Design of MLLM"></a>Rethinking Design of MLLM</h2><p>从Flamingo并入LLM主路的Cross Attention, 再到BLIP-2里Q - Former, 再到LLaVA直接用简单的MLP就完成视觉信号输入, 确实可以得出一些关于MLLM设计的结论, 并引导MLLM的设计走向.</p><h3 id="Prismatic-VLMs"><a href="#Prismatic-VLMs" class="headerlink" title="Prismatic VLMs"></a>Prismatic VLMs</h3><ul><li>论文: <a href="https://arxiv.org/abs/2402.07865" target="_blank" rel="noopener">Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models</a>.</li></ul><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结16.png" style="zoom:50%"><p>作者探寻了四个维度的VLM设计, 并给出了一些结论:</p><ul><li><strong>Optimization Procedure</strong>:<ul><li>Multi - Stage Training: <strong>LLaVA</strong>中是分两个阶段训练, 第一个阶段训练Projection Layer, 第二个阶段全Finetune. 本文作者发现其实直接略过第一阶段也可以, 这样还可以节省训练时间.</li><li>Full Finetuning through Visual Backbones: 在上述单阶段过程中, <strong>必须要把Visual Backbone Freeze住</strong>, 不然模型会崩掉, 尤其是Location Task(这个结论和训练数据也可能有关).</li></ul></li><li><strong>Image Processing and Pretrained Visual Representations</strong>:<ul><li>Choosing a Pretrained Vision Representation: 同规模下, 用VL Contrastive训练过的<strong>CLIP</strong>, SigLIP比纯视觉无监督DINOv2和有监督的ViT效果要好.</li><li>Image Processing across Visual Backbones: 对于CLIP而言, 简单的Resize比Crop-Resize要好, SigLIP则在简单的Resize和Letterbox Padding上表现类似. 不同模型似乎不太一样, 无法得出一个确信的结论.</li><li>Scaling Image Resolution: 高分辨率确实好.</li><li>Ensembling Different Visual Representations: 集成DINOv2和SigLIP的特征效果最好.</li></ul></li><li><strong>Language Models</strong>:<ul><li>Base vs. Instruct - Tuned LMs: 没有经过指令微调的LLM和经过指令微调的LLM表现近似, 并且没有经过指令微调的base LLM出现幻觉更少.</li><li>Do Better LMs Lead to Better VLMs?: LLM本身的表现与VLM最终表现不一定有很强的关联.</li><li>Co-training on Language-only Safety Data: 加入Text-Only的安全数据后, 性能只有一点点下降, 但是增加了不少在对话过程中的安全性.</li></ul></li><li><strong>Scaling Properties Around Training Time and Data</strong>:<ul><li>Are we Undertraining?: 很多方法都只Train了一个epoch, 实际上有些模型没有充分训练, 还可以继续训练第二个epoch.</li><li>Adding Additional Vision-Language Data: 更多样性的数据堆Scaling VLM很重要.</li></ul></li></ul><h3 id="MM1"><a href="#MM1" class="headerlink" title="MM1"></a>MM1</h3><ul><li>论文: <a href="https://arxiv.org/abs/2403.09611" target="_blank" rel="noopener">MM1: Methods, Analysis &amp; Insights from Multimodal LLM Pre-training</a>.</li></ul><p>Apple团队的工作, 除了从模型角度揭示了一些MLLM的设计表现, 也从数据角度进行了分析.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结17.png" style="zoom:67%"><p>这里直接给出论文中得出的结论:</p><ul><li><strong>Visual Encoder</strong>: <strong>图像分辨率</strong>影响最大, 然后才是Visual Backbone的Size和数据.</li><li><strong>VL Adapter</strong>: <strong>Visual Token的数量和图像分辨率</strong>最关键, <strong>VL Adapter的类型反而不太重要</strong>.</li><li><strong>Data</strong>:<ol><li><strong>交错数据</strong>对提升Few - Shot和Text - Only影响很大, Caption数据对Zero - Shot提升比较大.</li><li><strong>Text-Only</strong>数据对Few - Shot和Text - Only性能提升有帮助.</li><li><strong>合理图文数据配比</strong>能带来更好的MM性能并保持住纯文本任务上的性能.</li><li><strong>合成数据</strong>有助于Few - Shot Learning.</li></ol></li></ul><h3 id="Idefics2"><a href="#Idefics2" class="headerlink" title="Idefics2"></a>Idefics2</h3><ul><li>论文: <a href="http://arxiv.org/abs/2405.02246" target="_blank" rel="noopener">What matters when building vision-language models?</a>.</li></ul><p>来自抱抱脸团队的工作. 文中对MLLM的可能设计做了探索, 最终探索得到的模型结构如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结15.png" style="zoom:45%"><p>输入图像 -&gt; Visual Encoder -&gt; Projection + Pooling -&gt; 大图分割成小图, 且可能和文本交错 -&gt; LLM -&gt; 输出文本.</p><p>一些关键结论:</p><ol><li>固定参数量下, <strong>LLM的质量</strong>相比于Visual Backbone的质量对VLM的影响更大(这点和Prismatic VLMs有冲突).</li><li>Cross - Attention(Flamingo并入LLM主干的Cross Attention)比Autoregressive(指不改动Decoder Only的LLM本身, 比如LLaVA)的表现在单模态Backbone冻结的时候效果要好, 但当Backbone解冻的时候, 反而<strong>Autoregressive Manner会更好一些</strong>, 即使Cross - Attention吃了更多参数.</li><li>Fully Autoregressive的架构的预训练Backbone<strong>如果都解冻会导致模型发散</strong>, 用LoRA可以缓解这个问题.</li><li><strong>Visual Token过多</strong>时, 通过<strong>Learnable Pooling来减少Visual Token的数量</strong>可以很明显的增加下游任务上的推理效果和训练 / 推理速度.</li><li>对在固定大小的正方形图像预训练的Visual Backbone, 采用LoRA等方式来<strong>保留图像的原始长宽比和分辨率</strong>, 不会降低性能, 并且同时还能加速训练和推理并减少显存.</li><li>在训练时将<strong>大图变为拆分出的多张小子图和大图本身</strong>, 都作为Visual Token输入到模型当中, 可以在推理阶段用推理效率换来更多的推理性能, 尤其是在涉及到阅读图像中文本的任务中更为明显.</li></ol><hr><p>经过一系列摸索以后发现, 之前纠结的VL Adapter形式似乎并不是很重要, <strong>图像分辨率和Visual Token数</strong>是重要的, 这也会成为未来(现在已经)的主流发展方向.</p><p>除去上述三篇论文以外, 还建议大家阅读一下这个知乎问题, 里面也有很多真知灼见:</p><ul><li><a href="https://www.zhihu.com/question/626796690" target="_blank" rel="noopener">多模态大语言模型（MLLM）为什么最近的工作中用BLIP2中Q - Former结构的变少了？</a>.</li><li>也有一些工作对Q - Former的问题做了一些分析:<ul><li><a href="https://arxiv.org/abs/2405.20985" target="_blank" rel="noopener">DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models</a>.</li><li><a href="https://arxiv.org/abs/2406.08487" target="_blank" rel="noopener">Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models</a>.</li></ul></li></ul><h2 id="Do-not-Stop-Generation"><a href="#Do-not-Stop-Generation" class="headerlink" title="Do not Stop Generation!"></a>Do not Stop Generation!</h2><p>LLM主要还是面向AIGC, 生成还是有很多要做的工作.</p><h3 id="NExT-GPT"><a href="#NExT-GPT" class="headerlink" title="NExT-GPT"></a>NExT-GPT</h3><ul><li>论文: <a href="https://arxiv.org/abs/2309.05519" target="_blank" rel="noopener">NExT-GPT: Any-to-Any Multimodal LLM</a>.</li></ul><p>上面讲的都是一些VLM, 有没有扩展到更多模态的工作?</p><p>NExT-GPT提出了从<strong>文本 / 图像 / 语音 / 视频</strong>作为输入再到上述四种模态生成的LLM框架:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结18.png" style="zoom:33%"><p>能非常明显的从图中观察到, <strong>NExT-GPT是纯粹的LLM Centric</strong>, 所有其他模态的编码和生成都围绕LLM来构建. <strong>所有模态都跟Language进行了对齐</strong>, 所以可以完成任意模态到Text再到任意模态的生成.</p><p>当进行对话时, NExT-GPT能够根据对话过程中生成的<strong>特殊Token及其表示</strong>来生成对应模态的内容:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结19.png" style="zoom:40%"> <img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结20.png" style="zoom:40%"><p>特殊的Token有<code>&lt;IMG_i&gt;</code>, <code>&lt;AUD_i&gt;</code>, <code>&lt;VID_i&gt;</code>, 这些特殊Token的表示会被作为每种模态的Diffusion里面的<strong>Condition</strong>来引导对应模态生成.</p><p>作者用ImageBird当做编码器, 同时编码Image, Audio, Video作为输入.</p><p>在优化模型时, 主要做了Encoder和Decoder两侧的模态对齐.</p><p>因为要将所有模态都和Text对齐, 所以可以拿除文本以外的模态Encoder抽取出的表示经过Projection, 在各模态Caption的过程中完成<strong>Encoder侧对齐</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结21.png" style="zoom:40%"><p>同样的, 由于有各模态Diffusion的Caption, 可以将Diffusion的Text Encoder对Caption编码后的表示(Condition)与LLM输出的特殊Token经过Projection后的表示对齐, 从而完成<strong>Decoder侧对齐</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结22.png" style="zoom:40%"><p>为了让NExT-GPT在对话过程中拥有模态迁移的能力, 还需要进行Modality-switching Instruction Tuning. 对LLM做LoRA Tuning, 并设计几种<strong>模态切换</strong>的对话场景:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结23.png" style="zoom:50%"><p>指令数据集根据GPT4和各种模态的Caption共同构造, 之后再人工筛选一下.</p><h3 id="DreamLLM"><a href="#DreamLLM" class="headerlink" title="DreamLLM"></a>DreamLLM</h3><ul><li>论文: <a href="https://arxiv.org/abs/2309.11499" target="_blank" rel="noopener">DREAMLLM: SYNERGISTIC MULTIMODAL COMPREHENSION AND CREATION</a>.</li></ul><p>DreamLLM提出了一种能够处理图文交错数据的方法, 并且这种方法能在原始多模态空间中采样生成:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm%E6%80%BB%E7%BB%9324.png" alt=""></p><p>当DreamLLM需要生成图像的时候, 会生成一个特殊的Dream Token <code>&lt;dream&gt;</code>.</p><p>一旦生成这个Token, 就说明LLM需要生成图像了. 接着会有若干个Dream Query以Autoregressive Manner喂给到LLM里, 最后这组Dream Query的表示会被用于生成Diffusion Model的Condition. 生成的图像会被Visual Encoder和Projection重新提取为Visual Embedding输入到模型中.</p><p>由于这种方式是直接生成或处理的图像, 不需要用CLIP之类的模型对Visual Embedding进行对齐, 所以整个生成过程都是在原始多模态空间中进行的.</p><p>训练分为三个阶段, 整个过程Diffusion Model都是冻住的:</p><ol><li><strong>Alignment Training</strong>: Linear Visual Projector, Linear Condition Projector和Learnable Dream Embedding解冻, 用30M图文对对齐所有Projector和Dream Query, 在这个期间做图文理解和文图生成. 这样就可以把除了LLM以外的所有组件初步对齐到一个偏于LLM的Multimodal Space.</li><li><strong>I-GPT Pretraining</strong>: 解冻LLM, 做大规模文本图像的联合生成式建模, 将所有组件对齐到真正的Multimodal Space.</li><li><strong>Supervised Fine-tuning</strong>: 加上指令按下游任务Finetune.</li></ol><h3 id="Joint-Interleaved-Image-Text-Generation"><a href="#Joint-Interleaved-Image-Text-Generation" class="headerlink" title="Joint / Interleaved Image-Text Generation"></a>Joint / Interleaved Image-Text Generation</h3><p><a href="https://arxiv.org/abs/2310.12973" target="_blank" rel="noopener">本身LLM其实是一个Universal Decoder</a>, 它的能力一定不止于Text Generation. 如果能够<strong>直接统一Visual Token和Text Token生成的方式</strong>, 则会带来更灵活的生成效果. <strong>有多种方式可以达到这个效果</strong>, 其中一种就是用VQ(Vector Quantization)把视觉图像变成离散化Token, 将Codebook里的离散Token.</p><blockquote><p>当然, VQ这个过程不是必要的, 因为有很多种方式可以达到这个效果, 从DreamLLM里就可以看出来.</p></blockquote><p>因此, 在需要生成图像的时候, 只需要MLLM预测出对应的Visual Token, 然后再交给Diffusion之类的Visual Decoder做个解码, 就能拿到需要的图像了. 这样做的好处是整个Token Space对Vision和Language都是Joint Training的, 能达到非常灵活的输出效果…</p><p>比如说在对话过程中, 模型发现用户难理解就直接出个图, 甚至也可以从用户输入的图像中截取一部分做辅助说明(生成Visual Token, 然后再Decode回去), 也可以同时在对话中处理图像并返还给用户.</p><p>再结合Flamingo, Idefics2和MM1中发现的交错数据带来的提升, 以及DreamLLM这种交错生成框架的出现, 如果这条线能做Work的话, 能够推测出Joint / Interleaved Generation的时代就要来了, 我比较看好它和Diffusion结合在一起的发挥.</p><p>这个部分暂时没什么时间, 没法串起来讲一下, 因为补这条线需要从VQ-VAE先开始讲起, 而且它还在不断演进中… 所以只能给大家一个阅读思路:</p><ul><li>预备知识:<ul><li>VQ-VAE: <a href="https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html" target="_blank" rel="noopener">Neural Discrete Representation Learning</a>.</li><li>VQ-GAN: <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.html?ref=" target="_blank" rel="noopener">Taming Transformers for High-Resolution Image Synthesis</a>.</li></ul></li><li>SPAE: <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/a526cc8f6ffb74bedb6ff313e3fdb450-Abstract-Conference.html" target="_blank" rel="noopener">SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs</a>.</li><li>VLTokenizer: <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhu_Beyond_Text_Frozen_Large_Language_Models_in_Visual_Signal_Comprehension_CVPR_2024_paper.html" target="_blank" rel="noopener">Beyond Text: Frozen Large Language Models in Visual Signal Comprehension</a>.</li><li>LaVIT: <a href="https://arxiv.org/abs/2309.04669" target="_blank" rel="noopener">UNIFIED LANGUAGE-VISION PRETRAINING IN LLM WITH DYNAMIC DISCRETE VISUAL TOKENIZATION</a>.</li><li>SEED系列:<ul><li>SEED: <a href="https://arxiv.org/abs/2307.08041" target="_blank" rel="noopener">Planting a SEED of Vision in Large Language Model</a>.</li><li>SEED-LLaMA: <a href="https://arxiv.org/abs/2310.01218" target="_blank" rel="noopener">Making llama see and draw with seed tokenizer</a>.</li><li>SEED-X: <a href="https://arxiv.org/abs/2404.14396" target="_blank" rel="noopener">SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation</a></li></ul></li><li>Chameleon: <a href="https://arxiv.org/abs/2405.09818" target="_blank" rel="noopener">Chameleon: Mixed-Modal Early-Fusion Foundation Models</a>.</li><li>TiTok: <a href="https://arxiv.org/abs/2406.07550" target="_blank" rel="noopener">An Image is Worth 32 Tokens for Reconstruction and Generation</a>.</li><li>SETOKIM: <a href="https://arxiv.org/abs/2406.05127" target="_blank" rel="noopener">Towards Semantic Equivalence of Tokenization in Multimodal LLM</a>.</li></ul></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/64567.html">https://ADAning.github.io/posts/64567.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/MLLM/"><span class="chip bg-color">MLLM</span> </a><a href="/tags/MM/"><span class="chip bg-color">MM</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/18380.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/18.jpg" class="responsive-img" alt="Introduction: Vector Quantization"> <span class="card-title">Introduction: Vector Quantization</span></div></a><div class="card-content article-content"><div class="summary block-with-text">Introduction: Vector QuantizationVector QuantizationAutoEncoder(AE)由Encoder和Decoder组成, Encoder将图像压缩为一个低维的隐向量(Latent), 再由</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2024-07-16 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/VQ/"><span class="chip bg-color">VQ</span> </a><a href="/tags/VQ-VAE/"><span class="chip bg-color">VQ-VAE</span> </a><a href="/tags/VQ-GAN/"><span class="chip bg-color">VQ-GAN</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/683.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/12.jpg" class="responsive-img" alt="通用信息抽取(下) - UniEX, Mirror, RexUIE"> <span class="card-title">通用信息抽取(下) - UniEX, Mirror, RexUIE</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: 通用信息抽取(上) - UIE, USM, InstructUIE. 通用信息抽取(下) - UniEX, Mirror, RexUIE本文为介绍通用信息抽取领域经典模型的下篇, 将会介绍了UniEX, Mirror</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2024-05-29 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/NER/"><span class="chip bg-color">NER</span> </a><a href="/tags/EE/"><span class="chip bg-color">EE</span> </a><a href="/tags/IE/"><span class="chip bg-color">IE</span> </a><a href="/tags/UIE/"><span class="chip bg-color">UIE</span> </a><a href="/tags/RE/"><span class="chip bg-color">RE</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">430.8k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:andaning@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>