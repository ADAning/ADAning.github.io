<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="KBAT: Learning Attention-based Embeddings for Relation Prediction in KGs, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>KBAT: Learning Attention-based Embeddings for Relation Prediction in KGs | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/2.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">KBAT: Learning Attention-based Embeddings for Relation Prediction in KGs</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/KGE/"><span class="chip bg-color">KGE</span> </a><a href="/tags/GNN/"><span class="chip bg-color">GNN</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/" class="post-category">知识图谱</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2021-04-04</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2022-03-27</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 2.9k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 11 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p>本文前置知识:</p><ul><li>GAT: 详见<a href="https://adaning.github.io/posts/31958.html">图神经网络入门</a>.</li></ul></blockquote><h1 id="Learning-Attention-based-Embeddings-for-Relation-Prediction-in-Knowledge-Graphs"><a href="#Learning-Attention-based-Embeddings-for-Relation-Prediction-in-Knowledge-Graphs" class="headerlink" title="Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs"></a>Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs</h1><p>本文是论文<a href="https://arxiv.org/abs/1906.01195" target="_blank" rel="noopener">Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者观察到, 虽然基于CNN的模型能生成关于三元组更丰富的信息, 但处理三元组时是<strong>独立</strong>的, 并没有覆盖<strong>周围邻居</strong>的复杂信息. 现有的方法要么只关注实体特征, 要么不会对实体和关系的特征共同嵌入.</p><p>作者希望利用<strong>Attention机制</strong>来共同捕捉节点与邻居的实体和关系特征.</p><h2 id="KBAT"><a href="#KBAT" class="headerlink" title="KBAT"></a>KBAT</h2><h3 id="Graph-Attention-Network"><a href="#Graph-Attention-Network" class="headerlink" title="Graph Attention Network"></a>Graph Attention Network</h3><blockquote><p>这部分对GAT有基础的可以直接跳过, 作者在原论文中描述GAT的式子与GAT本身论文中没区别.</p></blockquote><p>对于有$N$ 个节点的图, 输入节点特征为$\mathbf{x}=\left\{\vec{x}_{1}, \vec{x}_{2}, \ldots, \vec{x}_{N}\right\}$, 在经过一层GNN变换后, 节点特征被更新为$\mathbf{x}^{\prime}=\left\{\vec{x}_{1}^{\prime}, \vec{x}_{2}^{\prime}, \ldots, \vec{x}_{N}^{\prime}\right\}$.</p><p>在注意力机制中, 边$(e_i, e_j)$ 的注意力值$e_{ij}$ 由下式决定:<br>$$<br>e_{i j}=a\left(\mathbf{W} \overrightarrow{x_{i}}, \mathbf{W} \overrightarrow{x_{j}}\right)<br>$$<br>$\mathbf{W}$ 为线性变换矩阵, 能将节点特征投影到更高的维度, $a$ 是可选择的Attention Function.</p><p>在更新节点隐态信息时, 将在GCN的基础上对节点的邻居<strong>加权聚合</strong>:<br>$$<br>\overrightarrow{x_{i}^{\prime}}=\sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j} \mathbf{W} \overrightarrow{x_{j}}\right)<br>$$<br>$\alpha_{ij}$ 是用<strong>Softmax</strong>对节点$i$ 所有邻居$j$ 的注意力值由$e_{ij}$ 计算得到的<strong>注意力权重</strong>.</p><p>与Transformer类似, GAT中引入了<strong>多头注意力</strong>, 更新方程如下:<br>$$<br>\overrightarrow{x_{i}^{\prime}}=\lVert_{k=1}^{K} \sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k} \overrightarrow{x_{j}}\right)<br>$$<br>$\lVert$ 代表Concat操作, $\sigma$ 为非线性函数, $K$ 代表多头的头数. $\alpha_{ij}^k$ 代表第$k$ 个头内归一化的注意力权重, $\mathbf{W}^k$ 是第$k$ 个头的线性变换矩阵.</p><p>最后一层将GAT输出作为Embedding, 将多头的信息通过<strong>平均</strong>的方式整合:<br>$$<br>\overrightarrow{x_{i}^{\prime}}=\sigma\left(\frac{1}{K} \sum_{k=1}^{K} \sum_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k} \overrightarrow{x_{j}}\right)<br>$$</p><h3 id="Relations-are-Important"><a href="#Relations-are-Important" class="headerlink" title="Relations are Important"></a>Relations are Important</h3><p>在KG中, 相同的实体在不同关系的作用下意义会变得完全不一样, 所以<strong>关系嵌入</strong>是非常重要的.</p><p>例如, 在下述场景中, <code>Christopher Nolan</code>出现在了两个不同的三元组中, 所对应的关系分别是<code>directed</code>和<code>brother_of</code>, 他的身份分别是导演和哥哥:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kbat1.jpg" style="zoom:50%"><p>因此, 在将GAT引入KGE时, 必须将<strong>关系嵌入</strong>也一并纳入注意力的计算范畴.</p><p>在每层GAT中, $\mathbf{H}, \mathbf{G}$ 分别代表输入时实体嵌入和关系嵌入矩阵. $\mathbf{H}^\prime, \mathbf{G}^\prime$ 为输出时实体嵌入, 关系嵌入矩阵. 特别定义$t_{ij}^k=(e_i, r_k, e_j)$ 为三元组.</p><p>我们从输入实体嵌入矩阵$\mathbf{H}$ 中取出实体表示$\vec{h_i}, \vec{h_j}$, 从输入关系嵌入矩阵$\mathbf{G}$ 中取出关系表示$\vec{g_k}$, 获取三元组$t_{ij}^k$ 整体在嵌入空间中的表示$\vec{c_{i j k}}$:<br>$$<br>\vec{c_{i j k}}=\mathbf{W}_{1}\left[\vec{h}_{i}\lVert\vec{h}_{j}\lVert \vec{g}_{k}\right]<br>$$<br>$\mathbf{W}_1$ 为线性变换矩阵.</p><p>然后利用整体表示再次经过变换, 对$c_{ijk}$ 变换后用<strong>LeakyReLU</strong>过滤得到Attention Score $b_{ijk}$:<br>$$<br>b_{i j k}=\operatorname{LeakyReLU}\left(\mathbf{W}_{2} c_{i j k}\right)<br>$$<br>$\mathbf{W}_2$ 是线性变换矩阵.</p><p>根据Attention Score, 用Softmax可以得出三元组的Attention Weight $\alpha_{ijk}$:<br>$$<br>\begin{aligned}<br>\alpha_{i j k} &amp;=\operatorname{softmax}_{j k}\left(b_{i j k}\right) \\<br>&amp;=\frac{\exp \left(b_{i j k}\right)}{\sum_{n \in \mathcal{N}_{i}} \sum_{r \in \mathcal{R}_{i n}} \exp \left(b_{i n r}\right)}<br>\end{aligned}<br>$$<br>$\mathcal{N}_i$ 代表节点$i$ 的邻居节点集, $\mathcal{R}_{in}$ 代表节点$i$ 与邻居$n$ 之间的关系集.</p><p>因此, KBAT的前半部分与GAT相比, 只是把关系嵌入$g_k$ 塞进了GAT中:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kbat3.jpg" style="zoom:33%"><p>在更新节点表示时, 对实体$i$ 为头实体的三元组$\vec{c_{ijk}}$ 加权求和得到新的节点表示:<br>$$<br>\overrightarrow{h_{i}^{\prime}}=\sigma\left(\sum_{j \in \mathcal{N}_{i}} \sum_{k \in \mathcal{R}_{i j}} \alpha_{i j k} \vec{c_{i j k}}\right)<br>$$<br>和GAT相同, 也可以采用<strong>多头注意力</strong>的方式来增强GAT的稳定性:<br>$$<br>\overrightarrow{h_{i}^{\prime}}=\lVert_{m=1}^{M} \sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j k}^{m} c_{i j k}^{m}\right)<br>$$<br>其中$\lVert$ 代表Concat操作, $M$ 为注意力头数.</p><p>我们已经获取了实体在单层GAT的更新后的表示, 也需要对关系做一些改变, 保证节点嵌入更新的同时, 关系嵌入也是在更新的:<br>$$<br>G^{\prime}=G \cdot \mathbf{W}^{R}<br>$$<br>$\mathbf{W}^R$ 是权重矩阵, 可以改变关系嵌入的维度.</p><h3 id="Final-Output-Layer"><a href="#Final-Output-Layer" class="headerlink" title="Final Output Layer"></a>Final Output Layer</h3><p>在最终KBAT输出时, 需要聚合多头注意力的信息, 并对节点表示做进一步处理.</p><p>输出节点表示时, 与GAT类似, 将多个头的信息直接<strong>求平均</strong>聚合作为节点表示:<br>$$<br>\overrightarrow{h_{i}^{\prime}}=\sigma\left(\frac{1}{M} \sum_{m=1}^{M} \sum_{j \in \mathcal{N}_{i}} \sum_{k \in \mathcal{R}_{i j}} \alpha_{i j k}^{m} c_{i j k}^{m}\right)<br>$$<br>但是初始信息可能会随着提取特征而丢失, 作者在最终的输出前把初始节点嵌入$\mathbf{H}^t$ 再加回来:<br>$$<br>\mathbf{H}^{\prime \prime}=\mathbf{W}^{E} \mathbf{H}^{t}+\mathbf{H}^{f}<br>$$<br>$\mathbf{W}^E$ 是为了让维度能够匹配的变换矩阵, $\mathbf{H}^f$ 为最终层前的KBAT节点表示输出.</p><blockquote><p>其实就是类似<strong>残差</strong>的方法, 但是在原论文中却对残差只字未提.</p></blockquote><p>至此, 整个KBAT的运算过程如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kbat4.jpg" style="zoom:50%"><p>图中绿色圆圈为关系嵌入, 黄色圆圈为实体嵌入. 对图中的流程总结描述如下:</p><ol><li>先将头实体, 尾实体, 关系的嵌入<strong>拼接</strong>起来.</li><li>经过GAT层的<strong>多头注意力</strong>后得到两个<strong>实体更新</strong>后的表示, 同时将<strong>关系嵌入</strong>做一次<strong>更新</strong>.</li><li>在下个GAT层前, 将更新后的头实体, 尾实体, 关系表示<strong>拼接</strong>起来.</li><li>再次经过<strong>GAT</strong>层. 得到两个实体的输出.</li><li>将最初的实体嵌入通过一次<strong>投影</strong>到与最终输出维度相匹配的维度, 和GAT输出的实体表示<strong>相加</strong>.</li><li>利用关系嵌入和实体嵌入共同计算<strong>Loss</strong>.</li></ol><p>最后, 作者谈了一下多层GAT能够聚合N阶邻居信息的优势:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kbat2.jpg" style="zoom:50%"><p>在图中, 多阶邻居的信息能够以多次迭代的方式被有可能相关联的实体所捕捉.</p><blockquote><p>这种优势也只来自于GAT本身, 并不是由于KBAT的特殊性.</p></blockquote><h3 id="Training-Objective"><a href="#Training-Objective" class="headerlink" title="Training Objective"></a>Training Objective</h3><p>训练目标和TransE完全一致, 损失函数采用<strong>Hinge Loss</strong>:<br>$$<br>L(\Omega)=\sum_{t_{i j} \in S} \sum_{t_{i j}^{\prime} \in S^{\prime}} \max \left\{d_{t_{i j}^{\prime}}-d_{t_{i j}}+\gamma, 0\right\}<br>$$<br>$\gamma$ 为间隔, 距离的度量函数采用TransE的一范数, 即$d_{t_{ij}} = \lVert \vec{h_i} + \vec{g_k} - \vec{h_j} \rVert_1$.</p><p>也同样使用了负采样, $S$ 为有效三元组, $S^\prime$ 为负样本三元组:<br>$$<br>S^{\prime}=\underbrace{\left\{t_{i^{\prime} j}^{k} \mid e_{i}^{\prime} \in \mathcal{E} \backslash e_{i}\right\}}_{\text {replace head entity }} \cup \underbrace{\left\{t_{i j^{\prime}}^{k} \mid e_{j}^{\prime} \in \mathcal{E} \backslash e_{j}\right\}}_{\text {replace tail entity }}<br>$$</p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>KBAT在做链接预测时仍然像其他图方法一样, 沿用了<strong>Encoder - Decoder</strong>架构.</p><p>作者的模型中使用<a href="https://adaning.github.io/posts/52280.html">ConvKB</a>作为解码器, 作者的目标是使用卷积层捕获三元组$t_{ij}^k$ 的全局信息. ConvKB的打分函数如下:<br>$$<br>f\left(t_{i j}^{k}\right)=\left(\prod_{m=1}^{\Omega} \operatorname{ReLU}\left(\left[\vec{h}_{i}, \vec{g}_{k}, \vec{h}_{j}\right] \ast \omega^{m}\right)\right) \cdot \mathbf{W}<br>$$<br>其中$\omega^m$ 为第$m$ 个卷积核, $\Omega$ 为卷积核总数, $\ast$ 为卷积操作. $\mathbf{W}$ 代表获取最终三元组得分的线性变换矩阵.</p><p>然后用软间隔损失来优化ConvKB:<br>$$<br>\mathcal{L}=\sum_{t_{i j}^{k} \in\left\{S \cup S^{\prime}\right\}} \log \left(1+\exp \left(l_{t_{i j}^{k}} \cdot f\left(t_{i j}^{k}\right)\right)\right)+\frac{\lambda}{2}|\mathbf{W}|_{2}^{2} \\<br>\text { where } l_{t_{i j}^{k}}=\left\{\begin{array}{ll}<br>1 &amp; \text { for } t_{i j}^{k} \in S \\<br>-1 &amp; \text { for } t_{i j}^{k} \in S^{\prime}<br>\end{array}\right.<br>$$</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文.</p><blockquote><p><strong>这篇论文的实验结果有很大问题, 在文末的Summary里会提到.</strong></p></blockquote><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><p>作者将训练过程分为两步:</p><ol><li>先用Hinge Loss单独训练KBAT作为<strong>Encoder</strong>, 使得GAT有编码能力.</li><li>再用Soft Margin Loss训练ConvKB作为<strong>Decoder</strong>, 使得ConvKB有解码能力.</li></ol><blockquote><p>原文并没有说是KBAT和ConvKB联合训练还是单独训练ConvKB, 但在代码中是ConvKB直接加载了已经训练好的KBAT的Embedding, 然后继续训练. 也就是直接使用KBAT的Embedding当做ConvKB的Embedding<strong>初始化</strong>, 同时训练Embedding和ConvKB, <strong>并没有对Embedding做冻结</strong>.</p><p>作者针对此问题有<a href="https://github.com/deepakn97/relationPrediction/issues/14" target="_blank" rel="noopener">回复</a>, 但我仍感觉这样做可能会把KBAT提取好的Embedding信息<strong>破坏</strong>掉, 而非在KBAT的基础上去找ConvKB的更优解.</p></blockquote><p>作者在五个常用数据集做了链接预测实验, 统计信息如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kbat5.jpg" style="zoom:50%"><p>在WN18RR和FB15k - 237上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kbat6.jpg" style="zoom:33%"><p>R - GCN在FB15k - 237和WN18RR上的结果没有超过ConvE, 但KBAT超过了. WN18RR上的Hits@1表现不太好. 作者在后面分析了原因.</p><p>在NELL - 995和Kinship上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kbat7.jpg" style="zoom:33%"><p>在这两个数据集上来看KBAT效果不错, R - GCN就不太行.</p><h3 id="PageRank-Analysis"><a href="#PageRank-Analysis" class="headerlink" title="PageRank Analysis"></a>PageRank Analysis</h3><p>作者假设复杂的多跳信息在稠密图中比在稀疏图中要更难捕获, 作者设计了类似ConvE中的实验, 探究了PageRank对模型的影响, 即测试将DistMult替换成KBAT后MRR的相对变化:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kbat10.jpg" style="zoom:33%"><p>基本上PageRank越高, KBAT进步就越明显. 作者将在WN18RR上表现不好归因于WN18RR的高度稀疏和分级结构.</p><h3 id="Attention-Values-vs-Epochs"><a href="#Attention-Values-vs-Epochs" class="headerlink" title="Attention Values vs Epochs"></a>Attention Values vs Epochs</h3><p>下图是FB15k - 237中注意力值随epoch增大的变化情况:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kbat8.jpg" style="zoom:50%"><p>下图是WN18RR中注意力值随epoch增大的变化情况:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kbat9.jpg" style="zoom:50%"><p>总体来说, 随epoch的增大, 多跳邻居会被分配给更多的注意力.</p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>作者逐渐移除模型中的信息, 做出NELL - 995上MR随epoch的变化曲线:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kbat11.jpg" style="zoom:50%"><p>红色是作者的模型曲线, 绿色是移除多跳路径信息的曲线, 蓝色是移除关系信息的曲线.</p><blockquote><p>作者也没说如何移除, 估计是用随机初始化来代替.</p></blockquote><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>KBAT将<strong>GAT</strong>引入了KGE领域, 结合KG中的十分重要的<strong>关系</strong>, 利用<strong>图注意力网络</strong>的聚合特性, 获取实体之间的<strong>多跳</strong>信息, 并取得了比R - GCN更好的效果.</p><p>虽然有实验, 但是本论文除了将GAT引入KG领域中, 感觉没有什么创新点… 文中有很大篇幅都在介绍GAT. 有一些小改动也没有给出说明.</p><p>后续有一些<a href="https://arxiv.org/abs/1911.03903" target="_blank" rel="noopener">论文</a>说明KBAT给出的代码导致了Test Data存在Leakage, 详见<a href="https://github.com/deepakn97/relationPrediction/issues/28" target="_blank" rel="noopener">此处</a>, 所以实验结果大多<strong>站不住脚</strong>, 很多疑问都没有必要再追究了. 但使用GAT在思路上应该仍然有效.</p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/37736.html">https://ADAning.github.io/posts/37736.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/KGE/"><span class="chip bg-color">KGE</span> </a><a href="/tags/GNN/"><span class="chip bg-color">GNN</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/21954.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/11.jpg" class="responsive-img" alt="ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for KGE"> <span class="card-title">ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for KGE</span></div></a><div class="card-content article-content"><div class="summary block-with-text">ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for KGE本文是论文ReInceptionE:</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2021-04-10 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/" class="post-category">知识图谱</a></span></div></div><div class="card-action article-tags"><a href="/tags/KGE/"><span class="chip bg-color">KGE</span> </a><a href="/tags/CNN/"><span class="chip bg-color">CNN</span> </a><a href="/tags/GNN/"><span class="chip bg-color">GNN</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/45769.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/0.jpg" class="responsive-img" alt="CompGCN: Composition-based Multi-Relational Graph Convolutional Networks"> <span class="card-title">CompGCN: Composition-based Multi-Relational Graph Convolutional Networks</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: GCN: 详见图神经网络入门 R - GCN: 详见R - GCN: Modeling Relational Data with Graph Convolutional Networks Composition-ba</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2021-04-03 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/" class="post-category">知识图谱</a></span></div></div><div class="card-action article-tags"><a href="/tags/KGE/"><span class="chip bg-color">KGE</span> </a><a href="/tags/GNN/"><span class="chip bg-color">GNN</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">433.1k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:andaning@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>