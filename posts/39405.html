<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="机器学习之逻辑回归与线性回归, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>机器学习之逻辑回归与线性回归 | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:.6"></i> <span>时间轴</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li><li><a href="/markdown"><i class="fab fa-markdown" style="margin-top:-20px;zoom:.6"></i> <span>Markdown</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li><li><a href="/markdown" style="margin-left:75px"><i class="fa fab fa-markdown" style="position:absolute;left:50px"></i> <span>Markdown</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/19.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">机器学习之逻辑回归与线性回归</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"><span class="chip bg-color">线性回归</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="post-category">机器学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2020-08-05</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2022-04-03</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 2k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 7 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p><strong>2020.08.22</strong>: 附加了后续的逻辑回归部分.</p></blockquote><h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>想要了解逻辑回归，必须了解线性回归.</p><h2 id="线性回归-Logistcs-Regression"><a href="#线性回归-Logistcs-Regression" class="headerlink" title="线性回归 Logistcs Regression"></a>线性回归 Logistcs Regression</h2><p>线性回归是监督学习中最简单的模型了, 它具有非常好的<strong>可解释性</strong>, 也有一种简洁的典雅美. 在机器学习中, 不再区分一元和多元线性回归. 线性回归可以用来解决回归问题, 当然也可以通过一个函数来解决<strong>分类</strong>问题.<br>$$<br>\hat{y} = w_1x_1 + w_2x_2+\cdots + w_nx_n + b<br>$$<br>每个$x$ 代表一维度的特征, $w$ 代表这一维特征对预测目标$\hat{y}$ 影响所占的权重. $b$ 是一个偏置.</p><p>将以上方程用矩阵形式描述:</p><p>$$<br>Y = W^TX +b<br>$$<br>我们要求解的就是权重矩阵$W$和偏置$b$. 假设损失函数为均方误差$\rm MSE$, 代入损失函数有:<br>$$<br>\ell(w_i, b) =\frac{1}{n}\sum\limits_{i=1}^n(w_ix_i+b-\hat{y})^2<br>$$<br>那么为了最小化损失函数, 最终目标为:<br>$$<br>(w_i’, b) = \mathop{\arg\min}_{w_i, b}\ell(w_i, b)<br>$$<br>也就是求出最终能使损失最小化的解$(w_i’, b)$. 如何求出这个最优解呢? 我们需要借助优化方法.</p><h2 id="最小二乘法-Least-Squares-Method"><a href="#最小二乘法-Least-Squares-Method" class="headerlink" title="最小二乘法  Least Squares Method"></a>最小二乘法 Least Squares Method</h2><p>最小二乘法其实是线性回归最早的调整权重的方法(估计高中都接触过). 最小二乘法的想法非常朴实, 既然是<strong>线性问题</strong>, 想要让目标函数最小, 直接对损失函数求导, 令其导数为0, 利用和特征数量相等的样本数, 解一个多元方程组, 自然能够得到所有参数的解. 那样本不够呢? 函数不是线性的呢? 而且看起来样本数量很大时直接解这个方程组也不是很现实. 这时候就必须换一种思路, 不是一步登天, 而是通过某种迭代的方式, 慢慢逼近结果.</p><h2 id="梯度下降-Gradient-Descent"><a href="#梯度下降-Gradient-Descent" class="headerlink" title="梯度下降 Gradient Descent"></a>梯度下降 Gradient Descent</h2><p>沿梯度的方向前进, 函数值上升的速度最快. 那么沿着<strong>负梯度</strong>的方向前进, 就能让函数值以最快的速度下降. 对于每个参数, 只需要求出损失函数对它的偏导数, 就能调整参数值.</p><p>$$<br>\nabla f(x, y)=(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y})<br>$$<br>我们可以手动设置一个学习率$\alpha$, 来分别调整每个$w_i$和每个$b$的参数值, 经过多次迭代使得其值收敛与某个能够成功拟合问题的值. 这里直接使用梯度下降, 快速调整参数.<br>$$<br>\begin{aligned}<br>w_i &amp;\leftarrow w_i - \alpha\frac{\partial\ell(w_i, b)}{\partial{w_i}}\\<br>b&amp;\leftarrow b - \alpha\frac{\partial\ell(w_i, b)}{\partial{b}}<br>\end{aligned}<br>$$</p><h3 id="批量梯度下降-Batch-gradient-descent"><a href="#批量梯度下降-Batch-gradient-descent" class="headerlink" title="批量梯度下降 Batch gradient descent"></a>批量梯度下降 Batch gradient descent</h3><p><strong>逐个地</strong>在每个数据点应用均方(或绝对)误差, 并重复这一流程很多次. 它对所有样本进行迭代实现了并行, 同时用总体样本指明了下降的方向, 很有可能找到全局最优. 缺点就是当样本很多时, 每次都要对所有样本迭代导致训练缓慢.</p><h3 id="随机梯度下降-Stochastic-gradient-descent"><a href="#随机梯度下降-Stochastic-gradient-descent" class="headerlink" title="随机梯度下降 Stochastic gradient descent"></a>随机梯度下降 Stochastic gradient descent</h3><p><strong>同时</strong>在每个数据点应用均方(或绝对)误差, 并重复这一流程很多次. 这样每次只对一个样本进行迭代, 训练速度会加快, 但不易于并行, 且由于个体样本的差异不容易学习到总体样本的特性.</p><h3 id="小批次梯度下降法-Mini-batch-gradient-descent"><a href="#小批次梯度下降法-Mini-batch-gradient-descent" class="headerlink" title="小批次梯度下降法 Mini batch gradient descent"></a>小批次梯度下降法 Mini batch gradient descent</h3><p>批量梯度下降和随机梯度下降的<strong>折中</strong>, 上述二者都是极端情况, 一个使用所有样本, 一次只使用一个样本. 所以为什么不去取折中的办法, 取一个合适的迭代样本数量<strong>batch_size</strong> , 每次都迭代batch_size个样本. 它具有上述两种算法的优点平均, 缺点仅有batch_size设置不当会导致问题.</p><ul><li>如果batch_size在合理范围内增大, 有以下好处<ol><li>内存利用率提高.</li><li>每个epoch所用的时间少了, 也就是训练时间减少.</li><li>准确率增加, 震荡情况减少.</li></ol></li><li>盲目增大batch_size的坏处:<ol><li>内存可能扛不住.</li><li>花费时间变多, 逐渐变向批次梯度下降.</li><li>batch_size当增大到一定阈值时, 下降的方向已经不发生变化, 这时盲目增大batch_size是无用功.</li></ol></li></ul><h2 id="逻辑回归-Logistics-Regression"><a href="#逻辑回归-Logistics-Regression" class="headerlink" title="逻辑回归 Logistics Regression"></a>逻辑回归 Logistics Regression</h2><p>逻辑回归是在线性回归基础上的延伸. 逻辑回归虽然叫做回归，可是却是一个<strong>分类模型</strong>. 线性模型虽然简单, 但却含有丰富的变化. 应该考虑用某个函数来使得线性回归来适应分类问题. 不然分类会变得十分困难，比如在拟合单位阶跃函数时，回归问题就不能迁移到分类问题上来.<br>$$<br>y=<br>\begin{cases}<br>0, \quad y&lt;z; \\<br>0.5, \quad z=0; \\<br>1, \quad z &gt;0;<br>\end{cases}<br>$$<br>线性模型虽然简单，但是在预测$y$ 的衍生物时却显得有些无力. 比如在学习$y$ 的对数作为目标时就显得有些无力. 但是可以对线性回归做相应的变化，使得其容易逼近.<br>$$<br>\ln y=W^TX+b<br>$$<br>这就是<strong>对数几率回归</strong>，在形式上仍然是线性回归，但实际上已经是在求输入到输出的<strong>非线性映射</strong>了. 这里的对数起到了关联输出和输入的作用. 对于更普遍的输出，只需要考虑单调可微的函数$g(\cdot)$ :<br>$$<br>y = g^{-1}(W^TX+b)<br>$$<br>这样称为<strong>广义线性模型</strong>，对任意要逼近的目标都是适用的.<br>阶跃函数是不连续，不可微的. 使用对数几率函数来代替这个函数，使得其满足可微性质.<br>$$<br>y=\frac{1}{1+e^{-z}}<br>$$<br>对数几率函数也是神经网络中常说的$\sigma$ 激活函数，在神经网络中起到至关重要的作用.<br>它能够将任意输入放缩到$(0, 1)$ 之间，视为概率.<br>将$\sigma$ 函数带入广义线性模型中，能够得到:<br>$$<br>y=\frac{1}{1+e^{-(W^TX+b)}}<br>$$<br>那么将其转换为线性回归的形式:<br>$$<br>\ln\frac{y}{1-y}=W^TX+b<br>$$<br>如果预测值$y$ 是样本$x$ 为正例的可能性，那么$1-y$ 即是其反例可能性. 二者的比值称为<strong>几率</strong>，再对其取对数，对数几率函数的名字便由此诞生了. 想要训练这个分类模型，就不能单单再用之前定义过的MSE作为损失函数了. 根据极大似然，只要以类别概率的对数作为损失函数，通过迭代更新参数就行了. 设数据集中样本数为$m$损失函数是:<br>$$<br>\ell(w,b)=\sum_{i=1}^m \ln p(y_i\mid x_i;w, b)<br>$$<br>如果想要实现多分类，有多种方法可以实现.</p><ol><li>将Sigmoid函数替换为Softmax函数即可，然后对应的改变损失函数.</li><li>采用<code>One vs All</code>的方法，假设要为三类，则要分别训练三个分类器，每个都将自己要识别的类视为正例，其余两个类全部视为反例. 最终预测时从三个分类器中取最高概率作为该样本的类别. 对于$k$ 类，只需要$k$个分类器. 这么做会导致天然的训练集样本不平衡. 当选择其中一个类为正类时，其余两个类的样本都是反类.</li><li>采用<code>One vs One</code>的方法，假设要分三类，则像车轮战一样，每次只训练两个类的样本，最后通过三个分类器的投票来决定样本点的类别. 最终需要$C_k^2$个分类器. 这样一定程度上改善了训练集不均衡的问题，但可能会增加训练资源开销.</li></ol></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/39405.html">https://ADAning.github.io/posts/39405.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"><span class="chip bg-color">线性回归</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/63092.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/15.jpg" class="responsive-img" alt="机器学习之朴素贝叶斯"> <span class="card-title">机器学习之朴素贝叶斯</span></div></a><div class="card-content article-content"><div class="summary block-with-text">朴素贝叶斯NB Naive Bayes朴素贝叶斯有一个非常Naive的假设: 所有特征都是相互独立的, 因此所有特征总的条件概率总是每个特征条件概率的乘积. 这个算法的核心就在于贝叶斯公式. 条件概率条件概率是贝叶斯定理的铺垫. 指的是事件</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-08-06 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="post-category">机器学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF/"><span class="chip bg-color">贝叶斯</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/40485.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/20.jpg" class="responsive-img" alt="计算机网络-自顶向下"> <span class="card-title">计算机网络-自顶向下</span></div></a><div class="card-content article-content"><div class="summary block-with-text">计算机网络计网复习笔记, 参考书籍为自顶向下. 计算机网络体系结构这里的概念都比较散, 大多是一些计网的基础概念和整体知识的框架. 计算机网络的功能: 数据通信 资源共享 分布式处理 提高可靠性 负载均衡 计算机网络的分类(距离分):</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-08-04 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/" class="post-category">计算机基础</a></span></div></div><div class="card-action article-tags"><a href="/tags/%E9%9D%A2%E8%AF%95/"><span class="chip bg-color">面试</span> </a><a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"><span class="chip bg-color">计算机网络</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">373.6k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:695439722@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>