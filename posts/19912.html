<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="KGE预警论文两则, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>KGE预警论文两则 | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:.6"></i> <span>时间轴</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li><li><a href="/markdown"><i class="fab fa-markdown" style="margin-top:-20px;zoom:.6"></i> <span>Markdown</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li><li><a href="/markdown" style="margin-left:75px"><i class="fa fab fa-markdown" style="position:absolute;left:50px"></i> <span>Markdown</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/3.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">KGE预警论文两则</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/KGE/"><span class="chip bg-color">KGE</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/" class="post-category">知识图谱</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2020-11-20</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2022-03-27</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 3.2k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 11 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><p>本文是两篇KGE方向的预警论文的阅读笔记和个人理解. 预警类的工作其实是比较少见的, 对领域的发展也非常有指导意义.</p><blockquote><p><strong>2020.11.22</strong>: 更新Reciprocal Relation.</p><p><strong>2021.05.13</strong>: 修正Reciprocal Relation描述.</p></blockquote><h2 id="A-Re-evaluation-of-Knowledge-Graph-Completion-Methods"><a href="#A-Re-evaluation-of-Knowledge-Graph-Completion-Methods" class="headerlink" title="A Re - evaluation of Knowledge Graph Completion Methods"></a>A Re - evaluation of Knowledge Graph Completion Methods</h2><p>首先来看第一篇论文<a href="http://arxiv.org/abs/1911.03903" target="_blank" rel="noopener">A Re-evaluation of Knowledge Graph Completion Methods</a>.</p><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>这篇论文是在深度学习参与时代背景下提出的. 有相当多的深度学习方法被当做<strong>黑箱</strong>使用在Knowledge Embedding中. 例如普通的CNN, RNN, GNN, 到现在的Attention, 甚至是胶囊网络也有被用于KGE的研究.</p><p>作者敏锐的观察到, 虽然基于DL的方法有非常明显的提升, 但有些方法呈现出在<strong>不同数据集</strong>上的<strong>不一致性</strong>. 作者基于这一个问题, 深度剖析了<strong>基于卷积神经网络</strong>的几种方法呈现错误实验结果的原因.</p><h3 id="Observations"><a href="#Observations" class="headerlink" title="Observations"></a>Observations</h3><h4 id="Inconsistent-Improvements-on-Different-DataSet"><a href="#Inconsistent-Improvements-on-Different-DataSet" class="headerlink" title="Inconsistent Improvements on Different DataSet"></a>Inconsistent Improvements on Different DataSet</h4><p>基于DL的方法在FB15k - 237上的MRR表现十分良好, 但到WN18RR上就出现了问题:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120154015009.png" style="zoom:33%"><p>仔细看实验结果, 传统的建模方法例如RotatE, TuckER在两个数据集上相比ConvE是都有提升的, 只是提升的幅度不同. 而有些基于DL的方法在WN18RR上居然出现了<strong>退化</strong>的现象, 并且ConvKB的退化居然这么明显. 即使WN18RR是比较<strong>困难</strong>的数据集, 也不应该呈现<strong>大幅度</strong>的表现不一致.</p><h4 id="Score-Functions"><a href="#Score-Functions" class="headerlink" title="Score Functions"></a>Score Functions</h4><p>作者在FB15k - 237上做了个测试, 作者发现很多正确三元组和负采样得来的三元组居然具有<strong>相同Score</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120154042981.png" style="zoom:33%"><p>这意味着<strong>无论输入是什么</strong>, <strong>输出始终恒定</strong>(这不是离谱吗), 作者进一步在三种基于CNN的方法上做了对比:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120153725829.png" style="zoom:33%"><p>从图中可以看到, 同是基于CNN的KGE方法, 刚才呈现退化的ConvKB和CapsE出现异常的次数非常多, 而一致提升的ConvE异常次数却非常少.</p><h3 id="Root-of-the-Problem"><a href="#Root-of-the-Problem" class="headerlink" title="Root of the Problem"></a>Root of the Problem</h3><p>经过作者的深入研究后, 发现居然是<strong>ReLU</strong>的锅:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120153853322.png" style="zoom:33%"><p>可能模型输出的结果, 但最后通过ReLU激活的时候, 这些负数全都被<strong>过滤成零</strong>了, 这也就导致了许多三元组的得分完全一致. 该实验曲线与上一个实验趋势是保持一致的.</p><blockquote><p>其实这个现象被称为<strong>Dead Neuron</strong>, 因为ReLU将数据滤掉后, 会导致在BP时一整条链上的导数全为0, 也就不再更新梯度, 神经元的权重将得不到更新. 我想出的解决方案是使用Leaky ReLU等在负值域上有<strong>斜率</strong>的函数, 但同时激活函数作为NN中很重要的部分, 在更换后可能会对模型产生不可知的影响.</p></blockquote><h3 id="New-Evaluation-Protocol"><a href="#New-Evaluation-Protocol" class="headerlink" title="New Evaluation Protocol"></a>New Evaluation Protocol</h3><p>因为很多三元组的Score都相同, 导致最后在排名进行挑选时会出现<strong>不公平</strong>的现象. 假设正确的三元组是在相同得分的候选三元组中<strong>稳定分布</strong>的, 作者提出了三种选择策略:</p><ul><li>TOP: 将正确的三元组插入待预测插入到分数相同的候选三元组<strong>前</strong>.</li><li>BOTTOM: 将正确的三元组插入待预测插入到分数相同的候选三元组<strong>后</strong>.</li><li>RANDOM: 将正确的三元组<strong>随机</strong>插入待预测插入到分数相同的候选三元组中.</li></ul><blockquote><ol><li>作者认为这样做有效的原因是, 在Link Prediction中, 我们先计算所有负采样三元组的得分, 并将其排列, 最后再计算正确三元组得分, 将其放到合适的位置. 在选择三元组时, 选择排行<strong>最靠前</strong>的三元组. 如果调整了正确三元组的插入位置, 就能从一定程度上解决CNN模型性能虚高的问题.</li><li>在论文中这部分的观点我不是很认同, 我更倾向于是<strong>模型本身</strong>的问题而非<strong>评估协议</strong>的问题. 当然在Evaluation Protocol上动手也可以矫正实验结果, 但不能从根本上解决问题.</li></ol></blockquote><p>在FB15k -237上实验结果如下(作者也在WN18RR上做了实验, 结果一致):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120153927434.png" style="zoom:33%"><p>作者提到, 在原来的论文中, ConvE, RotatE, TuckER使用的协议是类似于RANDOM的方法, 而ConvKB, CapsE, KBAT使用的是TOP.</p><p>通过观察, 基于TOP的结果都显示出<strong>虚高</strong>的性能, 而且刚才问题最严重的的CapsE性能虚高最为明显. 基于BOTTOM的结果都<strong>虚低</strong>. 而RANDOM的结果相对来说要<strong>公平</strong>一些.</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>经过研究发现基于DL的KGE方法仍然存在一些问题, 在某些数据集上呈现出性能虚高的问题, 这些方法都具有<strong>误导性</strong>, 作者鼓励用文中的方法进行评估, 相对来说更加<strong>公平</strong>. 质疑类的论文价值很高, 但我认为<strong>作者提出的问题并没有从根本上解决</strong>.</p><h2 id="YOU-CAN-TEACH-AN-OLD-DOG-NEW-TRICKS-ON-TRAINING-KNOWLEDGE-GRAPH-EMBEDDINGS"><a href="#YOU-CAN-TEACH-AN-OLD-DOG-NEW-TRICKS-ON-TRAINING-KNOWLEDGE-GRAPH-EMBEDDINGS" class="headerlink" title="YOU CAN TEACH AN OLD DOG NEW TRICKS! ON TRAINING KNOWLEDGE GRAPH EMBEDDINGS"></a>YOU CAN TEACH AN OLD DOG NEW TRICKS! ON TRAINING KNOWLEDGE GRAPH EMBEDDINGS</h2><p>然后再来看第二篇论文<a href="https://openreview.net/forum?id=BkxSmlBFvr" target="_blank" rel="noopener">YOU CAN TEACH AN OLD DOG NEW TRICKS! ON TRAINING KNOWLEDGE GRAPH EMBEDDINGS</a>.</p><h3 id="Background-1"><a href="#Background-1" class="headerlink" title="Background"></a>Background</h3><p>随着KGE受到大家的重视, 越来越多的方法涌现出来, 并且在实验中SOTA. 但实际上大家击败的Baseline可能并不是该模型发挥的真正水平, 因为模型的性能是一定会与<strong>超参选择</strong>和<strong>训练</strong>有关. 许多研究人员将优秀的模型参数设置<strong>移植</strong>过来, 事实上该参数设置可能并不能在自己的模型上达到良好的效果. 除此外, 模型之间采用不同的方法导致<strong>很难横向对比</strong>也是一个很大的问题.</p><p>因此, 作者希望能够采用更大的超参搜索范围和更多种训练技巧, 来对这些模型的性能<strong>量化和总结</strong>.</p><p>以下是在论文中将要比较的模型, 粗体表示首次出现:</p><p>作者将对以下模型进行汇总, 其中粗体代表该方法首次在各类模型中使用:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120165535529.png" style="zoom:33%"><p>作者的研究只关注纯KGE模型, 不包括引入辅助信息的KGE模型.</p><h3 id="Models-Traning-Evaluation"><a href="#Models-Traning-Evaluation" class="headerlink" title="Models, Traning, Evaluation"></a>Models, Traning, Evaluation</h3><p>该部分中, 对于常见的领域类问题被我跳过了, 即论文中的前四点:</p><ul><li>Multi - relational link prediction.</li><li>Knowledge graph embeddings (KGE).</li><li>Evaluation.</li><li>KGE models.</li></ul><h4 id="Training-Type"><a href="#Training-Type" class="headerlink" title="Training Type"></a>Training Type</h4><p>现在常用的有三种训练类型, 分别是:</p><ul><li><strong>负采样</strong>: 将正样本中的任一元素随机替换作为负样本(有些模型中只替换头实体和尾实体).</li><li><strong>1 vs ALL</strong>: 打乱头和尾实体的位置, 由单个三元组生成全部负例(略存疑).</li><li><strong>K vs ALL</strong>: 批量构建头实体或尾实体非空的一个Batch, 如果在训练集中出现则为正例, 否则为负例. 这种方法在ConvE中首次出现, 在其中被作者称为1 - N Score.</li></ul><h4 id="Loss-Functions"><a href="#Loss-Functions" class="headerlink" title="Loss Functions"></a>Loss Functions</h4><p>一般也只有下列四种Loss:</p><ul><li>MSE(Mean Square Error).</li><li>MR(Margin Ranking), 也称为Hinge Loss.</li><li>BCE(Binary Cross Entropy).</li><li>CE(Cross Entropy).</li></ul><h4 id="Reciprocal-Relations"><a href="#Reciprocal-Relations" class="headerlink" title="Reciprocal Relations"></a>Reciprocal Relations</h4><p>对于Link Prediction任务来说, 对于同一关系, 分别预测头实体和尾实体的<strong>打分函数</strong>应该是不同的.</p><p>但实际上我们不用规定不同的头尾实体打分函数, 我们可以给同种关系以<strong>顺逆区分</strong>, 即每种关系使用两种不同的Embedding, 将所有问题都转化为预测尾实体, 然后打分函数共享相同的实体Embedding, 这样也能最大限度的节省计算开销.</p><p>该方法现在已经广泛的应用于各类KGE方法中, 可能会带来性能提升.</p><h4 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h4><p>现在一般使用的是L2正则化, 个别研究人员推荐使用L3正则. TransE使用归一化. 关于DL的模型可以使用Dropout, 例如ConvE. 在作者的研究中, 还考虑了L1正则.</p><h4 id="Hyperparameters"><a href="#Hyperparameters" class="headerlink" title="Hyperparameters"></a>Hyperparameters</h4><p>现在要考虑的超参有Batch Size, Learning Rate, 负采样个数, 实体和关系的正则权重等.</p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><h4 id="Experiments-Setup"><a href="#Experiments-Setup" class="headerlink" title="Experiments Setup"></a>Experiments Setup</h4><p>实验的设置非常琐碎, 但这是作者的主要工作.</p><ul><li><strong>Dataset</strong>: 现在最常用的两个数据集FB15k - 237和WN18RR.</li><li><strong>Models</strong>: 选用RESCAL, TransE, DistMult, ComplEx, ConvE.</li><li><strong>Evaluation</strong>: MRR和HITS@10.</li><li><strong>Hyperparameters</strong>: 考虑前面提到的三种Training Type(负采样, 1 vs ALL, K vs ALL), 提到的正则(None, L1, L2, L3, Dropout), 优化器(Adam, Adagrad), Embedding Size(128, 256, 512), 并分别对实体和关系建立权重用于Dropout和正则. 这是作者已知的最大范围的超参搜索空间.</li><li><strong>Training</strong>: 最大400Epochs. 每5个Epoch算一次MRR, 并且在有50个Epoch中模型MRR没有5%以上的提升, 则触发早停.</li><li><strong>Model Selection</strong>: 作者通过一个框架, 对于每个模型和数据集<strong>随机生成</strong>(即<strong>随机超参搜索</strong>)了30中不同的配置, 每种配置都包含不同的Training Type和Loss Function. 在随机搜索后用<strong>贝叶斯优化</strong>对参数进一步调优.</li></ul><p>作者给出了一张汇总表(摘自附录):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120193313355.png" style="zoom:50%"><blockquote><p>Table 6 是随机搜索在FB15k - 237上的详细最优配置图, 是同级标题下的Impact of Hyperparameters -&gt; Best configurations (quasi-random search)中的第二幅图.</p></blockquote><h4 id="Comparison-of-Model-Performance"><a href="#Comparison-of-Model-Performance" class="headerlink" title="Comparison of Model Performance"></a>Comparison of Model Performance</h4><p>作者将搜索过后的模型(Ours)性能与之前发布的最初版模型(First), 以及最近的模型(Recent)和更大号的模型(Large)进行了比较:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120183435443.png" style="zoom:33%"><p>根据实验结果, 得到以下结论:</p><ol><li>经过重新超参搜索后的模型相比于原作者首次发布有了<strong>巨大提升</strong>.</li><li>在作者重新训练的这些模型之间, 性能差距逐渐<strong>缩小</strong>, 甚至发生<strong>逆转</strong>.</li><li>有些模型例如RESCAL, 相比于最近<strong>新发布</strong>的模型也<strong>没差多少</strong>.</li></ol><h4 id="Impact-of-Hyperparameters"><a href="#Impact-of-Hyperparameters" class="headerlink" title="Impact of Hyperparameters"></a>Impact of Hyperparameters</h4><h5 id="Anatomy-of-Search-Space"><a href="#Anatomy-of-Search-Space" class="headerlink" title="Anatomy of Search Space"></a>Anatomy of Search Space</h5><p>作者将搜索空间所有的模型的所有结果做了一张<strong>箱型图</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120183501722.png" style="zoom:33%"><p>从箱型图得到以下结论:</p><ol><li>不同类型的超参和训练方式给模型带来的影响巨大, 大多的模型性能<strong>均值</strong>和<strong>上限</strong>基本一致, 下限差的有点多.</li><li>TransE的结果不太好, 这是因为其他模型都有大概200种配置, 而TransE只有60种.</li><li>各个模型的表现在FB15k - 237上似乎要稳定一些, WN18RR上最优和最差差距很大.</li></ol><h5 id="Best-configurations-quasi-random-search"><a href="#Best-configurations-quasi-random-search" class="headerlink" title="Best configurations (quasi-random search)"></a>Best configurations (quasi-random search)</h5><p>作者给出了在随机搜索后得到最佳结果的配置(简略), 括号内是不使用该参数导致MRR减少的值:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120183522075.png" style="zoom:33%"><p>从表中发现不了什么规律, 每个模型所使用的最佳配置基本不同. 但<strong>随机搜索时CE性能似乎比较好</strong>.</p><p>各模型经过随机搜索在<strong>FB15k - 237</strong>上的详细最优配置(详细):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120193607208.png" style="zoom:50%"><p>各模型经过随机搜索在<strong>WN18RR</strong>上的最优配置(详细):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120193632189.png" style="zoom:50%"><h5 id="Best-configurations-Bayesian-optimization"><a href="#Best-configurations-Bayesian-optimization" class="headerlink" title="Best configurations (Bayesian optimization)"></a>Best configurations (Bayesian optimization)</h5><p>各模型经过贝叶斯优化在各数据集上的最优配置:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120200139031.png" style="zoom:33%"><p>贝叶斯优化后的效果比随机搜索的结果又稍稍<strong>提升</strong>了一点.</p><h5 id="Impact-of-Training-Type-and-Loss-Functions"><a href="#Impact-of-Training-Type-and-Loss-Functions" class="headerlink" title="Impact of Training Type and Loss Functions"></a>Impact of Training Type and Loss Functions</h5><p>作者在随机搜索上给出了不同训练技巧和不同损失函数的箱型图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120183542089.png" style="zoom:50%"><p>除了能体现出不同训练技巧和不同损失函数对不同模型的影响之外, 再次说明<strong>数据集</strong>也是一个非常重要的原因.</p><p>在图中还能看出来咖啡色组(<strong>1 vs ALL + CE</strong>)和粉色组(<strong>K vs ALL + CE</strong>)的上限非常高, 平均水平也不错. 这说明<strong>CE</strong>可能比其他损失函数要稍好些.</p><h3 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h3><p>这篇论文显示了参数配置在KGE模型上的重要性. 作者用了非常大的超参数搜索空间, 来说明有些模型的性能不一定像它在初始论文中看起来的表现一样. 在作者的更大范围的超参搜索和修改训练方式后, 模型之间的差距<strong>逐渐缩小</strong>, 甚至<strong>结果反转</strong>(有点NFL的意思).</p><p>本论文还有更多的结果在附录中, 主要是对更细粒度的组合做的箱型图, 其实都说明不了什么规律性的问题, 本来就没有规律可言. 给我们最大的启发就是KGE有些模型的性能需要<strong>重新审视</strong>, 对于不同的参数可能会出现截然不同的性能, 作者也鼓励使用<strong>更大参数搜索范围</strong>尽可能的将模型真实的性能展现出来, 方便大家做比较.</p><p>最后, 对作者的<strong>钻研精神</strong>和<strong>科研毅力</strong>致敬.</p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/19912.html">https://ADAning.github.io/posts/19912.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/KGE/"><span class="chip bg-color">KGE</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/52897.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/19.jpg" class="responsive-img" alt="KEPLER: Knowledge Embedding and Pre-trained Language Representation"> <span class="card-title">KEPLER: Knowledge Embedding and Pre-trained Language Representation</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: BERT(详见ELMo, GPT, BERT) KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representat</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-11-21 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/" class="post-category">知识图谱</a></span></div></div><div class="card-action article-tags"><a href="/tags/KGE/"><span class="chip bg-color">KGE</span> </a><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/BERT/"><span class="chip bg-color">BERT</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/60645.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/22.jpg" class="responsive-img" alt="Pytorch实现: Skip-Gram"> <span class="card-title">Pytorch实现: Skip-Gram</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: Pytorch基本操作 Word2Vec Pytorch实现: Skip-Gram本文用Pytorch实现了Skip - Gram, 它是Word2Vec的其中一种. 本文实现参考PyTorch 实现 Word2Ve</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-11-19 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/Word2Vec/"><span class="chip bg-color">Word2Vec</span> </a><a href="/tags/Pytorch/"><span class="chip bg-color">Pytorch</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">368.3k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:695439722@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>