<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="SpanBERT: Improving Pre-training by Representing and Predicting Spans, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>SpanBERT: Improving Pre-training by Representing and Predicting Spans | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/20.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">SpanBERT: Improving Pre-training by Representing and Predicting Spans</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/BERT/"><span class="chip bg-color">BERT</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2021-05-13</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2022-03-27</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 2.2k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 9 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p>本文前置知识:</p><ul><li>BERT: 详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>.</li></ul></blockquote><h1 id="SpanBERT-Improving-Pre-training-by-Representing-and-Predicting-Spans"><a href="#SpanBERT-Improving-Pre-training-by-Representing-and-Predicting-Spans" class="headerlink" title="SpanBERT: Improving Pre-training by Representing and Predicting Spans"></a>SpanBERT: Improving Pre-training by Representing and Predicting Spans</h1><p>本文是论文<a href="https://www.aclweb.org/anthology/2020.tacl-1.5/" target="_blank" rel="noopener">SpanBERT: Improving Pre-training by Representing and Predicting Spans</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者认为, 许多NLP任务中所涉及到的推理任务是关于<strong>多个</strong>Text Token之间的, 而非基于单个Token之间的.</p><p>例如, 在回答问题<code>Which NFL team won Super Bowl 50?</code>时, 直接给出答案<code>Denver Broncos</code>比在<code>Denver</code>后给出<code>Broncos</code>要困难的多, 但前者却更贴近与现实场景, 难度也更大.</p><p>作者尝试提出SpanBERT来解决这种Span Level Prediction的问题.</p><h2 id="SpanBERT"><a href="#SpanBERT" class="headerlink" title="SpanBERT"></a>SpanBERT</h2><p>SpanBERT通过三种方式来帮助模型理解语言:</p><ol><li>将Token Level Mask替换为<strong>Span Level</strong> Mask.</li><li>引入一种新的辅助目标来<strong>SBO</strong>帮助模型训练.</li><li>只使用<strong>单个句子</strong>训练, 而非BERT所使用的句子对.</li></ol><h3 id="Span-Masking"><a href="#Span-Masking" class="headerlink" title="Span Masking"></a>Span Masking</h3><p>在BERT中, 对于给定的输入序列$X=\left(x_{1}, x_{2}, \ldots, x_{n}\right)$, 其编码表示为:<br>$$<br>\operatorname{enc}\left(x_{1}, x_{2}, \ldots, x_{n}\right)=\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{n}<br>$$<br>但在BERT中, 打Mask是对单个Token做的. 在SpanBERT中, 作者使用Span Level的Mask, 对一连串的Token做指定长度的Mask. 通过<strong>均匀分布</strong>随机采样得到Span Mask的初始位置.</p><p>而Span Mask的长度是通过<strong>迭代随机采样</strong>的来的, 继续扩展Span的几率服从<strong>几何分布</strong>$\ell \sim \operatorname{Geo}(p=0.2)$, 最大的Span长度$\ell_{\max }=10$. 即连续采样, 每次都有$p=0.2$ 的几率继续扩展Span, 每次都有$q = 1 - p = 0.8$ 的几率停止扩展Span. 其概率分布如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spanbert2.jpg" style="zoom:33%"><p>那么Span长度的期望值为3.8, 其推导过程如下:<br>$$<br>\begin{aligned}<br>&amp;p = 0.2 \\<br>&amp;q = 1 - p = 0.8 \\<br>&amp;p^\prime = \frac{p}{1-q^{10}} = 0.224 \\<br>\end{aligned}<br>$$<br>$p^\prime$ 代表Span最大长度为10以下的概率的归一化. $1-q^{10}$ 为当前Span不超过10的概率和, 需要用$p$ 去除以这个值, 让$p^\prime$ 排除掉Span长度大于10的情况.</p><blockquote><p>参考<a href="https://zhuanlan.zhihu.com/p/75893972" target="_blank" rel="noopener">SpanBert：对 Bert 预训练的一次深度探索</a>的评论区.</p></blockquote><p>故限定Span最大长为10的期望为:<br>$$<br>\begin{aligned}<br>E(x) &amp;= p^\prime \sum_{n=1}^{10}n q^{n-1} \\<br>&amp;= p^\prime (1 + 2q + 3q^2 + \dots + 10q^9) \\<br>&amp;= 0.224 \cdot 16.9469=3.797 \approx3.8<br>\end{aligned}<br>$$<br>即Span长度期望为3.8, 如果向上取整就是4.</p><p>Span Masking将Token Level的Mask变更为了Span Level的Mask, 任务的训练难度更为复杂.</p><h3 id="Span-Boundary-Objective"><a href="#Span-Boundary-Objective" class="headerlink" title="Span Boundary Objective"></a>Span Boundary Objective</h3><p>作者希望Span的<strong>结尾</strong>能尽可能多的表示出Span<strong>内部</strong>的内容, 因此作者直接引入一个新的辅助目标来实现.</p><blockquote><p>SBO的提出应该也受到一些其他模型的启发, 例如<strong>ERNIE(Baidu)</strong>, <strong>BERT WWM</strong>等模型, 都是基于Span Level Mask做的额外处理.</p></blockquote><p>对于Span外部的起始边界和结束边界$(s, e)$, 作者希望通过某种方式$f(\cdot)$, 来根据其边界两侧的表示$\mathbf{x}_{s-1}, \mathbf{x}_{e+1}$, 以及<strong>每个Span内部的Token</strong> $x_i$ 所对应的位置编码$\mathbf{p}_{i-s+1}$ 来得到预测结果$\mathbf{y}_i$, 从而预测出Span内的每个Token:</p><p>$$<br>\mathbf{y}_{i}=f\left(\mathbf{x}_{s-1}, \mathbf{x}_{e+1}, \mathbf{p}_{i-s+1}\right)<br>$$</p><p>这里作者简单的使用两层FFN和GeLU来将$\mathbf{y}_i$ 转换为$x_i$:</p><p>$$<br>\begin{array}{l}<br>\mathbf{h}_{0}=\left[\mathbf{x}_{s-1} ; \mathbf{x}_{e+1} ; \mathbf{p}_{i-s+1}\right] \\<br>\mathbf{h}_{1}=\text { LayerNorm }\left(\operatorname{GeLU}\left(\mathbf{W}_{1} \mathbf{h}_{0}\right)\right) \\<br>\mathbf{y}_{i}=\text { LayerNorm }\left(\operatorname{GeLU}\left(\mathbf{W}_{2} \mathbf{h}_{1}\right)\right)<br>\end{array}<br>$$</p><p>$[\cdot]$ 代表拼接操作.</p><p>SBO将和MLM的损失函数共同优化, 二者之间是简单的加和关系即可:<br>$$<br>\begin{aligned}<br>\mathcal{L}\left(x_{i}\right) &amp;=\mathcal{L}_{\mathrm{MLM}}\left(x_{i}\right)+\mathcal{L}_{\mathrm{SBO}}\left(x_{i}\right) \\<br>&amp;=-\log P\left(x_{i} \mid \mathbf{x}_{i}\right)-\log P\left(x_{i} \mid \mathbf{y}_{i}\right)<br>\end{aligned}<br>$$<br>那么根据SBO, 模型可能会学到一些关于Span的内容, 因为SBO要求模型必须用边界信息来猜Span内部指定位置的内容.</p><p>下面给出一个说明SBO的例子:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spanbert1.jpg" style="zoom:50%"><p>在该例中, $\text{football}$ 被选中, 将在其附近给长度为4的区域打上Mask, 这个词应该根据被Mask的区域的<strong>边界表示</strong>$\mathbf{x}_3, \mathbf{x}_9$, 以及$\text{football}$ 在Span Mask中的<strong>相对位置</strong>的编码$\mathbf{p}_3$ 共同预测出来.</p><p>所以在该例中, 损失函数为MLM预测$\text{football}$ 和SBO预测$\text{football}$ 的损失之和:<br>$$<br>\begin{aligned}<br>\mathcal{L}(\text { football }) &amp;=\mathcal{L}_{\mathrm{MLM}}(\text { football })+\mathcal{L}_{\mathrm{SBO}}(\text { football }) \\<br>&amp;=-\log P\left(\text { football } \mid \mathbf{x}_{7}\right)-\log P\left(\text { football } \mid \mathbf{x}_{4}, \mathbf{x}_{9}, \mathbf{p}_{3}\right)<br>\end{aligned}<br>$$</p><h3 id="Single-Sequence-Training"><a href="#Single-Sequence-Training" class="headerlink" title="Single - Sequence Training"></a>Single - Sequence Training</h3><p>在SpanBERT中, 作者舍弃了BERT的句子对训练法, 仅使用<strong>单句训练</strong>, 并抛弃NSP任务, 理由如下:</p><ol><li>句子对的引入限制了<strong>单句最长文本长度</strong>. 使用单句训练, 最长文本长度可以直接<strong>翻倍</strong>.</li><li>句子对上下不相关时, 会引入非常大的<strong>噪声</strong>.</li></ol><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文.</p><h3 id="Extractive-Question-Answering"><a href="#Extractive-Question-Answering" class="headerlink" title="Extractive Question Answering"></a>Extractive Question Answering</h3><p>抽取式QA的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spanbert3.jpg" style="zoom:33%"> <img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spanbert4.jpg" style="zoom:33%"><p>在这些数据集上的提升趋势是一致的, 每个数据集都有提升.</p><h3 id="Coreference-Resolution"><a href="#Coreference-Resolution" class="headerlink" title="Coreference Resolution"></a>Coreference Resolution</h3><p><strong>指代消解</strong>的任务目标是将文本中提到的同一实体的不同表述找出来, 即<strong>将同一事物的不同自然语言描述链接到文本中的同一事物</strong>.</p><p>对指代消解任务, 每个Mention Span $x$, 它与之前文本中对应的多个定长Span $y^\prime \in Y$ 都有一个得分$s(x, y)$, 根据得分使用Softmax能够判断出$y$ 是哪个Span的指代:</p><p>$$<br>P(y)=\frac{e^{s(x, y)}}{\sum_{y^{\prime} \in Y} e^{s\left(x, y^{\prime}\right)}}<br>$$</p><p>其中Span Pair的打分函数$s(x, y)$ 是由FFN得来的:</p><p>$$<br>\begin{aligned}<br>s(x, y) &amp;=s_{m}(x)+s_{m}(y)+s_{c}(x, y) \\<br>s_{m}(x) &amp;=\mathrm{FFNN}_{m}\left(\mathbf{g}_{\mathrm{x}}\right) \\<br>s_{c}(x, y) &amp;=\mathrm{FFNN}_{c}\left(\mathbf{g}_{\mathrm{x}}, \mathbf{g}_{\mathbf{y}}, \phi(x, y)\right)<br>\end{aligned}<br>$$</p><p>在这里, $\mathbf{g}_x, \mathbf{g}_y$ 代表两个Transformer提取出Span<strong>两个端点</strong>的隐态输出和Span内部的<strong>Attention的加权求和</strong>后的拼接向量, $\text{FFNN}_m, \text{FFNN}_c$ 代表两个有一层隐层的前馈神经网络, $\phi(x, y)$ 代表人工构建的特征.</p><blockquote><p>和论文<a href="https://arxiv.org/abs/1908.09091" target="_blank" rel="noopener">BERT for Coreference Resolution: Baselines and Analysis</a>的使用方法一致, BERT系列指代消解模型是BiLSTM + Attention系列模型在C2F - Coref上的升级.</p><p>更多关于神经网络指代消解的论文, 还可以参考:</p><ul><li><a href="https://arxiv.org/abs/1707.07045" target="_blank" rel="noopener">End-to-end Neural Coreference Resolution</a></li><li><a href="https://arxiv.org/abs/1804.05392" target="_blank" rel="noopener">Higher-order Coreference Resolution with Coarse-to-fine Inference</a></li></ul></blockquote><p>指代消解的数据集OntoNotes上表现如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spanbert5.jpg" style="zoom:33%"><p>相较于其他的BERT Baseline, SpanBERT有些许提升, 并且提升在每个数据集上都是一致的.</p><h3 id="Relation-Extraction"><a href="#Relation-Extraction" class="headerlink" title="Relation Extraction"></a>Relation Extraction</h3><p>在关系抽取的数据集TACRED上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spanbert6.jpg" style="zoom:33%"><p>SpanBERT在关系抽取上的标现达到平均水平, 在Recall上进步比较大.</p><h3 id="GLUE"><a href="#GLUE" class="headerlink" title="GLUE"></a>GLUE</h3><p>GLUE上的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spanbert7.jpg" style="zoom:33%"><p>相较于其他BERT Baseline, SpanBERT提升也是全面的. 尤其是在二分类数据集QNLI上提升比较大, 我认为这可能是SBO带来的提升.</p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><h4 id="Masking-Schemes"><a href="#Masking-Schemes" class="headerlink" title="Masking Schemes"></a>Masking Schemes</h4><p>作者将Subword, Whole Words, Named Entities, Noun Phrases, Geometric Spans几种Mask策略放在一起做了对比:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spanbert8.jpg" style="zoom:33%"><p>能够看出, SpanBERT中的Geometric Spans几乎是最有效的, 但在Coreference上似乎没有与其他提升一致. 但其他的策略也都不如BERT最开始使用的Subword Tokens策略要好.</p><h4 id="Auxiliary-Objectives"><a href="#Auxiliary-Objectives" class="headerlink" title="Auxiliary Objectives"></a>Auxiliary Objectives</h4><p>不同的辅助目标对Span BERT的影响如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spanbert9.jpg" style="zoom:33%"><p>如果使用NSP任务, 性能会比不使用NSP并单句训练的SpanBERT有损, 加上SBO任务后会使得Coreference上的表现大幅提升(相较于上表).</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>SpanBERT作为一种BERT的改进方案, 提出了更完备的Span Masking方案. SpanBERT加入了SBO任务, 使用边界信息和被Mask的Token在Span内的相对位置信息能预测出Span内部的内容. 也没有使用句子对作为训练方式, 抛弃了对性能有损害的NSP任务.</p><p>SpanBERT在各类任务上性能皆超过BERT, 应该算作是一种比较有效的改进方式, 方法也比较巧妙.</p><blockquote><p>我感觉效果全面提升的原因主要是增大了模型的训练难度, 并且包含有一定的随机性, 在预测时, 需要对每个Span内的Token都做预测, 大大的强化了Span左右两端判断Span内部内容的能力, 这样比漫无目的的Mask要有效得多, 在不同的位置编码下, 需要判断Span内部不同的内容, 进一步的提高了模型对两端信息的利用率, 使得两端的隐态更有意义, 更有内容.</p></blockquote></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/34019.html">https://ADAning.github.io/posts/34019.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/BERT/"><span class="chip bg-color">BERT</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/46395.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/1.jpg" class="responsive-img" alt="MASS: Masked Sequence to Sequence Pre - training for Language Generation"> <span class="card-title">MASS: Masked Sequence to Sequence Pre - training for Language Generation</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识; BERT: 详见ELMo, GPT, BERT. Transformer: 详见Transformer精讲. MASS: Masked Sequence to Sequence Pre-training for La</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2021-06-08 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/BERT/"><span class="chip bg-color">BERT</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/51848.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/14.jpg" class="responsive-img" alt="StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding"> <span class="card-title">StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: BERT: 详见ELMo, GPT, BERT. StructBERT: Incorporating Language Structures into Pre-training for Deep Language U</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2021-05-04 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">425.9k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:andaning@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>