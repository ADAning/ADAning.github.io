<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:.6"></i> <span>时间轴</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li><li><a href="/markdown"><i class="fab fa-markdown" style="margin-top:-20px;zoom:.6"></i> <span>Markdown</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li><li><a href="/markdown" style="margin-left:75px"><i class="fa fab fa-markdown" style="position:absolute;left:50px"></i> <span>Markdown</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/5.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/PLM/"><span class="chip bg-color">PLM</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2022-04-22</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2022-08-20</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 4.8k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 17 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p>本文前置知识:</p><ul><li>Transformer: <a href="https://adaning.github.io/posts/6744.html">Transformer精讲</a>.</li><li>BERT, GPT: <a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>.</li></ul></blockquote><h1 id="Exploring-the-Limits-of-Transfer-Learning-with-a-Unified-Text-to-Text-Transformer"><a href="#Exploring-the-Limits-of-Transfer-Learning-with-a-Unified-Text-to-Text-Transformer" class="headerlink" title="Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"></a>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</h1><p>本文是论文<a href="https://arxiv.org/abs/1910.10683" target="_blank" rel="noopener">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a> 的阅读笔记和个人理解. T5的论文更像是一篇详细扎实的实验报告, 所以本文不再按照常规结构讲解, 所有详细的实验参数和训练过程请参照原论文.</p><p>所谓<strong>万物皆可生成</strong>, 任何任务都可以转化为Sequence to Sequence的形式完成, 只是不同的任务转化为生成后的任务难度有所差异.</p><p>T5是谷歌在2019年, 后BERT时代, 面向生成的Transformer的一次全面探索, 甚至是一次无穷尽的探索, 再一次展示了唯一真理”Money is All You Need”.</p><blockquote><p>其实T5在此基础上还有个小升级T5 1.1, 基本大差不差. 后来又有个mT5是T5的多语言版本, 重新构建了多国语言预训练数据集, 模型沿用的是T5 1.1, 感兴趣的可以自行了解, 咱们在这就只说说T5初始版本.</p></blockquote><h2 id="Text-to-Text-Framework"><a href="#Text-to-Text-Framework" class="headerlink" title="Text - to - Text Framework"></a>Text - to - Text Framework</h2><p><strong>T5</strong>全称为<strong>T</strong>ext-<strong>t</strong>o-<strong>T</strong>ext <strong>T</strong>ransfer <strong>T</strong>ransformer, 无论是何种任务(MT, QA, 文本分类等等), 全部使用文本作为输入, 并且用生成的文本作为输出. 这种方式最大的好处在于提供了预训练时和微调时的<strong>目标一致性</strong>. 所以谷歌才要探索这种基于生成式的预训练方式.</p><p>如何将所有任务转化成text to text形式? 图中列举了几种将不同形式任务统一到text - to - text框架下的例子:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t51.png" style="zoom:33%"><blockquote><p>如果你能在其他NLP论文看到类似这样的图, 左边几个大框框右边几个小框框中间一个模型名字框框, 不出意外它一定是用T5或者T5的改进版做的预训练.</p></blockquote><ul><li><strong>机器翻译</strong>: 直接在训练数据前面加上<code>translate X to Y</code>, <code>X</code> 和<code>Y</code> 是不同种的语言.</li><li><strong>文本分类</strong>: 让模型输出一个单词, 代表不同种类的标签.</li><li><strong>情感分析</strong>: 加上任务前导名, 并在句子前加上<code>sentence:</code>.</li><li><strong>文本摘要</strong>: 在文本前面加上<code>summarize:</code>.</li><li><strong>语义相似度</strong>: 最特殊的一类任务. 加上任务前导名, 在两个句子前分别加上<code>sentence1:</code>, <code>sentence2:</code>.</li></ul><p>它们的输出都直接是任务中的<strong>标签文本</strong>, 注意是标签文本. 也就是说, 像STS - B这种<strong>语义相似度</strong>的任务, <strong>也是靠直接输出文本来完成的</strong>, 而非加个回归头输出数值. STS - B的相似度标签大多是在1 - 5之间的, 作者按0.2为一个增量步长把它改为了一个21分类任务, 如果有相近的值全部四舍五入到相应的类别里.</p><p>直觉上来讲这是比较离谱的, 但其实想想这样的统一也才符合T5的终极定位, 所有任务都用文本输入, 文本输出.</p><p>对于所有的分类问题(包括STS - B), 如果T5在分类任务中输出了不属于Label的单词, 直接记为错误.</p><p>不同文本的预处理方式实际上是个超参, 所以作者没有做探索, 因为方式实在是太多了.</p><p>在经过数据预处理后, 在这种任务大一统的情况下, 直接允许作者使用相同的模型架构, 损失函数, 超参等, 完成所有的NLP任务.</p><blockquote><p>详细的任务预处理例子请参照原论文附录D.</p></blockquote><h2 id="Differences-between-Transformer-and-T5"><a href="#Differences-between-Transformer-and-T5" class="headerlink" title="Differences between Transformer and T5"></a>Differences between Transformer and T5</h2><p>T5做的所有实验均是基于Transformer架构的, 和普通Transformer仅有一些不显著的不同:</p><ul><li><p><strong>Relative Positional Injection</strong>: T5中给模型注入的是<strong>相对位置信息</strong>, 但并没有采用在输入时直接加上位置编码引入位置信息的方式. Query和Key的绝对位置差值对应着不同的相对位置. T5令每个注意力头<strong>直接学习不同相对位置的Attention Score</strong>(就是在Softmax之前的logits), 在模型计算完Attention Score后直接加到上面. 每个注意力头都有自己的相对位置Embedding, 但所有层都共享同一套相对位置Embedding.</p><blockquote><p>注意! 既然该操作是直接加到Attention Score上的, 这意味着<strong>每次</strong>Attention都会完成一次注入. 这强化了T5对相对位置的感知.</p><p>其实, T5的相对位置其实不是一个值, 一个<strong>相对位置区间</strong>, 例如Query和Key的绝对位置差分别为<code>1, 2, 3, 4</code>, <code>1</code> 可能对应着最近的Attention Score, 而<code>2, 3</code> <strong>同时</strong>对应着中距离的Attention Score, <code>4</code> 对应着最远的Attention Score. 这点可以参考<a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py" target="_blank" rel="noopener">Hugging face的T5实现</a>中的<code>_relative_position_bucket</code>函数.</p></blockquote></li><li><p><strong>Activation Function</strong>: 从BERT使用的GeLU又退回了<strong>ReLU</strong>.</p></li><li><p><strong>No Bias Item</strong>: 从Attention到FFN, T5去掉了<strong>所有</strong>Linear的bias.</p></li><li><p><strong>Normalization</strong>: T5使用的是<strong>Pre Norm</strong>, 而非Post Norm. 此外, 与上一条相关, 把Layer Norm替换为了 <a href="https://arxiv.org/abs/1910.07467" target="_blank" rel="noopener">RMS Norm</a>, 即去掉了跟均值相关的那项:</p></li></ul><p>$$<br>\begin{aligned}<br>y_{i,j,k} = \frac{x_{i,j,k} - \mu_{i,j}}{\sqrt{\sigma_{i,j}^2 + \epsilon}}\times\gamma_k + \beta_k,\quad \mu_{i,j} = \frac{1}{d}&amp;\sum_{k=1}^d x_{i,j,k},\quad \sigma_{i,j}^2 = \frac{1}{d}\sum_{k=1}^d (x_{i,j,k}-\mu_{i,j})^2 \\<br>\Downarrow \\<br>y_{i,j,k} = \frac{x_{i,j,k}}{\sqrt{\sigma_{i,j}^2 + \epsilon}}\times\gamma_k,&amp;\quad \sigma_{i,j}^2 = \frac{1}{d}\sum_{k=1}^d x_{i,j,k}^2<br>\end{aligned}<br>$$</p><blockquote><p>除去上述几点外, 其实还有一个区别, 计算Attention Score的时候不用除以$\sqrt{d}$, 这么做的具体原因在<a href="https://kexue.fm/archives/8620" target="_blank" rel="noopener">浅谈Transformer的初始化、参数化与标准化</a>中提到, 强烈建议阅读.</p></blockquote><h2 id="C4-Colossal-Clean-Crawled-Corpus"><a href="#C4-Colossal-Clean-Crawled-Corpus" class="headerlink" title="C4: Colossal Clean Crawled Corpus"></a>C4: Colossal Clean Crawled Corpus</h2><p>不光有T5, 这篇文章还扔出来个<strong>C4</strong>(<strong>C</strong>olossal <strong>C</strong>lean <strong>C</strong>rawled <strong>C</strong>orpus), 这个名字起得挺搞的, 译为巨大号清洁爬下来的语料库. 它是爬虫从大量公开可用的<strong>HTML</strong>里爬下来的, 还没清洗时有20T大小.</p><p>爬下来的数据非常不干净, 它做了如下<strong>清洗</strong>工作:</p><ul><li>仅保留结尾有标点的句子.</li><li>去掉少于5个句子的页面, 保留至少3个单词的行.</li><li>去掉有脏话, 涩涩之类坏词的页面.</li><li>去掉包含Javascript的句子, 和网页警告的是否开启JS有关.</li><li>去掉包含<code>lorem ipsum</code> 占位符的页面, 和测试网页排版有关.</li><li>去掉所有包含花括号的页面, 和代码有关, 比如JS, Java, C都广泛使用花括号.</li><li>以三句为一个单位去重.</li></ul><p>清洗出的文本大约有750G, 作为预训练数据.</p><h2 id="Experimentsss…"><a href="#Experimentsss…" class="headerlink" title="Experimentsss…"></a>Experimentsss…</h2><p>T5的结构和训练细节是单纯考实验得来的, 每次做完一部分实验, 就确定T5的一部分细节. 待T5的训练细节固定后, 又对C4和训练策略做了探索.</p><h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p>不同体系的模型主要区别就在<strong>Attention Mask</strong>上:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t52.png" style="zoom:33%"><p>图中横轴为Attention的输入, 纵轴为输出. 黑色代表可以对该元素给予注意力, 白色代表不能对该元素给予注意力.</p><ul><li><strong>Fully - Visible</strong>: 输入对每个时间步的输出完全可见, 即Transformer Encoder的Attention Mask.</li><li><strong>Causal</strong>: 输出仅能依赖当前时间步及之前的输入, 即Transformer Decoder的Attention Mask.</li><li><strong>Casual with Prefix</strong>: 是前面二者的混合, 输入的一部分Fully - Visible, 一部分是Casual.</li></ul><p>不同类型的Attention Mask造就了Transformer各类不同结构的诞生:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t53.png" style="zoom:33%"><p>此处的x 和y代表输入序列和输出序列.</p><ul><li>Left: 标准的<strong>Encoder - Decoder</strong>结构, Encoder是全可见的, Decoder解码时可以看见全部的Encoder信息和已经解码的Decoder信息. Vallina Transformer就是这种结构, 后来这个流派又多了MASS, BART这样的模型.</li><li>Middle: 比较纯粹的<strong>语言模型</strong>, <strong>完全的自回归式生成</strong>. 当然这种模型也可以通过强行拼接不同的任务前导文本来执行不同的任务, 并入text to text框架中. 这个流派比较典型的是GPT.</li><li>Right: 就是上面Encoder和Decoder的<strong>共生体</strong>, 它的最大特点就是<strong>不做Encoder和Decoder的区分</strong>, 即共用一个Transformer. 而且它在解码的时候只能使用<strong>同层</strong>的Token表示, 换句话说, <strong>它的编码和解码过程是同步进行的</strong>, 这一流派的代表是UniLM.</li></ul><blockquote><p>一个非常有意思的看法是, XLNet这类的<strong>Permuted Language Model</strong>可以视为Prefix LM, 结合XLNet比较特殊的Attention Mask, 或许你会认同这种观点. 该观点来自于<a href="https://zhuanlan.zhihu.com/p/254821426" target="_blank" rel="noopener">张俊林</a>.</p></blockquote><p>下面直接看实验结果:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t54.png" style="zoom:33%"><ul><li><p>标准的Encoder - Decoder最好, 但是参数最多. 所以T5选择Encoder - Decoder作为架构.</p></li><li><p>共享Encoder和Decoder的参数似乎没有对结构有太多影响, 这点在ALBERT里也有说.</p></li><li><p>在非生成类任务上, 完全自回归的结构不太给力, 参数量和计算成本一个好处也没捞到, 大多数任务的性能还被人甩了好几条街.</p></li><li><p>Denoisiong作为训练目标确实提升了模型的能力, 几乎所有模型所有数据集表现都比LM要好.</p></li></ul><h4 id="Denoising-Object-for-Text-to-Text-Framework"><a href="#Denoising-Object-for-Text-to-Text-Framework" class="headerlink" title="Denoising Object for Text to Text Framework"></a>Denoising Object for Text to Text Framework</h4><p>这个Denoising是啥呢? 为了适配text to text框架, 受BERT中打Mask和Word Dropout启发, 作者提出了一种适用于text to text框架的Mask方法. 该方法从原文本中随机采样15%的Token变为<strong>不同的Mask</strong>, 不同被Mask掉的Token会被替换成不同类型的Mask. 模型将被要求生成Mask Token和与之对应被Mask掉的原内容, 以及最终的结束Token:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t55.png" style="zoom:33%"><p>在这个例子中, 原文本的<code>for inviting</code> 被替换为<code>&lt;X&gt;</code>, <code>last</code> 被替换为<code>&lt;Y&gt;</code>, 结束Token为<code>&lt;Z&gt;</code>. 从例子中来看, 多个连续Token如果同时被Mask, 只需要被替换为一个Mask Token, 因为生成类模型不需要多个Mask Token就能恢复出被遮掉的内容.</p><p>在下一节中, 该方法将会与其他训练方式对比.</p><h3 id="Unsupervised-Objectives"><a href="#Unsupervised-Objectives" class="headerlink" title="Unsupervised Objectives"></a>Unsupervised Objectives</h3><p>以<code>Thank you for inviting me to your party last week.</code> 为例, 作者列举了其他的无监督训练目标:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t56.png" style="zoom:33%"><blockquote><p>BERT - style的Inputs中的<code>apple</code> 是由<code>last</code> 随机替换得到的.</p></blockquote><h4 id="Disparate-High-Level-Approaches"><a href="#Disparate-High-Level-Approaches" class="headerlink" title="Disparate High - Level Approaches"></a>Disparate High - Level Approaches</h4><p>首先从Prefix LM, Denosing, Deshuffling三大类的角度来看它们的差距:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t57.png" style="zoom:33%"><p>BERT - style比其他两类都要好, Prefix LM在翻译上还算可以, 但理解类任务不够出色, Deshuffling可能因为完全颠倒语序语义损失太多了, 训练时全是乱序的句子, 下游任务全是正确的句子, 表现比较差.</p><h4 id="Simplifying-the-BERT-objective"><a href="#Simplifying-the-BERT-objective" class="headerlink" title="Simplifying the BERT objective"></a>Simplifying the BERT objective</h4><p>既然BERT - style是最好的, 那就把所有Denoising方法放在一起对比:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t58.png" style="zoom:33%"><p>总的来说保留Mask Token比较好, 和MASS - style对比, 连续的Mask只替换成一个Token就好, 不需要保留多个.</p><h4 id="Varying-the-Corruption-Rate"><a href="#Varying-the-Corruption-Rate" class="headerlink" title="Varying the Corruption Rate"></a>Varying the Corruption Rate</h4><p>很明显不同的<strong>打Mask几率</strong>有不同的效果:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t59.png" style="zoom:33%"><p>从实验上来看, 15%是一个比较好的值, 看来BERT开了一个好头. 比较大的打Mask率似乎对GLUE的影响比较大, SGLUE掉了一个点, 对其他任务影响好像都不是特别特别明显? 按理说给句子遮上一半会损失特别多的语义.</p><h4 id="Corrupting-spans"><a href="#Corrupting-spans" class="headerlink" title="Corrupting spans"></a>Corrupting spans</h4><p>出于对训练效率的考虑, 每次发生打Mask的动作时, 都会给要预测的<strong>目标序列</strong>多加一个<strong>引导预测</strong>的Mask Token, <strong>被Mask掉的Token不连续的越多</strong>, <strong>目标序列就越长</strong>.</p><p>对每个Token单独判断是否要Mask似乎不如<strong>直接Mask掉Span</strong>来得快, 这样可以<strong>保证目标序列里的Mask Token占比较少</strong>, 训练速度就比较快. 同时, 这个想法曾在SpanBERT上被验证为有益于BERT预训练, 所以作者做了直接Mask掉Span的实验.</p><p>作者所设计的方法是将给Token打Mask的比例和要Mask掉的Span个数设为超参, 以此来控制平均Mask掉的Span长度.</p><p>咱举个例子, 当前序列有500个Token, 所设置的被Mask掉的Token占比为15%, 要Mask掉25个Span, 则总共有500 x 15% = 75个Token会被Mask, Mask掉的Span的平均长度为75 / 25 = 3, 即<code>[Mask]</code>和被Mask掉的Token比例为1:3. 同时, 在固定被Mask掉的内容的Span长度的情况下, 它们全是不连续的, 目标序列长为75 + 25 = 100.</p><p>同样在前面所述的环境下, 采用独立同分布(长度为1)打Mask的方式, 仍是在500个Token中Mask掉15%的Token, 则目标序列长为75 + 75 = 150.</p><p>实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t510.png" style="zoom:33%"><p>在1, 2, 3, 5的设定下来看差异真的不是特别大, Mask掉长度为3的Span似乎比独立同分布的Mask掉Token在除了MT以外的任务要好一些. 但是Mask掉Span的训练速度比较快, 所以作者采用了Span长度为3的Mask方法.</p><h4 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h4><p>作者对无监督目标做出的搜索路径如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t511.png" style="zoom:33%"><p>作者选择的路径是 <code>BERT - style -&gt; Replace spans -&gt; 15% -&gt; 3</code>.</p><h3 id="Pre-training-Datasets"><a href="#Pre-training-Datasets" class="headerlink" title="Pre - training Datasets"></a>Pre - training Datasets</h3><h4 id="Unlabeled-datasets"><a href="#Unlabeled-datasets" class="headerlink" title="Unlabeled datasets"></a>Unlabeled datasets</h4><p>作者希望比较不同数据集给模型预训练带来的影响, 顺便评估一下C4.</p><p>不同预训练数据集结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t512.png" style="zoom:33%"><p>我粗略的介绍一下数据集, C4 unfiltered是指不适用启发式规则过滤的C4, 它的噪声比C4要大的多的多. RealNews和WebText的like版本指的是使用C4的启发式过滤规则过滤后的数据集.</p><p>结论如下:</p><ul><li>C4的<strong>启发式过滤</strong>还是挺有效的, 没过滤的C4明显效果拉了.</li><li>即使域内数据没有标签, <strong>特定领域数据集预训练对特定任务有效</strong>, 感兴趣的可以细搜这些数据集覆盖的区域和不同任务的测试数据来源.</li></ul><h4 id="Pre-training-Datasets-Size"><a href="#Pre-training-Datasets-Size" class="headerlink" title="Pre - training Datasets Size"></a>Pre - training Datasets Size</h4><p>创建C4的核心在于扩大数据规模, 并尽量保证样本的<strong>不重复性</strong>. 作者尝试探索预训练期间多次重复样本对下游任务的影响. 作者尝试缩小C4的规模, 以达到多次重复数据的目的. 实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t513.png" style="zoom:33%"><p>结果表明, 越多次重复, 下游任务表现越差.</p><blockquote><p>至于把C4的规模缩小会不会导致真正有用的样本被减少, 我个人觉得C4实在太大了, 所以截断C4可能不会导致其样本多样性的缺失… 而且每次Mask都是随机的, 所以模型也不会看到完全相同的样本.</p></blockquote><p>作者怀疑模型随着重复次数过多, 学会直接死记硬背预训练数据集, 导致下游任务学习的时候效果就变差了, 作者做出了各规模数据集的Training Loss:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t514.png" style="zoom:33%"><p>Training Loss随着数据集的缩小而减小, 这就意味着可能模型有<strong>记忆现象</strong>, 因为数据集越小, 重复的次数就越容易多, 但是做下游任务的时候就越不容易.</p><h3 id="Training-Strategy"><a href="#Training-Strategy" class="headerlink" title="Training Strategy"></a>Training Strategy</h3><h4 id="Fine-Tuning-Methods"><a href="#Fine-Tuning-Methods" class="headerlink" title="Fine - Tuning Methods"></a>Fine - Tuning Methods</h4><p>不同的训练策略会给模型在下游任务的表现带来不同影响.</p><p>作者在这里列举了两种高级一点的Fine Tuning方法:</p><ul><li><strong>Adapter</strong>: 在Fine Tuning的时候不调原来模型参数, 而是只调一些在原模型基础上加进去的<strong>即插即用插件</strong>, 以代替更新预训练模型的参数, 详情请见论文<a href="https://arxiv.org/abs/1902.00751" target="_blank" rel="noopener">Parameter-efficient transfer learning for NLP</a>.</li><li><strong>Gradual Unfreezing</strong>: 随时间<strong>逐步解冻</strong>模型的参数, 逐步使得模型所有参数都参与更新. 先只微调模型的最后一层, 训练一定次数后解冻倒数第二层, 直到整个模型都解封.</li></ul><p>结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t515.png" style="zoom:33%"><p>各种大小的Adapter似乎都不如Gradual Unfreezing, 根据作者所述, Adapter只在资源比较少的任务上设置比较小的d时效果比较好. 但总的来说还是<strong>微调全部参数</strong>效果最好.</p><h4 id="Multi-Task-Learning"><a href="#Multi-Task-Learning" class="headerlink" title="Multi - Task Learning"></a>Multi - Task Learning</h4><p>前面测试的都是预训练 + 微调的表现, 还没有测试过多任务下的表现. 在text to text下, T5的多任务训练比较简单, 只需要把不同任务的数据集混到一起. 而MTL比较关键的地方在于需要在每个任务之间做权衡, 不能让某个任务的数据过少或过多. 因此作者想了一些办法来控制每类任务的数据集大小, 详情请参照原论文.</p><p>最终结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t516.png" style="zoom:33%"><p>总的来说, 用直接多任务训练不如预训练 + 微调.</p><blockquote><p>此处的MTL应该指的是<strong>不做预训练</strong>, 把各类任务的数据混到一块训练, 所以和预训练必然差着很大的数据量级.</p></blockquote><h4 id="Combining-Multi-Task-Learning-with-Fine-Tuning"><a href="#Combining-Multi-Task-Learning-with-Fine-Tuning" class="headerlink" title="Combining Multi - Task Learning with Fine - Tuning"></a>Combining Multi - Task Learning with Fine - Tuning</h4><p>作者将多任务学习扩展到预训练上, 即同时对所有任务预训练, 然后有监督的对单任务微调.</p><p>把预训练 + 微调和MTL放到一起对比. 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t517.png" style="zoom:33%"><p>单任务学习的预训练 + 微调是最好的, <strong>多任务预训练 + 微调几乎和它一样</strong>. 但是多任务预训练可以在预训练期间就监视下游任务的性能, 所以作者最终为T5采纳了多任务预训练的策略.</p><p>对于翻译这种任务来说, 有监督比无监督要好, 除去翻译任务外无监督效果更好一些.</p><h3 id="Scaling"><a href="#Scaling" class="headerlink" title="Scaling"></a>Scaling</h3><p>大模型和更长的训练时间, 一般都会带来提升. 作者测试了不同规模的模型, 不同训练时间, 以及集成后的模型性能:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t518.png" style="zoom:33%"><p>结论显而易见, 大模型, 长时间都会让性能更好. 但是似乎增大batch size相较于提高训练步长没有那么大提升. <strong>模型分别预训练再微调集成比使用相同的预训练再微调集成效果好</strong>.</p><blockquote><p>当然这些讨论都是建立在像C4这样大规模数据集上的, 模型可能没有完全收敛. 如果数据不够多, 模型可能在训练一段时间就收敛了, 训练再长时间也没用.</p></blockquote><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>总的来说, T5给出了以下的主要结论:</p><ol><li>模型架构: 原生Transformer最好.</li><li>训练目标: 类似BERT的Denosing, 15%的Mask几率.</li><li>训练策略: 全参数微调最好, 多任务预训练后微调和无监督预训练后微调性能相当.</li><li>模型大小: 小模型给长训练时间不如大模型给短训练时间.</li></ol><p>最后再给大家看一眼T5论文的最后一页:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t519.png" style="zoom:33%"><p>确实看着让人头皮发麻, 这足以证明T5是在”Money is All You Need”, “全面探索”两个前提下的结果. 不得不说这种研究只有大公司能做得起.</p><p>我不太建议去读T5的原文, 因为实在是太长了, 但T5中涉及到的引文还是值得看看的, 因为这篇论文几乎把所有当时比较火的预训练模型做了个大串烧, BERT, GPT, MASS, BART, UniLM, ALBERT, 甚至还有SpanBERT, 扩展的话XLNet也算… 这些文章我也都做过笔记, 感兴趣的可以去看下.</p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/58173.html">https://ADAning.github.io/posts/58173.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/PLM/"><span class="chip bg-color">PLM</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/53442.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/11.jpg" class="responsive-img" alt="PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction"> <span class="card-title">PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: CasRel: 详见CasRel: A Novel Cascade Binary Tagging Framework for Relational Triple Extraction. TPLinker: 详见TPLin</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2022-05-30 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/RTE/"><span class="chip bg-color">RTE</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/49404.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/17.jpg" class="responsive-img" alt="Introduction: Deep Reinforcement Learning"> <span class="card-title">Introduction: Deep Reinforcement Learning</span></div></a><div class="card-content article-content"><div class="summary block-with-text">2022.04.14: AlphaGo施工完成. Introduction: Deep Reinforcement Learning本文介绍的是Deep Reinforcement Learning(深度强化学习)入门相关知识, 适合想</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2022-04-02 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/RL/"><span class="chip bg-color">RL</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">378.1k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:695439722@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>