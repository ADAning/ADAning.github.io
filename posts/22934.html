<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="ConvBERT: Improving BERT with Span-based Dynamic Convolution, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>ConvBERT: Improving BERT with Span-based Dynamic Convolution | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:.6"></i> <span>时间轴</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li><li><a href="/markdown"><i class="fab fa-markdown" style="margin-top:-20px;zoom:.6"></i> <span>Markdown</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li><li><a href="/markdown" style="margin-left:75px"><i class="fa fab fa-markdown" style="position:absolute;left:50px"></i> <span>Markdown</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/medias/featureimages/8.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">ConvBERT: Improving BERT with Span-based Dynamic Convolution</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/CNN/"><span class="chip bg-color">CNN</span> </a><a href="/tags/Attention/"><span class="chip bg-color">Attention</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2021-02-12</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2022-03-17</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 2.3k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 8 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><h1 id="ConvBERT-Improving-BERT-with-Span-based-Dynamic-Convolution"><a href="#ConvBERT-Improving-BERT-with-Span-based-Dynamic-Convolution" class="headerlink" title="ConvBERT: Improving BERT with Span-based Dynamic Convolution"></a>ConvBERT: Improving BERT with Span-based Dynamic Convolution</h1><blockquote><p>本文前置知识:</p><ul><li>Light Weight Convolution: 详见<a href="https://adaning.github.io/posts/40162.html">基于轻量级卷积和动态卷积替代的注意力机制</a>.</li><li>Depthwise Separable Convolution, Group Convolution: 详见<a href="https://adaning.github.io/posts/13629.html">深度可分离卷积与分组卷积</a>.</li><li>BERT: 详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>.</li></ul></blockquote><p>本文是论文<a href="https://arxiv.org/abs/2008.02496" target="_blank" rel="noopener">ConvBERT: Improving BERT with Span-based Dynamic Convolution</a>的阅读笔记和个人理解. 属于随缘填坑系列.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者发现, BERT的中所使用的Self Attention每次请求都需要全局的信息, 但实际上并不是每次Attention都需要全局信息, 有时只需要<strong>局部信息</strong>即可, 所以Attention总是伴随着<strong>计算冗余</strong>, 总体来看BERT也没有高度局部化的操作, 但很多头部都只学习了自然语言局部信息.</p><p>因此, 作者希望能够使用一种<strong>直接捕捉局部信息</strong>的方法, 从而降低Attention的复杂度.</p><h2 id="ConvBERT"><a href="#ConvBERT" class="headerlink" title="ConvBERT"></a>ConvBERT</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>既然作者是希望能够获得捕捉局部信息的能力, 还对参数压缩有一定的需求, 那么很容易就想到了用<strong>卷积</strong>.</p><p>在经典Transformer中的Attention机制的公式为:</p><p>$$<br>\operatorname{Self}-\operatorname{Attn}(Q, K, V)=\operatorname{softmax}\left(\frac{Q^{\top} K}{\sqrt{d_{k}}}\right) V<br>$$</p><p>即最简单的缩放点积, 在这里就不多做说明了.</p><h4 id="Parameters-Redundancy"><a href="#Parameters-Redundancy" class="headerlink" title="Parameters Redundancy"></a>Parameters Redundancy</h4><p>Self - Attention在处理问题时可能会存在参数冗余的问题:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/convbert2.jpg" style="zoom:50%"><p>从BERT的注意力中能够看到, 相当多的权重被分配在在<strong>对角线邻近</strong>的位置上, 说明了自然语言结构中的邻近信息在很多时候是起作用的, 这也引起了相当多的权重<strong>冗余</strong>.</p><h4 id="Convolutional-based-Attention"><a href="#Convolutional-based-Attention" class="headerlink" title="Convolutional based Attention"></a>Convolutional based Attention</h4><p>在前面的研究工作中介绍过, 使用卷积能够一定程度上来缓解参数冗余的问题, 例如<a href="https://adaning.github.io/posts/40162.html">轻量级卷积</a>:<br>$$<br>\operatorname{LConv}(X, W, i)=\sum_{j=1}^{k} W_{j} \cdot X_{\left(i+j-\left\lceil\frac{k+1}{2}\right\rceil\right)}<br>$$</p><p>以及以轻量级卷积为基础, 做出一定改进的动态卷积:</p><p>$$<br>\operatorname{DConv}\left(X, W_{f}, i\right)=\operatorname{LConv}\left(X, \operatorname{softmax}\left(W_{f} X_{i}\right), i\right)<br>$$</p><h3 id="Span-based-Dynamic-Convolution"><a href="#Span-based-Dynamic-Convolution" class="headerlink" title="Span - based Dynamic Convolution"></a>Span - based Dynamic Convolution</h3><p>在<a href="https://adaning.github.io/posts/40162.html">基于轻量级卷积和动态卷积替代的注意力机制</a>中提到过:</p><blockquote><p>注意力权重的生成只取决于<strong>当前时刻的输入</strong>, 而与前时刻和后时刻输入无关, 这是一个严重缺陷.</p></blockquote><p>基于这个改进点, 作者将当前时刻输入的邻近信息也加入了动态调整当前时刻输出的机制, 并称之为<strong>区间动态卷积</strong>.</p><p>自注意力, 动态卷积, 区间动态卷积这三者之间的差别能够很容易的用如下图示对比, 也可以顺带引出区间动态卷积的本质:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/convbert1.jpg" style="zoom:50%"><p>自注意力与所有基于卷积方法, 特征之间有更加<strong>稠密</strong>的影响, 但在作者的论文中认为许多交互是不必要的, 即存在特征冗余, 导致了模型参数的冗余. 动态卷积的当前时刻输出仅会取决于<strong>当前时刻输入</strong>, 结合<strong>周围语义</strong>动态调整权重的能力比较差, 因为它分配权重时完全没有结合上下文信息. 区间动态卷积算是在二者之间取了个<strong>折中</strong>, 能结合<strong>小区间范围</strong>内的信息对卷积核的参数做更好的权重分配, 即能够契合语言局部性的特点, 也能结合<strong>语境</strong>做出判断.</p><p>因此, 考虑将区间动态卷积和自注意力兼容, 需要考虑一种结合区间信息<strong>动态生成卷积核</strong>的方法. 作者使用输入$X$ 用Self - Attention的方式生成对应的$Q$ 和 $V$, 接着使用<strong>深度可分离卷积</strong>抽取与区间内容相关的$K_s$, 然后动态地生成卷积核权重:</p><p>$$<br>f\left(Q, K_{s}\right)=\operatorname{softmax}\left(W_{f}\left(Q \odot K_{s}\right)\right)<br>$$</p><p>$\odot$ 为逐元素点乘, $W_f$ 为可训练的权重矩阵.</p><p>自注意力, 动态卷积, 区间动态卷积三者的运算流程结构图如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/convbert3.jpg" style="zoom:33%"><p>明显的能看出来, 在附加了局部信息后, 区间动态卷积与动态卷积相比更像自注意力, 仅替换了某些线性算子, 相较动态卷积而言去掉了GLU, 不会引入过多的参数.</p><p>将这种生成卷积核的方式融入轻量级卷积, 称为区间动态卷积:<br>$$<br>\operatorname{SDConv}\left(Q, K_{s}, V ; W_{f}, i\right)=\operatorname{LConv}\left(V, \operatorname{softmax}\left(W_{f}\left(Q \odot K_{s}\right)\right), i\right)<br>$$</p><p>与动态卷积的式子相比, 将$\operatorname{LConv}$ 中的第一个参数由输入$X$ 替换为自注意力模式中的$V$, 并将生成卷积核参数的方式替换为结合区间信息的$\operatorname{softmax}\left(W_{f}\left(Q \odot K_{s}\right)\right)$.</p><h3 id="ConvBERT-Architecture"><a href="#ConvBERT-Architecture" class="headerlink" title="ConvBERT Architecture"></a>ConvBERT Architecture</h3><h4 id="Mixed-Attention"><a href="#Mixed-Attention" class="headerlink" title="Mixed Attention"></a>Mixed Attention</h4><p>因为自注意力机制和区间动态卷积是可以兼容的, 所以可以将这二者以某种形式混合起来.</p><p>混合Attention就是将区间动态卷积与Self - Attention做了混合, 这二者之间是没有交互的, 直接通过一个Concat操作将二者拼接起来:</p><p>$$<br>\text {Mixed-Attn}\left(K, Q, K_{s}, V ; W_{f}\right)=\operatorname{Cat}\left(\text {Self-Attn}(Q, K, V), \operatorname{SDConv}\left(Q, K_{s}, V ; W_{f}\right)\right)<br>$$</p><p>这样的设计可以让模型不仅限于局部特征的捕捉, 而是以<strong>多角度</strong>来捕捉整个文本的信息.</p><p>自注意力和区间动态卷积共享相同的$Q, V$, 使用不同的方式来生成$K$.</p><h4 id="Bottleneck-Design-for-Self-Attention"><a href="#Bottleneck-Design-for-Self-Attention" class="headerlink" title="Bottleneck Design for Self - Attention"></a>Bottleneck Design for Self - Attention</h4><p>针对冗余参数, Bottleneck的设计能够有助于模型学习到更<strong>紧凑</strong>的信息:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/convbert4.jpg" style="zoom:33%"><p>作者提出的Bottleneck加入<strong>缩放因子</strong>$\gamma$ , 当$\gamma&gt;1$ 时, 将特征维数缩小为$d/\gamma$, 并降低注意力头的数量为原来的$1/\gamma$.</p><p>同时, 这样的设计也可以保证模块的有序堆叠, 输入大小和输出大小一致.</p><h4 id="Grouped-Feed-Forward-Module"><a href="#Grouped-Feed-Forward-Module" class="headerlink" title="Grouped Feed - Forward Module"></a>Grouped Feed - Forward Module</h4><p>因为FFN中占了很多参数, 所以作者希望通过<strong>分组</strong>的方式来减小开销. 与多头注意力类似, 分组卷积分别将特征分组提取后再Concat起来:</p><p>$$<br>\displaylines{<br>M=\Pi_{i=0}^{g}\left[f_{\frac{d}{g} \rightarrow \frac{m}{g}}^{i}\left(H_{\left[:, i-1: i \times \frac{d}{g}\right]}\right)\right], M^{\prime}=\operatorname{GeLU}(M)<br>\\<br>H^{\prime}=\Pi_{i=0}^{g}\left[f_{\frac{m}{g} \rightarrow \frac{d}{g}}^{i}\left(M_{\left[:, i-1: i \times \frac{m}{g}\right]}^{\prime}\right)\right]<br>}<br>$$</p><p>其中$H, H^\prime \in R^{n\times n}$, $M, M^\prime \in R^{n\times n}$, $f_{d_1 \rightarrow d_2}(\cdot)$ 表示将$d_1$ 维映射到$d_2$ 维的FC层. $g$ 为分组的组数, $\Pi$ 为Concat操作.</p><p>后续的实验表明, 精度下降可以忽略不计.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>在实验中, 作者使用了开源网页数据集OpenWebText(32G)来对标BERT所用的训练数据. 作者采用$\gamma=2$ 来缩小特征维度, 注意力头的数量也为原来的一半. 其余详细的实验设置请参考原论文.</p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>消融实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/convbert5.jpg" style="zoom:33%"><p>小Trick对提升模型性能有一些帮助, 但总体来说区间动态卷积的提升比较大.</p><h4 id="Kernel-Size"><a href="#Kernel-Size" class="headerlink" title="Kernel Size"></a>Kernel Size</h4><p>对于Kernel Size实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/convbert6.jpg" style="zoom:50%"><p>模型性能先是随着卷积核的大小提升而提升, 在达到一定阈值后性能反而下降. 很有可能是感受野逐渐扩大, 覆盖到整个输入序列, 此前性能均在提升. 而Kernel Size覆盖整个输入序列时, 模型效果稍有下降.</p><p>这个实验再一次佐证了作者的猜想.</p><h4 id="Ways-to-integrate-convolution"><a href="#Ways-to-integrate-convolution" class="headerlink" title="Ways to integrate convolution"></a>Ways to integrate convolution</h4><p>如下作者探究不同形式的卷积对模型性能的影响:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/convbert7.jpg" style="zoom:33%"><p>肯定是区间动态卷积比较好, 因为区间动态卷积本身就是由其他卷积形式变换而来的.</p><h3 id="Comparison-results"><a href="#Comparison-results" class="headerlink" title="Comparison results"></a>Comparison results</h3><h4 id="GLUE"><a href="#GLUE" class="headerlink" title="GLUE"></a>GLUE</h4><p>其中标有十字架标志的模型是基于知识蒸馏的方法, 在GLUE上的结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/convbert8.jpg" style="zoom:33%"><p>ConvBERT在低资源的情况下结果良好.</p><h4 id="SQuAD"><a href="#SQuAD" class="headerlink" title="SQuAD"></a>SQuAD</h4><p>在问答数据集SQuAD上的结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/convbert9.jpg" style="zoom:33%"><p>延长训练后, 虽然训练所需的计算量下降了, 但是仍然能够达到近似ELECTRA的效果. 极大地减少了训练成本.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>本文中仍然是一篇NLP领域的论文, 我读它的原因主要是为了获取在Attention上改动的灵感.</p><p>作者在轻量级卷积的基础上进一步的将卷积核的权重生成动态化. 设计了一种新型Bottleneck, 进一步的优化了BERT的结构, 并在其中加入了卷积操作, 大幅的降低了训练成本, 但似乎这种方法<strong>不够简洁</strong>.</p><p>作者没有将自注意力完完全全的去掉, 也说明了NLP中Self - Attention必然是不能被其他方法直接取代的.</p><p>后面实验主要也对比的Baseline主要也是小模型系列, 不知与大模型相比效果如何(虽然有点不公平).</p><p>训练过程中也使用了一些小Trick, 没有这些小Trick可能不会有特别亮眼的效果.</p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">AnNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/22934.html">https://ADAning.github.io/posts/22934.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">AnNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/CNN/"><span class="chip bg-color">CNN</span> </a><a href="/tags/Attention/"><span class="chip bg-color">Attention</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/21282.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/medias/featureimages/11.jpg" class="responsive-img" alt="LightCAKE: A Lightweight Framework for Context-Aware Knowledge Graph Embedding"> <span class="card-title">LightCAKE: A Lightweight Framework for Context-Aware Knowledge Graph Embedding</span></div></a><div class="card-content article-content"><div class="summary block-with-text">2021.3.9: 修正关于引入逆三元组的影响. 2021.4.18: 更新一篇更早的类似论文GAKE. LightCAKE: A Lightweight Framework for Context-Aware Knowledge Gr</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2021-03-08 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/" class="post-category">知识图谱</a></span></div></div><div class="card-action article-tags"><a href="/tags/KGE/"><span class="chip bg-color">KGE</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/11653.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/medias/featureimages/1.jpg" class="responsive-img" alt="PPKE: Knowledge Representation Learning by Path-based Pre-training"> <span class="card-title">PPKE: Knowledge Representation Learning by Path-based Pre-training</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: BERT(Transformer Encoder). PPKE: Knowledge Representation Learning by Path-based Pre-training本文是论文PPKE: Know</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2021-01-18 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/" class="post-category">知识图谱</a></span></div></div><div class="card-action article-tags"><a href="/tags/KGE/"><span class="chip bg-color">KGE</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">AnNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">234.4k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:695439722@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/instantpage/instantpage.js" type="module"></script></body></html>