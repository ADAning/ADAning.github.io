<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="Pytorch实现: BERT, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>Pytorch实现: BERT | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:.6"></i> <span>时间轴</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li><li><a href="/markdown"><i class="fab fa-markdown" style="margin-top:-20px;zoom:.6"></i> <span>Markdown</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li><li><a href="/markdown" style="margin-left:75px"><i class="fa fab fa-markdown" style="position:absolute;left:50px"></i> <span>Markdown</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/16.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">Pytorch实现: BERT</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/Transformer/"><span class="chip bg-color">Transformer</span> </a><a href="/tags/Pytorch/"><span class="chip bg-color">Pytorch</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2021-03-12</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2022-04-05</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 3.3k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 16 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p>本文前置知识:</p><ul><li>Transformer: 详见<a href="https://adaning.github.io/posts/6744.html">Transformer精讲</a></li><li>BERT: 详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a></li><li><a href="https://adaning.github.io/posts/63679.html">Pytorch实现: Transformer</a></li></ul><p><strong>2022.04.03</strong>: 修正Pre Norm效果好于Post Norm的错误描述.</p></blockquote><h1 id="Pytorch实现-BERT"><a href="#Pytorch实现-BERT" class="headerlink" title="Pytorch实现: BERT"></a>Pytorch实现: BERT</h1><p>本文是BERT的Pytorch版本实现. 实现并没有完全参照BERT原论文中的设置, 有些细枝末节的地方可能没有考虑进去, 每个人实现的方法可能也不同, 可以不必过于纠结这些. BERT的实现比Transformer更简单, 因为不用考虑Decoder.</p><p>本文参考如下文章:</p><ul><li><a href="https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html#BertModel" target="_blank" rel="noopener">Hugging Face的BERT实现</a></li><li><a href="https://wmathor.com/index.php/archives/1457/" target="_blank" rel="noopener">BERT 的 PyTorch 实现</a></li></ul><p>本文的代码已经放到了Colab上, 打开设置GPU就可以复现(需要科学上网).</p><p><a href="https://colab.research.google.com/drive/13PZiE-UQlGy-gSF_KZ-L9wq9RYsBbXDj?usp=sharing" target="_blank" rel="noopener"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="右键我在COLAB中打开!"></a></p><p>如果你不能科学上网, 应该看不到<code>Open in Colab</code>的图标.</p><h2 id="Preparing"><a href="#Preparing" class="headerlink" title="Preparing"></a>Preparing</h2><p>先导包:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> random
<span class="token keyword">import</span> re
<span class="token keyword">from</span> math <span class="token keyword">import</span> sqrt <span class="token keyword">as</span> msqrt

<span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">import</span> Adadelta
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader<span class="token punctuation">,</span> Dataset<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>定义与BERT相关的参数:</p><pre class="line-numbers language-python"><code class="language-python">max_len <span class="token operator">=</span> <span class="token number">30</span>
max_vocab <span class="token operator">=</span> <span class="token number">50</span>
max_pred <span class="token operator">=</span> <span class="token number">5</span>

d_k <span class="token operator">=</span> d_v <span class="token operator">=</span> <span class="token number">64</span>
d_model <span class="token operator">=</span> <span class="token number">768</span>  <span class="token comment" spellcheck="true"># n_heads * d_k</span>
d_ff <span class="token operator">=</span> d_model <span class="token operator">*</span> <span class="token number">4</span>

n_heads <span class="token operator">=</span> <span class="token number">12</span>
n_layers <span class="token operator">=</span> <span class="token number">6</span>
n_segs <span class="token operator">=</span> <span class="token number">2</span>

p_dropout <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token number">1</span>
<span class="token comment" spellcheck="true"># BERT propability defined</span>
p_mask <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token number">8</span>
p_replace <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token number">1</span>
p_do_nothing <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">-</span> p_mask <span class="token operator">-</span> p_replace


device <span class="token operator">=</span> <span class="token string">"cuda"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span>
device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><p>max_len: 输入序列的最大长度.</p></li><li><p>max_vocab: 字典的最大大小.</p></li><li><p>max_pred: Mask时最大的Mask数量.</p></li><li><p>d_k, d_v: 自注意力中K和V的维度, Q的维度直接用K的维度代替, 因为这二者必须始终相等.</p></li><li><p>d_model: Embedding的大小.</p></li><li><p>d_ff: 前馈神经网络的隐藏层大小, 一般是d_model的四倍.</p></li><li><p>n_heads: 多头注意力的头数.</p></li><li><p>n_layers: Encoder的堆叠层数.</p></li><li><p>n_segs: 输入BERT的句子段数. 用于制作Segment Embedding.</p></li><li><p>p_dropout: BERT中所有dropout的概率.</p></li><li><p>p_mask, p_replace, p_do_nothing:</p><ul><li><p><strong>80%</strong>的概率将被选中的单词替换为<code>[MASK]</code>.</p></li><li><p><strong>10%</strong>的概率将被选中的单词替换为<strong>随机词</strong>.</p></li><li><p><strong>10%</strong>的概率对被选中的单词<strong>不进行替换</strong>.</p></li></ul></li></ul><h2 id="Mask-and-GELU"><a href="#Mask-and-GELU" class="headerlink" title="Mask and GELU"></a>Mask and GELU</h2><p>在BERT中没有Decoder, 所以我们的Mask实际上只为<strong>Padding</strong>服务.</p><h3 id="Mask"><a href="#Mask" class="headerlink" title="Mask"></a>Mask</h3><p>因为预期的Token输入序列大小为<code>[batch, seq_len]</code>, 如果token中的索引与pad索引相同, 那么则</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_pad_mask</span><span class="token punctuation">(</span>tokens<span class="token punctuation">,</span> pad_idx<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">'''
    suppose index of [PAD] is zero in word2idx
    tokens: [batch, seq_len]
    '''</span>
    batch<span class="token punctuation">,</span> seq_len <span class="token operator">=</span> tokens<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
    pad_mask <span class="token operator">=</span> tokens<span class="token punctuation">.</span>data<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>pad_idx<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
    pad_mask <span class="token operator">=</span> pad_mask<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> seq_len<span class="token punctuation">)</span>
    <span class="token keyword">return</span> pad_mask<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="GELU"><a href="#GELU" class="headerlink" title="GELU"></a>GELU</h3><p>在BERT中采用GELU作为激活函数, 它与ReLU相比具有一些概率上的性质:<br>$$<br>\displaylines{<br>\operatorname{GELU}(x)=x P(X \leq x)= x \Phi(x)=x \cdot \frac{1}{2}[1+\operatorname{erf}(x / \sqrt{2})] \\<br>or \\<br>0.5 x\left(1+\tanh \left[\sqrt{2 / \pi}\left( x+ 0.044715 x^{3}\right)\right]\right)<br>}<br>$$<br>第二行的是GELU的近似表达式. 它实现起来非常简单:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">gelu</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">'''
    Two way to implements GELU:
    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))
    or
    0.5 * x * (1. + torch.erf(torch.sqrt(x, 2))) 
    '''</span>
    <span class="token keyword">return</span> <span class="token punctuation">.</span><span class="token number">5</span> <span class="token operator">*</span> x <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">.</span> <span class="token operator">+</span> torch<span class="token punctuation">.</span>erf<span class="token punctuation">(</span>x <span class="token operator">/</span> msqrt<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>两种方式均可, 我这里采用第一种.</p><h2 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h2><p>BERT中含有三种编码, Word Embedding, Position Embedding, Segment Embedding:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert2.jpg" style="zoom:50%"><p>其中Position Embedding不像Transformer中是用正余弦编码计算得到, 而是通过学习获得.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Embeddings</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>Embeddings<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>seg_emb <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>n_segs<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>word_emb <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>max_vocab<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pos_emb <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>max_len<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p_dropout<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> seg<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">'''
        x: [batch, seq_len]
        '''</span>
        word_enc <span class="token operator">=</span> self<span class="token punctuation">.</span>word_emb<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token comment" spellcheck="true"># positional embedding</span>
        pos <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>long<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
        pos <span class="token operator">=</span> pos<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        pos_enc <span class="token operator">=</span> self<span class="token punctuation">.</span>pos_emb<span class="token punctuation">(</span>pos<span class="token punctuation">)</span>

        seg_enc <span class="token operator">=</span> self<span class="token punctuation">.</span>seg_emb<span class="token punctuation">(</span>seg<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>word_enc <span class="token operator">+</span> pos_enc <span class="token operator">+</span> seg_enc<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># return: [batch, seq_len, d_model]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里的LayerNorm有些版本的实现加了, 有些没有加, 看个人爱好吧.</p><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>这里的点积缩放注意力和多头注意力完全和Transformer一致, 不再细说, 直接照搬过来就行.</p><h3 id="Scaled-DotProduct-Attention"><a href="#Scaled-DotProduct-Attention" class="headerlink" title="Scaled DotProduct Attention"></a>Scaled DotProduct Attention</h3><p>$$<br>\operatorname{Attention}(Q, K, V) = \operatorname{softmax}(\frac{QK^T}{\sqrt{d_k}})V<br>$$</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">ScaledDotProductAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>ScaledDotProductAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> Q<span class="token punctuation">,</span> K<span class="token punctuation">,</span> V<span class="token punctuation">,</span> attn_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
        scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>Q<span class="token punctuation">,</span> K<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">/</span> msqrt<span class="token punctuation">(</span>d_k<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># scores: [batch, n_heads, seq_len, seq_len]</span>
        scores<span class="token punctuation">.</span>masked_fill_<span class="token punctuation">(</span>attn_mask<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1e9</span><span class="token punctuation">)</span>
        attn <span class="token operator">=</span> nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span>scores<span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># context: [batch, n_heads, seq_len, d_v]</span>
        context <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>attn<span class="token punctuation">,</span> V<span class="token punctuation">)</span>
        <span class="token keyword">return</span> context<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi - Head Attention"></a>Multi - Head Attention</h3><p>$$<br>\begin{aligned}<br>\operatorname{MultiHead}(Q, K, V) &amp;= \operatorname{Concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_h)W^O \\<br>\text{where } \text{head}_i &amp;= \operatorname{Attention}(QW^Q_i, KW^K_i, VW^V_i)<br>\end{aligned}<br>$$</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>MultiHeadAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_Q <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_k <span class="token operator">*</span> n_heads<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_K <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_k <span class="token operator">*</span> n_heads<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_V <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_v <span class="token operator">*</span> n_heads<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>n_heads <span class="token operator">*</span> d_v<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> Q<span class="token punctuation">,</span> K<span class="token punctuation">,</span> V<span class="token punctuation">,</span> attn_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">'''
        Q, K, V: [batch, seq_len, d_model]
        attn_mask: [batch, seq_len, seq_len]
        '''</span>
        batch <span class="token operator">=</span> Q<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token triple-quoted-string string">'''
        split Q, K, V to per head formula: [batch, seq_len, n_heads, d_k]
        Convenient for matrix multiply opearation later
        q, k, v: [batch, n_heads, seq_len, d_k / d_v]
        '''</span>
        per_Q <span class="token operator">=</span> self<span class="token punctuation">.</span>W_Q<span class="token punctuation">(</span>Q<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> d_k<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        per_K <span class="token operator">=</span> self<span class="token punctuation">.</span>W_K<span class="token punctuation">(</span>K<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> d_k<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        per_V <span class="token operator">=</span> self<span class="token punctuation">.</span>W_V<span class="token punctuation">(</span>V<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> d_v<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>

        attn_mask <span class="token operator">=</span> attn_mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># context: [batch, n_heads, seq_len, d_v]</span>
        context <span class="token operator">=</span> ScaledDotProductAttention<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>per_Q<span class="token punctuation">,</span> per_K<span class="token punctuation">,</span> per_V<span class="token punctuation">,</span> attn_mask<span class="token punctuation">)</span>
        context <span class="token operator">=</span> context<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>
            batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads <span class="token operator">*</span> d_v<span class="token punctuation">)</span>

        <span class="token comment" spellcheck="true"># output: [batch, seq_len, d_model]</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>context<span class="token punctuation">)</span>
        <span class="token keyword">return</span> output<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Feed-Forward-Neural-Network"><a href="#Feed-Forward-Neural-Network" class="headerlink" title="Feed Forward Neural Network"></a>Feed Forward Neural Network</h2><p>BERT中的FFN实现将激活函数换为了GELU:<br>$$<br>\operatorname{FFN}(x)=\operatorname{GELU}(xW_1+b_1)W_2 + b_2<br>$$</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">FeedForwardNetwork</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>FeedForwardNetwork<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_ff<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_ff<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p_dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>gelu <span class="token operator">=</span> gelu

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>gelu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我把残差部分移到了EncoderLayer的设计中, 见下节.</p><h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p>我对Encoder的实现进行了调整. 在Encoder中控制两个Sub Layer的Layer Norm和Residual connection.</p><p>在论文<a href="https://openreview.net/pdf?id=B1x8anVFPr" target="_blank" rel="noopener">ON LAYER NORMALIZATION IN THE TRANSFORMER ARCHITECTURE</a>中曾经提到, Transformer中Layer Norm的位置加的有问题, 在Sub Layer后加Layer Norm被称为Post Norm, 它是不规范的. Layer Norm如果调整到Sub Layer前会对训练有很大帮助, 称为<strong>Pre Norm</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/prenorm.jpg" style="zoom:50%"><blockquote><p>事实上, 截止到2022年4月3日, 研究表明, Pre Norm并不是比Post Norm效果好, 而是因为Pre Norm比Post Norm更好训练. 实际上, 如果能够训练完全, Post Norm效果是比Pre Norm好的, 原因见<a href="https://spaces.ac.cn/archives/9009" target="_blank" rel="noopener">为什么Pre Norm的效果不如Post Norm？</a>.</p></blockquote><p>我这里采用了Pre Norm版本的实现:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">EncoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>EncoderLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>enc_attn <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ffn <span class="token operator">=</span> FeedForwardNetwork<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> pad_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">'''
        pre-norm
        see more detail in https://openreview.net/pdf?id=B1x8anVFPr

        x: [batch, seq_len, d_model]
        '''</span>
        residual <span class="token operator">=</span> x
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>enc_attn<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> pad_mask<span class="token punctuation">)</span> <span class="token operator">+</span> residual
        residual <span class="token operator">=</span> x
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>ffn<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x <span class="token operator">+</span> residual<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Pooler"><a href="#Pooler" class="headerlink" title="Pooler"></a>Pooler</h2><p>Pooler是<a href="https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html#BertModel" target="_blank" rel="noopener">Hugging Face实现BERT</a>时加上的额外组件. NSP任务需要提取<code>[CLS]</code>处的特征, Hugging Face的做法是将<code>[CLS]</code>处的输出接上一个FC, 并用tanh激活, 最后再接上二分类输出层. 他们将这一过程称为”<strong>Pool</strong>“.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Pooler</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>Pooler<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>tanh <span class="token operator">=</span> nn<span class="token punctuation">.</span>Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">'''
        x: [batch, d_model] (first place output)
        '''</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>因为额外添加了一个FC层, 所以能增强表达能力, 同样提升了训练难度.</p><h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p>现在大框架中的Embedding, EncoderLayer, Pooler已经定义好了, 只需要额外定义输出时需要的其他组件. 在NSP任务输出时需要额外定义一个二分类输出层<code>next_cls</code>, 还有MLM任务输出所需的<code>word_classifier</code>, 以及前向传递<code>forward</code>.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">BERT</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>BERT<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> Embeddings<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>encoders <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>
            EncoderLayer<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_layers<span class="token punctuation">)</span>
        <span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pooler <span class="token operator">=</span> Pooler<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>next_cls <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>gelu <span class="token operator">=</span> gelu
        <span class="token comment" spellcheck="true"># Sharing weight between some fully connect layer</span>
        shared_weight <span class="token operator">=</span> self<span class="token punctuation">.</span>pooler<span class="token punctuation">.</span>fc<span class="token punctuation">.</span>weight
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc<span class="token punctuation">.</span>weight <span class="token operator">=</span> shared_weight
        <span class="token comment" spellcheck="true"># Sharing weight between word embedding and classifier</span>
        shared_weight <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>word_emb<span class="token punctuation">.</span>weight
        self<span class="token punctuation">.</span>word_classifier <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> max_vocab<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>word_classifier<span class="token punctuation">.</span>weight <span class="token operator">=</span> shared_weight

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tokens<span class="token punctuation">,</span> segments<span class="token punctuation">,</span> masked_pos<span class="token punctuation">)</span><span class="token punctuation">:</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>tokens<span class="token punctuation">,</span> segments<span class="token punctuation">)</span>
        enc_self_pad_mask <span class="token operator">=</span> get_pad_mask<span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>
        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>encoders<span class="token punctuation">:</span>
            output <span class="token operator">=</span> layer<span class="token punctuation">(</span>output<span class="token punctuation">,</span> enc_self_pad_mask<span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># output: [batch, max_len, d_model]</span>

        <span class="token comment" spellcheck="true"># NSP Task</span>
        hidden_pool <span class="token operator">=</span> self<span class="token punctuation">.</span>pooler<span class="token punctuation">(</span>output<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        logits_cls <span class="token operator">=</span> self<span class="token punctuation">.</span>next_cls<span class="token punctuation">(</span>hidden_pool<span class="token punctuation">)</span>

        <span class="token comment" spellcheck="true"># Masked Language Model Task</span>
        <span class="token comment" spellcheck="true"># masked_pos: [batch, max_pred] -> [batch, max_pred, d_model]</span>
        masked_pos <span class="token operator">=</span> masked_pos<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>

        <span class="token comment" spellcheck="true"># h_masked: [batch, max_pred, d_model]</span>
        h_masked <span class="token operator">=</span> torch<span class="token punctuation">.</span>gather<span class="token punctuation">(</span>output<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> index<span class="token operator">=</span>masked_pos<span class="token punctuation">)</span>
        h_masked <span class="token operator">=</span> self<span class="token punctuation">.</span>gelu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>h_masked<span class="token punctuation">)</span><span class="token punctuation">)</span>
        logits_lm <span class="token operator">=</span> self<span class="token punctuation">.</span>word_classifier<span class="token punctuation">(</span>h_masked<span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># logits_lm: [batch, max_pred, max_vocab]</span>
        <span class="token comment" spellcheck="true"># logits_cls: [batch, 2]</span>

        <span class="token keyword">return</span> logits_cls<span class="token punctuation">,</span> logits_lm<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>做个说明:</p><ol><li>为了减少模型训练上的负担, 这里对<code>pooler</code>的<code>fc</code>和MLM输出时使用的<code>fc</code>做了权重共享, 也对Word Embedding和<code>word_classifier</code>的权重做了共享.</li><li><code>torch.gather</code>能收集特定维度的指定位置的数值. <code>h_masked</code>使用的<code>gather</code>是为了检索<code>output</code>中 <code>max_len</code>维度上被Mask的位置上的表示, 总大小<code>[batch, max_pred, d_model]</code>. 因为<code>masked_pos</code>大小为<code>[batch, max_pred, d_model]</code>. 可能我表述不太清楚, 请参照<a href="https://adaning.github.io/posts/1216.html">Pytorch之张量进阶操作</a>中的例子理解.</li><li>没在模型中使用<code>Softmax</code>和<code>Simgoid</code>的原因是<code>nn.CrossEntropyLoss</code>自带将Logits转为概率的效果.</li></ol><h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>这部分主要是准备数据.</p><h3 id="Text-data"><a href="#Text-data" class="headerlink" title="Text data"></a>Text data</h3><p>采用一段简单的对话来作为原始数据. 为了方便起见, 这里没有采用<strong>Subword</strong>.</p><pre class="line-numbers language-python"><code class="language-python">test_text <span class="token operator">=</span> <span class="token punctuation">(</span>
    <span class="token string">'Hello, how are you? I am Romeo.\n'</span>  <span class="token comment" spellcheck="true"># R</span>
    <span class="token string">'Hello, Romeo My name is Juliet. Nice to meet you.\n'</span>  <span class="token comment" spellcheck="true"># J</span>
    <span class="token string">'Nice meet you too. How are you today?\n'</span>  <span class="token comment" spellcheck="true"># R</span>
    <span class="token string">'Great. My baseball team won the competition.\n'</span>  <span class="token comment" spellcheck="true"># J</span>
    <span class="token string">'Oh Congratulations, Juliet\n'</span>  <span class="token comment" spellcheck="true"># R</span>
    <span class="token string">'Thank you Romeo\n'</span>  <span class="token comment" spellcheck="true"># J</span>
    <span class="token string">'Where are you going today?\n'</span>  <span class="token comment" spellcheck="true"># R</span>
    <span class="token string">'I am going shopping. What about you?\n'</span>  <span class="token comment" spellcheck="true"># J</span>
    <span class="token string">'I am going to visit my grandmother. she is not very well'</span>  <span class="token comment" spellcheck="true"># R</span>
<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># we need [MASK] [SEP] [PAD] [CLS]</span>
word2idx <span class="token operator">=</span> <span class="token punctuation">{</span>f<span class="token string">'[{name}]'</span><span class="token punctuation">:</span> idx <span class="token keyword">for</span> idx<span class="token punctuation">,</span>
            name <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'PAD'</span><span class="token punctuation">,</span> <span class="token string">'CLS'</span><span class="token punctuation">,</span> <span class="token string">'SEP'</span><span class="token punctuation">,</span> <span class="token string">'MASK'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
<span class="token comment" spellcheck="true"># {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}</span>

sentences <span class="token operator">=</span> re<span class="token punctuation">.</span>sub<span class="token punctuation">(</span><span class="token string">"[.,!?\\-]"</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">,</span> test_text<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">)</span>
word_list <span class="token operator">=</span> list<span class="token punctuation">(</span>set<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

holdplace <span class="token operator">=</span> len<span class="token punctuation">(</span>word2idx<span class="token punctuation">)</span>
<span class="token keyword">for</span> idx<span class="token punctuation">,</span> word <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>word_list<span class="token punctuation">)</span><span class="token punctuation">:</span>
    word2idx<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token operator">=</span> idx <span class="token operator">+</span> holdplace

idx2word <span class="token operator">=</span> <span class="token punctuation">{</span>idx<span class="token punctuation">:</span> word <span class="token keyword">for</span> word<span class="token punctuation">,</span> idx <span class="token keyword">in</span> word2idx<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
vocab_size <span class="token operator">=</span> len<span class="token punctuation">(</span>word2idx<span class="token punctuation">)</span>
<span class="token keyword">assert</span> len<span class="token punctuation">(</span>word2idx<span class="token punctuation">)</span> <span class="token operator">==</span> len<span class="token punctuation">(</span>idx2word<span class="token punctuation">)</span>

token_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> sentence <span class="token keyword">in</span> sentences<span class="token punctuation">:</span>
    token_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">[</span>
        word2idx<span class="token punctuation">[</span>s<span class="token punctuation">]</span> <span class="token keyword">for</span> s <span class="token keyword">in</span> sentence<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里需要编写两个函数:</p><ul><li><code>padding</code>: 句子长度不够时, 用<code>[PAD]</code>补全.</li><li><code>masking_produce</code>: 按照BERT论文中提到的Mask方式.</li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">padding</span><span class="token punctuation">(</span>ids<span class="token punctuation">,</span> n_pads<span class="token punctuation">,</span> pad_symb<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> ids<span class="token punctuation">.</span>extend<span class="token punctuation">(</span><span class="token punctuation">[</span>pad_symb <span class="token keyword">for</span> _ <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_pads<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">masking_procedure</span><span class="token punctuation">(</span>cand_pos<span class="token punctuation">,</span> input_ids<span class="token punctuation">,</span> masked_symb<span class="token operator">=</span>word2idx<span class="token punctuation">[</span><span class="token string">'[MASK]'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    masked_pos <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    masked_tokens <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> pos <span class="token keyword">in</span> cand_pos<span class="token punctuation">:</span>
        masked_pos<span class="token punctuation">.</span>append<span class="token punctuation">(</span>pos<span class="token punctuation">)</span>
        masked_tokens<span class="token punctuation">.</span>append<span class="token punctuation">(</span>input_ids<span class="token punctuation">[</span>pos<span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> p_mask<span class="token punctuation">:</span>
            input_ids<span class="token punctuation">[</span>pos<span class="token punctuation">]</span> <span class="token operator">=</span> masked_symb
        <span class="token keyword">elif</span> random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token punctuation">(</span>p_mask <span class="token operator">+</span> p_replace<span class="token punctuation">)</span><span class="token punctuation">:</span>
            rand_word_idx <span class="token operator">=</span> random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> vocab_size <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span>
            input_ids<span class="token punctuation">[</span>pos<span class="token punctuation">]</span> <span class="token operator">=</span> rand_word_idx

    <span class="token keyword">return</span> masked_pos<span class="token punctuation">,</span> masked_tokens<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我们需要给句子加上<code>[CLS], [&#39;SEP&#39;], [MASK]</code>来符合BERT的输入格式. 并且保持样本中句子相邻和不相邻的比例是半对半, 即有一半样本中输入句子对是相邻的, 有一半不相邻.</p><p>这里简单的用两个句子的index是否相邻来判断上下文是否相邻, 不是很严谨, 只用于自己实现测试.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">make_data</span><span class="token punctuation">(</span>sentences<span class="token punctuation">,</span> n_data<span class="token punctuation">)</span><span class="token punctuation">:</span>
    batch_data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    positive <span class="token operator">=</span> negative <span class="token operator">=</span> <span class="token number">0</span>
    len_sentences <span class="token operator">=</span> len<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span>
    <span class="token comment" spellcheck="true"># 50% sampling adjacent sentences, 50% sampling not adjacent sentences</span>
    <span class="token keyword">while</span> positive <span class="token operator">!=</span> n_data <span class="token operator">/</span> <span class="token number">2</span> <span class="token operator">or</span> negative <span class="token operator">!=</span> n_data <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">:</span>
        tokens_a_idx <span class="token operator">=</span> random<span class="token punctuation">.</span>randrange<span class="token punctuation">(</span>len_sentences<span class="token punctuation">)</span>
        tokens_b_idx <span class="token operator">=</span> random<span class="token punctuation">.</span>randrange<span class="token punctuation">(</span>len_sentences<span class="token punctuation">)</span>
        tokens_a <span class="token operator">=</span> sentences<span class="token punctuation">[</span>tokens_a_idx<span class="token punctuation">]</span>
        tokens_b <span class="token operator">=</span> sentences<span class="token punctuation">[</span>tokens_b_idx<span class="token punctuation">]</span>

        input_ids <span class="token operator">=</span> <span class="token punctuation">[</span>word2idx<span class="token punctuation">[</span><span class="token string">'[CLS]'</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">+</span> tokens_a <span class="token operator">+</span> <span class="token punctuation">[</span>word2idx<span class="token punctuation">[</span><span class="token string">'[SEP]'</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">+</span> tokens_b <span class="token operator">+</span> <span class="token punctuation">[</span>word2idx<span class="token punctuation">[</span><span class="token string">'[SEP]'</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
        segment_ids <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>
            <span class="token number">1</span> <span class="token operator">+</span> len<span class="token punctuation">(</span>tokens_a<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token number">1</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> len<span class="token punctuation">(</span>tokens_b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>

        n_pred <span class="token operator">=</span> min<span class="token punctuation">(</span>max_pred<span class="token punctuation">,</span> max<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> int<span class="token punctuation">(</span>len<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        cand_pos <span class="token operator">=</span> <span class="token punctuation">[</span>i <span class="token keyword">for</span> i<span class="token punctuation">,</span> token <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>
                    <span class="token keyword">if</span> token <span class="token operator">!=</span> word2idx<span class="token punctuation">[</span><span class="token string">'[CLS]'</span><span class="token punctuation">]</span> <span class="token operator">and</span> token <span class="token operator">!=</span> word2idx<span class="token punctuation">[</span><span class="token string">'[SEP]'</span><span class="token punctuation">]</span><span class="token punctuation">]</span>

        <span class="token comment" spellcheck="true"># shuffle all candidate position index, to sampling maksed position from first n_pred</span>
        masked_pos<span class="token punctuation">,</span> masked_tokens <span class="token operator">=</span> masking_procedure<span class="token punctuation">(</span>
            cand_pos<span class="token punctuation">[</span><span class="token punctuation">:</span>n_pred<span class="token punctuation">]</span><span class="token punctuation">,</span> input_ids<span class="token punctuation">,</span> word2idx<span class="token punctuation">[</span><span class="token string">'[MASK]'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        <span class="token comment" spellcheck="true"># zero padding for tokens</span>
        padding<span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> max_len <span class="token operator">-</span> len<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span><span class="token punctuation">)</span>
        padding<span class="token punctuation">(</span>segment_ids<span class="token punctuation">,</span> max_len <span class="token operator">-</span> len<span class="token punctuation">(</span>segment_ids<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token comment" spellcheck="true"># zero padding for mask</span>
        <span class="token keyword">if</span> max_pred <span class="token operator">></span> n_pred<span class="token punctuation">:</span>
            n_pads <span class="token operator">=</span> max_pred <span class="token operator">-</span> n_pred
            padding<span class="token punctuation">(</span>masked_pos<span class="token punctuation">,</span> n_pads<span class="token punctuation">)</span>
            padding<span class="token punctuation">(</span>masked_tokens<span class="token punctuation">,</span> n_pads<span class="token punctuation">)</span>

        <span class="token keyword">if</span> <span class="token punctuation">(</span>tokens_a_idx <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">==</span> tokens_b_idx <span class="token operator">and</span> positive <span class="token operator">&lt;</span> <span class="token punctuation">(</span>n_data <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            batch_data<span class="token punctuation">.</span>append<span class="token punctuation">(</span>
                <span class="token punctuation">[</span>input_ids<span class="token punctuation">,</span> segment_ids<span class="token punctuation">,</span> masked_tokens<span class="token punctuation">,</span> masked_pos<span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            positive <span class="token operator">+=</span> <span class="token number">1</span>
        <span class="token keyword">elif</span> <span class="token punctuation">(</span>tokens_a_idx <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">!=</span> tokens_b_idx <span class="token operator">and</span> negative <span class="token operator">&lt;</span> <span class="token punctuation">(</span>n_data <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            batch_data<span class="token punctuation">.</span>append<span class="token punctuation">(</span>
                <span class="token punctuation">[</span>input_ids<span class="token punctuation">,</span> segment_ids<span class="token punctuation">,</span> masked_tokens<span class="token punctuation">,</span> masked_pos<span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            negative <span class="token operator">+=</span> <span class="token number">1</span>

    <span class="token keyword">return</span> batch_data<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>除了Tokens需要添加Zero Padding, Mask也要添加Zero Padding, 因为每个样本中添加Mask的数量是不定的.</p><p>此外, 这里实现逻辑不是很好, 可以针对循环优化.</p></blockquote><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>其实上面已经把数据准备的工作做完了, 下面就直接继承<code>Dataset</code>实现自己的数据集:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">BERTDataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_ids<span class="token punctuation">,</span> segment_ids<span class="token punctuation">,</span> masked_tokens<span class="token punctuation">,</span> masked_pos<span class="token punctuation">,</span> is_next<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>BERTDataset<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>input_ids <span class="token operator">=</span> input_ids
        self<span class="token punctuation">.</span>segment_ids <span class="token operator">=</span> segment_ids
        self<span class="token punctuation">.</span>masked_tokens <span class="token operator">=</span> masked_tokens
        self<span class="token punctuation">.</span>masked_pos <span class="token operator">=</span> masked_pos
        self<span class="token punctuation">.</span>is_next <span class="token operator">=</span> is_next

    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>input_ids<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>input_ids<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>segment_ids<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>masked_tokens<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>masked_pos<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>is_next<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Training-and-Evaluation"><a href="#Training-and-Evaluation" class="headerlink" title="Training and Evaluation"></a>Training and Evaluation</h2><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>下面是训练代码, 没有什么值得注意的地方.</p><pre class="line-numbers language-python"><code class="language-python">batch_size <span class="token operator">=</span> <span class="token number">6</span>
batch_data <span class="token operator">=</span> make_data<span class="token punctuation">(</span>token_list<span class="token punctuation">,</span> n_data<span class="token operator">=</span>batch_size<span class="token punctuation">)</span>
batch_tensor <span class="token operator">=</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>ele<span class="token punctuation">)</span> <span class="token keyword">for</span> ele <span class="token keyword">in</span> zip<span class="token punctuation">(</span><span class="token operator">*</span>batch_data<span class="token punctuation">)</span><span class="token punctuation">]</span>

dataset <span class="token operator">=</span> BERTDataset<span class="token punctuation">(</span><span class="token operator">*</span>batch_tensor<span class="token punctuation">)</span>
dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

model <span class="token operator">=</span> BERT<span class="token punctuation">(</span>n_layers<span class="token punctuation">)</span>
lr <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span>
epochs <span class="token operator">=</span> <span class="token number">500</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> Adadelta<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>
model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># training</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> one_batch <span class="token keyword">in</span> dataloader<span class="token punctuation">:</span>
        input_ids<span class="token punctuation">,</span> segment_ids<span class="token punctuation">,</span> masked_tokens<span class="token punctuation">,</span> masked_pos<span class="token punctuation">,</span> is_next <span class="token operator">=</span> <span class="token punctuation">[</span>ele<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token keyword">for</span> ele <span class="token keyword">in</span> one_batch<span class="token punctuation">]</span>

        logits_cls<span class="token punctuation">,</span> logits_lm <span class="token operator">=</span> model<span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> segment_ids<span class="token punctuation">,</span> masked_pos<span class="token punctuation">)</span>
        loss_cls <span class="token operator">=</span> criterion<span class="token punctuation">(</span>logits_cls<span class="token punctuation">,</span> is_next<span class="token punctuation">)</span>
        loss_lm <span class="token operator">=</span> criterion<span class="token punctuation">(</span>logits_lm<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> max_vocab<span class="token punctuation">)</span><span class="token punctuation">,</span> masked_tokens<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        loss_lm <span class="token operator">=</span> <span class="token punctuation">(</span>loss_lm<span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
        loss <span class="token operator">=</span> loss_cls <span class="token operator">+</span> loss_lm
        <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">10</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'Epoch:{epoch + 1} \t loss: {loss:.6f}'</span><span class="token punctuation">)</span>

        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h3><p>这里只采用了单个样本模拟Evaluation的过程.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Using one sentence to test</span>
test_data_idx <span class="token operator">=</span> <span class="token number">3</span>
model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    input_ids<span class="token punctuation">,</span> segment_ids<span class="token punctuation">,</span> masked_tokens<span class="token punctuation">,</span> masked_pos<span class="token punctuation">,</span> is_next <span class="token operator">=</span> batch_data<span class="token punctuation">[</span>test_data_idx<span class="token punctuation">]</span>
    input_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    segment_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>segment_ids<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    masked_pos <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>masked_pos<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    masked_tokens <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>masked_tokens<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    logits_cls<span class="token punctuation">,</span> logits_lm <span class="token operator">=</span> model<span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> segment_ids<span class="token punctuation">,</span> masked_pos<span class="token punctuation">)</span>
    input_ids<span class="token punctuation">,</span> segment_ids<span class="token punctuation">,</span> masked_tokens<span class="token punctuation">,</span> masked_pos<span class="token punctuation">,</span> is_next <span class="token operator">=</span> batch_data<span class="token punctuation">[</span>test_data_idx<span class="token punctuation">]</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"========================================================"</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Masked data:"</span><span class="token punctuation">)</span>
    masked_sentence <span class="token operator">=</span> <span class="token punctuation">[</span>idx2word<span class="token punctuation">[</span>w<span class="token punctuation">]</span> <span class="token keyword">for</span> w <span class="token keyword">in</span> input_ids <span class="token keyword">if</span> idx2word<span class="token punctuation">[</span>w<span class="token punctuation">]</span> <span class="token operator">!=</span> <span class="token string">'[PAD]'</span><span class="token punctuation">]</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>masked_sentence<span class="token punctuation">)</span>

    <span class="token comment" spellcheck="true"># logits_lm: [batch, max_pred, max_vocab]</span>
    <span class="token comment" spellcheck="true"># logits_cls: [batch, 2]</span>
    cpu <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cpu'</span><span class="token punctuation">)</span>
    pred_mask <span class="token operator">=</span> logits_lm<span class="token punctuation">.</span>data<span class="token punctuation">.</span>max<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>cpu<span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
    pred_next <span class="token operator">=</span> logits_cls<span class="token punctuation">.</span>data<span class="token punctuation">.</span>max<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>data<span class="token punctuation">.</span>to<span class="token punctuation">(</span>cpu<span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

    bert_sentence <span class="token operator">=</span> masked_sentence<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>
    original_sentence <span class="token operator">=</span> masked_sentence<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>masked_pos<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        pos <span class="token operator">=</span> masked_pos<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
        <span class="token keyword">if</span> pos <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token keyword">break</span>
        bert_sentence<span class="token punctuation">[</span>pos<span class="token punctuation">]</span> <span class="token operator">=</span> idx2word<span class="token punctuation">[</span>pred_mask<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span>
        original_sentence<span class="token punctuation">[</span>pos<span class="token punctuation">]</span> <span class="token operator">=</span> idx2word<span class="token punctuation">[</span>masked_tokens<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"BERT reconstructed:"</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>bert_sentence<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Original sentence:"</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>original_sentence<span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"===============Next Sentence Prediction==============="</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'Two sentences are continuous? {True if is_next else False}'</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'BERT predict: {True if pred_next else False}'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出:</p><pre><code>========================================================
Masked data:
[&#39;[CLS]&#39;, &#39;team&#39;, &#39;[MASK]&#39;, &#39;[MASK]&#39;, &#39;you&#39;, &#39;i&#39;, &#39;am&#39;, &#39;romeo&#39;, &#39;[SEP]&#39;, &#39;hello&#39;, &#39;romeo&#39;, &#39;my&#39;, &#39;name&#39;, &#39;is&#39;, &#39;juliet&#39;, &#39;nice&#39;, &#39;to&#39;, &#39;meet&#39;, &#39;you&#39;, &#39;[SEP]&#39;]
BERT reconstructed:
[&#39;[CLS]&#39;, &#39;hello&#39;, &#39;how&#39;, &#39;are&#39;, &#39;you&#39;, &#39;i&#39;, &#39;am&#39;, &#39;romeo&#39;, &#39;[SEP]&#39;, &#39;hello&#39;, &#39;romeo&#39;, &#39;my&#39;, &#39;name&#39;, &#39;is&#39;, &#39;juliet&#39;, &#39;nice&#39;, &#39;to&#39;, &#39;meet&#39;, &#39;you&#39;, &#39;[SEP]&#39;]
Original sentence:
[&#39;[CLS]&#39;, &#39;hello&#39;, &#39;how&#39;, &#39;are&#39;, &#39;you&#39;, &#39;i&#39;, &#39;am&#39;, &#39;romeo&#39;, &#39;[SEP]&#39;, &#39;hello&#39;, &#39;romeo&#39;, &#39;my&#39;, &#39;name&#39;, &#39;is&#39;, &#39;juliet&#39;, &#39;nice&#39;, &#39;to&#39;, &#39;meet&#39;, &#39;you&#39;, &#39;[SEP]&#39;]
===============Next Sentence Prediction===============
Two sentences are continuous? True
BERT predict: True</code></pre></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">AnNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/52648.html">https://ADAning.github.io/posts/52648.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">AnNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/Transformer/"><span class="chip bg-color">Transformer</span> </a><a href="/tags/Pytorch/"><span class="chip bg-color">Pytorch</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/63236.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/23.jpg" class="responsive-img" alt="SACN: End-to-end Structure-Aware Convolutional Networks for KBC"> <span class="card-title">SACN: End-to-end Structure-Aware Convolutional Networks for KBC</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: GNN: 详见图神经网络入门 ConvE: 详见ConvE: Convolutional 2D Knowledge Graph Embeddings ConvKB: 详见ConvKB: A Novel Embedding</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2021-03-15 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/" class="post-category">知识图谱</a></span></div></div><div class="card-action article-tags"><a href="/tags/KGE/"><span class="chip bg-color">KGE</span> </a><a href="/tags/GNN/"><span class="chip bg-color">GNN</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/20145.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/6.jpg" class="responsive-img" alt="ConvR: Adaptive Convolution for Multi-Relational Learning"> <span class="card-title">ConvR: Adaptive Convolution for Multi-Relational Learning</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: ConvE: 详见ConvE: Convolutional 2D Knowledge Graph Embeddings ConvR: Adaptive Convolution for Multi-Relational</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2021-03-09 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/" class="post-category">知识图谱</a></span></div></div><div class="card-action article-tags"><a href="/tags/KGE/"><span class="chip bg-color">KGE</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">AnNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">312.5k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:695439722@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>