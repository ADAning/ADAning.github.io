<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="指针网络家族, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>指针网络家族 | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:.6"></i> <span>时间轴</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li><li><a href="/markdown"><i class="fab fa-markdown" style="margin-top:-20px;zoom:.6"></i> <span>Markdown</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li><li><a href="/markdown" style="margin-left:75px"><i class="fa fab fa-markdown" style="position:absolute;left:50px"></i> <span>Markdown</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/13.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">指针网络家族</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/Attention/"><span class="chip bg-color">Attention</span> </a><a href="/tags/%E6%91%98%E8%A6%81%E7%94%9F%E6%88%90/"><span class="chip bg-color">摘要生成</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2020-09-28</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2022-03-27</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 3.7k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 15 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><p>本文介绍了Pointer Network, CopyNet, Pointer-Generator Network以及Coverage机制在文本摘要与对话系统中的应用, 既可以作为知识点介绍, 也可以作为论文阅读笔记. 此外, 该部分内容为外部知识引入NLP任务中提供了思路.</p><blockquote><p>本文阅读所需的前置知识包括:</p><ul><li>Seq2Seq</li><li>Attention</li><li>Bi - directional RNN</li></ul></blockquote><h2 id="Pointer-Network"><a href="#Pointer-Network" class="headerlink" title="Pointer Network"></a>Pointer Network</h2><p>指针网络出自<a href="https://arxiv.org/abs/1506.03134" target="_blank" rel="noopener">Pointer Networks</a>. 在Seq2seq中, 经常有一种模型<strong>输出严重依赖输入</strong>的问题, 一旦问题规模发生变化, 那就必须重新训练网络. 作者利用Attention机制和Seq2Seq进行结合, 克服了这个问题.</p><p>“输出严重依赖输入”指的是输出往往是输入的子集. 比如在机器翻译中, 输出向量大小必须取决于字典长度, 并不能根据Encoder输入内容而发生变化. 在求凸包(Convex Hull)问题中, 输入和输出都是坐标序列, 规模是不固定的. <strong>可变大小序列的排序</strong>和<strong>各种组合优化</strong>都是这类问题.</p><blockquote><p>凸包是计算图形学中的概念, 通俗一点说凸包问题就是根据给定的二维平面点集找到能够包含点集中所有点的最外层点的连接线.</p></blockquote><p>而”指针”的命名来自于其Attention产生的权重直接决定了Decoder的输出对应着哪个Encoder的数据输入, 这样输出就从输入中进行选择, 能很好的解决该问题. 该结构非常简单, 与加权平均的注意力机制不同, Ptr - Net并非将Attention机制对信息进行筛选, 而是<strong>直接指出</strong>输出信息. 指针网络直接将Attention的权重最高者直接作为Decoder的输出, 如下图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ptrnet.jpg" style="zoom:67%"><p>实际上这种结构不仅仅能用于解决凸包问题, 作者还在论文中给出利用Ptr - Net 解决三角形分割(Delaunay Triangulation), 旅行商问题(Travelling Salesman Problem). 作者在调整参数后, 取得了比LSTM好得多的结果:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ptrnet1.jpg" style="zoom:50%"><p>指针网络因为具有解决输出严重依赖输入问题的能力, 如果应用于NLP的摘要生成任务中, 可以从原文中复现重要的细节, 在某些程度上解决预训练词典大小不足的问题(也称为<strong>OOV问题</strong>, 即Out of Vocabulary). 虽然这种结构在处理特定问题上有了优势, 但仍然受<strong>结构局限</strong>, 无法完全应用到通用任务当中.</p><h2 id="CopyNet"><a href="#CopyNet" class="headerlink" title="CopyNet"></a>CopyNet</h2><p>CopyNet出自论文<a href="http://arxiv.org/abs/1603.06393" target="_blank" rel="noopener">Incorporating Copying Mechanism in Sequence-to-Sequence Learning</a>. 该论文作者将PtrNet使用于文本<strong>摘要提取</strong>和<strong>对话任务</strong>当中.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/copynet1.jpg" style="zoom:50%"><p>在原文中作者提到Copy机制与Seq2Seq结合的难点:</p><blockquote><p>From a cognitive perspective, the copying mechanism is related to rote memorization, requiring less understanding but ensuring high literal fidelity. From a modeling perspective, the copying operations are more rigid and symbolic, making it more difficult than soft attention mechanism to integrate into a fully differentiable neural model.</p></blockquote><ul><li>从<strong>认知角度</strong>来看, Copy与死记硬背有关, 不需要理解, 但复制可以确保很高的字面保真度.</li><li>从<strong>建模角度</strong>来看, Copy更加僵化和符号化, 使其比软注意力机制更难集成到完全可区分的神经模型中.</li></ul><p>因此, 作者提出了另一种复制机制, 能够端到端的只通过梯度下降训练Seq2Seq模型.</p><p>前面提到的指针网络只能重复原文内容, 而非从已有的字典中提取内容, 天生就受到了极大的限制. 如果想要破除这种劣势, 就必须让之前的对话生成结构与这种Pointer(或者说Copy)机制相结合.</p><blockquote><p>注: Copy和Pointer的作用都是一致的, 都是将某个时刻的输出调整为先前某个时刻的输入, Copy也是将先前输入作为输出, 指针也是同样效果.</p></blockquote><p>在摘要生成中, 通过复现原文内容的摘要生成称为”<strong>抽取式</strong>“摘要生成, 而从外部词典中取出的内容叫”<strong>生成式</strong>“摘要生成. 而作者用复制机制将这两种方式实现了软结合.</p><p>沿用Seq2Seq结构, 仍然分为Encoder和Decoder两个部分.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/copynet.jpg" style="zoom:67%"><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>Encoder采用双向RNN提取句子信息, 并将每个时刻与输入$x_t$ 相对应的hidden state集合$\left\{\mathbf{h}_{1}, \ldots, \mathbf{h}_{T_{S}}\right\}$ 称为短期记忆$\mathrm{M}$ .</p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><h4 id="Predition"><a href="#Predition" class="headerlink" title="Predition"></a>Predition</h4><p>作者将预测分为<strong>生成模式</strong>和<strong>复制模式</strong>两种, 当前时刻的预测结果应该是两种模式混合的结果. g代表生成模式, c代表copy模式. 而每个模式的预测结果由Decoder在当前时刻的隐藏状态输出$s_t$, 上个时刻的预测结果$y_{t-1}$, Attention生成的当前时刻的上下文向量$c_t$, 以及短时记忆$\mathrm{M}$ 共同决定.<br>$$<br>\begin{array}{r}<br>p\left(y_{t} \mid \mathbf{s}_{t}, y_{t-1}, \mathbf{c}_{t}, \mathbf{M}\right)=p\left(y_{t}, g \mid \mathbf{s}_{t}, y_{t-1}, \mathbf{c}_{t}, \mathbf{M}\right)<br>+p\left(y_{t}, c \mid \mathbf{s}_{t}, y_{t-1}, \mathbf{c}_{t}, \mathbf{M}\right)<br>\end{array}<br>$$<br>作者将词语分为在词典中的词语集合$\mathcal{V}=\left\{v_{1}, \ldots, v_{N}\right\}$, 源序列的单词集合$X=\left\{x_{1}, \ldots, x_{T_{S}}\right\}$, OOV单词记为$\mathrm{UNK}$, 每个模式生成的结果计算方式如下:<br>$$<br>\begin{array}{l}<br>p\left(y_{t}, g \mid \cdot\right)=\left\{\begin{array}{cc}<br>\frac{1}{Z} e^{\psi_{g}\left(y_{t}\right)}, &amp; y_{t} \in \mathcal{V} \\<br>0, &amp; y_{t} \in \mathcal{X} \cap \bar{V} \\<br>\frac{1}{Z} e^{\psi_{g}(\mathrm{UNK})} &amp; y_{t} \notin \mathcal{V} \cup \mathcal{X}<br>\end{array}\right. \\<br>p\left(y_{t}, \mathrm{c} \mid \cdot\right)=\left\{\begin{array}{cc}<br>\frac{1}{Z} \sum_{j: x_{j}=y_{t}} e^{\psi_{c}\left(x_{j}\right)}, &amp; y_{t} \in \mathcal{X} \\<br>0 &amp; \text { otherwise }<br>\end{array}\right.<br>\end{array}<br>$$<br>其中$Z$ 为生成模式和copy模式共享的归一化项, 作者通过设计归一化项让两种模式通过Softmax来<strong>互相竞争</strong>(将其代入原式的分母中就是Softmax的形式):<br>$$<br>Z=\sum_{v \in \mathcal{V} \cup\{\mathrm{UNK}\}} e^{\psi_{g}(v)}+\sum_{x \in X} e^{\psi_{c}(x)}<br>$$<br>$\psi_{g}(\cdot)$ 和 $\psi_{c}(\cdot)$ 是两种打分函数, 会在后面提到如何计算.</p><p>作者还给出了图加以辅助说明计算概率时所对应的不同情况:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/copynet2.jpg" style="zoom:33%"><p>生成模式下采用Attention, 打分公式为:<br>$$<br>\psi_{g}\left(y_{t}=v_{i}\right)=\mathbf{v}_{i}^{\top} \mathbf{W}_{o} \mathbf{s}_{t}, \quad v_{i} \in \mathcal{V} \cup \mathrm{UNK}<br>$$<br>$W_o$ 和$\mathbf{v}_{i}$ 相乘后能获得$v_i$ 的独热编码, 它与$s_t$ 乘后得到一个分数.</p><p>copy模式下, 打分公式为:<br>$$<br>\psi_{c}\left(y_{t}=x_{j}\right)=\sigma\left(\mathbf{h}_{j}^{\top} \mathbf{W}_{c}\right) \mathbf{s}_{t}, \quad x_{j} \in \mathcal{X}<br>$$<br>$W_c$ 是训练得到的, 与$\mathbf h_j$ 相乘经过$\sigma$ (原文使用的是tanh)添加非线性, 将$h_j$ 和 $s_t$ 投射到同一个语义空间.</p><p>综上, 当$y_t$ 没出现在源序列中时, $p\left(y_{t}, c \mid \cdot\right)=0$, 只启动生成模式, 当$y_t$ 只出现在源序列时, $p\left(y_{t}, g \mid \cdot\right)=0$, 只启动copy模式.</p><h4 id="State-Update-and-Reading-M"><a href="#State-Update-and-Reading-M" class="headerlink" title="State Update and Reading M"></a>State Update and Reading M</h4><p>作者在copy机制下对Decoder的状态更新做了改良, 普通Decoder的状态更新为:<br>$$<br>\begin{array}{l}<br>\mathbf{s}_{t}=f\left(y_{t-1}, \mathbf{s}_{t-1}, \mathbf{c}\right) \\<br>p\left(y_{t} \mid y_{&lt;t}, X\right)=g\left(y_{t-1}, \mathbf{s}_{t}, \mathbf{c}\right)<br>\end{array}<br>$$<br>作者将$y_{t-1}$ 用$y_{t-1}$ 的Embedding $\mathbf{e}(y_{t-1})$ 和$\zeta\left(y_{t-1}\right)$ 来表示, 并将其concat起来, 即:<br>$$<br>\left[\mathbf{e}\left(y_{t-1}\right) ; \zeta\left(y_{t-1}\right)\right]^{\top}<br>$$<br>因为使用的是双向RNN, 所以$\mathrm M$ 中既包含了上下文信息, 也包含了位置信息, 这对状态更新很重要, $\zeta\left(y_{t-1}\right)$ 是其中的核心内容, 计算方式如下:<br>$$<br>\begin{array}{l}<br>\zeta\left(y_{t-1}\right)=\sum_{\tau=1}^{T_{S}} \rho_{t \tau} \mathbf{h}_{\tau} \\<br>\rho_{t \tau}=\left\{\begin{array}{cc}<br>\frac{1}{K} p\left(x_{\tau}, \mathrm{c} \mid \mathbf{s}_{t-1}, \mathbf{M}\right), &amp; x_{\tau}=y_{t-1} \\<br>0 &amp; \text { otherwise }<br>\end{array}\right. \\<br>K = \sum_{\tau^{\prime}: x_{\tau^{\prime}}=y_{t-1}} p\left(x_{\tau^{\prime}}, c \mid \mathbf{s}_{t-1}, \mathbf{M}\right)<br>\end{array}<br>$$<br>$K$ 是归一化项, 作者将这个过程称为Selective Read, 即用$\zeta\left(y_{t-1}\right)$ 对$\mathbf h_\tau$ 进行加权求和. 从式子直观上来理解, 仅当Encoder接受的输入在上个Decoder输出时刻相同时, 这个$\rho$ 才有意义. 通过这种方式, 对于在前文已经出现的单词, Decoder就能拿到<strong>额外的上下文信息和位置信息</strong>. 一旦$\zeta$ 有值, 当前时刻的输出就会倾向于copy模式, 因为上个时刻的输出在原文中能够找到, 那么当前时刻的内容也有相当大的概率从原文中copy.</p><blockquote><p>这个$\zeta$ 似乎和$c_t$ 差不多, 不知道有没有信息上的<strong>冗余</strong>.</p></blockquote><p>综上, 将更新过程总结如下:<br>$$<br>\zeta\left(y_{t-1}\right) \stackrel{\text { update }}{\longrightarrow} \mathbf{s}_{t} \stackrel{\text { predict }}{\longrightarrow} y_{t} \stackrel{\text { sel. read }}{\longrightarrow} \zeta\left(y_{t}\right)<br>$$</p><blockquote><p>在原文中最后的实验对比结果中. CopyNet对于Copy任务做的效果都碾压基础模型, 但唯独对于结束符的生成准确率较差, 这也为<strong>大量生成重复内容</strong>埋下了隐患.</p></blockquote><h2 id="Pointer-Generator-Network"><a href="#Pointer-Generator-Network" class="headerlink" title="Pointer - Generator Network"></a>Pointer - Generator Network</h2><p>指针生成网络受到了PtrNet和CopyNet的影响, 仍然沿用原输入的复现能力与基于已有知识的文本生成能力做结合的思路, 并针对指针生成网络出现的问题做了改进. 与CopyNet相比更加简洁.</p><p>该结构出自<a href="https://arxiv.org/abs/1704.04368" target="_blank" rel="noopener">Get To The Point: Summarization with Pointer-Generator Networks</a>, 这是一篇非常不错的论文, 如果时间不充裕我建议阅读这篇, 作者逻辑清晰, 图片也简单易懂.</p><p>该模型应用于<strong>摘要生成</strong>任务中, 作者在论文开头便提出了现存模型在摘要生成中出现的问题:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ptrgennet1.jpg" style="zoom:67%"><ol><li>对于图中的红色字, 存在内容抽取<strong>不准确</strong>的情况.</li><li>对于图中的绿色字, 存在多次与原文中生成<strong>重复且无意义内容</strong>的情况.</li></ol><p>指针生成网络沿用了CopyNet生成式和抽取式并存的思想, 分为抽取式摘要生成和基于单词表的生成式摘要生成, 并可以在生成模式和抽取模式之间更灵活的切换.</p><blockquote><p>Our pointer-generator network is a hybrid betweenour baseline and a pointer network, as it allows both copying words via pointing, and generating words from a fixed vocabulary.</p></blockquote><h3 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h3><p>在原论文中, 作者用标准的Seq2Seq + Attention作为Baseline:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ptrgennet2.jpg" style="zoom:67%"><p>这是一种非常直觉性的办法. Seq2Seq架构下用双向RNN提取隐藏状态$h_i$, Decoder解码得到$s_t$, 用Bahdanau Attention提取上下文信息$h^\ast_t$:<br>$$<br>\begin{array}{l}<br>e_{i}^{t}=v^{T} \tanh \left(W_{h} h_{i}+W_{s} s_{t}+b_{\text {attn }}\right) \\<br>a^{t}=\operatorname{softmax}\left(e^{t}\right) \\<br>h_{t}^{\ast}=\sum_{i} a_{i}^{t} h_{i}<br>\end{array}<br>$$<br>然后根据得出的上下文信息和Decoder当前时刻解码信息, 经过Softmax得到当前时刻的词语概率分布$P_{\text {vocab }}$:<br>$$<br>P_{\text {vocab }}=\operatorname{softmax}\left(V^{\prime}\left(V\left[s_{t}, h_{t}^{\ast}\right]+b\right)+b^{\prime}\right)<br>$$<br>为了后面体现出指针生成网络和Baseline的差异, 直接令$P_{\text{vocab}}$就是最终结果:<br>$$<br>P(w)=P_{\text {vocab }}(w)<br>$$<br>然后用对数似然做损失, 计算总共的Loss:<br>$$<br>\begin{aligned}<br>\operatorname{loss}_{t}&amp;=-\log P\left(w_{t}^{\ast}\right) \\<br>\operatorname{loss}&amp;=\frac{1}{T} \sum_{t=0}^{T} \operatorname{loss}_{t}<br>\end{aligned}<br>$$</p><h3 id="Pointer-Generator-Network-1"><a href="#Pointer-Generator-Network-1" class="headerlink" title="Pointer - Generator Network"></a>Pointer - Generator Network</h3><p>作者基于Baseline的缺点, 给出了指针生成网络:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ptrgennet3.jpg" style="zoom:67%"><p>与Baseline的图相对比, 最大的变化就是多了一个$p_{\text{gen}}$, 导致有其他很多连带的内容不同.</p><p>和之前一样, 我们仍然计算Attention和绿色对应的词表概率分布, 但同时根据当前时刻Decoder的输入$x_t$ 能额外得到一个概率$p_{\text{gen}} \in [0, 1]$:<br>$$<br>p_{\mathrm{gen}}=\sigma\left(w_{h^{\ast}}^{T} h_{t}^{\ast}+w_{s}^{T} s_{t}+w_{x}^{T} x_{t}+b_{\mathrm{ptr}}\right)<br>$$<br>引入$p_{\text{gen}}$ 就是想在Baseline的基础上将生成式与抽取式利用<strong>开关</strong>做个<strong>软结合</strong>, 从而改善最终结果:<br>$$<br>P(w)=p_{\mathrm{gen}} P_{\mathrm{vocab}}(w)+\left(1-p_{\mathrm{gen}}\right) \sum_{i: w_{i}=w} a_{i}^{t}<br>$$<br>生成模式的概率是$p_{\text{gen}}$, 那么copy模式的概率就是$1 - p_{\text{gen}}$. copy模式中用到了$a^t$, 当单词$w$ 在之前的输入中出现过时候, 我们将其Attention累加起来, 所以在原文中出现次数越多的词使用copy模式的概率就越大, 被添入摘要的概率也就越大. 如果$w$ 直接OOV了, 则$P_{\mathrm{vocab}}(w)=0$, 如果$w$ 没在原文中出现, 那么令$\sum_{i: w_{i}=w} a_{i}^{t}=0$.</p><p>模型非常简单, 图画的也特别好, 以至于让人一目了然.</p><h2 id="Coverage-Mechanism"><a href="#Coverage-Mechanism" class="headerlink" title="Coverage Mechanism"></a>Coverage Mechanism</h2><p>覆盖(汇聚?)机制与指针生成网络出自同一篇论文. 在原文中作者提到:</p><blockquote><p>Repetition is a common problem for sequence-to-sequence models, and is especially pronounced when generating multi-sentence text. We adapt the coverage model of Tu et al. (2016) to solve the problem.</p></blockquote><p>对于Seq2Seq模型来说, <strong>重复</strong>是一个非常令人头疼的问题, 作者在指针生成网络的基础上提出了Converage Mechanism. 思想非常简单, 作者通过Coverage Mechanism来记录之前时刻Attention的和, 作为当前时刻对原文单词关注位置的依据, 令其为$c^t$:</p><p>$$<br>c^{t}=\sum_{t^{\prime}=0}^{t-1} a^{t^{\prime}}<br>$$</p><blockquote><p>注: 这里的$c^t$ 不是上下文关系, 而是coverage vector, 在本论文中采用$h_t^{\ast}$ 作为上下文关系.</p></blockquote><p>$c^t$ 代表了先前模型对这些单词关注的覆盖程度(不知道是不是”覆盖”二字的来源), 模型先前越有可能copy过, 那么覆盖向量就越大.</p><p>将Coverage Mechanism加入Attention中去, 将模型对单词的关注程度也作为Attention的依据:<br>$$<br>e_{i}^{t}=v^{T} \tanh \left(W_{h} h_{i}+W_{s} s_{t}+w_{c} c_{i}^{t}+b_{\mathrm{attn}}\right)<br>$$<br>单单影响Attention不能产生太大作用, 因为模型并不知道要往哪个方向进行优化, 还需要给模型一个引导, 将相关的内容作为Loss体现:<br>$$<br>\displaylines{<br>\operatorname{covloss}_{t}=\sum_{i} \min \left(a_{i}^{t}, c_{i}^{t}\right) \\<br>\operatorname{covloss}_{t} \leq \sum_{i} a_{i}^{t}=1<br>}<br>$$<br>$\text{covloss}$ 使得模型更容易选择不同的单词做copy, 而非大量重复copy. 若重复copy, $a^t$ 和 $c^t$ 都会很高, 模型会被惩罚的更严重.</p><p>令$\lambda$ 为超参数加权, 将$\text{covloss}$ 加入总体损失中:<br>$$<br>\operatorname{loss}_{t}=-\log P\left(w_{t}^{\ast}\right)+\lambda \sum_{i} \min \left(a_{i}^{t}, c_{i}^{t}\right)<br>$$</p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/36009.html">https://ADAning.github.io/posts/36009.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/Attention/"><span class="chip bg-color">Attention</span> </a><a href="/tags/%E6%91%98%E8%A6%81%E7%94%9F%E6%88%90/"><span class="chip bg-color">摘要生成</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/42255.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/17.jpg" class="responsive-img" alt="Pytorch学习: 张量基础操作"> <span class="card-title">Pytorch学习: 张量基础操作</span></div></a><div class="card-content article-content"><div class="summary block-with-text">Pytorch学习: 张量基础操作整理内容顺序来自龙龙老师的&lt;深度学习与PyTorch入门实战教程&gt;, 根据个人所需情况进行删减或扩充. 如果想要自己创建新的模块, 这些操作都是基本功, 需要掌握扎实. 张量数据类型下表摘自Py</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-10-02 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/%E7%BC%96%E7%A8%8B/"><span class="chip bg-color">编程</span> </a><a href="/tags/Pytorch/"><span class="chip bg-color">Pytorch</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/6744.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/3.jpg" class="responsive-img" alt="Transformer精讲"> <span class="card-title">Transformer精讲</span></div></a><div class="card-content article-content"><div class="summary block-with-text">2020.10.05: 更新训练技巧. 2020.09.27: 更新Masked Multi - Head Attention理解. 2021.06.08: 更新Teacher Forcing. TransformerTransform</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-09-21 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/Attention/"><span class="chip bg-color">Attention</span> </a><a href="/tags/Transformer/"><span class="chip bg-color">Transformer</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">367.8k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:695439722@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>