<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="KEPLER: Knowledge Embedding and Pre-trained Language Representation, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>KEPLER: Knowledge Embedding and Pre-trained Language Representation | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:.6"></i> <span>时间轴</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li><li><a href="/markdown"><i class="fab fa-markdown" style="margin-top:-20px;zoom:.6"></i> <span>Markdown</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li><li><a href="/markdown" style="margin-left:75px"><i class="fa fab fa-markdown" style="position:absolute;left:50px"></i> <span>Markdown</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/19.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">KEPLER: Knowledge Embedding and Pre-trained Language Representation</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/BERT/"><span class="chip bg-color">BERT</span> </a><a href="/tags/KGE/"><span class="chip bg-color">KGE</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/" class="post-category">知识图谱</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2020-11-21</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2022-03-27</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 3k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 11 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p>本文前置知识:</p><ul><li>BERT(详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>)</li></ul></blockquote><h1 id="KEPLER-A-Unified-Model-for-Knowledge-Embedding-and-Pre-trained-Language-Representation"><a href="#KEPLER-A-Unified-Model-for-Knowledge-Embedding-and-Pre-trained-Language-Representation" class="headerlink" title="KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation"></a>KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation</h1><p>本文是论文<a href="http://arxiv.org/abs/1911.06136" target="_blank" rel="noopener">KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation</a> 的阅读笔记和个人理解.</p><blockquote><p>顺带吐槽一下这论文第一版和第二版差的也太大了…</p></blockquote><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者指出, PLM不能直接从文本中<strong>获取常识</strong>. 相反, KGE经常能获得知识图谱中实体和关系的有效表示, 却不能<strong>捕捉上下文</strong>. 基于这个简单的观察, 作者希望将常识注入PLM, 这样PLM不但能学到常识, 还能学到有效而丰富的信息<strong>表示</strong>.</p><p>并且在已经注入知识的PLM中, 作者还观察到以下问题:</p><ul><li>实体嵌入和语言分离, 不方便表示空间的对齐. KGE模型很少将KG结构作为输入, 并很少结合文本信息, 因此无法帮助PLM.</li><li>需要实体链接器, 在传播时容易出现错误.</li><li>与普通PLM相比, 查找实体的表示会带来额外的开销.</li></ul><p>而文本描述有与实体相关的丰富信息, 能够帮助文本语义空间与KG的符号空间对齐:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler1.jpg" style="zoom:25%"><p>作者将实体结合其<strong>描述</strong>用PLM编码, 对KRL和PLM的目标进行<strong>联合优化</strong>.</p><h2 id="KEPLER"><a href="#KEPLER" class="headerlink" title="KEPLER"></a>KEPLER</h2><p>KEPLER(<strong>K</strong>nowledge <strong>E</strong>mbedding and <strong>P</strong>re - trained <strong>L</strong>anguag<strong>E</strong> <strong>R</strong>epresentation)是一个KGE和PLM表示<strong>统一</strong>的模型. 所以它包含了PLM和KE的联合优化目标.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler2.jpg" style="zoom:50%"><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>作者使用了Transformer Encoder(其实就是BERT)作为<strong>文本编码器</strong>. 即Transformer Encoder将$N$ 个Token的序列$(x_1,\dots, x_N)$作为输入, 然后经过$L$ 层Transformer Encoder的堆叠计算, 得到$d$ 维的上下文表示$\mathbf{H}_i, 1\leq i \leq L$. 每层编码器$\mathrm{E}_i$由多头自注意力和前馈神经网络组成, 每层的Encoder表示记为:<br>$$<br>\mathbf{H}_i=\mathrm{E}_i(\mathbf{H}_{i-1})<br>$$<br>对于任意文本$\text{text}$, 作者希望将经过编码后的$\mathrm{[CLS]}$ 处的输出$\mathrm{E}_{[\mathrm{CLS}]} $ 做为文本的表示.</p><p>在KEPLER中, 作者和RoBERTa一样使用了<strong>BPE</strong>, 与之前出现的Knowledge - Enhanced Model相比较, 这里没有使用额外的<strong>实体连接器</strong>或者知识集成层.</p><h3 id="Knowledge-Embedding"><a href="#Knowledge-Embedding" class="headerlink" title="Knowledge Embedding"></a>Knowledge Embedding</h3><p>与其他KGE方法一样, KEPLER将实体和关系映射进一个$d$ 维的空间中, 并且使用打分函数训练.</p><p>但是KEPLER和普通的KGE方法又不一样, 它不再<strong>存储</strong>Embedding, 而是将实体结合它们本身的描述<strong>编码</strong>做为Embedding. 作者设计了两种结合实体描述的方法:</p><ul><li>只用<strong>实体描述</strong>.</li><li>使用<strong>实体描述</strong>和<strong>关系描述</strong>.</li></ul><h4 id="Using-Entity-Descriptions"><a href="#Using-Entity-Descriptions" class="headerlink" title="Using Entity Descriptions"></a>Using Entity Descriptions</h4><p>对于三元组$(h, r, t)$, 只使用三元组就是对头实体$h$ 的描述$\text{text}_h$和尾实体$t$ 的描述$\text{text}_t$ 分别进行编码, 然后再将关系$r$ 单独嵌入:</p><p>$$<br>\begin{aligned}<br>\mathbf{h} &amp;=\mathrm{E}_{[\mathrm{CLS}]}\left(\operatorname{text}_{h}\right) \\<br>\mathbf{t} &amp;=\mathrm{E}_{[\mathrm{CLS}]}\left(\operatorname{text}_{t}\right) \\<br>\mathbf{r} &amp;=\mathbf{T}_{r}<br>\end{aligned}<br>$$</p><p>其中$\mathbf{T}_r$ 代表关系$r$ 的Embedding权重.</p><h4 id="Using-Entity-and-Relation-Descriptions"><a href="#Using-Entity-and-Relation-Descriptions" class="headerlink" title="Using Entity and Relation Descriptions"></a>Using Entity and Relation Descriptions</h4><p>与只使用实体描述不一样, 因为BERT是可以对两段文字联合编码的, 所以这种方法可以将头实体描述和关系放在一起使用:</p><p>$$<br>\mathbf{h}_{r}=\mathrm{E}_{[\mathrm{CLS}]}\left(\operatorname{text}_{h, r}\right)<br>$$</p><p>具体一点, $\mathrm{text}_{h,r}$ 代表头实体$h$ 和其三元组关系$r$的描述, 在二者之间像BERT输入两段信息一样加上特殊的分隔符$[\mathrm{SEP}]$ 做为区分. 尾实体仍然单独输入进BERT.</p><blockquote><p>我认为该方法最少有两个问题:</p><ol><li>头实体获得关系描述加成(也有可能是污染), 尾实体没有被考虑到.</li><li>BERT捕捉上下文的能力被<strong>局限</strong>了, 仅能体现在<strong>描述</strong>的上下文当中, 并不能根据语境调整实体的表示.</li></ol></blockquote><h4 id="Konwledge-Embedding-Loss-and-Score-Function"><a href="#Konwledge-Embedding-Loss-and-Score-Function" class="headerlink" title="Konwledge Embedding Loss and Score Function"></a>Konwledge Embedding Loss and Score Function</h4><p>KE部分的损失如下:</p><p>$$<br>\mathcal{L}_{\mathrm{KE}} =-\log \sigma\left(\gamma-d_{r}(\mathbf{h}, \mathbf{t})\right)<br>-\sum_{i=1}^{n} \frac{1}{n} \log \sigma\left(d_{r}\left(\mathbf{h}_{\mathbf{i}}^{\prime}, \mathbf{t}_{\mathbf{i}}^{\prime}\right)-\gamma\right)<br>$$</p><p>其中$(h_i^\prime, r, t_i^\prime)$是负采样得到的样本, $\sigma$ 是Sigmoid函数, $\gamma$ 是间隔, $d_r$ 是打分函数, KEPLER沿用TransE的打分函数:</p><p>$$<br>d_{r}(\mathbf{h}, \mathbf{t})=\lVert\mathbf{h}+\mathbf{r}-\mathbf{t}\rVert_{p}<br>$$</p><p>作者使用的是一阶范数, 即$p=1$.</p><h3 id="Masked-Language-Modeling"><a href="#Masked-Language-Modeling" class="headerlink" title="Masked Language Modeling"></a>Masked Language Modeling</h3><p>在Masked Language Model上, KEPLER沿用了BERT和RoBERTa的<strong>MLM</strong>训练损失, 结构和Mask方式都<strong>没有发生任何变化</strong>. 在分类时仍然是在Encoder的最后一层输出$\mathbf{H}_{L,j}$ 后接上和<strong>字典</strong>等同大小的$W$ 路分类器.</p><p>因为RoBERTa的效果比较好, 所以直接用$\text{RoBERTa}_{\mathrm{BASE}}$ 的参数初始化.</p><h3 id="Training-Objectives"><a href="#Training-Objectives" class="headerlink" title="Training Objectives"></a>Training Objectives</h3><p>训练目标就是之前提到过的KGE部分和MLM部分之和:</p><p>$$<br>\mathcal{L}=\mathcal{L}_{\mathrm{KE}}+\mathcal{L}_{\mathrm{MLM}}<br>$$</p><p>这两部分目标是<strong>共享Encoder</strong>的, 在训练时可以采样<strong>不同类型</strong>的文本(倾向于优化KE或者MLM)作为训练数据.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h3><p>作者需要使用不同数据优化KE和MLM.</p><h4 id="KE-Objective-Wikidata5M"><a href="#KE-Objective-Wikidata5M" class="headerlink" title="KE Objective: Wikidata5M"></a>KE Objective: Wikidata5M</h4><p>因为作者需要大规模的KG, 并且必须包含对应的实体和关系<strong>描述</strong>, 最好还要支持Inductive Setting, 这样的数据集基本不存在, 所以作者自己根据<strong>Wikidata</strong>和<strong>Wikipedia</strong>构建了一个新的包含实体关系描述文本的大规模KG数据集<strong>Wikidata5M</strong>.</p><p>Wikidata5M比现在的常用数据集大得多, 并几乎涵盖了所有领域:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler3.jpg" style="zoom:25%"><p>Wikidata5M中实体类型统计:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler4.jpg" style="zoom:20%"><h5 id="DataSplit"><a href="#DataSplit" class="headerlink" title="DataSplit"></a>DataSplit</h5><p>数据可以按照两种设置进行划分:</p><ul><li><p><strong>Transductive Setting</strong>: 在绝大多数KG数据集中使用, 对于训练集, 验证集, 测试集<strong>共享所有实体</strong>, 但<strong>并不知道完整三元组</strong>.</p></li><li><p><strong>Inductive Setting</strong>: 训练集, 验证集, 测试集中, <strong>实体和三元组都不共享</strong>. 这更考验模型的<strong>推断</strong>能力, 也更困难. 但它更符合<strong>现实世界</strong>的应用情况. 具体数据集划分情况如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler5.jpg" style="zoom:25%"></li></ul><h5 id="Benchmark"><a href="#Benchmark" class="headerlink" title="Benchmark"></a>Benchmark</h5><p>作者用之前常用的KGE模型测试了Wikidata5M的难度, 分别比较了它们在Wikidata5M上的MRR, MR, HITS@1, 3, 10.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler6.jpg" style="zoom:33%"><p>确实非常难, 很多流行的数据集都没取得太好的效果. 因为Wikidata包括了各种不同类型的实体和关系, 作者也建议大家使用<strong>大规模数据集</strong>以确保模型能够被充分测试.</p><h4 id="MLM-Objective"><a href="#MLM-Objective" class="headerlink" title="MLM Objective"></a>MLM Objective</h4><p>作者使用优化MLM的数据集有:</p><ul><li>BookCorpus.</li><li>English WIkipedia.</li></ul><h3 id="Pre-Training-Settings"><a href="#Pre-Training-Settings" class="headerlink" title="Pre - Training Settings"></a>Pre - Training Settings</h3><p>关于参数设置我就不再提了, 详见原文. 主要叙述一下在后面实验经常比较的内容.</p><h4 id="KE-Settings"><a href="#KE-Settings" class="headerlink" title="KE Settings"></a>KE Settings</h4><p>对于KE的优化, 作者设计了三种设置:</p><ol><li><strong>KEPLER - Wiki</strong>: 用<strong>Wikidata5M</strong>训练KEPLER, 总使用描述的前512个Token. 当使用实体和关系一起作为输入时(Using Entity and Relation Descriptions), 模型被称为KEPLER - Wiki - rel.</li><li><strong>KEPLER - WordNet</strong>: 用<strong>WordNet</strong>训练KEPLER, 作者尝试将更多的语言知识融入进去, 或许会有益于NLP任务. WordNet中的关系数量相对来说非常少, 所以只采用实体描述.</li><li><strong>KEPLER - W + W</strong>: 联合训练Wikidata5M和WordNet. 损失函数相应的发生变化:<br>$$<br>\mathcal{L}=\mathcal{L}_{\mathrm{Wiki}}+ \mathcal{L}_{\mathrm{WordNet}}+\mathcal{L}_{\mathrm{MLM}}<br>$$</li></ol><h4 id="RoBERTa"><a href="#RoBERTa" class="headerlink" title="RoBERTa"></a>RoBERTa</h4><p>KEPLER是基于RoBERTa的, RoBERTa采用了更大的语料库进行训练. 为公平起见, 作者训练了<strong>RoBERTa*</strong>, 也是用RoBERTa的权重进行初始化, 但用相同的语料库, 并只对它的MLM目标继续优化.</p><blockquote><p>这样设计实验应该是为了凸显出加入KE Loss带来的变化.</p></blockquote><h3 id="NLP-Tasks"><a href="#NLP-Tasks" class="headerlink" title="NLP Tasks"></a>NLP Tasks</h3><p>在NLP任务中, 作者主要和其他Knowledge Enhanced Model在NLP任务上<strong>横向对比</strong>.</p><h4 id="Relation-Classification"><a href="#Relation-Classification" class="headerlink" title="Relation Classification"></a>Relation Classification</h4><h5 id="TACRED"><a href="#TACRED" class="headerlink" title="TACRED"></a>TACRED</h5><p>TACRED是人为标注用于关系分类的数据集. 在TACRED上表现如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler7.jpg" style="zoom:25%"><p>KEPLER - Wiki表现很棒, 与RoBERTa*相比有很大进步, 但KEPLER - WordNet表现只比RoBERTa*好一点点. KEPLER - W + W和KEPLER - Wiki表现相当, 作者认为是WordNet限制了性能.</p><h4 id="FewRel"><a href="#FewRel" class="headerlink" title="FewRel"></a>FewRel</h4><p>FewRel是用于体现Few - Shot能力的关系分类数据集, 其2.0版本添加了更多的领域. 在FewRel上表现如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler8.jpg" style="zoom:33%"><p>就作者给出的结果来看, KEPLER - Wiki表现很不错. 作者认为在FewRel1.0和2.0上的差异是因为2.0版本加入了医疗类的数据.</p><h4 id="Entity-Typing"><a href="#Entity-Typing" class="headerlink" title="Entity Typing"></a>Entity Typing</h4><p>KEPLER在OpenEntity上的表现如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler11.jpg" style="zoom:20%"><p>从F1 Score上来看, KEPLER表现不错.</p><h4 id="GLUE"><a href="#GLUE" class="headerlink" title="GLUE"></a>GLUE</h4><p>GLUE(General Language Understanding Evaluation)是用来评判<strong>语言理解能力</strong>的测试, 这种任务不需要知识, 但需要<strong>理解能力</strong>. 作者通过GLUE尝试证明KEPLER对NLU任务表现没有退化.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler9.jpg" style="zoom:33%"><p>在比较大的数据集上KEPLER表现相对正常, 在比较小的数据集上例如RTE, KEPLER退化的比较严重. 一般来说 , KEPLER对NLU没有明显副作用.</p><blockquote><p>即然RoBERTa*训练与KEPLER使用的是相同数据集, 那么按理说会因描述类的文本数据扩充而提升文本理解能力, 实际上却没有. 从这个角度来看, PTM和KE并不能给NLU任务带来增益, 甚至还会有<strong>减益</strong>. 应该还有更深层的原因.</p></blockquote><h3 id="KG-Tasks"><a href="#KG-Tasks" class="headerlink" title="KG Tasks"></a>KG Tasks</h3><p>作者在KG的Task上不能使用常用数据集, 因为它们都没有高质量的实体描述, 并且不支持Inductive Setting.</p><h4 id="Transductive-Setting"><a href="#Transductive-Setting" class="headerlink" title="Transductive Setting"></a>Transductive Setting</h4><p>在该设置中, 所有实体在所有阶段均是可见的, 但三元组不可见. 作者将其与TransE性能进行比较:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler10.jpg" style="zoom:33%"><blockquote><p>用TransE当Baseline是不是有点太不公平了？TransE没有使用任何额外辅助信息, 实体关系描述肯定算引入辅助信息了. 作者在文中列出三条理由, 但我不是很认同.</p></blockquote><h4 id="Inductive-Setting"><a href="#Inductive-Setting" class="headerlink" title="Inductive Setting"></a>Inductive Setting</h4><p>在该设置中, 所有实体和三元组在各阶段都不共享. 作者将也引入实体描述的DKRL(也是作者提出的模型)作为Baseline进行比较:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler14.jpg" style="zoom:33%"><p>KEPLER - Wiki - rel比KEPLER - Wiki要强大许多, 并且比DKRL提升巨大.</p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>KEPLER是一个<strong>多任务学习</strong>的模型, 作者希望探究对KEPLER性能提升的因素.<br>作者先将RoBERTa, RoBERTa*(仅使用MLM Loss), KEPLER - KE(仅使用KE Loss), KEPLER - Wiki在TACRED上进行测试:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler12.jpg" style="zoom:25%"><p>相比于RoBERTa, RoBERTa*和KEPLER - KE性能下降了, 作者说这证明了KE Loss和MLM Loss都是必不可缺的.</p><p>作者希望进一步量化的去看看KEPLER到底学到了多少知识, 在TACRED中在<strong>对实体进行Mask</strong>(ME, Masked Entity)和<strong>只保留实体</strong>(Only Entity)的情况下, 重新对关系进行分类, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler13.jpg" style="zoom:25%"><p>KEPLER - Wiki学习到一些知识, 比用同等数据训练出来的RoBERTa*效果要更好一些.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>作者分别从PLM和KGE两个方面总结了KEPLER的优点:</p><ul><li>KEPLER作为PLM, 将常识集成进了语言表示, 并且具有强大的语言理解能力, 并且增强了抽取知识的能力, 能够直接适应很多NLP任务.</li><li>KEPLER作为KEM, 能够使用丰富的文本信息, 能在文本描述的引导下预测从没见过的实体.</li></ul><p>在我看来, KEPLER其实很一般. 而且它有一些很明显的<strong>缺陷</strong>, 它没有很好地将实体描述中的相关做进一步<strong>扩展</strong>, 例如它没有很好地利用KG作为<strong>图</strong>的优势. 而且它的上下文提取能力获取的实体和关系表示是<strong>静态</strong>的, 并不能根据上下文改变实体的表达.</p><p>除去KEPLER本身外, 作者贡献了一个大规模附带文本描述的数据集.</p><p>最后, 就注入知识是否有益于NLU这个问题来说, 答案还是不明确. 从直觉的角度来说, KEPLER本身就能从上下文中利用BERT的结构学到一些语法知识, 在注入知识的情况下应该进一步提升NLU能力, 而现在很多工作实验结果则不然.</p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">AnNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/52897.html">https://ADAning.github.io/posts/52897.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">AnNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/BERT/"><span class="chip bg-color">BERT</span> </a><a href="/tags/KGE/"><span class="chip bg-color">KGE</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/63679.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/18.jpg" class="responsive-img" alt="Pytorch实现: Transformer"> <span class="card-title">Pytorch实现: Transformer</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: Pytorch基本操作 Transformer: 详见Transformer精讲 2022.04.03: 去掉了Pre Norm比Post Norm效果好的表述. Pytorch实现: Transformer本文是T</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-11-23 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/Transformer/"><span class="chip bg-color">Transformer</span> </a><a href="/tags/Pytorch/"><span class="chip bg-color">Pytorch</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/19912.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/3.jpg" class="responsive-img" alt="KGE预警论文两则"> <span class="card-title">KGE预警论文两则</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文是两篇KGE方向的预警论文的阅读笔记和个人理解. 预警类的工作其实是比较少见的, 对领域的发展也非常有指导意义. 2020.11.22: 更新Reciprocal Relation. 2021.05.13: 修正Reciprocal</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-11-20 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/" class="post-category">知识图谱</a></span></div></div><div class="card-action article-tags"><a href="/tags/KGE/"><span class="chip bg-color">KGE</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">AnNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">325.7k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:695439722@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>