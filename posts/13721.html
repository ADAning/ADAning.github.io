<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="Integrating Image-Based and Knowledge-Based Representation Learning, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>Integrating Image-Based and Knowledge-Based Representation Learning | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:.6"></i> <span>时间轴</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li><li><a href="/markdown"><i class="fab fa-markdown" style="margin-top:-20px;zoom:.6"></i> <span>Markdown</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li><li><a href="/markdown" style="margin-left:75px"><i class="fa fab fa-markdown" style="position:absolute;left:50px"></i> <span>Markdown</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/3.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">Integrating Image-Based and Knowledge-Based Representation Learning</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/KGE/"><span class="chip bg-color">KGE</span> </a><a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"><span class="chip bg-color">多模态</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/" class="post-category">知识图谱</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2020-11-13</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2022-03-27</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 3.7k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 14 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p>本文前置知识:</p><ul><li>AlexNet(详见<a href="https://adaning.github.io/posts/38085.html">卷积神经网络发展史</a>)</li><li>Attention(详见<a href="https://adaning.github.io/posts/40071.html">Seq2Seq和Attention</a>)</li><li>TransE(详见<a href="https://adaning.github.io/posts/53023.html">TransE: Translating Embeddings for Modeling Multi-relational Data</a>)</li></ul></blockquote><h1 id="Integrating-Image-Based-and-Knowledge-Based-Representation-Learning"><a href="#Integrating-Image-Based-and-Knowledge-Based-Representation-Learning" class="headerlink" title="Integrating Image-Based and Knowledge-Based Representation Learning"></a>Integrating Image-Based and Knowledge-Based Representation Learning</h1><p>本文是论文<a href="https://ieeexplore.ieee.org/abstract/document/8689107/" target="_blank" rel="noopener">Integrating Image-Based and Knowledge-Based Representation Learning</a>的阅读笔记和个人理解. 这篇论文是刘志远老师&lt;知识图谱与深度学习&gt;中2.8节提到的模型.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>研究人员发现, 语言的理解和生成是由大脑<strong>不同位置</strong>的区域负责的, 这些区域对应了许多现实生活中的事物.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113210117233.png" style="zoom:25%"><p>有时, 图片之间也能暗含关系:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113210138922.png" style="zoom:25%"><p>即若A是B的一部分, 我们的视觉也认为A的周围应该有B.</p><p>作者认为, 我们理解世界是通过<strong>Knowledge Base</strong>(实体和关系的结构化三元组), 和<strong>Image Representation</strong>(通过Deep Convolutional Networks). 所以作者研究了基于<strong>图片</strong>的知识表示模型IKRL Model(<strong>I</strong>mage - <strong>B</strong>ased Knowledge <strong>R</strong>epresentation <strong>L</strong>earning Model).</p><p>在先前的KRL方法中, 只使用了KG中的关系信息. 然而KG中的结构化信息经常<strong>过于简单</strong>, 或者<strong>不完整</strong>, 会限制知识表示在下游任务中的表现. KRL是允许向表示中添加实体图像信息的, 而基于结构和图片的表示能够从<strong>多方面</strong>表示实体.</p><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>受到大脑的启发, 通过编码和图像两种手段来对实体进行表示, $\mathbf{h}_S, \mathbf{t}_S$ 是<strong>基于结构化的表示(SBR)</strong>的头实体和尾实体, $\mathbf{h}_I, \mathbf{t}_I$ 是<strong>基于图片的表示(IBR)</strong>的头实体和尾实体.</p><h3 id="Joint-Energy-Function"><a href="#Joint-Energy-Function" class="headerlink" title="Joint Energy Function"></a>Joint Energy Function</h3><p>我们将基于结构化的表示(SBR)和基于图片的表示(IBR), 融合起来, 形成一个IKRL Model, <strong>联合能量函数</strong>如下:<br>$$<br>E(h, r, t)=E_{S S}+E_{S I}+E_{I S}+E_{I I}<br>$$</p><p>联合能量函数中有四项, 这四项分别是:</p><ul><li>$E_{S S}=\lVert\mathbf{h}_{S}+\mathbf{r}-\mathbf{t}_{S}\rVert$: 和<strong>TransE</strong>的能量函数一模一样.</li><li>$E_{II}=\lVert\mathbf{h}_{I}+\mathbf{r}-\mathbf{t}_{I}\rVert$: 与TransE的能量函数也一样, 但是是<strong>图片版本</strong>的.</li><li>$E_{SI}=\lVert\mathbf{h}_{S}+\mathbf{r}-\mathbf{t}_{I}\rVert$, $E_{IS}=\lVert\mathbf{h}_{I}+\mathbf{r}-\mathbf{t}_{S}\rVert$: 这两项希望能将SBR和IBR投入<strong>相同语义空间</strong>.</li></ul><p>其中实体向量$\mathbf{h}_S, \mathbf{h}_I, \mathbf{t}_S, \mathbf{t}_I$ 都是<strong>归一化</strong>过的, 关系$\mathbf{r}$ 不是, TransE论文中曾提到关系的归一化对学习到关系没太大影响. 但是请注意, 在这里SBR和IBR共用同一个关系向量$r$, 关系的表示是<strong>不能从图像中直接学到</strong>的. 共享关系向量能够作为两种实体表示之间的<strong>转换</strong>, 也能方便它们嵌入到同一语义空间中.</p><p>模型结构的概览图如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113210155434.png" style="zoom:50%"><p>因为考虑到融入了IBR, 所以每个实体的多个图片都被送入Image Encoder中, 然后在通过注意力机制让模型考虑每张图片的重要性, 然后SBR和IBR联合学习整个能量函数.</p><h3 id="Image-Encoder"><a href="#Image-Encoder" class="headerlink" title="Image Encoder"></a>Image Encoder</h3><p>图像对IKRL来说是非常重要的, 因为图像能够从外观等多个方面刻画实体. 此外, <strong>多张图片</strong>可能从不同角度提供同一个实体的不同特性, 我们用$I_{k}=\left\{\mathrm{img}_{1}^{(k)}, \mathrm{img}_{2}^{(k)}, \ldots, \mathrm{img}_{n}^{(k)}\right\}$ 来表示多张图片.</p><p>既然涉及到图像, 那么比较成熟的方案肯定是用<strong>CNN</strong>提取视觉特征了, 用CNN对每张图像构建特征表示.</p><p>Image Encoder由<strong>图像表示模块</strong>和<strong>图像投影模块</strong>组成:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113210231408.png" style="zoom:25%"><h4 id="Image-Representation-Module"><a href="#Image-Representation-Module" class="headerlink" title="Image Representation Module"></a>Image Representation Module</h4><p>图像表示模块主要依赖于CNN对特征进行抽取, 将实体在<strong>图像空间</strong>中进行表示. 作者在这里只使用ALexNet(确实比较老), AlexNet是一个只由5层Conv层和2层全连接层组成的神经网络:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/alexnet.jpg" style="zoom:50%"><p>将图像Reshape成$224\times 224$ 的大小, 然后接上5层Conv层, 2层全连接层就得到了实体的图像表示.</p><blockquote><p>这里使用AlexNet肯定不是最好的选择, 我猜测作者出于<strong>实验性目的</strong>才使用了AlexNet.</p></blockquote><h4 id="Image-Projection-Module"><a href="#Image-Projection-Module" class="headerlink" title="Image Projection Module"></a>Image Projection Module</h4><p>因为IBR和SBR不在同一个语义空间中, 所以需要用一次变换将它们投入相同的实体空间中:<br>$$<br>\mathbf{p}_{i}=\mathbf{M} \cdot f\left(\mathrm{img}_{i}\right)<br>$$<br>$\mathbf{p}_i$ 指图片表示, $f(\cdot)$ 表示神经网络的输出. 当然, 这个投影矩阵$\mathbf{M}$ 在每个实例之间<strong>共享</strong>.</p><h3 id="Attention-Based-Multi-Instance-Learning"><a href="#Attention-Based-Multi-Instance-Learning" class="headerlink" title="Attention Based Multi - Instance Learning"></a>Attention Based Multi - Instance Learning</h3><p>Attention在IKRL中起到了非常大的作用. 因为Attention让更多信息性的图片对IKRL有更多的贡献. Attention在IKRL中被用于<strong>多实例学习</strong>, 因为绝大多数实体都有不止一张不同的图片, 但视觉信息经常伴随着<strong>噪声</strong>, 所以相当有必要对实体所对应的图片进行<strong>选择</strong>. 论文后面实验会多次说明这一点.</p><p>细想一下, 其实我们也是这样的, 我们善用注意力去选择表征实例, 而滤掉不相关的实例.</p><p>在IKRL中, 实例级别的注意力能将每个实例与实体进行<strong>匹配</strong>, 得到实例对实体的<strong>权重</strong>:<br>$$<br>\operatorname{att}\left(\mathbf{p}_{i}^{(k)}, \mathbf{e}_{S}^{(k)}\right)=\frac{\exp \left(\mathbf{p}_{i}^{(k)} \cdot \mathbf{e}_{S}^{(k)}\right)}{\sum_{j=1}^{n} \exp \left(\mathbf{p}_{j}^{(k)} \cdot \mathbf{e}_{S}^{(k)}\right)}<br>$$</p><p>其中$\mathbf{e}_{S}^{(k)}$ 代表第$k$ 个实体的SBR.</p><p>在获取了权重后, 对图像表示<strong>加权求和</strong>, 得到IBR:<br>$$<br>\mathbf{e}_{I}^{(k)}=\sum_{i=1}^{n} \frac{\operatorname{att}\left(\mathbf{p}_{i}^{(k)}, \mathbf{e}_{S}^{(k)}\right) \cdot \mathbf{p}_{i}^{(k)}}{\sum_{j=1}^{n} \operatorname{att}\left(\mathbf{p}_{j}^{(k)}, \mathbf{e}_{S}^{(k)}\right)}<br>$$<br>除了上述普通的Attention外, 作者还用两种方法与之进行比较:</p><ul><li>如果对每张实例分配相同权重, 称为$\text{AVG}$.</li><li>如果只选权重最大的实例作为$\mathbf{p}_{i}^{(k)}$, 称为$\text{MAX}$.</li></ul><p>在后续的实验中, 会比较这三种方法之间的性能.</p><h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><p>作者使用最大化间隔的Hinge Loss来训练:</p><p>$$<br>L=\sum_{(h, r, t) \in T} \sum_{\left(h^{\prime}, r^{\prime}, t^{\prime}\right) \in T^{\prime}} \max \left(\gamma+E(h, r, t)-E\left(h^{\prime}, r^{\prime}, t^{\prime}\right), 0\right)<br>$$</p><p>其中$\gamma$ 代表间隔. 与现在的其他KRL模型训练一样, 都有<strong>负采样</strong>:<br>$$<br>T^{\prime}=\left\{\left(h^{\prime}, r, t\right) \mid h^{\prime} \in E\right\} \cup\left\{\left(h, r, t^{\prime}\right) \mid t^{\prime} \in E\right\}<br>\cup\left\{\left(h, r^{\prime}, t\right) \mid r^{\prime} \in R\right\}, \quad(h, r, t) \in T<br>$$<br>但这里的负采样不光替换实体, 而是随机替换三元组中的<strong>实体</strong>和<strong>关系</strong>.</p><h3 id="Optimization-and-Implementation-Details"><a href="#Optimization-and-Implementation-Details" class="headerlink" title="Optimization and Implementation Details"></a>Optimization and Implementation Details</h3><p>IKRL模型中, 所有的参数为$\theta=(\mathbf{E}, \mathbf{R}, \mathbf{W}, \mathbf{M})$, $\mathbf{E}$ 代表SBR的Embedding, 包括$\mathbf{h}_s, \mathbf{t}_s$. $\mathbf{R}$ 代表关系嵌入. $\mathbf{W}$ 代表神经网络的参数, $\mathbf{M}$ 是投影矩阵的参数. 作者使用SGD来优化模型, $\mathbf{E}, \mathbf{R}$ 直接用TransE的参数初始化, $\mathbf{M}$ 随机初始化.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细试验参数设置请参考原论文, 不过我认为本论文是一篇<strong>实验性</strong>的工作, 参数设置上没有太多意义. SBR的实体和关系Embedding维度$d_s=50$, 并且最多为每个实体使用10张图片.</p><h3 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h3><p>因为没有现成的Image Based Knowledge Dataset, 所以作者团队自己搞了一个WN9 - IMG. 这个数据集先从WN18中抽出一部分三元组, 然后从ImageNet中抽出来一部分图片, 组成了基于图片的数据集. 这个数据集中只有9种关系, 6555个实体. 不同关系的数据集分布如下所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113210532097.png" style="zoom:25%"><p>8类实体数量如下所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113210614662.png" style="zoom:25%"><h3 id="Entity-Prediction"><a href="#Entity-Prediction" class="headerlink" title="Entity Prediction"></a>Entity Prediction</h3><p>在训练时, 仍然使用IBR和SBR<strong>混合</strong>的方式进行训练. 只是在<strong>测试</strong>时对使用的信息进行改动:</p><ul><li>SBR: 在测试时只使用基于结构的表示.</li><li>IBR: 在测试时只使用基于图像的表示.</li><li>UNION: 二者都可以使用.</li></ul><p>在与TransE和TransR的比较中, IKRL的表现非常好:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113210715845.png" style="zoom:25%"><p>从结果中得到以下结论:</p><ul><li>无论是哪种IKRL, 全面碾压TransE和TransR, 证明了实体图像中丰富的视觉信息能帮助更深入的理解实体.</li><li>相比于只使用SBR训练(TransE), 融合了IBR的表示训练有极大的提升. 因为IKRL能通过能量函数中的两种表示进行混合项训练, 也间接地学习到了一部分图像信息, 这也是为什么测试时只使用SBR就能得到很大提升.</li><li>在MR上显示出的效果提升比较大. 作者认为MR是更多关注Embedding在空间上的<strong>整体效果</strong>, 而Hits@10对错误比例更加敏感. IKRL因为融入了图片信息, 能从图片中间接的发现KG中没有体现的直接关系, 可以利用图片的中发现的<strong>潜在关系</strong>.</li><li>IKRL是基于TransE进行训练的, 但仍然比TransR效果好, 说明IKRL有更好的<strong>鲁棒性</strong>, 能更好的用在基于平移模型的改进模型上.</li></ul><blockquote><p>我有些惊讶, 仅仅只使用IBR带来的效果居然这么好. 是不是说明了模型在训练阶段使用了SBR后, 在不同任务上SBR测试阶段的意义是不同的? 可能有些任务不需要SBR?</p></blockquote><p>作者还对Attention的三种方式的效果进行了比较:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113210928871.png" style="zoom:33%"><p>MAX最差, 这是因为单一图片提供的视觉信息有限. AVG其次, 虽然考虑到了所有实例, 但不可避免的引入了噪声. 普通的Attention是最好的, 它能够根据图片的质量来对实例进行<strong>筛选</strong>.</p><h3 id="Triple-Classification"><a href="#Triple-Classification" class="headerlink" title="Triple Classification"></a>Triple Classification</h3><p>在三元组分类任务上, 作者对每种关系依据验证集设置一个阈值$\delta_r$, 用阈值来对三元组分类是否正确进行分类. 例如当$\lVert\mathbf{h}+\mathbf{r}-\mathbf{t}\rVert&gt;\delta_r$ 时, 分类错误, 反之分类正确. 其他模型同样按照自己的评分函数分类判断.</p><p>在不同的Attention种类下, 结果如下所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113211050295.png" style="zoom:50%"><p>仍然是与Entity Prediction相似的结论. 融入视觉信息的表示是一件非常重要的事情, 并同时证明了Attention保证了图像的质量, 充分利用实体的多样性, 增强了模型的<strong>鲁棒性</strong>.</p><h3 id="Representation-Analysis"><a href="#Representation-Analysis" class="headerlink" title="Representation Analysis"></a>Representation Analysis</h3><h4 id="SBR-and-IBR"><a href="#SBR-and-IBR" class="headerlink" title="SBR and IBR"></a>SBR and IBR</h4><p>作者分别基于SBR和IBR计算了数据集中所有类别的实体之间的<strong>协方差</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113211018302.png" style="zoom:67%"><p>从中不难发现, IBR能对不同类别的实体相<strong>区分</strong>(不同类之间相关性差), SBR将少数类别的实体相<strong>连接</strong>(有些类相关性强). 但在IBR中, Plant和Object似乎具有很高的相关性.</p><p>IBR更容易<strong>基于外观</strong>区分实体, SBR更善于<strong>基于功能</strong>区分实体, 如下图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113211141244.png" style="zoom:50%"><p>左侧”sport”从图像上来看并不相似, 但功能相似. 右侧”aritifact”从图像上看上去非常相似, 但功能不相似.</p><p>作者分别对SBR和IBR进行了PCA降维成2维后绘制出实体在坐标系中的位置, 并用RSA做可解释性分析:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113211120889.png" style="zoom:67%"><p>SBR(左), IBR(中), 主成分可解释性(右). IBR能够明显的将各关系的实体分开, SBR效果就差一些. 从数据可解释性来看, 除了随维度增大可解释性增强, 但从表示类型来看, IBR也是始终要强于IBR的.</p><h4 id="Relation-Analysis"><a href="#Relation-Analysis" class="headerlink" title="Relation Analysis"></a>Relation Analysis</h4><p>作者将关系也做PCA和RSA:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113211207797.png" style="zoom:50%"><p>能很清楚的看到, 反义关系往往处于某个主成分正交轴的<strong>对立面</strong>. RSA图中看到, 在主成分没有达到8时可解释性已经收敛, 说明目前学到的关系复杂度是比需求要大的.</p><h3 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h3><p>作者在这里对案例进行了分析.</p><h4 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h4><p>首先是对Attention对<strong>实例筛选</strong>的结果进行了可视化:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113211241891.png" style="zoom:50%"><p>例如”cycling”这个实体, 真正的骑行图像被赋予了高注意力, 而没有自行车的图像被赋予低注意力. “typewirter”中, 整体打印机的图片被赋予高注意力, 而打印机局部细节的图片被赋予低注意力. 在”riding”中, 人类骑马的图片被赋予高注意力, 马群自己走的照片被赋予低注意力.</p><p>这证明了注意力能自动从图像中学习知识表示, 减少低质量图片的噪声.</p><h4 id="Semantic-Translation-in-Image-Representation-Space"><a href="#Semantic-Translation-in-Image-Representation-Space" class="headerlink" title="Semantic Translation in Image Representation Space"></a>Semantic Translation in Image Representation Space</h4><p>然后, 作者发现, 在跨模态的图像 - 知识空间中, 像Word2Vec一样, 也含有<strong>语义平移</strong>规则:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113211321972.png" style="zoom:33%"><p>在图像的表示中, 柜子和抽屉的差, 与钢琴和琴键之间的差大致相等, 表示出”属于”的关系. 这体现了编码的<strong>语义规则</strong>性.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>IKRL介绍了一种<strong>基于图像</strong>的知识表示方法. 将<strong>基于结构化的表示(SBR)</strong>和<strong>基于图片的表示(IBR)</strong>融合在了一起, 并用<strong>Attention</strong>做图片的自动过滤, 提高模型的<strong>性能</strong>和<strong>鲁棒性</strong>.</p><p>我认为, 这篇文章比较有价值的有以下几个部分:</p><ul><li>论文中提出的<strong>多实例学习</strong>思路确实不错, 或许多模态都可以用多实例学习作为接口, 将实体与不同模态之间的数据进行自动对齐和筛选.</li><li>做了很多的<strong>分析类实验</strong>, 虽然使用的模型都是最原始最简单的, 但这些<strong>可视化探究</strong>都是非常有价值的, 我认为相当多的可视化都<strong>得益于图像</strong>.</li><li>提供了基于图像的知识数据集<strong>WN9 - IMG</strong>.</li></ul><p>我认为的缺点有:</p><ul><li>在处理SBR时只用了TransE, 在处理IBR时只用了非常早的AlexNet. 那么在对关系建模时就肯定会遇到对<strong>多关系</strong>建模的痛点. 但想必这篇论文也只是一次<strong>尝试和探索</strong>, 而非追求性能.</li><li>没有与更多的KRL方法进行对比, 但想必在后续肯定会有相关工作.</li></ul><p>我认为实验还揭示了另一个点, 或许在某些任务上<strong>融入语言特征</strong>并不能起到很大的作用. 比如本论文提到的模型, 如果只用IBR, 也能取得相当好的效果. 当然不排除作者提出的KRL模型没有对三元组使用其他方法优化的因素. SBR确实能够在某些任务上使模型完成<strong>更复杂</strong>的任务, 并与IBR<strong>互补</strong>. 但从本论文的结果来看, <strong>并不能很大幅度的提升性能</strong>.</p><p>希望<strong>脑科学和神经科学</strong>能快快进步, 给深度学习发展带来更多动力和想法.</p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">AnNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/13721.html">https://ADAning.github.io/posts/13721.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">AnNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/KGE/"><span class="chip bg-color">KGE</span> </a><a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"><span class="chip bg-color">多模态</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/24649.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/1.jpg" class="responsive-img" alt="RoBERTa: A Robustly Optimized BERT Pretraining Approach"> <span class="card-title">RoBERTa: A Robustly Optimized BERT Pretraining Approach</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: BERT(详见ELMo, GPT, BERT) RoBERTa: A Robustly Optimized BERT Pretraining Approach本文是论文RoBERTa: A Robustly Opti</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-11-18 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/BERT/"><span class="chip bg-color">BERT</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/53023.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/11.jpg" class="responsive-img" alt="TransE: Translating Embeddings for Modeling Multi-relational Data"> <span class="card-title">TransE: Translating Embeddings for Modeling Multi-relational Data</span></div></a><div class="card-content article-content"><div class="summary block-with-text">TransE: Translating Embeddings for Modeling Multi-relational Data本文是论文Translating Embeddings for Modeling Multi-relation</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-11-13 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/" class="post-category">知识图谱</a></span></div></div><div class="card-action article-tags"><a href="/tags/KGE/"><span class="chip bg-color">KGE</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">AnNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">332.4k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:695439722@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>