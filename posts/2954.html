<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="R-MeN: A Relational Memory-based Embedding Model, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>R-MeN: A Relational Memory-based Embedding Model | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/0.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">R-MeN: A Relational Memory-based Embedding Model</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/KGE/"><span class="chip bg-color">KGE</span> </a><a href="/tags/Attention/"><span class="chip bg-color">Attention</span> </a><a href="/tags/%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C/"><span class="chip bg-color">记忆网络</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/" class="post-category">知识图谱</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2020-12-10</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2022-03-27</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 2.6k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 10 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p>本文前置知识:</p><ul><li>Self - Attention: 详见<a href="https://adaning.github.io/posts/6744.html#Self-Attention">Transformer精讲</a>.</li></ul><p><strong>2020.12.14</strong>: 修正错误.</p></blockquote><h1 id="A-Relational-Memory-based-Embedding-Model-for-Triple-Classification-and-Search-Personalization"><a href="#A-Relational-Memory-based-Embedding-Model-for-Triple-Classification-and-Search-Personalization" class="headerlink" title="A Relational Memory-based Embedding Model for Triple Classification and Search Personalization"></a>A Relational Memory-based Embedding Model for Triple Classification and Search Personalization</h1><p>本文是论文<a href="https://arxiv.org/abs/1907.06080" target="_blank" rel="noopener">A Relational Memory-based Embedding Model for Triple Classification and Search Personalization</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者认为, 现在的KGE方法普遍在<strong>记忆有效的三元组问题</strong>上受限, 并不能<strong>有效的捕捉三元组之间的潜在依赖关系</strong>.</p><p>现在的KGE普遍都用Link Prediction评估模型的性能, 但在<strong>实际应用</strong>中, 经常以<strong>三元组分类</strong>和<strong>个性化搜索</strong>为主要目标. 所以作者希望通过记忆提升这两个场景中的KGE性能, 关系记忆能够编码关系三元组之间的潜在依赖关系.</p><blockquote><p>作者给出以下问题的定义:</p><ul><li>三元组分类: 判断给定的三元组是否<strong>有效</strong>.</li><li>个性化搜索: 针对用户给出的<strong>请求</strong>, 对系统给出的搜索结果进行<strong>重排</strong>.</li></ul></blockquote><h2 id="R-MeN"><a href="#R-MeN" class="headerlink" title="R - MeN"></a>R - MeN</h2><p>在R - MeN中, 作者考虑将三元组视为一个长度为3的<strong>时序序列</strong>看待, 通过<strong>Self - Attention</strong>对记忆<strong>循环交互</strong>.</p><p>整体流程是Embedding -&gt; Memory Attention interact -&gt; CNN decode:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rmen1.jpg" style="zoom:50%"><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>作者假设三元组$(s, r, o)$ 的<strong>相对位置关系</strong>对于推断三元组是否有效是十分必要的, 因此需要在三元组输入之前对它们都加以位置编码, 用$\mathbf{v}_{s}, \mathbf{v}_{r}, \mathbf{v}_{o} \in \mathbb{R}^{d}$ 分别代表$s,r,o$ 的嵌入, 加上位置编码后, 再经过一次线性变换的三元组向量$\left\{\mathbf{x}_{1}, \mathbf{x}_{2}, \mathbf{x}_{3}\right\}$ 为:<br>$$<br>\begin{aligned}<br>\mathbf{x}_{1} &amp;=\mathbf{W}\left(\mathbf{v}_{s}+\mathbf{p}_{1}\right)+\mathbf{b} \\<br>\mathbf{x}_{2} &amp;=\mathbf{W}\left(\mathbf{v}_{r}+\mathbf{p}_{2}\right)+\mathbf{b} \\<br>\mathbf{x}_{3} &amp;=\mathbf{W}\left(\mathbf{v}_{o}+\mathbf{p}_{3}\right)+\mathbf{b}<br>\end{aligned}<br>$$<br>其中$\mathbf{W} \in \mathbb{R}^{k\times d}$ 为权重矩阵, $\mathbf{p}_{1}, \mathbf{p}_{2} \mathbf{p}_{3} \in \mathbb{R}^{d}$ 分别它们的位置编码, 其中的$k$ 代表每个Memory Slot的维度大小(下文会提到). 在文中, 并没有提及位置编码的实现方式.</p><h3 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h3><p>作者假设有一个记忆$M \in \mathbb{R}^{N \times k}$, 它是由$N$ 行Memory Slot组成的, 每个Slot的大小为$k$. 用$M^{(t)}$ 代表$t$ 时刻的$M$ 矩阵, 那么$M_{i,:}^{(t)} \in \mathbb{R}^{k}$ 就代表其中第$i$ 个Memory Slot在$t$ 时刻存储的所有Memory.</p><p>那么每个Memory Slot中的记忆就一定会根据在$t$ 时刻的新输入$\mathbf{x}_t$ 来<strong>更新</strong>当前时刻的记忆$\hat{M}_{i,:}^{(t+1)}$.</p><p>如果使用跟Transformer中的<strong>多头</strong>类似的机制, 对每个Slot中的Memory进行分割:<br>$$<br>\hat{M}_{i,:}^{(t+1)}=\left[\hat{M}_{i,:}^{(t+1), 1} \oplus \hat{M}_{i,:}^{(t+1), 2} \oplus\right.\left.\ldots \oplus \hat{M}_{i,:}^{(t+1), H}\right]<br>$$<br>其中$\oplus$ 代表Concat操作, $H$ 代表总头数.</p><p>每个头的记忆$\hat{M}_{i,:}^{(t+1), h}$ 使用Self - Attention将过去时刻的记忆与当前时刻输入<strong>加权交互</strong>:<br>$$<br>\hat{M}_{i,:}^{(t+1), h}=\alpha_{i, N+1, h}\left(\mathbf{W}^{h, V} \mathbf{x}_{t}\right)+\sum_{j=1}^{N} \alpha_{i, j, h}\left(\mathbf{W}^{h, V} M_{j,:}^{(t)}\right)<br>$$<br>其中$\mathbf{W}^{h, V} \in \mathbb{R}^{n \times k}$ 是Value的投影矩阵, $n$ 是每个头的大小, 满足$k = nH$, 这样能保证Concat后的记忆矩阵中每个Slot的大小仍然是$k$ 维. $\left\{\alpha_{i, j, h}\right\}_{j=1}^{N}, \alpha_{i, N+1, h}$ 是注意力权重. 具体细节在下一小节介绍.</p><blockquote><p>三元组中的三个元素被分别输入后, 记忆将被<strong>清除</strong>.</p></blockquote><h3 id="Self-Attention-Interaction"><a href="#Self-Attention-Interaction" class="headerlink" title="Self - Attention Interaction"></a>Self - Attention Interaction</h3><p>与Self - Attention大同小异, 但在<strong>记忆存储机制</strong>的影响下, 归一化一定要包含<strong>过去记忆</strong>和<strong>当前时刻新输入</strong>两个部分:<br>$$<br>\begin{aligned}<br>\alpha_{i, j, h} &amp;=\frac{\exp \left(\beta_{i, j, h}\right)}{\sum_{m=1}^{N+1} \exp \left(\beta_{i, m, h}\right)} \\<br>\alpha_{i, N+1, h} &amp;=\frac{\exp \left(\beta_{i, N+1, h}\right)}{\sum_{m=1}^{N+1} \exp \left(\beta_{i, m, h}\right)}<br>\end{aligned}<br>$$<br>$\beta_{i, j, h}$ 是Attention得分, 也分别对应着过去记忆和当前时刻新输入两部分:<br>$$<br>\begin{aligned}<br>\beta_{i, j, h} &amp;=\frac{\left(\mathbf{W}^{h, Q} M_{i,:}^{(t)}\right)^{\top}\left(\mathbf{W}^{h, K} M_{j,:}^{(t)}\right)}{\sqrt{n}} \\<br>\beta_{i, N+1, h} &amp;=\frac{\left(\mathbf{W}^{h, Q} M_{i,:}^{(t)}\right)^{\top}\left(\mathbf{W}^{h, K} \mathbf{x}_{t}\right)}{\sqrt{n}}<br>\end{aligned}<br>$$<br>其中$\mathbf{W}^{h, Q} \in \mathbb{R}^{n \times k}$ , $\mathbf{W}^{h, K} \in \mathbb{R}^{n \times k}$ 分别是Query和Key的投影矩阵. 此外, 还在$\mathbf{x}_{t}$ 和$\hat{M}_{i,:}^{(t+1), h}$ 之间添加了Residual Connection和MLP, Gating, 具体添加方式作者未说明, 应该和关系记忆网络一致:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rmen7.jpg" style="zoom:33%"><p>图片来自于关系记忆网络的论文<a href="https://arxiv.org/abs/1806.01822" target="_blank" rel="noopener">Relational recurrent neural networks</a>.</p><p>对于输入$\mathbf{x}_t$ 的最后编码产生的结果记为$\mathbf{y}_t \in \mathbb{R}^k$.</p><h3 id="Score-Function"><a href="#Score-Function" class="headerlink" title="Score Function"></a>Score Function</h3><p>将之前得出的$\left\{\mathbf{y}_{1}, \mathbf{y}_{2}, \mathbf{y}_{3}\right\}$ Concat起来, 接着用一个基于CNN的Decoder来解码:<br>$$<br>f(s, r, o)=\max \left(\operatorname{ReLU}\left(\left[\mathbf{y}_{1}, \mathbf{y}_{2}, \mathbf{y}_{3}\right] \ast \mathbf{\Omega}\right)\right)^{\top} \mathbf{w}<br>$$<br>$\left[\mathbf{y}_{1}, \mathbf{y}_{2}, \mathbf{y}_{3}\right] \in \mathbb{R} ^ {k \times 3}$, $\Omega$ 代表大小为$\mathbb{R} ^ {m \times 3}$ 的卷积核集合, 其中$m$ 为卷积核的窗口大小, $\ast$ 代表卷积操作. $\mathbf{w} \in \mathbb{R}^{\left| \Omega \right|}$ 是一个权重向量, 能将所有卷积核卷积得到的结果变成标量(其实就是得分). $\operatorname{max}$ 在这里代表<strong>最大池化</strong>. 作者认为这样能捕捉Feature map中的最重要特征, 并减小参数量.</p><h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><p>采用Adam优化, 损失函数如下:<br>$$<br>\mathcal{L}=\sum_{(s, r, o) \in\left\{\mathcal{G} \cup \mathcal{G}^{\prime}\right\}} \log \left(1+\exp \left(-t_{(s, r, o)} \cdot f(s, r, o)\right)\right)<br>$$<br>其中$t_{(s, r, o)}$ 是一个符号函数:<br>$$<br>t_{(s, r, o)}=\left\{\begin{array}{l}<br>1 \text { for }(s, r, o) \in \mathcal{G} \<br>-1 \text { for }(s, r, o) \in \mathcal{G}^{\prime}<br>\end{array}\right.<br>$$<br>其中$\mathcal{G}$ 代表正确的知识图谱, $\mathcal{G}^{\prime}$ 代表被替换后污染的知识图谱(负例的知识图谱, 其中包含负例三元组).</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>作者主要面向实际应用中比较多的三元组分类和个性化搜索两个任务进行实验. 详细的模型参数设置请参考原论文.</p><h3 id="Triple-Classification"><a href="#Triple-Classification" class="headerlink" title="Triple Classification"></a>Triple Classification</h3><p>三元组分类的问题是给定一个三元组, 通过模型来判断它们是否有效. 作者在WN11和FB13数据集上测试了R - MeN的三元组分类性能. 结果取决于一个阈值$\theta_r$, 如果高于阈值则视为三元组有效, 否则无效.</p><p>虽然前面介绍的Memory可以包含多个Slots, 但通过实验后, 作者发现对于所有的数据集, Memory Slot为1的时候效果都是最好的. 所以在之后的实验中只考虑使用单一的Memory Slot.</p><blockquote><p>如果Memory Slot = 1, 关系记忆网络应该类似于GRU, 感觉有点退化的意思. 我始终没有想明白Single slot能work的理由.</p></blockquote><p>实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rmen2.jpg" style="zoom:50%"><p>其中下划线代表效果次高的模型, 最下面一栏是借助了关系路径推理的模型.</p><p>R - MeN在这两个数据集上平均表现最好, 并且比TransE要好许多.</p><p>作者将TransE和R - MeN在WN11和FB13上针对关系的准确率进行了对比:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rmen3.jpg" style="zoom:50%"><p>在WN11中, 对于一对一的关系<code>similar_to</code>, R - MeN比TransE有非常强的学习能力. 在<strong>作者给出的</strong>FB13关系准确率中, R - MeN比TransE要好.</p><h3 id="Search-Personalization"><a href="#Search-Personalization" class="headerlink" title="Search Personalization"></a>Search Personalization</h3><p>个性化搜索的问题被定义为根据用户给出的搜索请求, 目标是根据搜索系统给出的搜索结果进行重排. 这个搜索场景能被视为是三元组$(query, user, document)$, 所以R - MeN也能用于个性化搜索任务上.<br>作者在SEARCH17数据集上进行了实验, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rmen4.jpg" style="zoom:25%"><p>KGE方法在个性化搜索任务上比传统方法要好一些, R - MeN显示出更好的性能.</p><h3 id="Effects-of-Hyper-Parameters"><a href="#Effects-of-Hyper-Parameters" class="headerlink" title="Effects of Hyper-Parameters"></a>Effects of Hyper-Parameters</h3><p>作者将多头的头大小$n$, 头的个数$H$ 在各个数据集上的性能进行对比:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rmen5.jpg" style="zoom:80%"><p>在三个数据集上, 使用更大的头有利于性能提升. 而在头的个数上, WN11和FB13中使用多头效果要更好, 在SEARCH17中单头效果更好, 作者认为SEARCH17是单意图的, 所以通常用1个头的效果会比较好. 因为头越多, 注意力就越分散, 就越容易Over Fitting.</p><h3 id="Ablation-Analysis"><a href="#Ablation-Analysis" class="headerlink" title="Ablation Analysis"></a>Ablation Analysis</h3><p>在消融实验中, 作者尝试去掉R - MeN的位置编码, 不使用关系记忆网络:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rmen6.jpg" style="zoom:50%"><p>在不使用关系记忆网络时, 打分函数直接就变为了接收最原始的实体和关系Embedding $\mathbf{v}_{s}, \mathbf{v}_{r}, \mathbf{v}_{o}$ 作为输入:<br>$$<br>f(s, r, o)=\max \left(\operatorname{ReLU}\left(\left[\mathbf{v}_{s}, \mathbf{v}_{r}, \mathbf{v}_{o}\right] \ast \mathbf{\Omega}\right)\right)^{\top} \mathbf{w}<br>$$<br>去掉位置编码后, 在SEARCH17上的表现退化比较大, WN11没有变化, FB13有一点点下降. 在不使用关系记忆网络时, 所有性能都有退化, 关系记忆网络体现出比较大的作用.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>作者将<strong>关系记忆网络</strong>用于KGE中, 并将三元组视为<strong>时序序列输入</strong>来看待. 并同时指出了KGE模型普遍采用Link Prediction来评估KGE模型的问题.</p><p>普通的KGE方法不能间接捕捉三元组内的联系, 只能用知识图谱中给出的实体间关系<strong>显式</strong>的学习它们之间的联系. 但通过关系记忆网络, 能够从三元组内的其他元素遗留下来的信息中学到一些潜在的联系.</p><p>循环记忆机制一直都有一个诟病: 自回归. 也就是当前时刻输入必须使用必须以上一个时刻的输出, 导致整个过程只能<strong>串行</strong>而不能并行.</p><blockquote><p>另外, 在使用Single Slot的时候, Memory可能就退化成类似GRU的结构了, 有点像Triple Level的RNN… 暂时还不知道这个看法的对错.</p></blockquote></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/2954.html">https://ADAning.github.io/posts/2954.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/KGE/"><span class="chip bg-color">KGE</span> </a><a href="/tags/Attention/"><span class="chip bg-color">Attention</span> </a><a href="/tags/%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C/"><span class="chip bg-color">记忆网络</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/62547.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/2.jpg" class="responsive-img" alt="Generative Adversarial Zero-Shot Relational Learning for KGs"> <span class="card-title">Generative Adversarial Zero-Shot Relational Learning for KGs</span></div></a><div class="card-content article-content"><div class="summary block-with-text">Generative Adversarial Zero-Shot Relational Learning for Knowledge Graphs本文是论文Generative Adversarial Zero-Shot Relationa</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-12-11 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/" class="post-category">知识图谱</a></span></div></div><div class="card-action article-tags"><a href="/tags/KGE/"><span class="chip bg-color">KGE</span> </a><a href="/tags/GAN/"><span class="chip bg-color">GAN</span> </a><a href="/tags/ZSL/"><span class="chip bg-color">ZSL</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/40162.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/10.jpg" class="responsive-img" alt="基于轻量级卷积和动态卷积替代的注意力机制"> <span class="card-title">基于轻量级卷积和动态卷积替代的注意力机制</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: Depthwise Convolution: 详见深度可分离卷积与分组卷积. Attention: 详见Seq2Seq和Attention. Transformer: 详见Transformer精讲. 本文是论文PA</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-12-05 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/CNN/"><span class="chip bg-color">CNN</span> </a><a href="/tags/Attention/"><span class="chip bg-color">Attention</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">390.5k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:andaning@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>