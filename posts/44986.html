<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="Vision &amp; Language Pretrained Model 总结, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>Vision &amp; Language Pretrained Model 总结 | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/5.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">Vision &amp; Language Pretrained Model 总结</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/MM/"><span class="chip bg-color">MM</span> </a><a href="/tags/VLP/"><span class="chip bg-color">VLP</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2023-07-18</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2024-07-03</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 4k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 16 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p>2024.4.21: 添加CoCa, 并修改对WPA的描述.</p><p>2024.4.23: 增加了BLIP-2的部分描述.</p></blockquote><h1 id="Vision-amp-Language-Pretraining-总结"><a href="#Vision-amp-Language-Pretraining-总结" class="headerlink" title="Vision &amp; Language Pretraining 总结"></a>Vision &amp; Language Pretraining 总结</h1><p>本文只是以<strong>总结</strong>的形式梳理了近期比较有代表性的VLP模型结构和预训练任务, 推荐有基础后再阅读.</p><h2 id="UNITER"><a href="#UNITER" class="headerlink" title="UNITER"></a>UNITER</h2><ul><li>论文: <a href="https://arxiv.org/abs/1909.11740" target="_blank" rel="noopener">UNITER: UNiversal Image-TExt Representation Learning</a>.</li></ul><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%931.png" alt=""></p><p>UNITER是典型的单塔模型, 直接把Region Feature也变成Token Level Feature和Token Emebdding(Text)一并交给Transformer完成各类预训练任务.</p><p>UNITER有四个预训练任务:</p><ul><li><strong>MLM</strong> - Masked Language Modeing: 和BERT相同, 对文本打<code>[MASK]</code> 预测原来的Token.</li><li><strong>MRM</strong> - Masked Region Modeling: 把Region Vector变成全0, 但是由于Visual Feature是连续的, 没法像NLP的Token一样被多分类预测出来, 所以作者提出了三个MRM的变体:<ul><li><strong>MRFR</strong> - Masked Region Feature Regression: 用一个Linear层把被Mask的ROI Feature直接恢复出来, 用L2回归损失.</li><li><strong>MRC</strong> - Masked Region Classification: 用Linear预测ROI Feature的Semantic Class. 这里没有Ground Truth Label, 直接用Fast RCNN预测的物体类别当做Ground Truth Label.</li><li><strong>MRC-kl</strong> - Masked Region Classification with KL-Divergence: 由于MRC中的Label过于硬了, 所以MRC-kl用Soft Label当做Ground Truth.</li></ul></li><li><strong>ITM</strong> - Image-Text Matching: 采样若干负样本, 用<code>[CLS]</code>接一个Linear来判断图文是否匹配.</li><li><strong>WRA</strong> - Word Region Aligment: 粒度比ITM更细, 对齐每个Text Token和Region. 由于图文在正样本里是匹配的, 所以可以最小化<strong>最优传输</strong>的分配代价, 来达到细粒度对齐的目的. 可以把最优传输看成是计算Text Modality到Vision Modality的距离, 最小化这两个分布的距离即可.</li></ul><p>当预训练的时候, 每次只Mask掉一个模态的Token, 使另一个模态的信息能充分交互.</p><h2 id="Oscar"><a href="#Oscar" class="headerlink" title="Oscar"></a>Oscar</h2><ul><li>论文: <a href="https://arxiv.org/abs/2004.06165" target="_blank" rel="noopener">Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks</a>.</li></ul><p>出发点:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%932.png" alt=""></p><p>Region Feature可能不能很好的区分开不同的物体, 例如图中的dog和couch实际上很大部分重叠到了一起, 但是在Word Embedding Space上二者是分开的.</p><p>单塔, 把Region Tag的语义标签也加入到模型的预训练过程中:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%933.png" alt=""></p><p>Tag本身具备Language的语义, 但实际上描述的是Visual信息, 所以<strong>Tag打通了V&amp;L的桥梁</strong>.</p><blockquote><p>和现在普遍使用”<strong>中间过渡模态</strong>“的思想非常像.</p></blockquote><p>两个预训练任务, 都与Tag相关</p><ul><li>Dictionary View: Tag和Text都属于同一个Word Dictionary下, 所以沿用BERT的MLM任务, 要求模型用剩余的Tag或Text, Region Feature来恢复.</li><li>Modality View: Tag和Region Feature同属Visual Signal, 作者令训练阶段有50%的概率替换Tag为负样本, 用<code>[CLS]</code>来预测Tag是否被替换.</li></ul><h2 id="CLIP"><a href="#CLIP" class="headerlink" title="CLIP"></a>CLIP</h2><ul><li>论文: <a href="http://proceedings.mlr.press/v139/radford21a" target="_blank" rel="noopener">Learning Transferable Visual Models From Natural Language Supervision</a>.</li></ul><p>CLIP的模型和训练方法非常简单, A picture is worth a thousand words:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%934.png" alt=""></p><p>采用<strong>双塔</strong>结构, 最大化主对角线上的匹配的图文对之间的相似度, 最小化对角线两侧其他文本的相似度即可, 力大砖飞.</p><p>在做Zero Shot Image Classification时, 直接用prompt <code>A photo of a {object}</code> 来预测Image和每个Class的Prompt之间的相似度.</p><p>伪代码:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># image_encoder - ResNet or Vision Transformer </span>
<span class="token comment" spellcheck="true"># text_encoder - CBOW or Text Transformer </span>
<span class="token comment" spellcheck="true"># I[n, h, w, c] - minibatch of aligned images </span>
<span class="token comment" spellcheck="true"># T[n, l] - minibatch of aligned texts </span>
<span class="token comment" spellcheck="true"># W_i[d_i, d_e] - learned proj of image to embed </span>
<span class="token comment" spellcheck="true"># W_t[d_t, d_e] - learned proj of text to embed </span>
<span class="token comment" spellcheck="true"># t - learned temperature parameter </span>

<span class="token comment" spellcheck="true"># extract feature representations of each modality </span>
I_f <span class="token operator">=</span> image_encoder<span class="token punctuation">(</span>I<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#[n, d_i] </span>
T_f <span class="token operator">=</span> text_encoder<span class="token punctuation">(</span>T<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#[n, d_t] </span>

<span class="token comment" spellcheck="true"># joint multimodal embedding [n, d_e] </span>
I_e <span class="token operator">=</span> l2_normalize<span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>I_f<span class="token punctuation">,</span> W_i<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> 
T_e <span class="token operator">=</span> l2_normalize<span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>T_f<span class="token punctuation">,</span> W_t<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> 

<span class="token comment" spellcheck="true"># scaled pairwise cosine similarities [n, n] </span>
logits <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>I_e<span class="token punctuation">,</span> T_e<span class="token punctuation">.</span>T<span class="token punctuation">)</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>t<span class="token punctuation">)</span> 

<span class="token comment" spellcheck="true"># symmetric loss function </span>
labels <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>n<span class="token punctuation">)</span> 
loss_i <span class="token operator">=</span> cross_entropy_loss<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span> 
loss_t <span class="token operator">=</span> cross_entropy_loss<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> 
loss <span class="token operator">=</span> <span class="token punctuation">(</span>loss_i <span class="token operator">+</span> loss_t<span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">2</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="ViLT"><a href="#ViLT" class="headerlink" title="ViLT"></a>ViLT</h2><ul><li>论文: <a href="https://proceedings.mlr.press/v139/kim21k.html" target="_blank" rel="noopener">ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision</a>.</li></ul><p>ViLT的核心思想: <strong>弱Embed重交互</strong>:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%935.png" alt=""></p><p>先前的模型在Embedding上是不均衡或者都非常重的, 反而是在交互上设计的比较弱. 作者认为弱Embedding重交互也可以得到很好的性能, 并且非常省时间.</p><p>模型非常简单, 直接单塔大一统:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%936.png" alt=""></p><p>经典预训练任务ITM, MLM. WPA与UNITER里的WRA类似, 算作是ITM Loss的一个附加Loss. 由于ViLT从输入的Region变成了Patch, 所以作者希望Text Token和Image Patch的匹配程度越高越好, 即总体距离越小越好.</p><h2 id="ALBEF"><a href="#ALBEF" class="headerlink" title="ALBEF"></a>ALBEF</h2><ul><li>论文: <a href="https://proceedings.neurips.cc/paper_files/paper/2021/hash/505259756244493872b7709a8a01b536-Abstract.html" target="_blank" rel="noopener">Align before Fuse: Vision and Language Representation Learning with Momentum Distillation</a>.</li></ul><p>出发点:</p><p>先前的模型(<strong>UNITER</strong>, <strong>Oscar</strong>)仍然在用Region based视觉特征, 整个模型性能受限于Detector的性能. 并且因为Detector是<strong>Freeze</strong>住的, 没有参与E2E的训练, 所以Visual Feature和Text Feature一起扔给一个单塔模型会导致Visual和Text没有<strong>对齐</strong>, 单塔学起来就比较困难.</p><p>模型框架:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%937.png" alt=""></p><p>双塔和单塔融合结构, 先双塔, 后单塔融合. BERT被拆成两半, 一半给文本用, 一半给多模态用.</p><p>至于为什么劈开的是BERT而不是ViT, 有些人认为在多模态任务中往往是视觉占主导地位, 所以对Visual这边要给的重一些.</p><blockquote><p>虽然叫Multimodal Encoder, 但实际上是个Decoder.</p></blockquote><p>ALBEF用了三个Loss: <strong>ITC</strong>, <strong>ITM</strong>, <strong>MLM</strong>, 都是多模态老传统了.</p><p>作者认为, 从Web上爬下来的数据都太脏了, 很有可能出现图文不匹配的情况. 所以直接用One Hot的Cross Entropy去计算对模型的伤害会比较大. 所以要用Momentum Distillation, 也就是<a href="https://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html" target="_blank" rel="noopener">MoCo</a>中提到的方式来缓解这种强惩罚.</p><p>假设图像Token为$\boldsymbol{v}$, 文本Token为$\boldsymbol{w}$. $g_v, g_w, g_v^\prime, g_w^\prime$ 都是Linear Projection. Momentum Encoder得到的V &amp; L归一化以后的Feature为$g_v^{\prime}\left(\boldsymbol{v}_{\mathrm{cls}}^{\prime}\right), g_w^{\prime}\left(\boldsymbol{w}_{\mathrm{cls}}^{\prime}\right)$.</p><p>ITC Loss计算如下. 计算主干和Momentum Encoder的图文匹配相似度:</p><p>$$<br>\begin{aligned}<br>s(I, T)=g_v\left(\boldsymbol{v}_{\mathrm{cls}}\right)^{\top} g_w^{\prime}\left(\boldsymbol{w}_{\mathrm{cls}}^{\prime}\right) \\<br>s(T, I)=g_w\left(\boldsymbol{w}_{\mathrm{cls}}\right)^{\top} g_v^{\prime}\left(\boldsymbol{v}_{\mathrm{cls}}^{\prime}\right)<br>\end{aligned}<br>$$</p><blockquote><p>其实和<a href="https://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html" target="_blank" rel="noopener">MoCo</a>是完全一样的, 拿Momentum Encoder和主干Encoder投影完做点积. 主要是为了<strong>扩大Dictionary Size</strong>.</p></blockquote><p>$M$ 为图文匹配对数量, 计算InfoNCE:</p><p>$$<br>\begin{aligned}<br>p_m^{\mathrm{i} 2 \mathrm{t}}(I)=\frac{\exp \left(s\left(I, T_m\right) / \tau\right)}{\sum_{m=1}^M \exp \left(s\left(I, T_m\right) / \tau\right)} \\<br>p_m^{\mathrm{t} 2 \mathrm{i}}(T)=\frac{\exp \left(s\left(T, I_m\right) / \tau\right)}{\sum_{m=1}^M \exp \left(s\left(T, I_m\right) / \tau\right)}<br>\end{aligned}<br>$$</p><p>如果使用Momentum Distillation, 则需要从计算One Hot损失变为加权计算Momentum Encoder得到的概率分布$\boldsymbol{q}^{\mathrm{i} 2 \mathrm{t}}, \boldsymbol{q}^{\mathrm{t} 2 \mathrm{i}}$ 和主干预测结果$\boldsymbol{p}_m^{\mathrm{i} 2 \mathrm{t}}, \boldsymbol{p}_m^{\mathrm{t} 2 \mathrm{i}}$ 之间的KL散度:<br>$$<br>\begin{aligned}<br>s(I, T)=&amp;g_v\left(\boldsymbol{v}_{\mathrm{cls}}\right)^{\top} g_w^{\prime}\left(\boldsymbol{w}_{\mathrm{cls}}^{\prime}\right) \\<br>s(T, I)=&amp;g_w\left(\boldsymbol{w}_{\mathrm{cls}}\right)^{\top} g_v^{\prime}\left(\boldsymbol{v}_{\mathrm{cls}}^{\prime}\right) \\<br>&amp;\Downarrow \\<br>s^\prime(I, T)=&amp;g_v^\prime\left(\boldsymbol{v}^{\prime}_{\mathrm{cls}}\right)^{\top} g_w^{\prime}\left(\boldsymbol{w}^{\prime}_{\mathrm{cls}}\right) \\<br>s^\prime(T, I)=&amp;g_w^\prime\left(\boldsymbol{w}^{\prime}_{\mathrm{cls}}\right)^{\top} g_v^{\prime}\left(\boldsymbol{v}^{\prime}_{\mathrm{cls}}\right) \\<br>\end{aligned}<br>$$</p><p>$$<br>\begin{aligned}<br>\mathcal{L}_{\mathrm{itc}}=&amp;\frac{1}{2} \mathbb{E}_{(I, T) \sim D}\left[\mathrm{H}\left(\boldsymbol{y}^{\mathrm{i} 2 \mathrm{t}}(I), \boldsymbol{p}^{\mathrm{i} 2 \mathrm{t}}(I)\right)+\mathrm{H}\left(\boldsymbol{y}^{\mathrm{t} 2 \mathrm{i}}(T), \boldsymbol{p}^{\mathrm{t} 2 \mathrm{i}}(T)\right)\right] \\<br>&amp;\Downarrow \\<br>\mathcal{L}_{\text {itc }}^{\mathrm{mod}}=&amp;(1-\alpha) \mathcal{L}_{\mathrm{itc}}+\frac{\alpha}{2} \mathbb{E}_{(I, T) \sim D}\left[\mathrm{KL}\left(\boldsymbol{q}^{\mathrm{i} 2 \mathrm{t}}(I) | \boldsymbol{p}^{\mathrm{i} 2 \mathrm{t}}(I)\right)+\mathrm{KL}\left(\boldsymbol{q}^{\mathrm{t} 2 \mathrm{i}}(T) | \boldsymbol{p}^{\mathrm{t} 2 \mathrm{i}}(T)\right)\right]<br>\end{aligned}<br>$$</p><p>MLM任务也做相应的改动:<br>$$<br>\begin{aligned}<br>\mathcal{L}_{\mathrm{mlm}}=&amp;\mathbb{E}_{(I, \hat{T}) \sim D} \mathrm{H}\left(\boldsymbol{y}^{\mathrm{msk}}, \boldsymbol{p}^{\mathrm{msk}}(I, \hat{T})\right) \\<br>&amp;\Downarrow \\<br>\mathcal{L}_{\mathrm{mlm}}^{\mathrm{mod}}=&amp;(1-\alpha) \mathcal{L}_{\mathrm{mlm}}+\alpha \mathbb{E}_{(I, \hat{T}) \sim D} \mathrm{KL}\left(\boldsymbol{q}^{\mathrm{msk}}(I, \hat{T}) | \boldsymbol{p}^{\mathrm{msk}}(I, \hat{T})\right)<br>\end{aligned}<br>$$<br>在ITM Task中, 作者采用前面计算ITC时除去匹配对相似度最高的图文对作为<strong>Hard Negative</strong>, 作为一个比较难的负样本让模型学习.</p><h2 id="BLIP"><a href="#BLIP" class="headerlink" title="BLIP"></a>BLIP</h2><ul><li>论文: <a href="https://proceedings.mlr.press/v162/li22n.html" target="_blank" rel="noopener">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a>.</li></ul><p>出发点:</p><ol><li>模型角度: encoder only的模型不擅长文本生成, 而encoder-decoder的模型又不擅长检索.</li><li>数据角度: Web端获得的图文匹配对有大量<strong>噪声</strong>.</li></ol><p>模型结构图:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%938.png" alt=""></p><p>不难看出, BLIP实际上也是延续<strong>ALBEF</strong>的一种单双塔融合模型(两篇论文均出自Salesforce之手), 在Image Encoder和Text Encoder完成了两个模态的对齐, 在这个基础上额外加了一个Text Decoder完成Language Modeling的任务, 取代了MLM.</p><p>作者共享了所有的Text Encoder的部分参数, 除了SA是独有的, CA和FFN在二者之间共享.</p><p>所以BLIP的预训练任务也是ITC, ITM, 以及新加入的LM.</p><p>在计算ITC的时候同样是延续ALBEF对ITM用了Hard Negative, 而知识蒸馏以另一种方式CapFilt在BLIP中体现.</p><blockquote><p>至于BLIP为什么要这么设计, 请看后面的CapFilt.</p></blockquote><p>由于大部分图文对都来自网络, 质量很低, 作者希望通过Captioning and Filtering来得到质量更高的数据:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%939.png" alt=""></p><p>在数据中, 人为标注的数据$\{(I_h, T_h)\}$ 肯定是质量非常高的, 但是从网上爬下来的图文对 “image and <strong>alt-text</strong> pairs” $\{(I_w, T_w)\}$ 质量就差很多, 包含了大量<strong>噪声</strong>.</p><p>这时候再看看作者设计的模型, 既然Image - grounded Text Decoder实现的是对图像的描述, Image - grounded Text Encoder完成的是对图文是否匹配的判断, 那么Captioning and Filtering的框架就呼之欲出了:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%9310.png" alt=""></p><p>作者首先在所有数据$\{(I_h, T_h)\} + \{(I_w, T_w)\}$ 上完成BLIP的预训练, 然后在人为标注的数据$\{(I_h, T_h)\}$ 上对Captioner和Filter进行微调.</p><p>微调后, Captioner为网上爬下来的图像$I_w$ 生成描述$T_w$, 再把这部分生成的图文对交给Filter. Filter将网上爬下来的图文对$\{(I_w, T_w)\}$ 和Captioner生成的图文对进行过滤, 最后将这两部分和人为标注的数据$\{(I_h, T_h)\}$ 一起作为新的数据集重新训练一个BLIP.</p><blockquote><p>参数共享在CapFilt阶段是不启用的, 作者在文中有实验说明.</p></blockquote><p>BLIP和ALBEF其实思想上非常相似, 从出发点到模型再到左脚踩右脚上天的思想都是一致的.</p><h2 id="CoCa"><a href="#CoCa" class="headerlink" title="CoCa"></a>CoCa</h2><ul><li>论文: <a href="https://arxiv.org/abs/2205.01917" target="_blank" rel="noopener">CoCa: Contrastive Captioners are Image-Text Foundation Models</a>.</li></ul><p>现有工作的缺点:</p><ol><li>Single Encoder: 代指纯视觉预训练模型, 这些模型不能处理VL Task.</li><li>Dual Encoder: 类似CLIP的双塔, 有利于检索, 但对VL的融合能力不足, 无法直接迁移到VL Understanding任务里.</li><li>Encoder - Decoder: Generative Pretraining, 在VL Alignment上很差.</li></ol><p>模型图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp总结17.png" style="zoom:67%"><p>Attentional Pooling就是一层Cross Attention, 将Query作为Q, Image Feature作为KV, 从而使得Query提炼图像中的特征.</p><p>Image Encoder可以是预训练的Visual Encoder, 在计算Contrastive Loss的时候, Query(Contrastive Query)数量设定为1, 这时候作用和[CLS]类似. 在计算Image Caption Loss的时候, 可能需要更细粒度的Visual Feature, 所以设定这种Query(Cap Query)有256个.</p><p>文中写到, 整个Text Decoder的前半部分的Cross Attention全部被忽略, 以便于编码文本单独的特征. 所以文本模态的Text Decoder其实就是Casual Mask + Self Attention.</p><p>损失函数:</p><ul><li>Single-Encoder Classification: 只对Image Encoder做各类图像相关的分类任务的损失, 在有标注数据下进行.</li><li>Dual-Encoder Contrastive Learning: 在Image Encoder和纯文本模态的Text Decoder上做的ITC Loss.</li><li>Encoder-Decoder Captioning: 两个Text Decoder都用上的Next Token Prediction Loss.</li></ul><p>伪代码:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp总结18.png" style="zoom:80%"><p>综上, 个人认为CoCa和上面讲过的BLIP很像, 甚至在对视觉信息的处理上是下面要说的BLIP-2里Q-Former的雏形.</p><h2 id="VLMo"><a href="#VLMo" class="headerlink" title="VLMo"></a>VLMo</h2><ul><li>论文: <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/d46662aa53e78a62afd980a29e0c37ed-Abstract-Conference.html" target="_blank" rel="noopener">VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts</a>.</li></ul><p>VLMo对不同模态使用了<strong>专家系统</strong>(MOE), 大框架如下:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%9311.png" alt=""></p><p>SA是可以被共享的, 当VLMo执行不同任务时, 不同的FFN会被启用:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%9312.png" alt=""></p><p>如图, VLMo预训练一共使用了三个Loss: ITC, ITM, MLM, 同样ITM也带有Hard Negative, 和<strong>ALBEF</strong>, <strong>BLIP</strong>一样. 做ITC时候, 只走V-FFN和L-FFN, 不使用VL-FFN, 此时VLMo更像双塔一些. 当做ITM和MLM时候, VLMo的前(L-F)层用V-FFN和L-FFN对Image Patch和Word Token分别编码, 后F层用VL-FFN把两种模态的数据整合, 此时VLMo更像ALBEF类的单双塔融合模型. 在Base和Large中F分别取2和3.</p><blockquote><p>实验中证明Shared SA比不Share性能要高很多, 这可能说明SA对模态并不是那么敏感, 而且只是用来控制数据流的一个组件而已, 与模态无关.</p></blockquote><p>由于VLMo使用了专家系统, 所以VLMo的使用方式可以十分灵活. VLMo将自己的训练过程拆分为多个阶段:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%9313.png" alt=""></p><p>模型首先在Image dataset上训练SA和V-FFN, 然后再把它们冻住, 去训练L-FFN(用MLM), 此时可以认为VLMo在VL Task上已经有一个良好的初始化, 并且L-FFN是兼容SA输入的. 所以在Image - Text Pair上进行预训练时, 直接开放全量微调.</p><h2 id="BLIP-2"><a href="#BLIP-2" class="headerlink" title="BLIP-2"></a>BLIP-2</h2><ul><li>论文: <a href="https://arxiv.org/abs/2301.12597" target="_blank" rel="noopener">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a>.</li></ul><p>出发点: 当前大规模模型在预训练期间的高额计算消耗太大, 数据也用的特别多.</p><p>作者引入一个lightweight Querying Transformer (Q-Former)来完成Visual &amp; Language模态的桥接过程:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%9314.png" alt=""></p><p>作者把Q-Former的训练拆分为两个阶段:</p><ul><li>首阶段: 让Q-Former从Freeze Image Encoder中学习VL表示.</li><li>次阶段: 从Freeze LLM中学习VL表示.</li></ul><p>Q-Former结构和首阶段预训练如下:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%9315.png" alt=""></p><p>Q-Former实际上由<strong>双塔</strong>的两个Transformer组成, 分别被称为Image Transformer和Text Transformer. 结构上类似于<strong>BLIP</strong>中的Image-Grounded Text Encoder和Text Encoder.</p><p>Image Transformer的SA和Text Transformer的SA参数是共享的(这点和<strong>VLMo</strong>出奇的一致). Learnable Query从Image Transformer给入, 通过CA来从Frozen Image Encoder中获取视觉信号.</p><blockquote><p>作者还在文中补充了一个小细节, 实际上的Visual Key Value采用的是Image Encoder的倒数第二层输出, 而不是最后一层, 效果会稍微好一点, 这点与大家使用Stable Diffusion的时候取CLIP的倒数第二层输出有点类似.</p></blockquote><p>所以但从结构上来看, 首阶段的训练目标是希望Query能够学到从Image Encoder中抽取对Text最有用的内容. 再看训练任务也是这样, 设计了三种:</p><ul><li><strong>ITC</strong>(Image-Text Contrastive Learning): 虽然说是老生常谈的Loss, 但因为Query经过Trm以后得到的表示有多个, 所以作者计算了多个Query与Text Transformer<code>[CLS]</code>的余弦相似度, 选择相似度最大的作为正样本. 为了避免<strong>信息泄露</strong>, 在做ITC的时候要保证Q和T之间是互相不可见的(最右侧Mask).</li><li><strong>ITG</strong>(Image-grounded Text Generation): 使得Q对T完全可见, T单独用Casual Mask, 然后生成图文匹配的文本段. 这就要求Query必须覆盖Image的全部信息, 且Query抽取出的信息必须是有效的(中间Mask). <code>[CLS]</code>也被换成<code>[DEC]</code>.</li><li><strong>ITM</strong>(Image-Text Matching): ITM也是常见Loss, Q必须拥有两个模态的信息才能一起判断图文是否匹配, 作者对所有Query都计算ITM Loss, 最后取平均作为Logits, 同时也使用Hard Negative.</li></ul><p>次阶段预训练, 直接用Q-Former完成图生文:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%9316.png" alt=""></p><p>由于在首阶段中Q-Former已经完成了Query从Image Encoder中抽取关键信息的学习, 这也就使得Visual Signal可以被Query以Soft Visual Prompt的形式传递给LLM. 所以Q-Former中的Text Transformer变得不再必要, 可以被<strong>丢弃</strong>. Query表示还需要过一层Linear Project和大模型输入维度对齐.</p><blockquote><p>如果不要首阶段直接硬学的话, 由于没有Text Transformer打辅助, 所以想要让Q-Former学到从Image中抽取出更多有关文本的信息会更难. 但文本模态在Q-Former首阶段训练中起到的实际上是一个Grounding的作用, 根据Language来让Learnable Query抽取更多有用的信息.</p></blockquote></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/44986.html">https://ADAning.github.io/posts/44986.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/MM/"><span class="chip bg-color">MM</span> </a><a href="/tags/VLP/"><span class="chip bg-color">VLP</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/4539.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/16.jpg" class="responsive-img" alt="2024-元旦"> <span class="card-title">2024-元旦</span></div></a><div class="card-content article-content"><div class="summary block-with-text">2024-元旦都有小半年没更新博客了, 已经鸽了好久了… 首先祝大家元旦快乐! 这半年来, 找工作和申博我都试了试, 最后是选择了自己觉得更合适的一条路, 也算是人生中做的一个关键的节点吧. 2023年是LLM横行霸道的一年, 我印象中光是</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2024-01-01 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E5%BF%83%E6%83%85%E9%9A%8F%E7%AC%94/" class="post-category">心情随笔</a></span></div></div><div class="card-action article-tags"><a href="/tags/%E5%BF%83%E6%83%85%E9%9A%8F%E7%AC%94/"><span class="chip bg-color">心情随笔</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/8982.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/21.jpg" class="responsive-img" alt="大模型并行优化"> <span class="card-title">大模型并行优化</span></div></a><div class="card-content article-content"><div class="summary block-with-text">大模型并行优化为什么要并行优化?大就是好, 虽然丛2019年人们的认识普遍就是大就是好, 这个概念在当今依然没有被改变, 只是有了更深刻的认识. 所以, 为什么要并行? 虽然大就是好, 模型太大显存吃不消(空间). 虽然大就是好</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2023-06-01 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/"><span class="chip bg-color">并行计算</span> </a><a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"><span class="chip bg-color">分布式</span> </a><a href="/tags/ZeRO/"><span class="chip bg-color">ZeRO</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">413.5k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:andaning@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>