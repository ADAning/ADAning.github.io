<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="Introduction: Vector Quantization, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>Introduction: Vector Quantization | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/18.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">Introduction: Vector Quantization</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/VQ/"><span class="chip bg-color">VQ</span> </a><a href="/tags/VQ-VAE/"><span class="chip bg-color">VQ-VAE</span> </a><a href="/tags/VQ-GAN/"><span class="chip bg-color">VQ-GAN</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2024-07-16</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2024-07-28</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 2.9k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 12 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><h1 id="Introduction-Vector-Quantization"><a href="#Introduction-Vector-Quantization" class="headerlink" title="Introduction: Vector Quantization"></a>Introduction: Vector Quantization</h1><h2 id="Vector-Quantization"><a href="#Vector-Quantization" class="headerlink" title="Vector Quantization"></a>Vector Quantization</h2><p>AutoEncoder(AE)由Encoder和Decoder组成, Encoder将图像压缩为一个低维的隐向量(Latent), 再由Decoder使用Latent将图像恢复出来. 在此基础上有向原始图像中加噪的Denoising AutoEncoder(DAE)和从将Latent规约为标准正态分布的生成式模型Variational AutoEncoder(VAE)两种. 我们在<a href="https://adaning.github.io/posts/53598.html">之前的博客</a>中已经讲述过了:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/introduction vq1.png" style="zoom:25%"><p>它们的Latent Vector都是连续的, <strong>能不能将中间的Latent从连续变成一个离散的状态呢</strong>?</p><p>因为有时, 物体的特征可能是一种离散状态, 此时我们不希望样本在Latent Space连续的分布.</p><p>例如人是否在坐着, 只能是坐与不坐两种状态, 不能介于二者之间变成一个即坐又不坐的状态. 如果是连续的Latent Space设计, 模型可能会学习到从坐到不坐的连续变化过程, 但实际上它并不应该存在.</p><p>我们可以通过<strong>Vector Quantization</strong>(VQ)来实现这一想法:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/introduction vq2.png" style="zoom:33%"><h2 id="VQ-VAE"><a href="#VQ-VAE" class="headerlink" title="VQ - VAE"></a>VQ - VAE</h2><ul><li>论文: <a href="https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html" target="_blank" rel="noopener">Neural Discrete Representation Learning</a>.</li></ul><h3 id="Discrete-Latent-Variables"><a href="#Discrete-Latent-Variables" class="headerlink" title="Discrete Latent Variables"></a>Discrete Latent Variables</h3><p>VQ - VAE仍然遵循AE的框架, 它也有Encoder $z_e$, Decoder $z_q$. 对图像用Encoder抽得一个表示$z_e(x)$, 然后用Decoder对输入$z_q(x)$ 解码, 得到原图像$x$.</p><p>与一般AE不同的是, VQ - VAE额外有一个<strong>Codebook</strong>, 存放了$K$ 个$D$ 维的Embedding $e_i \in \mathbb{R}^D$, 称为Latent Embedding Space $e \in \mathbb{R}^{K \times D}$. 这个Codebook也可以看做是VQ - VAE的先验分布, 只不过它是离散的.</p><p>在Latent处, 后验类别分布$q(z|x)$ 也不再是连续空间, 而是离散空间, 这与做一个$K$ 分类任务类似. 其分布可以用One - Hot来表示:</p><p>$$<br>q(z=k \mid x)= \begin{cases}1 &amp; \text { for } \mathrm{k}=\operatorname{argmin}_j\left||z_e(x)-e_j|\right|_2 \\\ 0 &amp; \text { otherwise }\end{cases}<br>$$</p><p>接下来, 将Encoder抽取到的图像表示$z_e(x)$ 直接替换为Codebook中离$z_q(x)$ 距离最近的$e_k$, 并作为Decoder的输入$z_q(x)$:</p><p>$$<br>z_q(x)=e_k, \quad \text {where} \quad k=\operatorname{argmin}_j\left||z_e(x)-e_j|\right|_2<br>$$</p><p>由于其中有$\text{argmin}$, 所以这里存在<strong>梯度断裂</strong>, Decoder侧的梯度是没法通过链式法则传到Encoder做更新的.</p><blockquote><p>注意, 这里Encoder抽取出的表示$z_e(x)$ 一般是一个用CNN获得的$m \times m \times D$ 的特征, 所以这张图像$x$ 对应的离散表示就是一个$m \times m$ 的二维矩阵. 如果$z_e(x) \in \mathbb{R}^D$, 在重构的时候就比较困难了, 这代表着Codebook里的某个Embedding对应了若干张训练集图片, 而不是图像中的某个部分.</p></blockquote><h3 id="Learning"><a href="#Learning" class="headerlink" title="Learning"></a>Learning</h3><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/introduction vq3.png" style="zoom:67%"><p>接上文, 因为Encoder的输出$z_e(x)$ 维度和Decoder输入$z_q(x)$ 维度是一致的, 作者使用<strong>Straight - Through Estimator</strong>(STE), 直接将Decoder input处$z_q(x)$ 的梯度Copy到Encoder output处$z_e(x)$即可, 这样就链接了Encoder和Decoder的梯度更新, 官方给出的代码如下:</p><pre class="line-numbers language-python"><code class="language-python">decoder_input <span class="token operator">=</span> z_e <span class="token operator">+</span> <span class="token punctuation">(</span>z_q <span class="token operator">-</span> z_e<span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>虽然作者引入的STE解决了Encoder和Decoder的梯度更新, 但如果按照上述代码来走, Codebook里面的Embedding的梯度没法通过Decoder拿到了, 因为<code>z_q</code> 的梯度被detach掉了. 这样就需要通过额外约束来优化Codebook.</p><p>Codebook中的Embedding $e$ 肯定是希望离Encoder的输出$z_e(x)$ 越近越好, 这样才说明Codebook的对应Embedding $e_i$ 跟Encoder输出匹配的更好, 也更有利于Decoder做生成, 因为$z_q(x) = e_k$ 嘛.</p><p>最终, Training Loss共包含三个部分:</p><p>$$<br>L=\underbrace{\log p\left(x \mid z_q(x)\right)}_{\text{Reconstruction}}+\underbrace{\left||\operatorname{sg}\left[z_e(x)\right]-e|\right|_2^2}_{\text{VQ}}+\beta\underbrace{\left||z_e(x)-\operatorname{sg}[e]|\right|_2^2}_{\text{Commitment}}<br>$$</p><p>其中, $\text{sg}$ 为Stop Gradient Operator.</p><ul><li>第一项为Reconstruction Loss, 就是由Decoder重建图像的Loss, 与AE损失相同. 并由STE链接Encoder和Decoder的梯度更新.</li><li>第二项为VQ Loss, 负责让Codebook中的Embedding $e$ 离Encoder output $z_e(x)$ 更近.</li><li>第三项为Commitment Loss, 这是与第二项对称的Loss, 前面多乘上了一个超参$\beta$ 用于调节比例, 负责让Encoder output$z_e(x)$ 也朝着$e$ 移动. 作者认为Codebook Embedding无量纲, 且Codebook和Encoder的参数更新速度不同添加的.</li></ul><p>另外, $\beta$ 文中作者取0.25, 并且作者发现从0.1取到2.0模型表现变化都不大.</p><blockquote><p>看到这里, 能够发现VQ - VAE实际上并不是一个VAE, 它和VAE没有任何关系, 只是一个用VQ实现的AE. 因为<strong>只靠VQ - VAE自己是不能直接生成若干张不同随机图像的</strong>, 因此它只能看做是一种<strong>压缩方式</strong>, 图像从原始输入大小$H \times W \times 3$变成了更小的二维矩阵$m \times m$. 如果想要做生成, 需要结合PixelCNN完成.</p></blockquote><h2 id="VQ-GAN"><a href="#VQ-GAN" class="headerlink" title="VQ - GAN"></a>VQ - GAN</h2><ul><li>论文: <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf" target="_blank" rel="noopener">Taming Transformers for High-Resolution Image Synthesis</a>.</li></ul><p>VQ - GAN实际是<strong>VQ-VAE</strong>的一种变体, 都是利用VQ, 但VQ - GAN用了Transformer做自回归生成. VQ - VAE的自回归生成采用的是PixelCNN, 这种级别的模型对于自回归生成来说还是比较脆弱的. 恰好, Transformer擅长自回归离散Token序列的生成, 所以Transformer理应成为Autoregressive Manner的首选.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/introduction vq4.png" style="zoom:25%"><h3 id="Learning-an-Effective-Codebook-of-Image-Constituents-for-Use-in-Transformers"><a href="#Learning-an-Effective-Codebook-of-Image-Constituents-for-Use-in-Transformers" class="headerlink" title="Learning an Effective Codebook of Image Constituents for Use in Transformers"></a>Learning an Effective Codebook of Image Constituents for Use in Transformers</h3><p>与VQ - VAE类似的, VQ - GAN中生成用的主体有Encoder $E$, Decoder $G$(因为这里是GAN, 所以Decoder是生成器$G$), 和Codebook $\mathcal{Z} = \set{z_k}^{K}_{k=1} \subset \mathbb{R}^{n_z}$.</p><p>对于给定的图片$x \in \mathbb{R}^{H \times W \times 3}$, 首先用CNN的Encoder $E$ 抽取出图像$x$ 的编码表示$\hat{z} = E(x) \in \mathbb{R}^{h \times w \times n_z}$.</p><p>然后, VQ - GAN会用一个由$h \times w$ 个$n_z$ 维的Embedding组成的二维特征矩阵$z_\mathbf{q} \in \mathcal{R}^{h \times w \times n_z}$ 作为CNN Decoder $G$ 的输入.</p><p>这里的VQ的过程与VQ - VAE是一样的, 都是用<strong>最近邻</strong>替换每个Spatial Code$\hat{z}_{ij} \in \mathbb{R}^{n_z}$:</p><p>$$<br>z_{\mathbf{q}}=\mathbf{q}(\hat{z}):=\left(\underset{z_k \in \mathcal{Z}}{\arg \min }\left||\hat{z}_{i j}-z_k|\right|\right) \in \mathbb{R}^{h \times w \times n_z}<br>$$</p><p>将$z_{\mathbf{q}}$ 作为Decoder $G$ 的输入, 重建图像$\hat{x}$:</p><p>$$<br>\hat{x}=G\left(z_{\mathbf{q}}\right)=G(\mathbf{q}(E(x)))<br>$$</p><p>与VQ - VAE同样的, 由于Decoder $G$ 和Encoder $E$ 中间的quantization存在$\text{argmin}$, 所以梯度不能从$G$ 传递到$E$, VQ - GAN也使用了STE, 与VQ - VAE形式一样的Loss $\mathcal{L}_{\text{VQ}}$如下:</p><p>$$<br>\mathcal{L}_{\mathrm{VQ}}(E, G, \mathcal{Z})=||x-\hat{x}||^2 +\left||\operatorname{sg}[E(x)]-z_{\mathbf{q}}|\right|_2^2+\beta\left||\operatorname{sg}\left[z_{\mathbf{q}}\right]-E(x)|\right|_2^2<br>$$</p><p>虽然MSE可以从像素角度来描述图像之间的相似性, 但并不能从抽象的特征角度描述图像重构的好不好, 所以作者将L2 Loss $|x-\hat{x}|^2$ 替换为<a href="https://arxiv.org/abs/1603.08155" target="_blank" rel="noopener">Perceptual Loss</a> $\mathcal{L}_{\text{rec}}$:</p><p>$$<br>\mathcal{L}_{\mathrm{VQ}}(E, G, \mathcal{Z})=\mathcal{L}_{\text{rec}} +\left||\operatorname{sg}[E(x)]-z_{\mathbf{q}}|\right|_2^2+\beta\left||\operatorname{sg}\left[z_{\mathbf{q}}\right]-E(x)|\right|_2^2<br>$$</p><blockquote><p>据说作者给的代码MSE和Perceptual Loss都用了.</p></blockquote><h4 id="Learning-a-Perceptually-Rich-Codebook"><a href="#Learning-a-Perceptually-Rich-Codebook" class="headerlink" title="Learning a Perceptually Rich Codebook"></a>Learning a Perceptually Rich Codebook</h4><p>因为是GAN嘛, 训练GAN除了有Generator, 还需要有一个<strong>Discriminator</strong>. 作者用一个Patch - based Discriminator $D$ (其实就是<a href="https://arxiv.org/abs/1803.07422" target="_blank" rel="noopener">PatchGAN</a>)来评估Decoder $G$ 将<strong>每个Code生成为Patch的真假</strong>, 即对$h \times w$ 个Patch的真假都做判断:</p><p>$$<br>\mathcal{L}_{\mathrm{GAN}}(\{E, G, \mathcal{Z}\}, D)=[\log D(x)+\log (1-D(\hat{x}))]<br>$$</p><p>最终目标函数为找到最优模型参数$\mathcal{Q}^\ast = \set{E^\ast, G^\ast, \mathcal{Z}^\ast}$:</p><p>$$<br>\mathcal{Q}^\ast=\underset{E, G, \mathcal{Z}}{\arg \min } \max _D \mathbb{E}_{x \sim p(x)}{\left[\mathcal{L}_{\mathrm{VQ}}(E, G, \mathcal{Z})\right.} \left.+\lambda \mathcal{L}_{\mathrm{GAN}}(\{E, G, \mathcal{Z}\}, D)\right]<br>$$</p><p>对应的Loss就是前面说过的VQ Loss和GAN Loss两项之和.</p><blockquote><p>GAN的Loss最外层是Minimize $E, G, \mathcal{Z}$, 最内层是Maximize $D$, 所以它的目标其实是在有一个具有真正判别能力的判别器$D$ 的情况下优化生成器$G$ 的参数.</p></blockquote><p>其中, $\lambda$ 为自适应权重:</p><p>$$<br>\lambda=\frac{\nabla_{G_L}\left[\mathcal{L}_{\mathrm{rec}}\right]}{\nabla_{G_L}\left[\mathcal{L}_{\mathrm{GAN}}\right]+\delta}<br>$$</p><p>这个式子里的$\mathcal{L}_{\text{rec}}$ 是Perceptual Loss, $\delta$ 为1e-6, 防止分母零除.</p><p>作者在论文中没有给出$\lambda$ 的作用, 我们姑且可以认为$\lambda$ 是为了平衡$\mathcal{L}_{\text{rec}}$ 与$\mathcal{L}_{\mathrm{GAN}}$ 的影响而存在的, 保证两个Loss的作用差不多:</p><ul><li>当$\mathcal{L}_{\text{rec}}$ 梯度大于$\mathcal{L}_{\mathrm{GAN}}$ 的梯度时, 说明模型生成的图像还不够好, 此时$\lambda &gt; 1$, $\mathcal{L}_{\text{rec}}$ 影响比较大, 所以需要加强$\mathcal{L}_{\mathrm{GAN}}$ 的权重.</li><li>当$\mathcal{L}_{\text{rec}}$ 梯度小于$\mathcal{L}_{\mathrm{GAN}}$ 的梯度时, 说明模型生成的图像足够好, 此时$\lambda &lt; 1$, $\mathcal{L}_{\text{rec}}$ 影响比较小, 所以需要削弱$\mathcal{L}_{\mathrm{GAN}}$ 的权重.</li></ul><h3 id="Learning-the-Composition-of-Images-with-Transformers"><a href="#Learning-the-Composition-of-Images-with-Transformers" class="headerlink" title="Learning the Composition of Images with Transformers"></a>Learning the Composition of Images with Transformers</h3><h4 id="Latent-Transformers"><a href="#Latent-Transformers" class="headerlink" title="Latent Transformers"></a>Latent Transformers</h4><p>在通过VQ将输入图像$x$ 转化为$z_{\mathbf{q}}=\mathbf{q}(E(x))$ 后, 它等价于由Codebook $\mathcal{Z}$ 中Token下标组成的序列$s \in\{0, \ldots,|\mathcal{Z}|-1\}^{n \times w}$:</p><p>$$<br>s_{i j}=k \text { such that }\left(z_{\mathbf{q}}\right)_{i j}=z_k<br>$$</p><p>所以可以直接用Decoder - Only的Transformer对离散序列的自回归生成:</p><p>$$<br>\mathcal{L}_{\text {Transformer }}=\mathbb{E}_{x \sim p(x)}[-\log {\prod_i p\left(s_i \mid s_{&lt;i}\right)}]<br>$$</p><h4 id="Conditioned-Synthesis"><a href="#Conditioned-Synthesis" class="headerlink" title="Conditioned Synthesis"></a>Conditioned Synthesis</h4><p>如果需要对图像合成附加上条件$c$, 只需要在Transformer生成序列前加上条件信息$c$, 接下来继续做自回归生成就好:</p><p>$$<br>p(s \mid c)=\prod_i p\left(s_i \mid s_{&lt;i}, c\right)<br>$$</p><p>如果$c$ 只是简单的类别信息, 那么$c$ 就是一个简单的Embedding. 但如果是有约束条件$r$, 例如一个要生成图像的草图或框架, 那么$c$ 是一由一个新的Codebook和另一个专门编码条件的VQ - GAN得到的Conditional Sequence, 然后继续以Autoregressive Manner生成图像就可以了.</p><h4 id="Generating-High-Resolution-Images"><a href="#Generating-High-Resolution-Images" class="headerlink" title="Generating High - Resolution Images"></a>Generating High - Resolution Images</h4><p>生成高清图像的话需要扩大每个图片对应的Code Embedding数量, 这样就会给Transformer做序列生成时候带来压力. 作者提出了一种基于<strong>Sliding window</strong>的方法, 每次生成Token的时候只关注一个局部的小窗来生成下一个Code:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/introduction vq5.png" style="zoom:33%"><p>只要数据集上满足近似平移不变, 或者附带空间信息, 就可以做到高清的图像生成了. 如果没有空间信息, 人为附加上一个空间信号, 依然可以用这种方法生成高清图像.</p><h2 id="Recommended"><a href="#Recommended" class="headerlink" title="Recommended"></a>Recommended</h2><p>VQ - VAE:</p><ul><li><p><a href="https://www.spaces.ac.cn/archives/6760" target="_blank" rel="noopener">VQ-VAE的简明介绍：量子化自编码器 - 科学空间|Scientific Spaces</a>.</p></li><li><p><a href="https://zhuanlan.zhihu.com/p/633744455" target="_blank" rel="noopener">轻松理解 VQ-VAE：首个提出 codebook 机制的生成模型 - 周弈帆的文章 - 知乎</a>.</p></li><li><p><a href="https://zhuanlan.zhihu.com/p/388299884s" target="_blank" rel="noopener">漫谈VAE和VQVAE，从连续分布到离散分布 - 陀飞轮的文章 - 知乎</a>.</p></li></ul><p>VQ - GAN:</p><ul><li><a href="https://zhuanlan.zhihu.com/p/637705399" target="_blank" rel="noopener">VQGAN 论文与源码解读：前Diffusion时代的高清图像生成模型 - 周弈帆的文章 - 知乎</a>.</li></ul><p>VQ + Image Generation串讲:</p><ul><li><a href="https://zhuanlan.zhihu.com/p/681895334" target="_blank" rel="noopener">AutoEncoder与图像生成 - vasgaowei的文章 - 知乎</a>.</li></ul><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>在不同的Modality里, 都可以用VQ来做:</p><ul><li>CV: VQ最主要的作用是压缩图像的序列长度.</li><li>NLP: Token Embedding的生成本身就遵循Autoregressive Manner, 本身就是离散的.</li><li>Audio: 在语音领域中, 语音信号也需要做离散化处理, 用一个Quantizer把连续语音信号转化成离散的Token是一种常用的操作.</li></ul><p>在以VL为主的MLLM时代, VQ因为使用Codebook能够统一离散形式的文本Token和图像生成而重新被人提起. 这种<strong>Multimodal Tokenization</strong>的方式或许能帮助MLLM更好的理解不同Modality之间的关系, 因此VQ或许能够在MLLM时代继续发挥它的光和热.</p><blockquote><p>另外, Codebook中的每个Code很容易出现利用率不均, 这里抛砖引玉的给出一篇文章, 是对VQ - GAN的改进:</p><ul><li><a href="https://arxiv.org/abs/2110.04627" target="_blank" rel="noopener">Vector-Quantized Image Modeling With Improved VQGAN</a>.</li></ul></blockquote></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/18380.html">https://ADAning.github.io/posts/18380.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/VQ/"><span class="chip bg-color">VQ</span> </a><a href="/tags/VQ-VAE/"><span class="chip bg-color">VQ-VAE</span> </a><a href="/tags/VQ-GAN/"><span class="chip bg-color">VQ-GAN</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/62916.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/11.jpg" class="responsive-img" alt="Pytorch实现: VQ-VAE"> <span class="card-title">Pytorch实现: VQ-VAE</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: VQ基本知识: Introduction: Vector Quantization Vector Quantization. Pytorch实现: VQ - VAE本文是VQ - VAE的Pytorch版本实现, 并</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2024-07-28 </span><span class="publish-author"><i class="fas fa-user fa-fw"></i> DaNing</span></div></div><div class="card-action article-tags"><a href="/tags/VQ-VAE/"><span class="chip bg-color">VQ-VAE</span> </a><a href="/tags/Pytorch/"><span class="chip bg-color">Pytorch</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/64567.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/10.jpg" class="responsive-img" alt="Multimodal Large Language Model 总结"> <span class="card-title">Multimodal Large Language Model 总结</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: Vision &amp; Language Pretrained Model 总结. Multimodal Large Language Model 总结最近MLLM的进展实在是太快了, 必须得赶紧写一篇博客出来了… 再</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2024-07-03 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/MLLM/"><span class="chip bg-color">MLLM</span> </a><a href="/tags/MM/"><span class="chip bg-color">MM</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">384.2k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:andaning@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>