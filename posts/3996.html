<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="ELMo, GPT, BERT, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>ELMo, GPT, BERT | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/17.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">ELMo, GPT, BERT</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"><span class="chip bg-color">词向量</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2020-10-04</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2022-03-27</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 8.2k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 31 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p>本文的前置知识:</p><ul><li>RNN</li><li>Transformer</li><li>Language Model</li></ul></blockquote><h1 id="ELMo-GPT-BERT"><a href="#ELMo-GPT-BERT" class="headerlink" title="ELMo, GPT, BERT"></a>ELMo, GPT, BERT</h1><p>本文是对ELMo, GPT, BERT三个模型的结构介绍以及个人理解, 多图预警.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>由于NLP领域的<strong>数据标注</strong>难度非常大, 成本很高, 所以必须要采用<strong>无监督或半监督</strong>的方法来在数据集不够充足的情况下提高模型的<strong>通用表示能力</strong>. 从Word2Vec(2013)到Glove(2014), 再到ELMo(2017), GPT-1(2018), BERT(2018), GPT-2(2019), GPT-3(2020)… 前人已经在预训练领域做了不少的探索. 由于NLP领域在自然语言的表示上非常困难, 特别注重于<strong>感知</strong>和<strong>理解</strong>, 现有的算法对机器要求又比较高, 所以NLP的发展进度比CV是滞后一些的, 在CV上早就出现了预训练模型, 也就不难理解为什么会在预训练上摸索了.</p><p>之所以把ELMo, GPT, BERT这三个模型放在一起说, 是因为BERT作为NLP中的又一个<strong>里程碑</strong>(并不是因为BERT模型本身创新点有多少, 而是它代表本阶段的所有技术融合)将这三个模型进行了对比. 这三个模型像是NLP近四年在<strong>预训练领域</strong>的探索过程的缩影, 从ELMo出现以来, NLP的ImageNet时代就来临了. 除此外, 它们三个都与<strong>单词结合上下文表示</strong>有关. 但无论哪种预训练模型, 都离不开强大的数据集和高算力支撑.</p><p>本文的图片来自于:</p><ul><li><p><a href="https://jalammar.github.io/illustrated-bert/" target="_blank" rel="noopener">The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)</a></p></li><li><p><a href="https://jalammar.github.io/illustrated-gpt2/" target="_blank" rel="noopener">The Illustrated GPT-2 (Visualizing Transformer Language Models)</a></p></li><li><p><a href="https://jalammar.github.io/how-gpt3-works-visualizations-animations/" target="_blank" rel="noopener">How GPT3 Works - Visualizations and Animations</a></p></li><li><p>ELMo, GPT, BERT的原论文.</p></li><li><p>芝麻街那几个小人的图片来源出自网络.</p><p>其余图片的具体来源不再注明是来自于哪篇文章的, 请自行到对应模型的文章查找. 上述三篇文章均来自<code>jalammar</code>, 选择他的文章做图片来源的理由太简单了, 并不是因为内容准确(自然语言不具有像数学语言一样的精确性), 而是因为颜值高.</p></li></ul><p>当然Word2vec我认为没必要再细说了, 现在所有的词嵌入都是基于Word2vec的, Word2vec更像是一个”<strong>死</strong>“的稠密向量, 单词嵌入后就是唯一的表示, 不能根据其在句子中的位置和上下文关系而改变其含义, 这就为处理<strong>一词多义</strong>问题添加了难度.</p><h2 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h2><p>ELMo全称是<strong>Embeddings from Language Models</strong>, 出自<a href="https://arxiv.org/abs/1802.05365" target="_blank" rel="noopener">Deep contextualized word representations</a>.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/elmo.jpg" style="zoom:25%"><p>上面这个小人就是芝麻街里的角色ELMo. ELMo是一种基于Embedding的高级词向量表示.</p><p>前面说过, Word2vec具有局限性, 不能够表示一个单词的多种意思. 作者认为一个好的词向量有两个优点:</p><ol><li>能表示复杂的单词特性, 例如语义和语法信息.</li><li>词向量能适应多种语境有不同的体现, 即结合语境的一词多义.</li></ol><p>ELMo也是基于这两个点出发, 尝试通过更深的方法来实现基于<strong>上下文</strong>来表示单词的模型, 能够学习到更复杂的语义特征, 所以也说它是更高级的词向量.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/elmo1.png" style="zoom:33%"><p>在上图中, ELMo指出对于不同的语义, 具有不同的词向量.</p><p>ELMo在执行不同的Task时或不同的语境时, 同一个词得到的词嵌入也可能是不同的.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/elmo2.png" style="zoom:50%"><h3 id="Bidirectional-Language-Models"><a href="#Bidirectional-Language-Models" class="headerlink" title="Bidirectional Language Models"></a>Bidirectional Language Models</h3><p>虽然ELMo自称是双向语言模型, 但实则不然. 先抛出这个结论, 在本小节结束的时候再点出.</p><p>论文中指出, 高层LSTM能够捕捉结合语境的<strong>词意</strong>信息, 而底层LSTM能捕捉<strong>语法</strong>上的特征, 把它们结合起来, 非常有利于下游任务的执行, 这也恰恰就是作者认为的优秀的词向量的特性.</p><p>ELMo采用<strong>无监督</strong>的方法, 利用<strong>语言模型</strong>的特点, 对句子本身的单词进行移位预测, 依赖串行结构以当前时刻以前的单词预测本时刻单词输出的性质也叫作<strong>自回归</strong>(Transformer的<strong>Decoder</strong>也是自回归结构). 文本数量非常庞大, 无需任何标签就能从中学习.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/elmo3.png" style="zoom:50%"><p>如图中, LSTM在已知前三个单词”let’s”, “stick”, “to”的情况下需要预测”improvisation”.</p><p>另外, 引入从左向右和从右向左的两个<strong>单向LSTM</strong>能更好的结合上下文, 获得更复杂的语义表示. 对于多层的LSTM, 每层输出的隐态被作为下一层的输入继续传递.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/elmo4.png" style="zoom:50%"><p>这两个单向而反向的LSTM的权重将在预训练完成后被保存, 在正式预测时派上用场.</p><p>对于上述过程, 我们用数学语言来描述. 对于序列中的$N$ 个Token, $\left(t_{1}, t_{2}, \ldots, t_{N}\right)$, 第$k$ 个Token前的序列被描述为$\left(t_{1}, \ldots, t_{k-1}\right)$, 那么根据语言模型, 序列$\left(t_{1}, t_{2}, \ldots, t_{N}\right)$ 的概率就应该表示为:<br>$$<br>p\left(t_{1}, t_{2}, \ldots, t_{N}\right)=\prod_{k=1}^{N} p\left(t_{k} \mid t_{1}, t_{2}, \ldots, t_{k-1}\right)<br>$$<br>如果把上面的方向叫做<strong>Forward</strong>, 相应的, 与之相反的方向就称为<strong>Backward</strong>, 描述如下:<br>$$<br>p\left(t_{1}, t_{2}, \ldots, t_{N}\right)=\prod_{k=1}^{N} p\left(t_{k} \mid t_{k+1}, t_{k+2}, \ldots, t_{N}\right)<br>$$<br>对与语言模型, 普遍采用<strong>极大似然</strong>来调整两个单向LSTM的参数, 只要最大化向前和向后的对数概率就好:<br>$$<br>\begin{array}{l}<br>\sum_{k=1}^{N}\left[\log p\left(t_{k} \mid t_{1}, \ldots, t_{k-1} ; \Theta_{x}, \overrightarrow{\Theta}_{L S T M}, \Theta_{s}\right)\right.<br>\left.+\log p\left(t_{k} \mid t_{k+1}, \ldots, t_{N} ; \Theta_{x}, \overleftarrow{\Theta}_{L S T M}, \Theta_{s}\right)\right]<br>\end{array}<br>$$<br>其中$\overrightarrow{\Theta}_{L S T M}$ 和 $\overleftarrow{\Theta}_{L S T M}$ 分别代表两个LSTM的参数, $\Theta_{x}$ 代表Token的表示, $\Theta_{s}$ 代表Softmax层在两个LSTM维护的参数. 在论文中提到, 在某些任务中加入L2正则$\lambda \lVert\mathbf{w}\rVert _{2}^{2}$ 效果可能会更好.</p><p>再强调一遍, 这两个LSTM是单向工作的:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/elmo7.png" style="zoom:67%"><h3 id="ELMo-Architecture"><a href="#ELMo-Architecture" class="headerlink" title="ELMo Architecture"></a>ELMo Architecture</h3><p>在ELMo中, 并非直接采用Word2vec作为输入, 而是在Embedding后采用了字符级的CNN作为替代输入. 作者认为字符级CNN是一种更加<strong>不敏感</strong>的上下文表示, 也有可能Word2vec本身就限制了多语义的表达.</p><p>如果输入用$\mathbf{x}_{k}^{L M}$ 表示, 一共有$L$ 层LSTM, 在论文中采取$L = 2$. 而Forward的LSTM最终的隐态输出为$\overrightarrow{\mathbf{h}}_{k, j}^{L M}$, Backward输出为$\overleftarrow{\mathbf{h}}_{k, j}^{L M}$, 那么$k$ 应该能被$2L+1$ 个参数表示, 称这个参数集合为$R_k$, 并将两个LSTM的隐态合并后有:<br>$$<br>\begin{aligned}<br>\mathbf{h}_{k, j}^{L M}&amp;=\left[\overrightarrow{\mathbf{h}}_{k, j}^{L M} ; \overleftarrow{\mathbf{h}}_{k, j}^{L M}\right] \\<br>R_{k} &amp;=\left\{\mathbf{x}_{k}^{L M}, \overrightarrow{\mathbf{h}}_{k, j}^{L M}, \overleftarrow{\mathbf{h}}_{k, j}^{L M} \mid j=1, \ldots, L\right\} \\<br>&amp;=\left\{\mathbf{h}_{k, j}^{L M} \mid j=0, \ldots, L\right\}<br>\end{aligned}<br>$$<br>为了更好的解释ELMo对下游任务是如何运作的, 将$R_k$和所有参数都表示为一个向量:<br>$$<br>\mathbf{E} \mathbf{L M o}_{k}=E\left(R_{k} ; \mathbf{\Theta}_{e}\right)<br>$$<br>对下游任务, ELMo对不同的Task采用了不同的权重.<br>$$<br>\mathbf{E} \mathbf{L} \mathbf{M} \mathbf{o}_{k}^{t a s k}=E\left(R_{k} ; \Theta^{t a s k}\right)=\gamma^{t a s k} \sum_{j=0}^{L} s_{j}^{t a s k} \mathbf{h}_{k, j}^{L M}<br>$$<br>$\text { s }^{\text {task}}$ 是经过Softmax标准化后的权重, $\gamma^{\text {task}}$ 是一个缩放因子. 对于每层LSTM, 它的标准化权重都是分别计算的, 最终是将所有层LSTM的隐态分别加以Softmax, 再相加求和, 这样对于不同的Task, ELMo就能学习到词向量在面对不同任务时的不同线性组合.</p><p>如下图, 最终的ELMo向量是由两个LSTM拼接后的向量和每层LSTM在经过Softmax后得到的权重加权求和而成的.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/elmo5.png" style="zoom:50%"><p>下面是ELMo在面对不同任务时Softmax所学习到的权重情况的可视化, 确实证明了ELMo对不同的任务下, 关注的权重不同, 对同样的词语有不同的表达:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/elmo6.jpg" style="zoom:67%"><blockquote><ul><li>SRL: Semantic role labeling.</li><li>Coref: Coreference resolution.</li><li>SNLI: Textual entailment.</li><li>SQuAD: Question Answer.</li><li>SST-5: Sentiment analysis.</li></ul></blockquote><h3 id="How-to-Use-ELMo"><a href="#How-to-Use-ELMo" class="headerlink" title="How to Use ELMo"></a>How to Use ELMo</h3><p>在使用ELMo的时候, 要冻结两个LSTM的参数. 最简单的用法就是把LSTM<strong>最后一层</strong>的隐态输出作为词向量使用, 也可以老老实实将每层的LSTM隐态输出都加权求和. 具体怎么使用, 还是要结合<strong>任务复杂程度</strong>来确定.</p><p>作者在论文中提到, 可以将$\mathbf{E} \mathbf{L} \mathbf{M} \mathbf{o}_{k}^{t a s k}$ 和 输入$\mathbf{x}_{k}$ 一起concat起来, 即$\left[\mathbf{x}_{k} ; \mathbf{E} \mathbf{L} \mathbf{M} \mathbf{o}_{k}^{t a s k}\right]$, 加强表示, 然后作为词向量送入RNN, 也可以将$\mathbf{x}_{k}$ 替换为$\mathbf{h}_{k}$, 和ELMo向量concat起来, 即$\left[\mathbf{h}_{k} ; \mathbf{E} \mathbf{L} \mathbf{M} \mathbf{o}_{k}^{t a s k}\right]$.</p><p>总而言之, <strong>怎么用都行</strong>.</p><h3 id="Fake-Bidirectional-Language-Model"><a href="#Fake-Bidirectional-Language-Model" class="headerlink" title="Fake Bidirectional Language Model"></a>Fake Bidirectional Language Model</h3><p>现在来说说小节开始时遇到的那个问题:</p><p>ELMo所谓的双向实际上是通过<strong>两个单向且反向的LSTM</strong>实现的, 并不是直接使用双向LSTM, ELMo并非是原论文中所称的”deep bidirectional language model (biLM)”, 这点在BERT论文中曾有提到过:</p><blockquote><ul><li><p>BERT uses masked language models to enable pre-trained <strong>deep bidirectional</strong> representations. This is also in <strong>contrast</strong> to Peters et al. (2018a), which uses a shallow concatenation of <strong>independently</strong> trained left-to-right and right-to-left LMs.</p></li><li><p>Similar to ELMo, their model is feature-based and <strong>not deeply bidirectional</strong>.</p></li></ul></blockquote><p>当你了解BERT后, 会发现作者虽然一直在强调双向语言模型, 但ELMo并不是真正意义上的双向语言模型.</p><p><strong>那么为什么不采用双向LSTM实现呢?</strong></p><p>两个单向的LSTM和一个双向LSTM之间的区别, 对于单层的LSTM, 双向和单向差别无非就是hidden state是否concat或者add到一起. 但对于多层来说, 涉及到序列前后<strong>信息泄露</strong>的问题, 深层双向LSTM会被泄露上下文词语的位置信息, 导致模型学到了不该学习的东西, 也就失去了预训练的效果.</p><h2 id="Transformer-Review"><a href="#Transformer-Review" class="headerlink" title="Transformer Review"></a>Transformer Review</h2><p>之所以把Transformer Review放在这里, 是因为剩下的两个模型和Transformer关系很大, 如果有基础的可以直接跳过.</p><h3 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h3><p>如果你还不了解Transformer, 在这个小节能帮你<strong>快速回顾</strong>关于Transformer的部分知识, 因此所有的描述都非常简陋, 更详细的内容请看我之前写的<code>&lt;Transformer精讲&gt;</code>.</p><p>Transformer采用Seq2Seq架构, 分为Encoder和Decoder两个部分:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview1.png" style="zoom:33%"><p>Encoder包括一个前FFN层和一个自注意力层, 结构如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview2.png" style="zoom:33%"><p>Decoder包括FFN层, 一个对接Encoder的自注意力层, 和一个Mask自注意力层, 结构如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview3.png" style="zoom:33%"><h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self - Attention"></a>Self - Attention</h3><h4 id="General-Self-Attention"><a href="#General-Self-Attention" class="headerlink" title="General Self - Attention"></a>General Self - Attention</h4><p>这个注意力说的是不加Mask的自注意力, 即出现在Encoder的第一层和Decoder的第二层. 每个自注意力层通常有多个头, 类似于CV中的卷积核, 能够抽取不同角度的特征. 我们先不考虑多头, 反正它的操作也是一致的, 最后再合并即可.</p><p>但是经过自注意力的vector的大小是不发生变化的(下图可能有shape错误). 自注意力机制一共有三步.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview4.png" style="zoom:33%"><p>首先, 每个Embedding的Token分别经过三个矩阵, 创建query vector, key vector, value vector.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview5.png" style="zoom:33%"><p>其次, 根据Transformer中提出的缩放点积注意力, 分别计算上下文其他Token对当前的Score, 并经过Softmax, 得到注意力权重:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview6.png" style="zoom:33%"><p>最后, 将注意力权重与对应的value vector加权求和得到结果.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview7.png" style="zoom:33%"><h4 id="Masked-Self-Attention"><a href="#Masked-Self-Attention" class="headerlink" title="Masked Self - Attention"></a>Masked Self - Attention</h4><p>在Encoder中, 所有Embedding是并行输入的, 这并不会影响Decoder的输出. 但我们提过, Decoder具有自回归性:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview13.png" style="zoom:33%"><p>在解码时, 解码是单向的, 如果我们将所有数据并行输入, 那么就会造成<strong>信息泄露</strong>.</p><p>Mask正是为保证Decoder的自回归性和计算并行而存在的, 因为在解码时, Decoder应该只能看到当前时刻应该解码的内容和之前解码过的内容, 如下图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview14.png" style="zoom:33%"><p>灰色的向量代表Decoder不应该看见那部分信息. 我们添加一个Mask, 使得并行输入的矩阵的对角线以上的部分都不能被Decoder所解码:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview9.png" style="zoom:33%"><p>我们来复现这个过程, 在Softmax之前先计算出Score矩阵:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview10.png" style="zoom:50%"><p>具体对Score加Mask的方法就是将主对角线上的所有元素都变为负无穷:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview11.png" style="zoom:50%"><p>这样在做Softmax时, 主对角线上的信息会自动被屏蔽为0:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview12.png" style="zoom:50%"><p>这也就等价于Decoder对当前时刻以后的内容是不可知的:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview8.png" style="zoom:33%"><h2 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h2><p>GPT的想法非常简单, 因为在NLP中有相当多种类的任务, 尽管有大量的未标注数据, 但用于指定任务的标注过的数据却很少, 这不能很好地评判已训练过的模型性能. GPT尝试用一种通用, <strong>任务无关</strong>的模型结构解决所有NLP问题. 对于不同的Task, 只需要在无监督的预训练后进行监督的Fine tune就行了, 这与CV界的Transfer Learning相同:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vggpretrain.png" style="zoom:50%"><p>GPT的三个模型几乎都是<strong>相同架构</strong>, 只是有非常非常少量的改动. 但一代比一代更大, 也更烧钱. 所以我对GPT系列的特点就是: <strong>钞能力, 大就完事了.</strong> 其影响力和花费的成本是成正比的.</p><p>先抛出三代GPT的论文出处:</p><ul><li>GPT - 1: <a href="https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf" target="_blank" rel="noopener">Improving Language Understanding by Generative Pre-Training</a></li><li>GPT - 2: <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" target="_blank" rel="noopener">Language Models are Unsupervised Multitask Learners</a> (可能需要魔法才能看)</li><li>GPT - 3: <a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener">Language Models are Few-Shot Learners</a></li></ul><p>因为架构相同, 后两篇论文大多数内容都是对增加参数后成果的展示. 时间不充裕建议只看第一篇, 因为每代区别不大, 本节<strong>一代为主</strong>, <strong>二代为辅</strong>, 三代先挖个坑, 以后会补.</p><blockquote><p>jalammar并没有做GPT - 1的图, 并且每代间又没有明显的结构区别, 所以就直接用二代的图了.</p></blockquote><p>GPT - 1其实并没有广泛的引起人们的关注, 反倒是GPT - 2和3让它火了一把. 最出名的就是GPT - 2生成的那篇关于独角兽的文章:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/独角兽.jpg" style="zoom:50%"><p>对此, 我个人认为除了生成的结果十分惊艳外, 确实有些炒作的成分. 惊艳是因为第一次见到机器能够生成如此有趣而附带一些逻辑的内容. 炒作主要原因是媒体夸大事实宣传, 博人眼球, 次要原因不难理解OpenAI也是需要科研资金的嘛. GPT目前<strong>相对于</strong>其他的NLP模型来说, 强是肯定的, 只是说代价太大了. 第二代和第三代强调了GPT有<strong>Few shot Learning</strong>和<strong>Zero shot Learning</strong>的潜力, 在巨大数据集的情况下, 模型甚至都没有收敛… 三代甚至不需要Fine tune…</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gpt13.jpg" style="zoom:67%"><blockquote><p>Few - shot: 给出一个自然语言任务的一些范例, 但这些范例不能更新权重(算是一个限制, 给出更多信息).</p><p>One - shot: 仅给出一个范例, 让模型给出结果.</p><p>Zero - shot: 不给任何范例, 直接让模型给出结果.</p><p>在论文中, GPT - 3的Zero - shot和One - shot在某些任务上确实超过SOTA, 但相较人类还有很大差距. Few - shot比前二者有很大提升. 说明在没给出足够多信息的条件下, GPT - 3对<strong>问题理解能力</strong>还是比较差的.</p></blockquote><h3 id="Why-Transformer-Decoder"><a href="#Why-Transformer-Decoder" class="headerlink" title="Why Transformer Decoder?"></a>Why Transformer Decoder?</h3><p>言归正传, 来说GPT的具体结构. 受Transformer影响, GPT(Generative Pre-Training)采用<strong>Transformer</strong>作为基本的Block结构. 作者指出LSTM将预测能力限制在<strong>短距离</strong>内, 所以才采用使用Attention的Transformer作为<strong>长距离</strong>的信息抽取器. 当然, 这里只使用<strong>Decoder</strong>, 就需要去掉Decoder对Encoder的自注意力层, <strong>Mask</strong>后的自注意力保护了Decoder的<strong>自回归性</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gpt3.jpg" style="zoom:33%"><p>在GPT一代中, 采用了12个Decoder堆叠.</p><h3 id="Unsupervised-pre-training"><a href="#Unsupervised-pre-training" class="headerlink" title="Unsupervised pre-training"></a>Unsupervised pre-training</h3><p>与标准语言模型一样, 对于无监督语料库中的Token $\mathcal{U}=\left\{u_{1}, \ldots, u_{n}\right\}$, 都采用<strong>极大似然</strong>优化:<br>$$<br>L_{1}(\mathcal{U})=\sum_{i} \log P\left(u_{i} \mid u_{i-k}, \ldots, u_{i-1} ; \Theta\right)<br>$$<br>其中$k$ 为上下文窗口, $\Theta$ 是神经网络参数, 它们会用<strong>随机梯度下降</strong>得到训练.</p><p>对于GPT来说, 选用Transformer的<strong>Decoder</strong>做最基本的Block. 将输入一层一层嵌套, 经过$n$ 个Transformer Block, 其数学表达为:<br>$$<br>\begin{aligned}<br>h_{0} &amp;=U W_{e}+W_{p} \\<br>h_{l} &amp;=\text { transformer_block }\left(h_{l-1}\right) \forall i \in[1, n] \\<br>P(u) &amp;=\operatorname{softmax}\left(h_{n} W_{e}^{T}\right)<br>\end{aligned}<br>$$<br>其中, $U=\left(u_{-k}, \ldots, u_{-1}\right)$ 是上下文向量, $n$ 为Decoder的层数.</p><p>$W_e$ 是Token的Embedding矩阵(一代嵌入维度只有768一种):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gpt4.jpg" style="zoom:50%"><p>$W_p$ 是Positional Embedding矩阵. 与Transformer不同, GPT的位置编码并非是通过三角函数计算来的, 而是通过训练学习到的.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gpt5.png" style="zoom:50%"><p>$h_0$ 是嵌入后的向量和学习到的位置编码向量之和:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gpt6.png" style="zoom:50%"><p>然后就经过Transformer Block的自回归得到Decoder部分的输出$h_l$ :</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gpt7.png" style="zoom:50%"><p>对Decoder部分的输出做个总结吧(因为重复太多次了, 省略了再跑一遍的过程, 想看完整的去<a href="https://jalammar.github.io/illustrated-gpt2/" target="_blank" rel="noopener">这里</a>):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gpt11.png" style="zoom:50%"><p>在得到最后输出前, 最后还需要再与<strong>Embedding相乘</strong>一次, 再通过Softmax得到结果$P(u)$:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gpt9.png" style="zoom:50%"><p>“logits”就是在Softmax前的值, 后一般接一个Softmax输出标准化的概率. 在GPT二代中, 不再像一代直接选择概率最高的单词, 而是从<code>top_k</code> 中以某个<strong>概率</strong>选择单词, 这样来避免陷入永无止境的循环之中.</p><p>参数量实在是太大了, 每个部分占到的参数都非常非常多:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gpt12.png" style="zoom:50%"><h3 id="Supervised-Fine-tuning"><a href="#Supervised-Fine-tuning" class="headerlink" title="Supervised Fine-tuning"></a>Supervised Fine-tuning</h3><p>首先要加载预训练的权重, 然后要让预训练模型适应特定的任务, 对<strong>额外参数</strong>进行微调, 所以还要继续在当前基础上再加Linear层适应输出. 假设给出数据集$\mathcal{C}=(x^{1}, \ldots, x^{m}, y)$, 在通过一堆Transformer Block后得到的最终输出为$h_{l}^{m}$, 最后加一个Linear层参数为$W_y$, 有:<br>$$<br>P\left(y \mid x^{1}, \ldots, x^{m}\right)=\operatorname{softmax}\left(h_{l}^{m} W_{y}\right)<br>$$<br>有了标注的数据, 仍然采用<strong>极大似然</strong>进行优化:<br>$$<br>L_{2}(\mathcal{C})=\sum_{(x, y)} \log P\left(y \mid x^{1}, \ldots, x^{m}\right)<br>$$<br>无监督学习反作用于监督学习可以有一些提升. 所以在Fine tune这里用了前面无监督预训练的Loss做<strong>辅助训练</strong>. $\lambda$ 是超参, 论文中设置$\lambda=0.5$.<br>$$<br>L_{3}(\mathcal{C})=L_{2}(\mathcal{C})+\lambda \ast L_{1}(\mathcal{C})<br>$$<br>在微调中, 只有最后外接的$W_y$ 和句子之间的代表分隔符的Token的Embedding(看Overview and GPT in Specific Task)是需要额外进行学习的.</p><h3 id="More-Details"><a href="#More-Details" class="headerlink" title="More Details"></a>More Details</h3><p>一些超参的设置请参考原论文, 这里只说容易被忽略的点.</p><h4 id="BPE"><a href="#BPE" class="headerlink" title="BPE"></a>BPE</h4><p>BPE(byte pair encode), 也称为<strong>字节对编码</strong>, 其主要目的是<strong>数据压缩</strong>, 现在已经被作为重要的提升NLP模型性能的算法. 这种编码拆除了语言学特性, 但通过统计学方法更好的解决语言类问题. 做法请参考论文<a href="https://arxiv.org/abs/1508.07909" target="_blank" rel="noopener">Neural Machine Translation of Rare Words with Subword Units</a>, 我认为日后有必要把几个subword技巧做个对比.</p><h4 id="GeLU"><a href="#GeLU" class="headerlink" title="GeLU"></a>GeLU</h4><p>在GPT中使用的激活函数是<strong>GeLU</strong>(<strong>G</strong>aussian <strong>E</strong>rror <strong>L</strong>inerar <strong>U</strong>nits)不是ReLU!</p><p>GeLU原文中作者给出的近似公式:<br>$$<br>\text{GeLU}(x) = 0.5x(1 + \text{tanh}[\sqrt{\frac{2}{\pi}}(x+0.044715x^3)])<br>$$<br>具体内容请见论文<a href="https://arxiv.org/abs/1606.08415" target="_blank" rel="noopener">Gaussian Error Linear Units (GELUs)</a>.</p><h4 id="WarmUp"><a href="#WarmUp" class="headerlink" title="WarmUp"></a>WarmUp</h4><p>Fine tune阶段使用了线性衰减的WarmUp, 请参考我在<code>&lt;Transformer精讲&gt;</code>中的内容.</p><h3 id="Overview-and-GPT-in-Specific-Task"><a href="#Overview-and-GPT-in-Specific-Task" class="headerlink" title="Overview and GPT in Specific Task"></a>Overview and GPT in Specific Task</h3><h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gpt1.jpg" style="zoom:50%"><p>GPT的结构非常简单, 实质就是12个Transformer Decoder的堆叠, 最后再根据任务需要<strong>接入结构</strong>以完成最终任务.</p><blockquote><p>这里能看出GPT的一个缺点, 虽然Self - Attention能够很好地利用全局的文章信息, 但是由于Decoder自回归性的限制, GPT是一个<strong>单向语言模型</strong>, 在Summary部分会继续与其他模型进行对比.</p></blockquote><h4 id="Input-Transformations"><a href="#Input-Transformations" class="headerlink" title="Input Transformations"></a>Input Transformations</h4><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gpt2.jpg" style="zoom:33%"><p>执行不同的任务时, 需要对输入进行相应的调整. 在微调的过程中, 对于每种任务的开始和结束都需要加入<strong>标记</strong>$\langle s\rangle,\langle e\rangle$, 下文不再强调, 两段不同的内容之间需要加入分隔符$ $ $ .</p><h5 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h5><p>做分类任务非常简单, 在原输入两侧加上开始结束标记即可, 可以直接使用原来的模型微调.</p><h5 id="Textual-entailment"><a href="#Textual-entailment" class="headerlink" title="Textual entailment"></a>Textual entailment</h5><p>文本蕴含任务, 将premise和hypothesis拼接起来, 在二者之间加入分隔符.</p><h5 id="Similarity"><a href="#Similarity" class="headerlink" title="Similarity"></a>Similarity</h5><p>句子相似度任务, 在句子A和句子B之间加上分隔符, 再交换句子A和句子B的位置, 分别传入Transformer, 将二者结果add起来, 传入Linear.</p><h5 id="Question-Answering-and-Commonsense-Reasoning"><a href="#Question-Answering-and-Commonsense-Reasoning" class="headerlink" title="Question Answering and Commonsense Reasoning"></a>Question Answering and Commonsense Reasoning</h5><p>QA和常识推理任务, 将内容文本和回答分别组合, 并在之间加上分隔符, 传入Transformer, 分别经过Linear, 经过Softmax, 最后得到概率分布.</p><p>GPT还能做其他的事情, 例如音乐生成之类的, 感兴趣自己了解下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gpt10.png" style="zoom:50%"><h3 id="Difference-Between-GPT-1-and-GPT-2"><a href="#Difference-Between-GPT-1-and-GPT-2" class="headerlink" title="Difference Between GPT - 1 and GPT - 2"></a>Difference Between GPT - 1 and GPT - 2</h3><p>GPT一代到二代仅发生了几个不同:</p><ol><li><p>用了更大的数据集, 尤其是网页文本, 40G.</p></li><li><p>增加了海量参数, 并推出了几个不同的版本, 一个比一个大:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gpt8.png" style="zoom:50%"><p>除了使用的Decoder个数不同, 它们的Embedding维度也是不同的.</p></li><li><p>直接去掉了Fine - tune层, 直接输入任务和所需的内容就能得到输出.</p></li><li><p>将Layer Norm放到了每个子层的输入前, 并且在最后一个自注意力层后添加了Layer Norm. 通过放缩权重更换了残差层的初始方式.</p></li></ol><h3 id="Postscript"><a href="#Postscript" class="headerlink" title="Postscript"></a>Postscript</h3><p>其实GPT一直都存在伦理上的问题. 有些人认为语言模型训练来自于语料, 语料的偏见会导致模型带有偏见. 而且GPT当初被OpenAI认为过于危险, 可能在只经过fine tune后被恶意滥用, 没有将这个庞然大物的参数开放.</p><blockquote><p>但可以通过下面几个链接去体验GPT - 2(可能需要魔法):</p><ul><li><a href="http://textsynth.org/" target="_blank" rel="noopener">Text Synth</a></li><li><a href="https://talktotransformer.com/" target="_blank" rel="noopener">Talk to Transformer</a></li></ul></blockquote><p>最后说一些自己的感想, 不想看的可以跳过这段.</p><p>GPT经历了三代, 有那么一点<strong>哲学</strong>和<strong>讽刺</strong>的意味. 第三代的1750亿参数几乎已经逼近了参数量的极限, 但实际上GPT产生的文章并没有媒体文章渲染的那么恐怖, GPT生成的内容还是经常犯一些常识性的<strong>错误</strong>, 即使是靠堆参数, 模型也并没有真的做到”Natural Language Understanding”, “大”真的意味着它<strong>智能</strong>了吗? 我们接触的世界是一个<strong>多模态</strong>的世界, 而计算机不能真正触及我们所接触的任何物体, 只能通过我们提供的<strong>数据</strong>来做到”认知”. 尽管我对AI发展持乐观态度, 但现在人们所强调的技术方法绝对不能有效的构造一个<strong>智能体</strong>, 虽是一条过渡的必经之路, 但有些<strong>矫枉过正</strong>. 至少人类距离强人工智能还有非常遥远的一段距离(如果非要形容, 距离可能是光年为单位), <strong>任重而道远</strong>.</p><h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p>也受到<strong>Transformer</strong>的影响, BERT像GPT一样把Transformer加入了自然语言理解中, 也尝试训练一个预训练模型, 只经过微调就能适应NLP领域的各种任务. 现在的BERT已经遍地开花, 很多NLP任务都是BERT或者BERT的魔改在屠榜.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert.jpg" style="zoom:25%"><p>上面这个小人就是BERT, 与ELMo同样都来自芝麻街. 其实除了BERT, 大家还拼凑过Grover, ERNIE, Big Bird…这些全是芝麻街的小人, 只不过有些凑的比较强行就是了.</p><p>BERT出自论文<a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>, 我认为这篇论文的<strong>附录</strong>才是本体… 正文着重于讲BERT的训练方式, 与前人模型的区别, 以及取得的效果. 虽然正文也很重要, 但附录里才有BERT的具体实现方法, 以及与ELMO, GPT的对比. 所以在看这篇论文时, 一定记得看Appendix.</p><p>BERT(Bidirectional Encoder Representations from Transformers)延续了GPT的Pre - train + Fine tune的思路, BERT也是冲着<strong>通用语言模型</strong>的目标去的, 并适配了一套对任何任务<strong>不用变更输入模式</strong>的训练方法.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert7.png" style="zoom:50%"><p>在预训练和微调好了之后, 只需要接上一层FFN和Softmax就能做到分类:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert8.png" style="zoom:50%"><p>另外, BERT可以<strong>抽取词向量</strong>, 近期已经被作为Word2vec的替代者了:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert18.png" style="zoom:50%"><p>在论文中, 作者对BERT提取的不同特征效果做了对比, 这也证实了BERT具有特征抽取能力:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert19.png" style="zoom:50%"><p>似乎将最后4层拼接起来的F1得分要高一些.</p><h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p>在BERT论文中指出了ELMo和GPT的不足: 它们不是<strong>双向语言模型</strong>, 所以BERT采用了<strong>多层双向的Transformer Encoder</strong>作为堆叠基础结构.</p><blockquote><p>所谓的双向Transformer实际上是Transformer Encoder, 所谓的”双向”是体现在MLM中(见Masked Language Model).</p><p>We note that in the literature the <strong>bidirectional Transformer</strong> is often referred to as a “<strong>Transformer encoder</strong>” while the left-context-only version is referred to as a “Transformer decoder” since it can be used for text generation.</p></blockquote><p>在论文中, BERT发表了两个不同的版本, $\mathbf{B E R T}_{\mathbf{B A S E}}$ 和 $\mathbf{B E R T}_{\mathbf{LARGE}}$.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert9.png" style="zoom:33%"><p>根据论文中的参数给出对比二者的对比:</p><table><thead><tr><th align="center">模型</th><th align="center">堆叠层数$L$</th><th align="center">隐藏层大小$H$</th><th align="center">自注意力层头数$A$</th><th align="center">共计参数</th></tr></thead><tbody><tr><td align="center">BERT (Base)</td><td align="center">12</td><td align="center">786</td><td align="center">12</td><td align="center">110M</td></tr><tr><td align="center">BERT (Large)</td><td align="center">24</td><td align="center">1024</td><td align="center">16</td><td align="center">340M</td></tr><tr><td align="center">Transformer</td><td align="center">6</td><td align="center">512</td><td align="center">8</td><td align="center">/</td></tr></tbody></table><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert10.png" style="zoom:33%"><p>此外, 在论文脚注中提到, FFN的大小为$4H$, 也采用了<strong>GeLU</strong>作为激活函数, 并使用了10000步的<strong>WarmUp</strong>.</p><h3 id="Input-Output-Representations"><a href="#Input-Output-Representations" class="headerlink" title="Input / Output Representations"></a>Input / Output Representations</h3><h4 id="CLS"><a href="#CLS" class="headerlink" title="[CLS]"></a>[CLS]</h4><p>在BERT中, 永远都将第一个位置输入<strong>分类提示符</strong><code>[CLS]</code>, 如果执行的是分类任务, 第一个位置最终会输出一个向量, 作为分类依据.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert11.png" style="zoom:50%"><p>BERT接受一些系列单词输入, 经过Transformer Encoder的堆叠, 得到输出.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert12.png" style="zoom:50%"><p>其实对于BERT来说, <code>[CLS]</code> 到底放在哪是无所谓的, 只是放在第一个习惯于我们理解.</p><p>在执行<strong>分类下游任务</strong>时(不是训练时), 其他位置无论有多少隐态输出, 我们都忽略, 只看<code>[CLS]</code> 对应位置上的输出, 也就是第一个位置:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert14.png" style="zoom:50%"><p>进行分类任务所搭建的网络也是在第一个位置上继续的, 结构也可以任意调整:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert15.png" style="zoom:50%"><h4 id="SEP"><a href="#SEP" class="headerlink" title="[SEP]"></a>[SEP]</h4><p>对于<strong>任何任务</strong>, BERT的输入永远都是将<strong>一对句子</strong>放在同一个序列中, 无论这对句子是真正连续的上下文还是随机拼接的. BERT句子和句子之间用分隔符<code>[SEP]</code> 隔开, 在结尾也要加上一个<code>[SEP]</code>.</p><h4 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h4><p>输入编码由三个部分组成:</p><ol><li>Token Embedding: 就是每个Token的Embedding.</li><li>Segment Embedding: 该Embedding起到了区分句子A和句子B的作用, 对A和B分别加以不同的编码.</li><li>Position Embedding: 与GPT一样, 位置信息也是学习来的, 而非公式计算.</li></ol><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert2.jpg" style="zoom:67%"><h3 id="Pre-trained-Task"><a href="#Pre-trained-Task" class="headerlink" title="Pre - trained Task"></a>Pre - trained Task</h3><p>BERT采用了两种无监督任务, 使BERT能学到双向的上下文信息, 而且这种信息不是通过Forward和Backward获取的, 而是一次性获得的, 这就促使BERT成为一个双向语言模型. 在论文中, 指出预训练BERT的损失函数为下述两个任务的损失和.</p><h4 id="Masked-Language-Model"><a href="#Masked-Language-Model" class="headerlink" title="Masked Language Model"></a>Masked Language Model</h4><p>在双向深度的结构中, 会伴随着信息泄露, 在原文中称为”<strong>See itself</strong>“, 模型能够通过深层网络来自两个方向的信息得知此处的内容. 为了训练这种网络, BERT训练时采用<strong>随机Mask</strong>的方法, 使BERT必须通过上下文预测出这个位置的Token, 然后用<strong>极大似然</strong>来调整参数. 这其实就是在模仿我们做<strong>完形填空</strong>, 该方法也就是BERT能被称之为双向语言模型的根本原因.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert16.png" style="zoom:50%"><blockquote><p><strong>这种方法为什么有效?</strong></p><p>因为BERT使用的是Transformer Encoder, 与Decoder不同, 是一种不带有自回归性的结构, 在Mask后, 无论怎么阅读句子, 对已经被Mask的位置内容都不可知, 只能强迫BERT根据上下文进行推测.</p></blockquote><p>因此, 该任务中, BERT直接根据上下文推测, 而非像ELMo一样采用两个单向的结构:<br>$$<br>P\left(w_{i} \mid w_{1}, \ldots, w_{i-1}, w_{i+1}, \ldots, w_{n}\right)<br>$$</p><blockquote><p>你看这式子, 是不是和<strong>CBOW</strong>非常像?</p></blockquote><p>但是在Fine tune的时候不可能对单词进行Mask, 这样就会导致预训练和微调的不匹配, 为缓解这种问题, 在每个句子中, 有15%的词会被选中, 在选中单词后有三种可能性:</p><ol><li><strong>80%</strong>的概率将被选中的单词替换为<code>[MASK]</code>.</li><li><strong>10%</strong>的概率将被选中的单词替换为<strong>随机词</strong>.</li><li><strong>10%</strong>的概率对被选中的单词<strong>不进行替换</strong>.</li></ol><p>这就给BERT一种非常迷惑的感觉, “<strong>即使没有Mask的单词仍然有可能是错的, 知道的也要预测, 不知道的还要预测</strong>“. 这就更加强迫BERT除了学到上下文关联外, 对每个词必须有理解能力.</p><p>另外, 随机替换在语料充足时并不会降低太多的模型性能, 因为它只有1.5%的几率发生.</p><p>当然, 因为每次只预测15%的Token, 模型的<strong>收敛速度</strong>会下降.</p><h4 id="Next-Sentence-Prediction"><a href="#Next-Sentence-Prediction" class="headerlink" title="Next Sentence Prediction"></a>Next Sentence Prediction</h4><p>另一种无监督任务比较好理解, 就是让BERT根据句子A和句子B, 在<code>[CLS]</code>处输出这两个句子是否是连贯的上下文. 无论是QA问题, 还是自然语言推理(NLI), 都是建立在理解相邻文本该关系基础之上的.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert17.png" style="zoom:50%"><p>在挑选两个句子时, 句子A与B是否相关各有50%的几率.</p><p>论文中给出示例如下:</p><pre><code>Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]
Label = IsNext
Input = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]
Label = NotNext</code></pre><blockquote><p>##less是word piece产生的, 先挖个坑以后填.</p></blockquote><h3 id="Fine-Tune"><a href="#Fine-Tune" class="headerlink" title="Fine - Tune"></a>Fine - Tune</h3><p>从预训练到微调, BERT可以很轻松的发生转换:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert1.jpg" style="zoom:50%"><p>$C$ 为<code>[CLS]</code>对应位置的最终输出, $T_i$ 代表第$i$ 个Token对应位置的输出. $E_i$ 代表Embedding.</p><p>在不同任务上的微调方式可能是不同的, 但仍然不用对模型结构进行改动:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert3.png"><p>图中的四类任务分别为:</p><ul><li><p>(a): 句子匹配分类任务.</p></li><li><p>(b): 单个句子分类任务.</p></li><li><p>(c): QA类任务. SQuAD中问题的答案一定在原文中会出现, 所以输出的是在原文中起始和结束的位置.</p></li><li><p>(d): 命名体识别任务.</p></li></ul><p>(a)和(b)是Sequence级的任务, (c)和(d)是Token级的任务. BERT在这些任务上都有具体的处理方法, 详情参见论文.</p><h3 id="BERT-in-One-Word"><a href="#BERT-in-One-Word" class="headerlink" title="BERT in One Word"></a>BERT in One Word</h3><p>总的来说, BERT像一个近些年人们在NLP上探索成果的<strong>融合</strong>, 但结合了自监督学习, Token Mask + 双向LM训练, Pre - train + Finetune的思想, 将Transformer, 位置编码一起使用. 现在NLP已经进入到<strong>BERT时代</strong>, 几乎由BERT魔改得到的模型都能取得显著的成果.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>这部分, 我们把ELMo, GPT, BERT三个模型放在一起来看, 可能有些零碎.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/elmogptbert.jpg" style="zoom:67%"><ul><li><p>在三个模型中, 只有BERT是真正能够捕捉所有层的上下文信息的. 这受益于Transformer中的自注意力机制, 将所有Token之间的距离直接缩短到1, 加权求和. ELMo和GPT都是单向捕捉信息的.</p></li><li><p>三个模型的Basic Block: LSTM, Transformer Decoder, Transformer Encoder. GPT和BERT都是用Transformer组件作为基础Block.</p></li><li><p>GPT在预训练时并没有引入<code>[CLS]</code>和<code>[SEP]</code>, BERT全程引入.</p></li><li><p>GPT和BERT是基于<strong>微调</strong>的方法, 模型结构不用发生变化, 而ELMo是基于<strong>特征</strong>的方法, 仅用于抽取特征.</p></li><li><p>对Input来讲, ELMo在Embedding后用<strong>字符级CNN</strong>, GPT采用<strong>BPE</strong>, BERT用了<strong>Word Piece</strong>. GPT有位置编码, BERT有位置编码和段编码. 这些不同也与模型的输入方式有关.</p></li></ul></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/3996.html">https://ADAning.github.io/posts/3996.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"><span class="chip bg-color">词向量</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/35276.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/5.jpg" class="responsive-img" alt="Transformer-XL与XLNet"> <span class="card-title">Transformer-XL与XLNet</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: Transformer(Masked Self - Attention和FFN) BERT(与XLNet做对比) Seq2Seq(AutoRegressive &amp; AutoEncoding) 2020.10.2</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-10-14 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/Transformer/"><span class="chip bg-color">Transformer</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/1216.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/20.jpg" class="responsive-img" alt="Pytorch学习: 张量进阶操作"> <span class="card-title">Pytorch学习: 张量进阶操作</span></div></a><div class="card-content article-content"><div class="summary block-with-text">2020.10.03: 因torch版本更新, 对gather描述进行了修正. 2021.03.11: 更新了对gather的描述. Pytorch学习: 张量进阶操作整理内容顺序来自龙龙老师的&lt;深度学习与PyTorch入门实战教</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-10-03 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/%E7%BC%96%E7%A8%8B/"><span class="chip bg-color">编程</span> </a><a href="/tags/Pytorch/"><span class="chip bg-color">Pytorch</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">413.5k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:andaning@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>