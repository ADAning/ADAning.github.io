<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="通用信息抽取(上) - UIE, USM, InstructUIE, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>通用信息抽取(上) - UIE, USM, InstructUIE | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:.6"></i> <span>时间轴</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li><li><a href="/markdown"><i class="fab fa-markdown" style="margin-top:-20px;zoom:.6"></i> <span>Markdown</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li><li><a href="/markdown" style="margin-left:75px"><i class="fa fab fa-markdown" style="position:absolute;left:50px"></i> <span>Markdown</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/8.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">通用信息抽取(上) - UIE, USM, InstructUIE</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/NER/"><span class="chip bg-color">NER</span> </a><a href="/tags/EE/"><span class="chip bg-color">EE</span> </a><a href="/tags/IE/"><span class="chip bg-color">IE</span> </a><a href="/tags/UIE/"><span class="chip bg-color">UIE</span> </a><a href="/tags/RE/"><span class="chip bg-color">RE</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2024-01-21</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2024-05-28</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 5.7k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 23 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p>2024.5.27: 稍微补充了UIE的其中一个改进版MetaRetriever.</p><p>本文前置知识:</p><ul><li><a href="https://adaning.github.io/posts/58173.html">T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>.</li></ul><p>扩展阅读:</p><ul><li><a href="https://adaning.github.io/posts/4431.html">UniRel: Unified Representation and Interaction for Joint Relational Triple Extraction</a>.</li></ul></blockquote><h1 id="通用信息抽取-上-UIE-USM-InstructUIE"><a href="#通用信息抽取-上-UIE-USM-InstructUIE" class="headerlink" title="通用信息抽取(上) - UIE, USM, InstructUIE"></a>通用信息抽取(上) - UIE, USM, InstructUIE</h1><p>本文为介绍<strong>通用信息抽取领域</strong>经典模型的上篇, 介绍了<strong>UIE</strong>, <strong>USM</strong>, <strong>InstructUIE</strong>三个模型:</p><ul><li><strong>UIE</strong>: <strong>ACL 2022</strong>, <a href="https://aclanthology.org/2022.acl-long.395/" target="_blank" rel="noopener">Unified Structure Generation for Universal Information Extraction</a>.</li><li><strong>USM</strong>: <strong>AAAI 2023</strong>, <a href="https://ojs.aaai.org/index.php/AAAI/article/view/26563" target="_blank" rel="noopener">Universal Information Extraction as Unified Semantic Matching</a>.</li><li><strong>InstructUIE</strong>: 暂挂arxiv, <a href="https://arxiv.org/abs/2304.08085" target="_blank" rel="noopener">InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction</a>.</li></ul><p>三个模型文中报告的实验效果依次递增, 碍于篇幅原因, <strong>本文不包含对实验部分的解读</strong>, 对实验感兴趣的读者还请自行阅读.</p><h2 id="UIE-Unified-Structure-Generation-for-Universal-Information-Extraction"><a href="#UIE-Unified-Structure-Generation-for-Universal-Information-Extraction" class="headerlink" title="UIE: Unified Structure Generation for Universal Information Extraction"></a>UIE: Unified Structure Generation for Universal Information Extraction</h2><p>第一个模型叫做UIE, 论文出自<strong>ACL 2022</strong>, <a href="https://aclanthology.org/2022.acl-long.395/" target="_blank" rel="noopener">Unified Structure Generation for Universal Information Extraction</a>.</p><p>UIE算是打开了近年通用信息抽取的新时代.</p><p>我印象中, 当时是Generative Information Extraction刚冒出头的时候. 比较出名的有NER里面的<a href="https://aclanthology.org/2021.acl-long.451/" target="_blank" rel="noopener">BARTNER</a>, 后来这种方法也被迁移到ABSA中叫<a href="https://aclanthology.org/2021.acl-long.188" target="_blank" rel="noopener">BARTABSA</a>, 它们可以直接从索引中抽取Span, 得到Target. 还有一类是直接做Text2Text的, 比如在EE里面的<a href="https://aclanthology.org/2021.acl-long.217/" target="_blank" rel="noopener">TEXT2EVENT</a>, <a href="https://arxiv.org/abs/2101.05779" target="_blank" rel="noopener">TANL</a>, RE里的<a href="https://aclanthology.org/2021.findings-emnlp.204/" target="_blank" rel="noopener">REBEL</a> 等等, 按照结构化的规则用<strong>Language Modeling</strong>的方式生成Target. 上述两种方法虽然略有不同, 但它们都遵循着将<strong>输出结构线性化, 并结构化生成</strong>的原则, 来处理各类IE task.</p><blockquote><p>这种生成式IE的发展, 和当时<strong>T5</strong>走上舞台以及人们对<strong>In Context Learning</strong>的探索脱不开关系.</p></blockquote><p>那么, 各类任务都可以通过Generative的方式来实现, 这些任务是不是也可以统一到一起呢?</p><p>作者认为, 各类任务是可以在任务形式上得到统一的:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie1.png" style="zoom:80%"><p>很明显的, 对于特定任务下的IE, 它们可以通过作者设定的结构化生成方式, 来将它们完成统一, 以此来利用不同IE task下跨任务的知识, 从而达到只用一个模型来兼顾各种任务的效果.</p><h3 id="Unified-Structure-Generation-for-Universal-Information-Extraction"><a href="#Unified-Structure-Generation-for-Universal-Information-Extraction" class="headerlink" title="Unified Structure Generation for Universal Information Extraction"></a>Unified Structure Generation for Universal Information Extraction</h3><h4 id="Structured-Extraction-Language-for-Uniform-Structure-Encoding"><a href="#Structured-Extraction-Language-for-Uniform-Structure-Encoding" class="headerlink" title="Structured Extraction Language for Uniform Structure Encoding"></a>Structured Extraction Language for Uniform Structure Encoding</h4><p>各类IE任务都可以被转化为text-to-structure的形式, 而任意IE结构生成的过程可以被拆分为两个原子操作:</p><ul><li><strong>Spotting</strong>: 从句子中<strong>定位</strong>任务需要的目标信息片段, 比如一个实体或者触发词的位置.</li><li><strong>Associating</strong>: 判断两个片段之间的<strong>关联</strong>, 比如实体之间的关系, 或者是事件中每个论元扮演的事件角色.</li></ul><p>作者将这两个操作在文本结构化生成中体现出来:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie2.png" style="zoom:67%"><p>上述结构化的文本被作者称为<strong>S</strong>tructured <strong>E</strong>xtraction <strong>L</strong>anguage(<strong>SEL</strong>). 所以SEL就是要找到<code>Spot Name</code>的Span, 并且生成Span和Span之间的<code>Asso Name</code>. 作者通过这种<strong>嵌套的结构化文本</strong>来表征任意的IE任务目标.</p><p>作者在这里展示了一个例子, 对于句子<code>Steve became CEO of Apple in 1997.</code>, 我们来一起完成RE, EE, NER三个经典IE任务:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie3.png" style="zoom:67%"><ul><li>蓝色: RE, 抽取出头实体<code>Steve</code>, 尾实体<code>Apple</code>, 他俩之间的关系, 也就是<code>Asso Name</code> 是<code>work for</code>.</li><li>红色: EE, 抽取出事件类型<code>start-position</code>, 触发词是<code>became</code>, 这个事件下对应的三个论元<code>Steve</code>, <code>Apple</code>, <code>1997</code> 分别扮演<code>employee</code>, <code>employer</code>, <code>time</code> 的事件角色. <strong>所以这里的触发词是以SPOT结构存在的</strong>.</li><li>黑色: NER, 实际上还有蓝色中也有一部分. 能抽取出<code>Steve</code>, <code>Apple</code>, <code>1997</code> 分别为<code>person</code>, <code>organization</code>, <code>time</code> 类型的实体.</li></ul><p>所以, SEL可以把任意的IE任务拆解为<strong>找Span</strong>, 和<strong>判断Span之间的关系</strong>这两个最小的操作.</p><h4 id="Structural-Schema-Instructor-for-Controllable-IE-Structure-Generation"><a href="#Structural-Schema-Instructor-for-Controllable-IE-Structure-Generation" class="headerlink" title="Structural Schema Instructor for Controllable IE Structure Generation"></a>Structural Schema Instructor for Controllable IE Structure Generation</h4><p>在上面一节中可以发现, 不同的任务有不同的Schema, 所以必须要用某种方式把Schema引入, 并令它控制要抽取的内容. 所以作者提出了<strong>S</strong>tructural <strong>S</strong>chema <strong>I</strong>nstructor(<strong>SSI</strong>), 很自然的就能想到把Schema作为Prompt(在这里也可以称为<strong>Prefix</strong>)放到模型当中.</p><p>UIE整体的形式化为, 对于输入文本序列$x=[x_1, \dots, x_{|x|}]$, 和结构化的Schema $s=[s_1, \dots, s_{|s|}]$ , UIE直接将Schema和文本拼接到一起作为输入:</p><p>$$<br>y = \text{UIE}(s \oplus x)<br>$$</p><p>$y=[y_1, \dots, y_{|y|}]$ 为SEL序列.</p><h5 id="Structural-Schema-Instructor"><a href="#Structural-Schema-Instructor" class="headerlink" title="Structural Schema Instructor"></a>Structural Schema Instructor</h5><p>SSI的目的是描述任务, 并引导UIE抽取出任务需要的SEL. 所以它包含三个部分:</p><ul><li>SPOTNAME: 比如NER中的<code>person</code>.</li><li>ASSONAME: 比如RE中的<code>work for</code>.</li><li>特殊Token: 比如<code>[spot]</code>, <code>[asso]</code>, <code>[text]</code>, 用来区分输入文本中的内容到底是归属于哪个部分.</li></ul><p>输入文本Text加上SSI以后的UIE在做RE, EE, NER的例子如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie4.png" style="zoom:67%"><p>UIE的完整输入形式化描述如下:</p><p>$$<br>\begin{aligned}<br>s \oplus x= &amp; {\left[s_1, s_2, \ldots, s_{|s|}, x_1, x_2, \ldots, x_{|x|}\right] } \\<br>= &amp; {[[\text{ spot }], \ldots[\text { spot }] \ldots, } \\<br>&amp; {[\text { asso }], \ldots,[\text { asso }] \ldots, } \\<br>&amp; {\left.[\text { text }], x_1, x_2, \ldots, x_{|x|}\right] }<br>\end{aligned}<br>$$</p><blockquote><p>这种SSI的方式在面对特别多数量的Spot或者Asso的时候, 会导致序列长度变得特别长, 附带一大坨东西, 详情可以看原论文的附录Table 11. 如果碰到这种情况, 对显存是不太友好的.</p></blockquote><h5 id="Structure-Generation-with-UIE"><a href="#Structure-Generation-with-UIE" class="headerlink" title="Structure Generation with UIE"></a>Structure Generation with UIE</h5><p>对于给定的SSI $s$ 和输入句子$x$, UIE用<strong>Encoder-Decoder</strong>架构(T5, BART等为代表)来实现从 <code>SSL + Text -&gt; SEL</code> 的转换. 作者在这里也是认为, 直接将结构化结果以生成式范式输出的好处在于, 可以直接<strong>共享预训练模型中的知识</strong>. 比如<code>location</code> 既可以表征它的自然语义, 也作为<code>[asso]</code>, 所以文本和标签的知识是可以被共享的.</p><p>首先用Encoder得到输入句子的每个Token的表示$\mathbf{H}=\left[\mathbf{s}_1, \ldots, \mathbf{s}_{|s|}, \mathbf{x}_1, \ldots, \mathbf{x}_{|x|}\right]$:</p><p>$$<br>\mathbf{H}=\operatorname{Encoder}\left(s_1, \ldots, s_{|s|}, x_1, \ldots, x_{|x|}\right)<br>$$</p><p>然后是用Decoder自回归的完成解码:</p><p>$$<br>y_i, \mathbf{h}_i^d=\operatorname{Decoder}\left(\left[\mathbf{H} ; \mathbf{h}_1^d, \ldots, \mathbf{h}_{i-1}^d\right]\right)<br>$$</p><p>在输出<code>&lt;eos&gt;</code> 后表示输出终止.</p><h3 id="Pre-training-and-Fine-tuning-for-UIE"><a href="#Pre-training-and-Fine-tuning-for-UIE" class="headerlink" title="Pre-training and Fine-tuning for UIE"></a>Pre-training and Fine-tuning for UIE</h3><h4 id="Pre-training-Corpus-Construction"><a href="#Pre-training-Corpus-Construction" class="headerlink" title="Pre-training Corpus Construction"></a>Pre-training Corpus Construction</h4><p>在预训练阶段的主要语料由以下三部分组成:</p><ul><li>$\mathcal{D}_{\text{pair}}$ 是按照Text-Structure准备好的平行语料, 记为$\mathcal{D}_{\text{pair}}=\set{(x, y)}$. 来自Wikipedia, 用于UIE的预训练, 赋予UIE这种文本转结构化输出的能力.</li><li>$\mathcal{D}_{\text{record}}$ 是只有$y$ 的结构化数据(也就是文中所说的SEL), 来自ConceptNet和Wikidata.</li><li>$\mathcal{D}_{\text{text}}$ 是无结构化文本, 来自Wikipedia.</li></ul><h4 id="Pre-training"><a href="#Pre-training" class="headerlink" title="Pre-training"></a>Pre-training</h4><p>作者以<strong>T5-v1.1</strong>为预训练模型, 完成Seq2Seq任务. 对于上述三种不同的数据格式构成的数据源, 作者分别拿它们过来做了不同的预训练:</p><ul><li>Text-to-Structure Pre-training using $\mathcal{D}_{\text{pair}}$: 作者认为, 只用Positive Schema可能会导致UIE的泛化能力不足, 所以作者还构建了<strong>Negative Schema</strong>来增强UIE的泛化能力. 记Positive Schema为$s_{+}=s_{\mathrm{S}+} \cup s_{\mathrm{a}+}$, 作者随机采样出Negative Spot $s_{\mathrm{S}-}$ 和Negative Asso $s_{\mathrm{a}-}$, 从而构建出Meta Schema $s_{\text{meta}}= s_{+} \cup s_{\mathrm{S}-} \cup s_{\mathrm{a}-}$, 然后在Meta Schema上完成训练:</li></ul><p>$$<br>\mathcal{L}_{\text {Pair }}=\sum_{(x, y) \in \mathcal{D}_{\text {pair }}}-\log p\left(y \mid x, s_{\text {meta }} ; \theta_e, \theta_d\right)<br>$$</p><p>​ 其中$\theta_e, \theta_d$ 分别为Encoder和Decoder的参数.</p><ul><li>Structure Generation Pre-training with $\mathcal{D}_{\text{record}}$: 因为$\mathcal{D}_{\text{record}}$ 只有$y$, 所以UIE Decoder这时候直接单独拿出来用, 当成传统意义上的Language Model来用, 直接生成SEL:</li></ul><p>$$<br>\mathcal{L}_{\text {Record }}=\sum_{y \in \mathcal{D}_{\text {record }}}-\log p\left(y_i \mid y_{&lt;i} ; \theta_d\right)<br>$$</p><ul><li>Retrofitting Semantic Representation using $\mathcal{D}_{\text{text}}$: $\mathcal{D}_{\text{text}}$ 是无结构化文本$x$. 在text-to-structure pre-training时, 作者也像T5一样使用了MLM的Denoising task, 来将损坏的文本恢复出来:</li></ul><p>$$<br>\mathcal{L}_{\text {Text }}=\sum_{x \in \mathcal{D}_{\text {text }}}-\log p\left(x^{\prime \prime} \mid x^{\prime} ; \theta_e, \theta_d\right)<br>$$</p><p>​ 其中$x^\prime$ 为Corrupted Text, $x^{\prime\prime}$ 为重建的目标Span.</p><blockquote><p><strong>作者提到, 这个任务可以有效的解决对<code>SPOTNAME</code> 和 <code>ASSONAME</code> Token语义的灾难性遗忘</strong>.</p><p>作者没有详细解释, 我个人的理解是, 因为Label和Text的知识被共享, 也确实插入了<code>[spot], [asso], [text]</code> 这样的Token来区分某个Token到底是Text还是SSI, 但<strong>Text仍然可能被插入的SSI所污染</strong>, 而且也<strong>容易导致Token在SSI和Text之间的语义混淆</strong>. <strong>MLM任务执行的时候是没有SSI插入的</strong>, 所以我更倾向于认为, 这里用一个MLM的任务是为了保证<strong>Token作为纯文本时的语义</strong>能被一定程度的保留. 而且这个任务是继承T5的, 感觉还是为了保持预训练时候的一些能力.</p></blockquote><p>如果把<code>(SSI, Input, Output)</code>看成一个三元组, 可以将上面三个任务归纳一下:</p><table><thead><tr><th>Data</th><th>SSI</th><th>Input</th><th>Output</th></tr></thead><tbody><tr><td>$\mathcal{D}_{\text{pair}}$</td><td>$s$</td><td>$x$</td><td>$y$</td></tr><tr><td>$\mathcal{D}_{\text{record}}$</td><td>None</td><td>None</td><td>$y$</td></tr><tr><td>$\mathcal{D}_{\text{text}}$</td><td>None</td><td>$x^\prime$</td><td>$x^{\prime\prime}$</td></tr></tbody></table><p>最后的Loss就是上面说的三者加在一起:</p><p>$$<br>\mathcal{L}=\mathcal{L}_{\text {Pair }}+\mathcal{L}_{\text {Record }}+\mathcal{L}_{\text {Text }}<br>$$</p><h4 id="On-Demand-Fine-tuning"><a href="#On-Demand-Fine-tuning" class="headerlink" title="On-Demand Fine-tuning"></a>On-Demand Fine-tuning</h4><p>在预训练过后, UIE需要在某个IE Task上做微调. 对于给定的有标签的语料$\mathcal{D}_{\text {Task }}=\set{(s, x, y)}$, 直接在语料上用Teacher-Forcing做Finetune即可:</p><p>$$<br>\mathcal{L}_{\mathrm{FT}}=\sum_{(s, x, y) \in \mathcal{D}_{\text {Task }}}-\log p\left(y \mid x, s ; \theta_e, \theta_d\right)<br>$$</p><p>为了<strong>缓解曝光偏差</strong>问题, 作者提出了<strong>拒绝机制</strong>. 即在SEL $y$ 中, 以概率$p_\epsilon$ 随机的插入一些Negative spot和Negative asso, 并将它们对应的info span设定为<code>[NULL]</code>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie5.png" style="zoom:80%"><p>如上例中, 如果模型错误的生成了<code>SPOTNAME: facility</code>, 模型仍然在自回归生成的下一步有可能生成<code>[NULL]</code> 来<strong>撤销</strong>这次错误的生成.</p><h3 id="Paddle-UIE"><a href="#Paddle-UIE" class="headerlink" title="Paddle-UIE"></a>Paddle-UIE</h3><p>非常值得一提的是, 这篇工作的一部分是本文的一作和二作在百度实习时完成的, 所以百度在这篇文章中稿后, 也发布了<a href="https://github.com/PaddlePaddle/PaddleNLP/tree/develop/model_zoo/uie" target="_blank" rel="noopener">Paddle版本的UIE</a>, 是一个开箱即用的模型, 并且有各种不同的规模, 非常的方便, 读者可以自行尝试.</p><p>不过Paddle版本的UIE的实现方式并不同, 只是利用了UIE的大致思想, 通过构建Prompt的方式, 并用两个<a href="https://github.com/PaddlePaddle/PaddleNLP/blob/17acf221b44fb5c6284acd5e45ffeac243ead13c/paddlenlp/transformers/ernie/modeling.py#L1222C7-L1222C7" target="_blank" rel="noopener">FFN</a>来<a href="https://github.com/PaddlePaddle/PaddleNLP/blob/17acf221b44fb5c6284acd5e45ffeac243ead13c/model_zoo/uie/deploy/python/infer.py#L71" target="_blank" rel="noopener">轮询每个Schema Element在文中对应的Span</a>, 基座用的则是<strong>ERNIE</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie6.png" style="zoom:80%"><blockquote><p>虽然UIE非常简单, 说白了就是T5 + 结构化生成来做IE任务, 但它确实成为了近两年通用IE中非常具有里程碑意义的方法.</p><p>不得不说, 在LLM兴起的前夕这个节骨眼下, UIE算是给信息抽取任务统一打了个样.</p></blockquote><p>另外, 在<strong>ACL Findings 2023</strong>上也有一个基于检索的UIE改进方法<a href="https://aclanthology.org/2023.findings-acl.251/" target="_blank" rel="noopener">MetaRetriever</a>, 用Meta Learning教会模型从Knowledge Base里面检索知识, 然后用检索到的知识作为上下文再做UIE:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie7.png" style="zoom:80%"><p>所以它是一种检索增强的方法. 如果单从效果上来讲, 不如我们接下来要讲的UIE续作USM.</p><h2 id="USM-Universal-Information-Extraction-as-Unified-Semantic-Matching"><a href="#USM-Universal-Information-Extraction-as-Unified-Semantic-Matching" class="headerlink" title="USM: Universal Information Extraction as Unified Semantic Matching"></a>USM: Universal Information Extraction as Unified Semantic Matching</h2><p>第二个模型叫做USM, 论文出自<strong>AAAI 2023</strong>, <a href="https://ojs.aaai.org/index.php/AAAI/article/view/26563" target="_blank" rel="noopener">Universal Information Extraction as Unified Semantic Matching</a>.</p><p>名字上一看就知道, USM和UIE一脉相承, 事实上USM是<strong>UIE</strong>的续作.</p><p>USM认为, UIE是一个Seq2Seq的黑盒, 非常难以确定什么时候UIE的知识迁移有效, 什么时候无效, 所以需要对知识的显式建模, 以明确知识迁移的有效性, 鲁棒性和可解释性.</p><p>因此, USM将所有IE任务解耦为两个操作, Structuring和Conceptualizing:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/usm1.png" alt=""></p><ul><li><strong>Structuring</strong>: 从句子中抽取和标签无关的基本子结构, 其实就是<strong>抽取文本和文本对</strong>. 比如抽取Entity Mention <code>Monet</code>, 事件触发词<code>born in</code>, 或者某种关系的实体对<code>(Monet, Paris)</code>, 或者事件的论元<code>(born in, Paris)</code>.</li><li><strong>Conceptualizing</strong>: 将子结构与目标语义标签对应, 也就是<strong>对文本或者文本对的标签做判定</strong>. 比如说使得语义标签<code>person</code> 和Entity Mention <code>Monet</code> 相对应, 它的含义就是判断<code>Monet</code> 的实体类型为 <code>person</code>.</li></ul><blockquote><p>其实这两个操作和UIE中的Spotting和Associating的本质是一样的, 都是<strong>找Span</strong>, 和<strong>判断Span之间的关系</strong>. 或者甚至可以说, 信息抽取各类任务的目标就是在找Span和Span之间的关系.</p></blockquote><p>当标签以<strong>语义Token</strong>的形式给定的时候, 上述两个操作可以统一的被抽象成一个<strong>Directed Token Linking</strong>的操作, 并且可以使得所有IE Task共享知识.</p><h3 id="Unified-Semantic-Matching-via-Directed-Token-Linking"><a href="#Unified-Semantic-Matching-via-Directed-Token-Linking" class="headerlink" title="Unified Semantic Matching via Directed Token Linking"></a>Unified Semantic Matching via Directed Token Linking</h3><p>USM的Linking包含三种类型, 再通过Schema约束的解码就可以得到目标输出:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/usm2.png" style="zoom:67%"><h4 id="Schema-Text-Joint-Embedding"><a href="#Schema-Text-Joint-Embedding" class="headerlink" title="Schema-Text Joint Embedding"></a>Schema-Text Joint Embedding</h4><p>首先, 我们用一个Encoder来同时编码<strong>Schema Token</strong> $l=\set{l_1, l_2, \dots, l_{||}}$ 和<strong>Text Token</strong> $t=\set{t_1, t_2, \dots, t_{|t|}}$ 一并拼接后的序列, 得到一个Label-Text联合编码的表示$\mathbf{H}=\left[\mathbf{h}_1, \mathbf{h}_2, \dots, \mathbf{h}_{|l|+|t|}\right]$:</p><p>$$<br>\mathbf{H}=\operatorname{Encoder}\left(l_1, l_2, \ldots, l_{|l|}, t_1, t_2, \ldots, t_{|t|}, \mathbf{M}\right)<br>$$</p><p>其中$\mathbf{M} \in \mathbb{R}^{(|l| + |t|) \times (|l| + |t|)}$ 为Attention的Mask矩阵, 来控制每个Token是否能给予其他Token注意力.</p><h4 id="Directed-Token-Linking"><a href="#Directed-Token-Linking" class="headerlink" title="Directed Token Linking"></a>Directed Token Linking</h4><h5 id="Token-Token-Linking-for-Structuring"><a href="#Token-Token-Linking-for-Structuring" class="headerlink" title="Token-Token Linking for Structuring"></a>Token-Token Linking for Structuring</h5><p>我们在之前说过, Structuring需要抽取句中的<strong>文本</strong>和<strong>文本对</strong>, 它们分别被称为<strong>Utterance</strong>和<strong>Association Pair</strong>(也是Span Pair, 记为<code>(Subject, Object)</code>), 它们均可以用Token Linking的方式实现:</p><ul><li>Utterance的抽取即同一个连续Span的<strong>Head</strong> Token和<strong>Tail</strong> Token的Linking(<strong>H2T</strong>). 比如某个实体的Entity Mention.</li><li>Association Pair的抽取即<strong>两个存在关联的Span</strong>的<strong>Head</strong> Token的Linking(<strong>H2H</strong>)和<strong>Tail</strong> Token的Linking(<strong>T2T</strong>), 也就是<strong>头对头尾对尾</strong>. 比如关系三元组的实体对, 或者是事件中的触发词和论元对.</li></ul><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/usm3.png" style="zoom:67%"><p>综上, 每个Token Pair $\left\langle t_i, t_j\right\rangle$ 是否存在Token-Token Linking (H2T, H2H, T2T), 可以通过计算得分$\mathbf{s}_{\mathrm{TTL}}\left(t_i, t_j\right)$ 来判断:</p><p>$$<br>\mathbf{s}_{\mathrm{TTL}}\left(t_i, t_j\right)=\operatorname{FFNN}_{\mathrm{TTL}}^l\left(\mathbf{h}_t^i\right)^T \mathbf{R}_{j-i} \operatorname{FFNN}_{\mathrm{TTL}}^r\left(\mathbf{h}_t^j\right)<br>$$</p><p>其中, $\operatorname{FFNN}^{l/r}$ 为输出大小为$d$ 的前馈神经网络, $\mathbf{R}_{j-i} \in \mathbb{R}^{d \times d}$ 为<a href="https://spaces.ac.cn/archives/8265" target="_blank" rel="noopener">苏神提出的RoPE</a>, 用于编码Token之间的相对位置关系.</p><h5 id="Label-Token-Linking-for-Utterance-Conceptualizing"><a href="#Label-Token-Linking-for-Utterance-Conceptualizing" class="headerlink" title="Label-Token Linking for Utterance Conceptualizing"></a>Label-Token Linking for Utterance Conceptualizing</h5><p>下面需要对Label和Token之间做Token Linking, 来完成前面提到的Conceptualizing, 这一小节中先处理Utterance.</p><p>对于给定的Label Token Embedding $\mathbf{h}^l_1, \mathbf{h}^l_2, \dots, \mathbf{h}^l_{|l|}$ 和Text Token Embedding $\mathbf{h}^t_1, \mathbf{h}^t_2, \dots, \mathbf{h}^t_{|t|}$, Utterance Conceptualizing需要判断Label Token和Utterance Token或者Association Pair中的<strong>候选Object</strong>之间是否有链接.</p><p>这一操作的就把Text Mention指向了Label, 比如NER中的<code>(person, Monet)</code>, <code>(country, France)</code>, 或者EE中事件类型和触发词构成的元组<code>(born, born in)</code>, RE中关系类型和<strong>尾实体</strong>构成的元组<code>(birth place, Paris)</code>.</p><p>其实上面的例子包含了两种情况:</p><ul><li>把<strong>Span的标签</strong>直接分配给Span, 比如NER的Entity Type分配给Entity Mention.</li><li>把<strong>Span Pair之间的关系标签</strong>分配给<strong>Association Pair中的Object</strong>, 比如RE中把关系类型分配给Object.</li></ul><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/usm4.png" style="zoom:67%"><p>如果做Label和Span的Linking, 则需要分别链接<strong>Label Head</strong> Token和<strong>Span Head</strong> Token(<strong>L2H</strong>), <strong>Label Tail</strong> Token和<strong>Span Tail</strong> Token(<strong>L2T</strong>).</p><p>与前面类似的, 每个Label-Text Token Pair $\left\langle l_i, t_j\right\rangle$ 是否存在Label-Token Linking(L2H, L2T), 通过计算得分$\mathbf{s}_{\mathrm{LTL}}\left(l_i, t_j\right)$ 来判断:</p><p>$$<br>\mathbf{s}_{\mathrm{LTL}}\left(l_i, t_j\right)=\operatorname{FFNN}_{\mathrm{LTL}}^{\text {label }}\left(\mathbf{h}_i^l\right)^T \mathbf{R}_{j-i} \operatorname{FFNN}_{\mathrm{LTL}}^{\text {text }}\left(\mathbf{h}_j^t\right)<br>$$</p><h5 id="Token-Label-Linking-for-Pairing-Conceptualizing"><a href="#Token-Label-Linking-for-Pairing-Conceptualizing" class="headerlink" title="Token-Label Linking for Pairing Conceptualizing"></a>Token-Label Linking for Pairing Conceptualizing</h5><p>这节来处理Association Pair的概念化.</p><p>因为在Utterance Conceptualizing中, 我们已经建立了Label到Association Pair中的Object的链接了, 所以我们只需要将Association Pair中的Subject再有向链接到Label, 就可以根据多条路径来确定一个三元组<code>(Subject, Association, Object)</code>.</p><p>这三条路径分别是:</p><ul><li>Token-Token Linking构造的<code>(Subject, Object)</code>.</li><li>Label-Token Linking构造的<code>(Association, Object)</code>.</li><li>即将在Token-Label Linking中构造的<code>(Subject, Association)</code>.</li></ul><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/usm5.png" style="zoom:67%"><p>如果要从Span链接到Label, 则需要分别链接<strong>Span Head</strong> Token和<strong>Label Head</strong> Token(<strong>H2L</strong>), 以及<strong>Span Tail</strong> Token和<strong>Label Tail</strong> Token(<strong>T2L</strong>).</p><p>类似的, 每个Text-Label Token Pair $\left\langle t_i, l_j\right\rangle$ 是否存在Token-Label Linking(H2L, T2L), 通过计算得分$\mathbf{s}_{\mathrm{TLL}}\left(t_i, l_j\right)$ 来判断:</p><p>$$<br>\mathbf{s}_{\mathrm{TLL}}\left(t_i, l_j\right)=\operatorname{FFNN}_{\mathrm{TLL}}^{\mathrm{text}}\left(\mathbf{h}_i^l\right)^T \mathbf{R}_{j-i} \mathrm{FFNN}_{\mathrm{TLL}}^{\text {label }}\left(\mathbf{h}_j^t\right)<br>$$</p><h4 id="Schema-constraint-Decoding-for-Structure-Composing"><a href="#Schema-constraint-Decoding-for-Structure-Composing" class="headerlink" title="Schema-constraint Decoding for Structure Composing"></a>Schema-constraint Decoding for Structure Composing</h4><p>跟着作者来看一个具体的例子, 感受一下Linking和解码的流程:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/usm6.png" style="zoom:67%"><p>对于输入的Schema<code>[L] person [L] country [L] birth place [L] capital [T]</code> 和输入句子 <code>Monet was born in Paris, the capital of France</code>, 有:</p><ol><li>首先经过Token-Token Linking, 抽取得到<code>Monet</code>, <code>Paris</code>, <code>France</code>, <code>(Monet, Pairs)</code>, <code>(France, Pairs)</code>.</li><li>然后通过Label-Token Linking, 得到<code>(person, Monet)</code>, <code>(country, France)</code>, <code>(birth place, Paris)</code>, <code>(capital, Paris)</code>, 这样就得到了Label对应的候选Object.</li><li>最后用Token-Label Linking, 得到<code>(Monet, birth place)</code>, <code>(France, capital)</code>, 完成Subject和Label的链接.</li><li>由于第一步, 抽取得到<code>(Monet, birth place)</code>, <code>(France, capital)</code>. 并且基于第一步的结果, 抽取得到两个三元组<code>(Monet, birth place, Paris)</code>,<code>(France, capital, Paris)</code>.</li></ol><p>该过程中, 抽取是高度并行的, 第2, 3步解码需要依赖第1步解码的结果.</p><blockquote><p>到这里, USM的模型部分就全部讲解完了. 不知道读者是否能联想到, USM实际上是<a href="https://adaning.github.io/posts/4431.html">UniRel</a>的超集.</p></blockquote><h3 id="Learning-from-Heterogeneous-Supervision"><a href="#Learning-from-Heterogeneous-Supervision" class="headerlink" title="Learning from Heterogeneous Supervision"></a>Learning from Heterogeneous Supervision</h3><h4 id="Pre-training-1"><a href="#Pre-training-1" class="headerlink" title="Pre-training"></a>Pre-training</h4><p>USM采用了三种有监督信号来预训练:</p><ul><li>$\mathcal{D}_{\text{task}}$: <strong>有标训练数据</strong>, 也就是IE特定的数据.</li><li>$\mathcal{D}_{\text{distant}}$: <strong>远程监督数据</strong>, 有文本和知识库对齐的数据, 在这个过程中也使用了UIE中的Meta Schema.</li><li>$\mathcal{D}_{\text{indirect}}$: <strong>间接监督数据</strong>, 由其他可能与IE相关任务的数据组成, 比如MRC相关, KBQA相关的数据.</li></ul><blockquote><p>我感觉这个间接监督用的挺巧, 我估计作者这里用的MRC数据大多是<strong>抽取式MRC</strong>的数据, 任务形式其实和信息抽取是完全一样的.</p><p>作者在文中写到, 每条MRC数据由<code>(question, context, answer)</code> 组成, question就可以当做Label Schema, answer当做Mention, Context当做输入文本. 这样还增强了USM的泛化能力.</p></blockquote><h4 id="Learning-function"><a href="#Learning-function" class="headerlink" title="Learning function"></a>Learning function</h4><p>在预训练时, 所有数据集都可以被表示为$\set{(x_i, y_i)}$, $x_i, y_i$ 分别为文本和USM的Linking标签(就是TTM, LTM, TLM对应的三张表). 因为表中会出现很多负样本Token Pair, 非常稀疏, 所以作者在这里采用了<a href="https://spaces.ac.cn/archives/7359" target="_blank" rel="noopener">苏神的了类别不平衡Loss</a>作为损失函数:</p><p>$$<br>\begin{aligned}<br>\mathcal{L}=\sum_{m \in \mathcal{M}} &amp; \log \left(1+\sum_{(i, j) \in m^{+}} e^{-s_m(i, j)}\right) +\log \left(1+\sum_{(i, j) \in m^{-}} e^{s_m(i, j)}\right)<br>\end{aligned}<br>$$</p><p>其中$\mathcal{M}$ 代表USM的三种Linking Type, $m^+$ 代表有链接的Token Pair, $m^-$ 代表没有链接的Token Pair. $s_m(i, j)$ 代表Linking操作$m$ 下Token Pair $(i, j)$ 之间的Linking得分.</p><p>在特定IE Task上使用时, 还需要继续Finetune.</p><h2 id="InstructUIE-Multi-task-Instruction-Tuning-for-Unified-Information-Extraction"><a href="#InstructUIE-Multi-task-Instruction-Tuning-for-Unified-Information-Extraction" class="headerlink" title="InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction"></a>InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction</h2><p>第三篇论文是InstructUIE, 论文暂挂了arxiv, <a href="https://arxiv.org/abs/2304.08085" target="_blank" rel="noopener">InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction</a>.</p><p>在LLM时代, 有UIE必有InstructUIE.</p><p>作者首先是对比了之前的<strong>UIE</strong>, <strong>USM</strong>, 和本文提出的InstructUIE的区别:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/instructuie1.png" style="zoom:50%"><blockquote><p>作者对于UIE和USM中写的”LLM”都是<strong>广义LLM</strong>, 比如T5-base / large或者BERT-base / large这种级别的模型, 而非LLaMA这样的狭义LLM, InstructUIE采用的才是7B及以上的这种狭义LLM.</p></blockquote><p>作者认为:</p><ul><li>UIE需要在每种任务上进行Finetune, 导致对没见过的Schema或者在低资源场景下表现会差.</li><li>USM有两个缺陷, 第一个是将IE转化为语义匹配任务, 使得USM难以应用在生成类模型下. 第二是需要对每个词都做语义匹配, 导致了训练和推理时间的增加.</li></ul><blockquote><p>我感觉, UIE的缺陷是因为它采用的T5-v1.1 base / large规模不够大, 而作者描述的USM的这两个缺陷也并不存在, 十几B的LLM自回归生成难道会比USM的二三百M的模型时间短吗?</p></blockquote><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>其实LLM来做UIE, 直接用图里的内容就能完全概括:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/instructuie2.png" style="zoom:50%"><p>就是用Instruct Tuning指挥LLM, 在自然语言驱动下完成各类IE任务. InstructUIE采用<strong>FlanT5 11B</strong>作为预训练基座.</p><h3 id="Task-Schema"><a href="#Task-Schema" class="headerlink" title="Task Schema"></a>Task Schema</h3><p>在预训练阶段, InstructUIE采用所有有标数据作为训练数据. 输入到LLM中的内容由以下内容组成:</p><ul><li><strong>Task Instruction</strong>: 用自然语言描述的能指挥LLM完成指定IE任务的Instruct. 比如NER任务中的其中一条Instruct为 <code>Please list all entity words in the text that fit the category. Output format is &quot;type1: word1; type2: word2&quot;.</code>.</li><li><strong>Options</strong>: 对输出标签做的约束, 比如NER的实体类型仅能为<code>person</code>, <code>organization</code> , <code>location</code> 等等.</li><li><strong>Text</strong>: 要预测的句子文本.</li></ul><p>LLM要输出的内容就是任务的直接结果. 不同任务的预期输出不同:</p><ul><li>NER: <code>entity tag: entity span</code>.</li><li>RE: <code>relationship: head entity, tail entity</code>.</li><li>EE: <code>event tag: trigger word, argument tag: argument span</code>.</li><li>当没有任何结构化信息时则输出<code>None</code>.</li></ul><h3 id="Auxiliary-Tasks"><a href="#Auxiliary-Tasks" class="headerlink" title="Auxiliary Tasks"></a>Auxiliary Tasks</h3><p>作者还提出了一系列辅助任务来保证InstructUIE的性能:</p><ul><li><strong>NER</strong>: 额外加入抽取Span, 对实体类型的判断.</li><li><strong>RE</strong>: 抽取有关系存在的实体对, 对实体对之间的关系进行判断.</li><li><strong>EE</strong>: 识别事件的触发词, 识别事件的论元.</li></ul><blockquote><p>其实就是把各类IE任务拆解为粒度更细的子任务, 来提升LLM对这个任务的理解能力.</p></blockquote><h3 id="IE-INSTRUCTIONS"><a href="#IE-INSTRUCTIONS" class="headerlink" title="IE INSTRUCTIONS"></a>IE INSTRUCTIONS</h3><p>作者提出了一个新的数据集IE INSTRUCTIONS, 该数据集收集了在NER, RE, EE上共32个公开可用的数据集, 并保证了它们的多样性, 数据来自于科学, 医疗, 社交媒体, 交通, 新闻百科等领域:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/instructuie3.png" style="zoom:80%"><p>作者主要针对上述数据做出了三项改动:</p><ol><li>统一了各类数据集中语义相同但表述不同的标签名称.</li><li>将特殊格式的标签转换为自然语言的语义标签, 可能需要去掉下划线, 转换缩写等.</li><li>保证了所有任务的输入输出格式均为文本到文本.</li></ol><blockquote><p>注: 读者在阅读InstructUIE的文章时, 需注意实验部分结果分为两部分, 一种是Supervised Settings, 另一种是Zero-shot Settings.</p><p>在Supervised Settings下, InstructUIE先在IE INSTRUCTIONS上做多任务预训练, 后仍会在特定数据集上<strong>全量微调</strong>. Zero-shot Settings下, InstructUIE则在与测试集不相交的数据集上训练, 没有微调.</p><p>所以实际上InstructUIE与UIE, USM相比, 在训练上并无优势, 因为它还是得在特定任务上Finetune.</p></blockquote><p>通用信息抽取未完待续, 挖的坑要填.</p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/11838.html">https://ADAning.github.io/posts/11838.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/NER/"><span class="chip bg-color">NER</span> </a><a href="/tags/EE/"><span class="chip bg-color">EE</span> </a><a href="/tags/IE/"><span class="chip bg-color">IE</span> </a><a href="/tags/UIE/"><span class="chip bg-color">UIE</span> </a><a href="/tags/RE/"><span class="chip bg-color">RE</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="far fa-dot-circle"></i>&nbsp;本篇</div><div class="card"><a href="/posts/11838.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/8.jpg" class="responsive-img" alt="通用信息抽取(上) - UIE, USM, InstructUIE"> <span class="card-title">通用信息抽取(上) - UIE, USM, InstructUIE</span></div></a><div class="card-content article-content"><div class="summary block-with-text">2024.5.27: 稍微补充了UIE的其中一个改进版MetaRetriever. 本文前置知识: T5: Exploring the Limits of Transfer Learning with a Unified Text-to</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2024-01-21 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/NER/"><span class="chip bg-color">NER</span> </a><a href="/tags/EE/"><span class="chip bg-color">EE</span> </a><a href="/tags/IE/"><span class="chip bg-color">IE</span> </a><a href="/tags/UIE/"><span class="chip bg-color">UIE</span> </a><a href="/tags/RE/"><span class="chip bg-color">RE</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/4539.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/16.jpg" class="responsive-img" alt="2024-元旦"> <span class="card-title">2024-元旦</span></div></a><div class="card-content article-content"><div class="summary block-with-text">2024-元旦都有小半年没更新博客了, 已经鸽了好久了… 首先祝大家元旦快乐! 这半年来, 找工作和申博我都试了试, 最后是选择了自己觉得更合适的一条路, 也算是人生中做的一个关键的节点吧. 2023年是LLM横行霸道的一年, 我印象中光是</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2024-01-01 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E5%BF%83%E6%83%85%E9%9A%8F%E7%AC%94/" class="post-category">心情随笔</a></span></div></div><div class="card-action article-tags"><a href="/tags/%E5%BF%83%E6%83%85%E9%9A%8F%E7%AC%94/"><span class="chip bg-color">心情随笔</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">368.5k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:695439722@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>