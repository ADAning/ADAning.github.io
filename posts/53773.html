<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="JiT: Back to Basics-Let Denoising Generative Models Denoise, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>JiT: Back to Basics-Let Denoising Generative Models Denoise | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/3.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">JiT: Back to Basics-Let Denoising Generative Models Denoise</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/Diffusion/"><span class="chip bg-color">Diffusion</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2025-12-18</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2025-12-18</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 2.7k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 11 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p>本文前置知识:</p><ul><li><strong>DDPM</strong>: <a href="https://adaning.github.io/posts/58818.html">DDPM: Denoising Diffusion Probabilistic Model</a>.</li><li><strong>Rectified Flow</strong>: <a href="https://adaning.github.io/posts/24725.html">ReFlow: Flow Straight and Fast-Learning to Generate and Transfer Data with Rectified Flow</a>.</li><li><strong>Flow Matching</strong>: <a href="https://adaning.github.io/posts/19143.html">Flow Matching: Flow Matching for Generative Modeling</a>.</li></ul></blockquote><h1 id="Back-to-Basics-Let-Denoising-Generative-Models-Denoise"><a href="#Back-to-Basics-Let-Denoising-Generative-Models-Denoise" class="headerlink" title="Back to Basics: Let Denoising Generative Models Denoise"></a>Back to Basics: Let Denoising Generative Models Denoise</h1><ul><li>论文: <a href="https://arxiv.org/abs/2511.13720" target="_blank" rel="noopener">Back to Basics: Let Denoising Generative Models Denoise</a>.</li><li>代码: <a href="https://github.com/LTH14/JiT" target="_blank" rel="noopener">GitHub - LTH14/JiT: PyTorch implementation of JiT https://arxiv.org/abs/2511.13720</a>.</li></ul><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>根据流形假设, 自然图像$\boldsymbol{x}$ 是位于高维空间中的低维流形上, 但是噪声$\boldsymbol{\epsilon}$ 是随机分布的, 不会散落在流形上. 由于速度$\boldsymbol{v}$ 一般由$\boldsymbol{x}$ 和 $\boldsymbol{\epsilon}$ 计算得到, 所以模型预测的$\boldsymbol{v}$ 也是流形外的:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/jit.png" style="zoom:67%"><p>本文JiT(<strong>J</strong>ust <strong>i</strong>mage <strong>T</strong>ransformers), 认为直接预测$\boldsymbol{x}$ 比预测$\boldsymbol{v}, \boldsymbol{\epsilon}$ 要好. 基于流形假设, 预测自然图像$\boldsymbol{x}$ 的难度比预测速度$\boldsymbol{v}$ 或者噪声$\boldsymbol{\epsilon}$ 都要低.</p><blockquote><p>之前相当多的工作中探讨过这个问题, 普遍认为$\boldsymbol{x}$ 要更加难以预测. 所以大多数工作才将预测放在Latent Space里面, 做$\boldsymbol{\epsilon}, \boldsymbol{v}$-pred.</p></blockquote><h2 id="On-Prediction-Outputs-of-Diffusion-Models"><a href="#On-Prediction-Outputs-of-Diffusion-Models" class="headerlink" title="On Prediction Outputs of Diffusion Models"></a>On Prediction Outputs of Diffusion Models</h2><h3 id="Background-Diffusion-and-Flows"><a href="#Background-Diffusion-and-Flows" class="headerlink" title="Background: Diffusion and Flows"></a>Background: Diffusion and Flows</h3><blockquote><p>没有基础的可以看<a href="https://adaning.github.io/posts/24725.html">Rectified Flow</a> / <a href="https://adaning.github.io/posts/19143.html">Flow Matching</a>.</p></blockquote><p>假设数据分布$\boldsymbol{x} \sim p_\text{data}(\boldsymbol{x})$, 噪声分布$\boldsymbol{\epsilon} \sim p_{\text{noise}}(\boldsymbol{\epsilon})$, 为先验分布, 例如$\boldsymbol{\epsilon} \sim \mathcal{N}(0, \boldsymbol{I})$. 训练阶段用线性插值采样一个含噪样本$\boldsymbol{z}_t = a_t \boldsymbol{x} + b_t \boldsymbol{\epsilon}$, 一般直接采用线性调度, $a_t=t, b_t=1-t$, 可得:</p><p>$$<br>\boldsymbol{z}_t=t \boldsymbol{x}+(1-t) \boldsymbol{\epsilon}<br>$$</p><p>当$t=1$ 时有$\boldsymbol{z}_t \sim p_{\text{data}}$. 同时令$\text{logit}(t) \sim \mathcal{N}(\mu, \sigma^2)$.</p><p>速度场$\boldsymbol{v}$ 定义为$\boldsymbol{z}$ 的时间导数, 即$\boldsymbol{v}_t = \boldsymbol{z}^\prime_t = a_t^\prime\boldsymbol{x} + b_t^\prime\boldsymbol{\epsilon}$, 根据$\boldsymbol{z}_t$, 有:</p><p>$$<br>\boldsymbol{v}=\boldsymbol{x} - \boldsymbol{\epsilon}<br>$$</p><p>Flow-based方法直接将速度$\boldsymbol{v}_\theta$ 直接由参数化网络$\theta$ 得到, 即$\boldsymbol{v}_\theta=\text{net}_\theta(\boldsymbol{z}_t, t)$, 然后计算网络预测的速度场和真实速度场之间的MSE:</p><p>$$<br>\mathcal{L}=\mathbb{E}_{t, \boldsymbol{x}, \boldsymbol{\epsilon}}\left\Vert\boldsymbol{v}_\theta\left(\boldsymbol{z}_t, t\right)-\boldsymbol{v}\right\Vert^2<br>$$</p><p>因此, 采样可以通过求解ODE完成:</p><p>$$<br>d \boldsymbol{z}_{t} / dt = \boldsymbol{v}_\theta (\boldsymbol{z}_t, t)<br>$$</p><p>从$\boldsymbol{z}_0 \sim p_{\text{noise}}$ 开始, 并从$t=1$ 结束.</p><h3 id="Prediction-Space-and-Loss-Space"><a href="#Prediction-Space-and-Loss-Space" class="headerlink" title="Prediction Space and Loss Space"></a>Prediction Space and Loss Space</h3><h4 id="Prediction-Space"><a href="#Prediction-Space" class="headerlink" title="Prediction Space"></a>Prediction Space</h4><p>网络啥都能学, 预测$\boldsymbol{x}, \boldsymbol{v}, \boldsymbol{\epsilon}$ 都可以, 也就是网络的<strong>Prediction Space</strong>. 对于网络预测的<strong>任意一项已知量</strong>, 可以通过附加两个额外约束来推导所有的三项未知量. 例如, 当网络预测$\boldsymbol{x}$ 时, 有:</p><p>$$<br>\left\{\begin{array}{l}<br>\boldsymbol{x}_\theta=\text {net}_\theta \\<br>\boldsymbol{z}_t=t \boldsymbol{x}_\theta+(1-t) \boldsymbol{\epsilon}_\theta \\<br>\boldsymbol{v}_\theta=\boldsymbol{x}_\theta-\boldsymbol{\epsilon}_\theta<br>\end{array}\right.<br>$$</p><p>可以求解得到$\boldsymbol{\epsilon}_\theta=(\boldsymbol{z}_t-t \boldsymbol{x}_\theta) / (1-t), \boldsymbol{v}_\theta=(\boldsymbol{x}_\theta- \boldsymbol{z}_t) / (1-t)$.</p><h4 id="Loss-Space"><a href="#Loss-Space" class="headerlink" title="Loss Space"></a>Loss Space</h4><p>由于所有量都可以通过网络预测推导而得, 所以网络即使预测$\boldsymbol{x}$ 也不一定非要计算$\boldsymbol{x}$ 的loss, 比如可以预测$\boldsymbol{x}$ 但是通过推导得到$\boldsymbol{v}$, 然后计算$\boldsymbol{v}$ 的loss, 这就是<strong>Loss Space</strong>.</p><p>综上, 作者对预测什么(Prediction Space), 用什么算Loss(Loss Space)做了个穷举:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/jit_1.png" style="zoom:67%"><p>虽然它们从公式上来说是可以互相推出来的, 但是网络每次学起来迭代的时候, 难易程度是不一样的.</p><p>主对角线其实代表了三大经典模型:</p><ul><li>$\boldsymbol{x}$-pred &amp; $\boldsymbol{x}$-loss: 早期所有直接预测原图, 并在原图上算Loss的模型(在Diffusion时代之前的模型大多为这种).</li><li>$\boldsymbol{\epsilon}$-pred &amp; $\boldsymbol{\epsilon}$-loss: DDPM这类以<strong>预测噪声</strong>为目标并优化的模型.</li><li>$\boldsymbol{v}$-pred &amp; $\boldsymbol{v}$-loss: <a href="https://adaning.github.io/posts/19143.html">Flow Matching</a> / <a href="https://adaning.github.io/posts/24725.html">Rectified Flow</a>这种以<strong>学习速度场</strong>为目标的生成类模型.</li></ul><p>但是不管pred / loss怎么组合, 训练完成后的推理的时候在 $\boldsymbol{x} / \boldsymbol{\epsilon} / \boldsymbol{v}$ 上做都可以, 此时称为<strong>Generator Space</strong>. 遵循Flow-based Model求解ODE的习俗, 在$\boldsymbol{v}$ 上做推理采样.</p><h3 id="Toy-Experiment"><a href="#Toy-Experiment" class="headerlink" title="Toy Experiment"></a>Toy Experiment</h3><p>作者希望在Toy Dataset上验证前文的低维流形假设. 令模型分别生成$\boldsymbol{x}, \boldsymbol{v}, \boldsymbol{\epsilon}$, 观察模型生成哪种量最简单.</p><p>具体的, 将低维流形$\hat{\boldsymbol{x}} \in \mathbb{R}^d$ 通过固定随机列正交矩阵$P \in \mathbb{R}^{D \times d}$ 嵌入到更高维的空间$D$ 中, 即将其转换为高维观测数据$\boldsymbol{x} = P \hat{\boldsymbol{x}}$, 然后让模型在未知$P$ 的情况下用$\boldsymbol{x}$ 生成$\hat{\boldsymbol{x}}$.</p><p>模型用的是5层ReLU激活的MLP, hidden size为256, 均采用$\boldsymbol{v}$-loss进行训练, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/jit_2.png" style="zoom:67%"><p>在所有$D$ 的设置下, $\boldsymbol{x}$-pred在降回到二维的时候, 仍然能保持住二维流形结构. 并且当$D=512$ 时(即$D$ 比MLP的hidden size大的时候), 其余两种方法出现了崩溃, 在二维空间中无法还原二维流形. 说明$\boldsymbol{x}$ 确实更有可能位于低维流形上, 因此生成起来要更容易.</p><blockquote><p>这个Toy Experiment说明JiT具有应用到更大Patch, 或者具有更高维数据的潜力.</p></blockquote><h2 id="“Just-Image-Transformers”-for-Diffusion"><a href="#“Just-Image-Transformers”-for-Diffusion" class="headerlink" title="“Just Image Transformers” for Diffusion"></a>“Just Image Transformers” for Diffusion</h2><h3 id="Just-Image-Transformers"><a href="#Just-Image-Transformers" class="headerlink" title="Just Image Transformers"></a>Just Image Transformers</h3><p>JiT Overview确实就是一个Image Transformer:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/jit_3.png" style="zoom:67%"><ul><li>Linear Embed: 构造一个低维的Bottleneck(例如128, 256, 具体大小视数据集大小而定), 将原始数据直接压缩到一个低维流形中, 然后将低维流形的数据投影到Trm Block(<strong>DiT</strong> Block)的Hidden Size.</li><li>将Embedding输入到串行的Trm Block中计算.</li><li>Linear Predict: 只是一层Linear, 直接从Hidden Size投影到像素域.</li><li>但是和其他生成类模型不同的是, JiT学习到的是预测像素域的值$\boldsymbol{x}$, 而不是噪声$\boldsymbol{\epsilon}$, 也不是速度$\boldsymbol{v}$.</li></ul><h3 id="What-to-Predict-by-the-Network"><a href="#What-to-Predict-by-the-Network" class="headerlink" title="What to Predict by the Network?"></a>What to Predict by the Network?</h3><h4 id="x-prediction-is-critical"><a href="#x-prediction-is-critical" class="headerlink" title="x-prediction is critical"></a>x-prediction is critical</h4><p>作者在九宫格设置上进行了实验, 结果如下, 红色代表崩溃:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/jit_4.png" style="zoom:67%"><ul><li>Patch size为4时: 对应的像素域维度为$4^2\times3=48$, 所有方法都可以在ImageNet 64×64上做Work, 各种方法差距不大. 此时JiT-B的Hidden size远大于像素空间维度.</li><li>Patch size为16时: 只有$\boldsymbol{x}$-pred是可以做Work的. 对应的像素空间维度为$16^2\times3 = 768$, 恰和JiT-B的Hidden size相同. 说明JiT的方法比其他方法有更强面对高维输入的处理能力.</li></ul><p>同时, 可以观察到$\boldsymbol{v}$-loss的表现比其他两种要好, 所以可以采用$\boldsymbol{v}$-loss进行训练.</p><blockquote><p>这个结论比较关键, 说明$\boldsymbol{\epsilon}, \boldsymbol{v}$ 在空间中确实不符合流形假设, 相较于$\boldsymbol{x}$ 的建模需要更高的Hidden size.</p></blockquote><h4 id="Noise-level-shift-is-not-sufficient"><a href="#Noise-level-shift-is-not-sufficient" class="headerlink" title="Noise-level shift is not sufficient"></a>Noise-level shift is not sufficient</h4><p>之前有一些方法建议采用<strong>Logit-Normal</strong>来采样$t$, 通过调整$\mu$ 来调整噪声的水平. 作者探究了不同噪声水平对三种pred方式的影响:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/jit_5.png" style="zoom:67%"><p>$\boldsymbol{x}$-pred可以从适当增加噪声中受益, 但通过调整噪声水平无法挽救$\boldsymbol{\epsilon}, \boldsymbol{v}$-pred的崩溃问题.</p><h4 id="Bottleneck-can-be-beneficial"><a href="#Bottleneck-can-be-beneficial" class="headerlink" title="Bottleneck can be beneficial"></a>Bottleneck can be beneficial</h4><p>基于低维流形假设, 作者显式的构造了<strong>Bottleneck</strong>, 然后进行了不同维度Bottleneck对结果的影响:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/jit_6.png" style="zoom:67%"><p>构造Bottleneck有利于提高生成表现, 维度适当的时候, 还可以提升模型性能. 当Bottleneck过小的时候也不会导致模型崩溃.</p><h3 id="JiT’s-Algorithm"><a href="#JiT’s-Algorithm" class="headerlink" title="JiT’s Algorithm"></a>JiT’s Algorithm</h3><p>因此, JiT的核心贡献, 就是下面这个由$\boldsymbol{x}$-pred推出来的$\boldsymbol{v}$-loss:</p><p>$$<br>\begin{aligned}<br>\mathcal{L}&amp;=\mathbb{E}_{t, \boldsymbol{x}, \boldsymbol{\epsilon}}\left\Vert\boldsymbol{v}_\theta\left(\boldsymbol{z}_t, t\right)-\boldsymbol{v}\right\Vert^2 \\<br>\boldsymbol{v}_\theta\left(\boldsymbol{z}_t, t\right)&amp;=\left(\operatorname{net}_\theta\left(\boldsymbol{z}_t, t\right)-\boldsymbol{z}_t\right) /(1-t)<br>\end{aligned}<br>$$</p><p>因为计算$\boldsymbol{v}_\theta(\boldsymbol{z}_t, t)$ 需要除$1-t$, 避免零除, 所以将$1-t$ 的最小值裁剪到0.05.</p><p>训练 / 采样推理伪代码如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/jit_7.png" style="zoom:67%"><ul><li>在Training的时候, 用模型预测<code>x_pred</code>, 并通过公式计算得到<code>v_pred</code>, 将<code>v_pred</code>用于$\boldsymbol{v}$-loss的计算.</li><li>在Inference的时候, 仍然用<code>x_pred</code>计算得到<code>v_pred</code>, 完成欧拉采样. JiT在论文中采用的实际上是Heun而不是Euler.</li></ul><h3 id="“Just-Advanced”-Transformers"><a href="#“Just-Advanced”-Transformers" class="headerlink" title="“Just Advanced” Transformers"></a>“Just Advanced” Transformers</h3><p>JiT里也使用了一些其他工作已经验证有效的方法来加速收敛. 例如<strong>SwiGLU</strong>, <strong>RMSNorm</strong>, <strong><a href="https://adaning.github.io/posts/50765.html">RoPE</a></strong>, <strong>QKNorm</strong>等. 下面是提升:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/jit_8.png" style="zoom:67%"><h2 id="Comparisons"><a href="#Comparisons" class="headerlink" title="Comparisons"></a>Comparisons</h2><p>下面是一些消融 / 包含比较的实验.</p><h3 id="High-resolution-generation-on-pixels"><a href="#High-resolution-generation-on-pixels" class="headerlink" title="High-resolution generation on pixels"></a>High-resolution generation on pixels</h3><p>作者在不同分辨率的ImageNet上, 用相同规模的JiT和不同的Patch size进行了实验:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/jit_9.png" style="zoom:67%"><p>作者发现, JiT的Patch size设置可以随分辨率提升而扩大, <strong>且同时完全不提升Token数量和Backbone参数量</strong>. 当Patch size开到32 / 64的时候, 像素空间维度为3072 / 12288, JiT-B仍然能正常工作.</p><blockquote><p>这种特性可以<strong>显著节省高分辨率图像生成时候的计算成本</strong>.</p></blockquote><p>此外, 从现象可以认为, <strong>JiT很大程度上可以与观测维度解耦</strong>, 所以JiT架构的Scaling增大模型的Hidden size可能也不是必须的.</p><h3 id="Scalability"><a href="#Scalability" class="headerlink" title="Scalability"></a>Scalability</h3><p>作者测试了JiT架构的Scaling能力:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/jit_10.png" style="zoom:67%"><p>除了越大效果越好外, 在JiT-G上的512×512甚至超过了256×256的性能.</p><h3 id="Reference-results-from-previous-works"><a href="#Reference-results-from-previous-works" class="headerlink" title="Reference results from previous works"></a>Reference results from previous works</h3><p>ImageNet 256×256上的结果:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/jit_11.png" style="zoom:67%"><p>ImageNet 512×512上的结果:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/jit_12.png" style="zoom:67%"><p>JiT的优势主要体现在计算量上要大大少于先前的模型, 却拥有差不多的生成质量.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>JiT是一篇争议比较大的论文, 自从它挂在Arxiv上, 知乎争论就不断. 因为它重新在Pixel Space上进行学习, 与目前用Pretrained-VAE在Latent Space上做生成的主流认知是相悖的. 能打破大家的思维定式就是一篇好文章, 无关故事讲的好不好.</p><p>早期的<strong>DiT</strong>之类的工作试过在Latent Space上做大Patch Diffusion, 但是失败了. JiT能Work, 可能说明<strong>目前模型的生成质量上限受制于VAE-Decoder</strong>, 同时也不需要Tokenizer了.</p><p>性能角度上来说, JiT取得了和其他$\boldsymbol{\epsilon}, \boldsymbol{v}$-pred的SOTA方法Comparable的结果. 但是它还是一个非常年轻的框架, 所以还有很大的改进空间.</p><p>个人比较看好的是, 它<strong>具有在大Patch / 高维度数据上建模的潜力</strong>, 而且不过多引入额外的计算成本, 包括但不限于各类模态 / 领域数据.</p><p>推荐继续阅读:</p><ul><li><a href="https://kexue.fm/archives/11428" target="_blank" rel="noopener">生成扩散模型漫谈（三十一）：预测数据而非噪声 - 科学空间||Scientific Spaces</a>.</li><li><a href="https://zhuanlan.zhihu.com/p/1977479109690032906" target="_blank" rel="noopener">何恺明团队新作 JiT 解读与复现：解决大 patch DiT 难以训练的问题 - 周弈帆的文章 - 知乎</a>.</li></ul></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/53773.html">https://ADAning.github.io/posts/53773.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/Diffusion/"><span class="chip bg-color">Diffusion</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="far fa-dot-circle"></i>&nbsp;本篇</div><div class="card"><a href="/posts/53773.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/3.jpg" class="responsive-img" alt="JiT: Back to Basics-Let Denoising Generative Models Denoise"> <span class="card-title">JiT: Back to Basics-Let Denoising Generative Models Denoise</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: DDPM: DDPM: Denoising Diffusion Probabilistic Model. Rectified Flow: ReFlow: Flow Straight and Fast-Learning t</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2025-12-18 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/Diffusion/"><span class="chip bg-color">Diffusion</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/46935.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/14.jpg" class="responsive-img" alt="TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis"> <span class="card-title">TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: Flow Matching: Flow Matching: Flow Matching for Generative Modeling. TCSinger: TCSinger: Zero-Shot Singing Voi</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2025-08-07 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/Audio/"><span class="chip bg-color">Audio</span> </a><a href="/tags/SVS/"><span class="chip bg-color">SVS</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">438k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:andaning@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>