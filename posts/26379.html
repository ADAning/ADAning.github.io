<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="NLP相关知识, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>NLP相关知识 | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon_new.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:.6"></i> <span>时间轴</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li><li><a href="/markdown"><i class="fab fa-markdown" style="margin-top:-20px;zoom:.6"></i> <span>Markdown</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li><li><a href="/markdown" style="margin-left:75px"><i class="fa fab fa-markdown" style="position:absolute;left:50px"></i> <span>Markdown</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/18.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">NLP相关知识</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B/"><span class="chip bg-color">词袋模型</span> </a><a href="/tags/Word2Vec/"><span class="chip bg-color">Word2Vec</span> </a><a href="/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"><span class="chip bg-color">特征工程</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="post-category">机器学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2020-08-17</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2022-04-03</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 2.7k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 10 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p><strong>2020.08.24</strong>: 更新word2vec的部分内容.</p></blockquote><h1 id="NLP相关知识"><a href="#NLP相关知识" class="headerlink" title="NLP相关知识"></a>NLP相关知识</h1><p>整个流程: 分词 Tokenize -&gt; 预处理 Preprocess -&gt; 特征工程 Feature engine -&gt; ML.</p><h2 id="分词-Tokenize"><a href="#分词-Tokenize" class="headerlink" title="分词 Tokenize"></a>分词 Tokenize</h2><p>就是把每个句子按照词语分开, 包括标点. 只有分词后才方便后续对句子的过滤. 中文分词和英文分词是不一样的. 英文分词只需要直接分离标点和空格就行, 中文分词常会因为不同NLP库的处理模式不同而结果不唯一. 有时候分词没那么容易, 在社交语言中和常常会有拼写错误, 缩写, URL, emoji, 单位名称书写不统一… 常常用正则一块处理掉. 值得注意的是, 有时去除它们不一定能带来好的效果.</p><h2 id="文本预处理"><a href="#文本预处理" class="headerlink" title="文本预处理"></a>文本预处理</h2><p>库这用<strong>NLTK</strong>用的多. 对于词形归一和词干提取, 无论是哪种方法都不能正确地处理所有的单词, 常常会引入噪声, 需要结合实际效果而定.</p><h3 id="词形归一-Lemma"><a href="#词形归一-Lemma" class="headerlink" title="词形归一 Lemma"></a>词形归一 Lemma</h3><p>把所有词的变形, 全部都归为一个形式. 通过语言学家的wordnet进行归一化. 但是Lemma的过程中常常会因为不考虑单词词性而归一错误, 这时候就需要附加标注的词性(Pos Tag)进行映射.</p><h3 id="词干提取-Stemming"><a href="#词干提取-Stemming" class="headerlink" title="词干提取 Stemming"></a>词干提取 Stemming</h3><p>简单来说就是直接把不影响词性的词根直接砍掉.</p><h3 id="停止词-Stopwords"><a href="#停止词-Stopwords" class="headerlink" title="停止词 Stopwords"></a>停止词 Stopwords</h3><p>停止词也叫停用词, 在英语里面遇到的a, the, or 等使用频率很高的词基本都是停止词. 去掉停止词后仍然可以表达出句子的意思, 去掉停止词可以节省大量的空间. 但是停止词也不是什么任务都去掉的, 比如判断文章相似度之类或者给文章打分的任务就不应该去除停止词, 因为去除后会导致句子结构发生变化.</p><h2 id="特征工程-Feature-Engineering"><a href="#特征工程-Feature-Engineering" class="headerlink" title="特征工程 Feature Engineering"></a>特征工程 Feature Engineering</h2><h3 id="基本语义特征"><a href="#基本语义特征" class="headerlink" title="基本语义特征"></a>基本语义特征</h3><p>主要是一些句子上的差别. 将各种描述句子的特征加加减减.</p><p>问题1和问题2的符号差异, 问号差异, 问题1和问题2分别的句子长度, 长度差异, 长度差异率, 字符数量差异, 字符差异率, 情感分析差异, 起始词(疑问词)的差异, 共享词的交, 并, 数量差异, 数量差异率, fuzz_qratio, fuzz_WRatio, fuzz_partial_ratio, partal_token_sort_ratio, token_set_ratio…</p><p>距离特征有cosine_word2vec, cityblock_distance, canberra_distance, euclidean_distance, braycurits_distance, minkowski_distance, skew_q1, skew_q2, kur_q1, kur_q2, wmd.</p><h3 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h3><p>TFIDF(Term Frequency - Inverse Document Frequency) 词频 - 逆文本频率. TF即词的词频, IDF可以帮助我们理解这个词的重要程度.</p><p>TF(Term Frequency): 一个term在文档中出现的频繁程度. 但是词频并不能反映这个词语的重要性, 有时候它们出现很多次, 但却没有什么意义比如停用词, 没有意义的词语没法起到文本分类的作用.</p><p>对于词语$i$, 在文档$j$ 有:<br>$$<br>TF_{i, j} = \frac{n_{i, j}}{\sum_k n_{k, j}} = \frac{某个词在文章中出现的次数}{文章的总词数}\<br>$$<br>所以就需要将他们乘个缩放因子, 来平衡掉出现次数过多但无意义的影响.</p><p>IDF(Inverse Document Frequency) 逆文档频率, 它与一个词常见程度成反比, 这样越普遍的词语, 越没有实际意义. 对于语料库文章总数$\left| D\right|$, 包含词语$j$ 的文档数$\left| j:t_i \in d_j \right|$ 有:<br>$$<br>IDF_i= \log{\frac{\left | D \right |}{1+\left| j:t_i \in d_j \right|}} = \log{\frac{语料库的文档总数}{包含该词的文档数+1}}<br>$$<br>加1是为了规避词语不在语料库中分母为0的情况.</p><p>而TF-IDF就是TF和IDF的乘积. 能够看出来, 某个词语在某篇文章出现的次数越多越重要, 在所有文章中出现的次数越多越不重要.<br>$$<br>TFIDF = TF \times IDF<br>$$<br>但是TF-IDF也有缺陷, 它忽略了文本中词语的位置信息. 有些文章的段首明显句首的权重更高. 其次有些文章的关键词可能只出现了1-2次.</p><h3 id="词袋模型-Bag-of-words-model"><a href="#词袋模型-Bag-of-words-model" class="headerlink" title="词袋模型 Bag of words model"></a>词袋模型 Bag of words model</h3><p>词袋模型(Bag of words model) 将每段文本都由装着词的袋子表示, 对于比如对于以下文本:</p><ol><li>John likes to watch movies. Mary likes movies too.</li><li>John also likes to watch football games.</li></ol><p>能够生成一个含有10个不同词语的词表:</p><p>[John, likes, to, watch, movies, also, football, games, Mary, too]</p><p>然后结合单词出现的次数, 能够将句子表示为:</p><ol><li>[1, 2, 1, 1, 2, 1, 1, 0, 0, 0]</li><li>[1, 1, 1, 1, 0, 0, 0, 1, 1, 1]</li></ol><p>词袋模型能够将文本转化为向量, 但是却没有保留文本之间的语序, N元语法对这个问题进行了改善.</p><h3 id="N-Gram"><a href="#N-Gram" class="headerlink" title="N-Gram"></a>N-Gram</h3><p>N元语法(N-Gram)基于N-1马尔科夫假设, 即句子中的第$n$个单词被认为和前$m$个单词相关, 即:<br>$$<br>P(x_1, x_2,… , x_n) = P(x_1)P(x_2|x_1)\cdots P(x_n|x_{n-m},…,x_{n-1})<br>$$<br>如果一个词出现只依赖于它前面的一个词, 称为Bi-gram, 如果依赖于前面的两个词, 称为Tri-gram.</p><p>一般就采用$N=2$或$N=3$即Bi-gram和Tri-gram. 用极大似然估计来计算频率, 有:<br>$$<br>\begin{aligned}<br>&amp;p(w_n|w_{n-1})=\frac{C(w_{n-1}w_n)}{C(w_{n})}\\<br>&amp;p(w_n|w_{n-1}w_{n-2})=\frac{C(w_{n-2}w_{n-1}w_n)}{C(w_{n-2}w_{n-1})}\\<br>&amp;p(w_n|w_{n-1}\cdots w_2w_1)=\frac{C(w_1w_2\cdots w_n)}{C(w_1w_2\cdots w_{n-1})}<br>\end{aligned}<br>$$<br>举个二元语法的例子:</p><blockquote><ol><li>oh I am Sam oh</li><li>oh Sam I am oh</li><li>oh I do not like eggs and ham. oh</li></ol></blockquote><p><code>I</code> 出现了三次, <code>I am</code> 出现了两次, 所以求得$p(am|I) = \frac{C(I\ am)}{C(am)}=\frac{2}{3}$, 同样计算出:<br>$$<br>\begin{aligned}<br>P(I|oh) &amp;= \frac{2}{3}, P(Sam|am)=\frac{1}{2}, P(oh|Sam)=\frac{1}{2}\\<br>P(do|I)&amp;=\frac{1}{3}, P(not|do)=\frac{1}{1}, P(like|not)=\frac{1}{1}<br>\end{aligned}<br>$$</p><h3 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h3><p>Word to vector是一种浅层神经网络, <strong>构建一个保留了单词的上下文相似性的低维向量来表示语料库中的文本</strong>. 在训练完成后, Word2Vec可以映射每个词到一个向量(也就是词向量). 词向量一般是稠密的, 低维(不像One-hot维数很高). Word2Vec是<strong>词嵌入</strong>(Word Embedding)的一种. 在词嵌入空间当中, 词意相似的词语通常具有相同的方向, 比如各种水果可能在空间中的位置类似. 值得一提的是, Embedding这种技术在人脸识别当中也叫作编码, 经常将人脸图片在空间中编码为一个向量, 与其他的向量进行比对, 也是利用相似度算法判断是否为该人, 原理实际一致.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/word2vec.jpg" style="zoom:50%"><p>整体步骤如下:</p><ol><li>在Input Layer, 某个词语被转化为One-Hot向量(高维, 稀疏)</li><li>在Hidden Layer, 稀疏向量被做了一次<strong>线性变换</strong>, 即$Wx+b$. 或者视为隐藏层神经元不激活.</li><li>获得对应词语在稠密空间的映射.</li></ol><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/embedding1.png" style="zoom:50%"><p>由于输入向量是一个独热稀疏向量, 那么实际上就可以直接获得Hidden Layer中的<strong>唯一神经元权重被激活</strong>, 也就对应了唯一的词向量, 完成这个映射过程, 也就获得了这个词所对应的embedding形式. 如下图左侧矩阵是某个词的独热向量, 右侧是词嵌入矩阵, 二者做矩阵乘法可以获得这个词所对应的嵌入式表示.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/embedding2.png" style="zoom:67%"><p>Xin Rong制作了展示<a href="https://ronxin.github.io/wevi/" target="_blank" rel="noopener">词嵌入是如何训练的</a>的网站.</p><p>现在基本上Word2Vec有两种变体用的是最多的, 一种是CBOW, 一种是Skip-gram. 两种算法在进行优化时所采用的函数不同, 这二者的结构是完全相反的. 详见<a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">原论文</a>.</p><h4 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h4><p>CBOW(Continuous Bag-of-Words Model), CBOW是<strong>通过上下文来预测中心词</strong>. 即在已知上文$w_{t-c}, w_{t-c+1}, \dots, w_{t-1}$和下文$w_{t+1}, w_{t+2}, \dots, w_{t+c}$1的情况下预测中心词$w_t$.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/cbow.jpg" style="zoom:50%"><p>在中间的隐藏层中, 我们对$w_{t-c}, \dots, w_{t-1}, w_{t+1}, \dots, w_{t+c}$的向量相加, 并除以$2c$. 最终输出层经过Softmax得到一个近似独热的向量.</p><p>根据极大似然, 对于单词集合$T$, 最终要最大化:<br>$$<br>L=\frac{1}{T}\sum_t{\log{P(w_t|w_{t-c}\cdots w_{t+c})}}<br>$$</p><h4 id="Skip-Gram"><a href="#Skip-Gram" class="headerlink" title="Skip-Gram"></a>Skip-Gram</h4><p>Skip-gram通过<strong>中心词预测上下文</strong>.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/skipgram.png" style="zoom:50%"><p>最后也是通过Softmax得出来很多近似独热的向量, 也是最大化如下函数:<br>$$<br>J=\frac{1}{T}\sum_{t=1}^T{\sum_{-c \leq j \leq c, j \neq 0}{\log p(w_{t+j}|w_t)}}<br>$$</p><h4 id="相似度算法"><a href="#相似度算法" class="headerlink" title="相似度算法"></a>相似度算法</h4><h5 id="余弦相似度"><a href="#余弦相似度" class="headerlink" title="余弦相似度"></a>余弦相似度</h5><p>对于余弦$\cos$想必都是了解的, 其实余弦相似度就是通过<strong>两个向量之间的余弦值</strong>来衡量相似度. 两个向量越不相似, 夹角就越大, 余弦值也越大. 这个向量可以是来自于任何文本向量化后的向量(比如TF-IDF和Word2Vec).</p><p>由向量内积:<br>$$<br>a\cdot b = \vert \vert a \vert \vert \ \vert \vert b \vert \vert \cos{\theta}<br>$$<br>有:<br>$$<br>similarity = \cos{\theta} = \frac{A \cdot B}{\vert\vert A\vert\vert \ \vert\vert B\vert\vert} = \frac{\sum\limits_{i=1}^nA_i\times B_i}{\sqrt{\sum\limits_{i=1}^n(A_i)^2}\times \sqrt{\sum\limits_{i=1}^n(B_i)^2}}<br>$$<br>相似度的范围在$(-1, 1)$之间, 1表示它们完全相同, 0表示相互独立, -1表示完全相反. 如果是$TF-IDF$下的向量, 由于不能为负数, 两个向量角度不大于90度.</p><h5 id="Jaccard相似度"><a href="#Jaccard相似度" class="headerlink" title="Jaccard相似度"></a>Jaccard相似度</h5><p>Jaccard相似度是在没有将文本数据向量化之前的一种度量. 它处理的是<strong>集合</strong>, 在NLP问题上就是两个句子的词语集合. Jaccard相似度是由Jaccard系数引出来的.<br>$$<br>\displaylines{<br>J(A, B) = \frac{|A \cap B|}{|A \cup B|}\\<br>d_j(A, B) = 1 - J(A, B) = \frac{|A \cup B| - |A\cap B|}{|A \cup B|}<br>}<br>$$<br>与余弦相似度不同的是, <strong>面对重复的词, Jaccard相似度不会有影响</strong>, 因为它是集合上的运算, 自带去重的效果, 如果是余弦相似度则会受到影响.</p><h5 id="词移距离"><a href="#词移距离" class="headerlink" title="词移距离"></a>词移距离</h5><p>词移距离WMD(Word Move Distance)是基于Word2vec特性开发出来的, 当单词经过Word2Vec映射成一个词向量的时候, 语义相近的单词距离会比较近, 比如king和queen在词向量空间中的距离就比sky和apple的近.</p><p>我在项目中用到的是直接用所有单词的词向量加权求和, 然后再用欧氏距离进行比较, 就能衡量两个句子的相似度, 还有用TF-IDF作为权重, 加权求和. 但是词移距离后续的叙述很麻烦, 不再详细说了, 因为涉及到求解和词转移代价等.</p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">AnNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/26379.html">https://ADAning.github.io/posts/26379.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">AnNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B/"><span class="chip bg-color">词袋模型</span> </a><a href="/tags/Word2Vec/"><span class="chip bg-color">Word2Vec</span> </a><a href="/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"><span class="chip bg-color">特征工程</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/59216.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/20.jpg" class="responsive-img" alt="DL目标检测"> <span class="card-title">DL目标检测</span></div></a><div class="card-content article-content"><div class="summary block-with-text">目标检测目标检测是CV里一个重要方向, 对于一张图片, 我们应该能够给出图中含有的物体(单个或多个)的位置以及他们的大小和类别. 目标定位假设我们已经能够利用CNN对一张图片是否含有某个物体而进行分类. 应该先搞清楚要的输出是什么. 在上</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-08-18 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/CV/"><span class="chip bg-color">CV</span> </a><a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"><span class="chip bg-color">目标检测</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/36969.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/1.jpg" class="responsive-img" alt="机器学习之XGBoost"> <span class="card-title">机器学习之XGBoost</span></div></a><div class="card-content article-content"><div class="summary block-with-text">XGBoostXGBoost是Extreme Gradient Boosting的缩写, 作者是陈天奇大神. XGB因为其高准确率, 易于使用而在各类数据科学竞赛譬如Kaggle, 天池等十分流行. XGB与GBDT十分相似, 可以将XGB</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-08-16 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="post-category">机器学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"><span class="chip bg-color">决策树</span> </a><a href="/tags/XGB/"><span class="chip bg-color">XGB</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">AnNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">362.2k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:695439722@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>