<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="CLAP: Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>CLAP: Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/15.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">CLAP: Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/Audio/"><span class="chip bg-color">Audio</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2025-02-07</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2025-02-07</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 1.7k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 7 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><h1 id="Large-scale-Contrastive-Language-Audio-Pretraining-with-Feature-Fusion-and-Keyword-to-Caption-Augmentation"><a href="#Large-scale-Contrastive-Language-Audio-Pretraining-with-Feature-Fusion-and-Keyword-to-Caption-Augmentation" class="headerlink" title="Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation"></a>Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation</h1><p>本文是论文<a href="https://ieeexplore.ieee.org/abstract/document/10095969" target="_blank" rel="noopener">Large-Scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation</a>的阅读笔记和个人理解, 论文本身比较简单.</p><p><a href="https://adaning.github.io/posts/44986.html#CLIP">CLIP</a> 打通了文本与视觉之间的连接桥梁, 不难想到在<strong>文本与音频</strong>这两个结合紧密的领域中也得有一个模型来完成跨模态连接的问题.</p><p>于是CLAP(Contrastive Language-Audio Pretraining)应运而生, 这个模型和名字很有意思, 因为在ICASSP 2023上有两篇同样名字为CLAP的模型同时出现.</p><p>分别是:</p><ul><li><strong>LAION-CLAP</strong>: <a href="https://ieeexplore.ieee.org/abstract/document/10095969" target="_blank" rel="noopener">Large-Scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation</a>, 来自LAION-AI.</li><li><strong>MS-CLAP</strong>: <a href="https://ieeexplore.ieee.org/abstract/document/10095889" target="_blank" rel="noopener">CLAP: Learning Audio Concepts From Natural Language Supervision</a>, 来自微软.</li></ul><p>在<a href="https://huggingface.co/docs/transformers/model_doc/clap" target="_blank" rel="noopener">抱抱脸上的提供的CLAP</a>是<strong>LAION-CLAP</strong>, 流传和影响力也更为广泛.</p><h2 id="CLAP"><a href="#CLAP" class="headerlink" title="CLAP"></a>CLAP</h2><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>CLAP框架图如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/clap1.png" style="zoom:50%"><p>明显的, CLAP也是遵循CLIP结构的<strong>双塔结构</strong>与训练方式:</p><ul><li>Audio Encoder会用Conv2d和一些其他的Encoder Layer(最后选择的起始是类似Swin Transformer的<a href="https://ieeexplore.ieee.org/abstract/document/9746312/" target="_blank" rel="noopener">HTS-AT</a>)完成对Audio的编码. 在时间长不同的情况下会有不同的处理流程.</li><li>Text Data被Keyword-to-Caption Augmentation增强后再送入Text Encoder中编码.</li><li>最后用Audio Encoder获得的表示和Text Encoder的表示通过MLP做Contrastive Learning.</li></ul><h3 id="Contrastive-Language-Audio-Pretraining"><a href="#Contrastive-Language-Audio-Pretraining" class="headerlink" title="Contrastive Language - Audio Pretraining"></a>Contrastive Language - Audio Pretraining</h3><p>与<a href="https://adaning.github.io/posts/44986.html#CLIP">CLIP</a>类似, 用两个两层的MLP将Audio Feature $X_i^a$ &amp; Text Feature$X_i^t$ 投影到多模态对齐的语义空间:</p><p>$$<br>\begin{aligned}<br>E_i^a &amp; =M L P_{\text {audio }}\left(f_{\text {audio }}\left(X_i^a\right)\right) \\<br>E_i^t &amp; =M L P_{\text {text }}\left(f_{\text {text }}\left(X_i^t\right)\right)<br>\end{aligned}<br>$$</p><p>其中$f(\cdot)$ 为Audio / Text Encoder.</p><p>接着用Contrastive Loss:</p><p>$$<br>L=\frac{1}{2 N} \sum_{i=1}^N\left(\log \frac{\exp \left(E_i^a \cdot E_i^t / \tau\right)}{\sum_{j=1}^N \exp \left(E_i^a \cdot E_j^t / \tau\right)}+\log \frac{\exp \left(E_i^t \cdot E_i^a / \tau\right)}{\sum_{j=1}^N \exp \left(E_i^t \cdot E_j^a / \tau\right)}\right)<br>$$</p><p>$\tau$ 为可学的温度系数, $N$ 为样本总数, 实际训练中为Batch Size.</p><h3 id="Downstream-Tasks-in-Inference-Stage"><a href="#Downstream-Tasks-in-Inference-Stage" class="headerlink" title="Downstream Tasks in Inference Stage"></a>Downstream Tasks in Inference Stage</h3><p>各类下游任务在Inference时候分别这样做:</p><ul><li>Text-to-Audio Retrieval: 获得Audio Embedding $E_p^a$, 并用余弦相似度找到在$E^t = \{E_1^t, \dots, E_M^t\}$ 内最相近的$E_q^t$.</li><li>Zero-shot Audio Classification: 对于M个Audio Class $C = \{C_1, \dots, C_M\}$, $X^t$, 构建M个Prompt, 比如”the sound of <code>class name</code>“,即$X^t = \{X_1^t, \dots, X_M^t\}$, 然后就可以像Text-to-Audio Retrieval一样, 对于给定的 $X_p^a$, 找到最临近的Text $X_q^t$, 以此完成Zero-shot Audio Classification.</li><li>Supervised Audio Classification: 对于给定的$X_p^a$, 获得它的Embedding $E_p^a$, 然后直接接一个Linear Head做有监督分类.</li></ul><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/clap2.png" style="zoom:33%"><h3 id="Audio-Encoders-and-Text-Encoders"><a href="#Audio-Encoders-and-Text-Encoders" class="headerlink" title="Audio Encoders and Text Encoders"></a>Audio Encoders and Text Encoders</h3><p>有很多种方法都可以当Audio Encoder和Text Encoder.</p><ul><li>Audio Encoder: PANN(基于CNN的模型), <a href="https://ieeexplore.ieee.org/abstract/document/9746312/" target="_blank" rel="noopener">HTS-AT</a>.</li><li>Text Encoder: <a href="https://adaning.github.io/posts/44986.html#CLIP">CLIP</a>, <a href="https://adaning.github.io/posts/3996.html#BERT">BERT</a>, <a href="https://adaning.github.io/posts/24649.html">RoBERTa</a>.</li></ul><p>作者会在实验中组合尝试.</p><h3 id="Feature-Fusion-for-Variable-Length-Audio"><a href="#Feature-Fusion-for-Variable-Length-Audio" class="headerlink" title="Feature Fusion for Variable - Length Audio"></a>Feature Fusion for Variable - Length Audio</h3><p>对于输入总长度为$T$ s的音频, 作者设定固定长$d=10$ s. 对于不同长度的音频, CLAP有不同的处理方法(其实是包含关系):</p><ul><li>对于$T\leq d$ s的音频, CLAP会将其直接重复若干次, 然后再做Zero Padding. 例如一条3s的音频, 将其重复3次, 然后再Padding 1s. 最后用一个Conv2d提取特征.</li><li>对于$T &gt; d$ s的音频, CLAP先将$T$ s的音频降采样到$d$ s作为全局输入, 随机选取三条$d$ s的音频切片, 分别从前, 中, 后各取一条. 全局输入会被用一个Conv2d做Projection(此时就是$T \leq d$ 的情况), 得到Global Feature $X_{\text {global }}^a$, 剩下三条局部输入会被用一个额外的Conv2d聚合, 得到$X_{\text {local }}^a$.</li></ul><p>最后再将两种特征融合:</p><p>$$<br>X_{\text {fusion }}^a=\alpha X_{\text {global }}^a+(1-\alpha) X_{\text {local }}^a<br>$$</p><p>其中系数$\alpha = f_{AFF}(X^a_{global}, X^a_{local})$ 由Attention Feature Fusion(AFF)结构得到:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/clap3.png" style="zoom:67%"><h3 id="Training-Dataset"><a href="#Training-Dataset" class="headerlink" title="Training Dataset"></a>Training Dataset</h3><p>作者使用了三个数据集作为训练数据:</p><ol><li>Audioset: 大约1.9M的音频样本, 每个音频只有标签.</li><li>AudioCaps+Clotho (AC+CL): 大约有55K的Audio-Text Pairs.</li><li>LAION-Audio-630K: 收集了共含63K的Audio-Text Pairs, 共计4325小时. 从公开源中收集了人类活动, 自然声音, 音频音效等. 其中也有对应的相关文本描述, 其大小与之前的公开数据集对比如下:</li></ol><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/clap4.png" style="zoom:33%"><p>作者设定所有音频的采样率均为48kHz, 并且将仅有类别标签可用的数据标签转换为”The sound of <code>label-1</code>, <code>label-2</code>, …, <code>label-n</code>“的Caption, 或者是用下面提到的Keyword-to-Caption Augmentation将这些Label转换为Caption.</p><h3 id="Keyword-to-Caption-Augmentation"><a href="#Keyword-to-Caption-Augmentation" class="headerlink" title="Keyword-to-Caption Augmentation"></a>Keyword-to-Caption Augmentation</h3><p>某些数据集提供了对音频的关键字, 作者使用T5将Keywords转换为Captions(Keyword-to-Caption Augmentation, <strong>K2C Aug</strong>.), 并去除了一些性别偏见, 并进行后处理. 因此可以将Audio Sample和Text Caption的总数扩充到2.5M.</p><p>转换完的一些例子如下所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/clap5.png" style="zoom:67%"><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的模型参数设置和实验设置请参考原论文.</p><h3 id="Text-to-Audio-Retrieval"><a href="#Text-to-Audio-Retrieval" class="headerlink" title="Text-to-Audio Retrieval"></a>Text-to-Audio Retrieval</h3><h4 id="Audio-and-Text-Encoders"><a href="#Audio-and-Text-Encoders" class="headerlink" title="Audio and Text Encoders"></a>Audio and Text Encoders</h4><p>为对比不同Text Encoder的效果, 作者采用PANN, <a href="https://ieeexplore.ieee.org/abstract/document/9746312/" target="_blank" rel="noopener">HTS-AT</a>作为Audio Encoder, 以<a href="https://adaning.github.io/posts/44986.html#CLIP">CLIP</a> 的Text Encoder, <a href="https://adaning.github.io/posts/3996.html#BERT">BERT</a>, <a href="https://adaning.github.io/posts/24649.html">RoBERTa</a>作为Text Encoder, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/clap6.png" style="zoom:50%"><p>CLIP Text Encoder表现很差, 我怀疑真实原因可能是因为它在VL上已经做过超大规模对齐. 作者也认为是CLIP在原来的VL大规模数据上过拟合严重. <a href="https://adaning.github.io/posts/24649.html">RoBERTa</a>表现最好, 这与它们在纯文本任务上的表现类似.</p><p>因此, CLAP的双塔实际上由<a href="https://ieeexplore.ieee.org/abstract/document/9746312/" target="_blank" rel="noopener">HTS-AT</a>和<a href="https://adaning.github.io/posts/24649.html">RoBERTa</a>共同组成.</p><h4 id="Dataset-Scaling-amp-Keyword-to-Caption-and-Feature-Fusion"><a href="#Dataset-Scaling-amp-Keyword-to-Caption-and-Feature-Fusion" class="headerlink" title="Dataset Scaling &amp; Keyword-to-Caption and Feature Fusion"></a>Dataset Scaling &amp; Keyword-to-Caption and Feature Fusion</h4><p>进一步观察Dataset Scaling和K2C &amp; Feature Fusion带来的影响, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/clap7.png" style="zoom:33%"><p>数据集增大后, 效果变得更好. 加入LA后再AudioCaps上反而有下降, 作者认为是Audio Encoder的预训练权重来自AudioSet, AudioCaps包含与AudioSet相似的音频. 所以此时CLAP泛化能力提高, 但在AudioCaps上的适应性变差了, 所以Clotho上才会有明显提高.</p><p>此外, 观察到单独加Fusion和单独加K2C aug后, 都能使得模型的表现更好.</p><h3 id="Zero-shot-and-Supervised-Audio-Classification"><a href="#Zero-shot-and-Supervised-Audio-Classification" class="headerlink" title="Zero-shot and Supervised Audio Classification"></a>Zero-shot and Supervised Audio Classification</h3><p>在音频分类数据集上的Supervised和Zero-Shot表现如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/clap8.png" style="zoom:33%"><p>Fusion和K2C加在CLAP上都分别有提升, 而且ZS表现相较于之前的模型有很大提升.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>CLAP作为音频模态的CLIP, 不光提供了一个非常好的<strong>Audio-Text Backbone</strong>, 也提供了一个大规模数据集<strong>LAION-Audio-630K</strong>. CLAP的存在使得后续一些下游任务和与多模态相关的音频任务获得了更多的可能.</p><p>另外, 同样的idea, LAION-CLAP和MS-CLAP做出来差别还是挺大的, 可能是因为大家在乎的点不太一样… 实验结果上显示MS-CLAP要弱于LAION-CLAP, 在其他Audio Research中LAION-CLAP也是用的更多些.</p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/21614.html">https://ADAning.github.io/posts/21614.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/Audio/"><span class="chip bg-color">Audio</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/50765.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/3.jpg" class="responsive-img" alt="RoPE / RoFormer: Enhanced Transformer with Rotary Position Embedding"> <span class="card-title">RoPE / RoFormer: Enhanced Transformer with Rotary Position Embedding</span></div></a><div class="card-content article-content"><div class="summary block-with-text">RoPE / RoFormer: Enhanced Transformer with Rotary Position Embedding本文是论文 RoFormer: Enhanced Transformer with Rotary Pos</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2025-02-12 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/RoPE/"><span class="chip bg-color">RoPE</span> </a><a href="/tags/LLM/"><span class="chip bg-color">LLM</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/22620.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/0.jpg" class="responsive-img" alt="EVA-GAN: Enhanced Various Audio Generation via Scalable Generative Adversarial Networks"> <span class="card-title">EVA-GAN: Enhanced Various Audio Generation via Scalable Generative Adversarial Networks</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: HiFi - GAN: HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis. EVA-</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2025-01-17 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/Audio/"><span class="chip bg-color">Audio</span> </a><a href="/tags/SVS/"><span class="chip bg-color">SVS</span> </a><a href="/tags/Vocoder/"><span class="chip bg-color">Vocoder</span> </a><a href="/tags/TTS/"><span class="chip bg-color">TTS</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">425.8k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:andaning@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>