<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="Pytorch实现: Transformer, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>Pytorch实现: Transformer | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/18.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">Pytorch实现: Transformer</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/Transformer/"><span class="chip bg-color">Transformer</span> </a><a href="/tags/Pytorch/"><span class="chip bg-color">Pytorch</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2020-11-23</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2024-03-11</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 3.8k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 18 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p>本文前置知识:</p><ul><li>Pytorch基本操作</li><li>Transformer: 详见<a href="https://adaning.github.io/posts/6744.html">Transformer精讲</a></li></ul><p><strong>2022.04.03</strong>: 去掉了Pre Norm比Post Norm效果好的表述.</p></blockquote><h1 id="Pytorch实现-Transformer"><a href="#Pytorch实现-Transformer" class="headerlink" title="Pytorch实现: Transformer"></a>Pytorch实现: Transformer</h1><p>本文是Transfomrer的Pytorch版本实现. 实现的过程中非常考验<strong>维度控制</strong>的功底. 本文实现参考<a href="https://wmathor.com/index.php/archives/1455/" target="_blank" rel="noopener">Transformer 的 PyTorch 实现</a>, 我对其在个别地方进行了修改, 并对所有的数据<strong>全部</strong>加上了维度注释.</p><p>本文的代码已经放到了Colab上, 打开设置GPU就可以复现(需要科学上网).</p><p><a href="https://colab.research.google.com/drive/1OAXor9Io7pk64ilo-HoFS6RnH9ImbaaF?usp=sharing" target="_blank" rel="noopener"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="右键我在COLAB中打开!"></a></p><p>如果你不能科学上网, 应该看不到<code>Open in Colab</code>的图标.</p><p>在开头需要说明的是:</p><ul><li>网上的所有流传的代码, 一般都会把<code>batch_size</code>放在第0维. 因为我们基本上不对batch维做操作, 放在最前面来防止影响后面总需要使用<code>transpose</code>移动.</li><li>如果对Transformer不熟悉, 最好熟悉后再来看这篇文章.</li><li>注意<code>view</code>和<code>transpose</code>拆维度时不要乱了.</li></ul><h2 id="Preparing"><a href="#Preparing" class="headerlink" title="Preparing"></a>Preparing</h2><p>按照惯例, 先导包:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch <span class="token keyword">import</span> optim
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils <span class="token keyword">import</span> data <span class="token keyword">as</span> Data
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>因为后面需要用到一些关于Transformer的超参数, 所以在开头就先全部定义出来:</p><pre class="line-numbers language-python"><code class="language-python">d_model <span class="token operator">=</span> <span class="token number">512</span> <span class="token comment" spellcheck="true"># embedding size </span>
max_len <span class="token operator">=</span> <span class="token number">1024</span> <span class="token comment" spellcheck="true"># max length of sequence</span>
d_ff <span class="token operator">=</span> <span class="token number">2048</span> <span class="token comment" spellcheck="true"># feedforward nerual network  dimension</span>
d_k <span class="token operator">=</span> d_v <span class="token operator">=</span> <span class="token number">64</span> <span class="token comment" spellcheck="true"># dimension of k(same as q) and v</span>
n_layers <span class="token operator">=</span> <span class="token number">6</span> <span class="token comment" spellcheck="true"># number of encoder and decoder layers</span>
n_heads <span class="token operator">=</span> <span class="token number">8</span> <span class="token comment" spellcheck="true"># number of heads in multihead attention</span>
p_drop <span class="token operator">=</span> <span class="token number">0.1</span> <span class="token comment" spellcheck="true"># propability of dropout</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果你对Transformer足够熟悉, 看变量名和注释一定能看出来它们的含义, 它们依次是:</p><ul><li>d_model: Embedding的大小.</li><li>max_len: 输入序列的最长大小.</li><li>d_ff: 前馈神经网络的隐藏层大小, 一般是d_model的四倍.</li><li>d_k, d_v: 自注意力中K和V的维度, Q的维度直接用K的维度代替, 因为这二者必须始终相等.</li><li>n_layers: Encoder和Decoder的层数.</li><li>n_heads: 自注意力多头的头数.</li><li>p_drop: Dropout的概率.</li></ul><h2 id="Mask"><a href="#Mask" class="headerlink" title="Mask"></a>Mask</h2><p>Mask分为两种, 一种是因为在数据中使用了padding, 不希望pad被加入到注意力中进行计算的Pad Mask for Attention, 还有一种是保证Decoder自回归信息不泄露的Subsequent Mask for Decoder.</p><h3 id="Pad-Mask-for-Attention"><a href="#Pad-Mask-for-Attention" class="headerlink" title="Pad Mask for Attention"></a>Pad Mask for Attention</h3><p>为了方便, 假设<code>&lt;PAD&gt;</code>在字典中的Index是0, 遇到输入为0直接将其标为True.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_attn_pad_mask</span><span class="token punctuation">(</span>seq_q<span class="token punctuation">,</span> seq_k<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">'''
  Padding, because of unequal in source_len and target_len.

  parameters:
  seq_q: [batch, seq_len]
  seq_k: [batch, seq_len]

  return:
  mask: [batch, len_q, len_k]

  '''</span>
  batch<span class="token punctuation">,</span> len_q <span class="token operator">=</span> seq_q<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
  batch<span class="token punctuation">,</span> len_k <span class="token operator">=</span> seq_k<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
  <span class="token comment" spellcheck="true"># we define index of PAD is 0, if tensor equals (zero) PAD tokens</span>
  pad_attn_mask <span class="token operator">=</span> seq_k<span class="token punctuation">.</span>data<span class="token punctuation">.</span>eq<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, 1, len_k]</span>

  <span class="token keyword">return</span> pad_attn_mask<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> len_q<span class="token punctuation">,</span> len_k<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, len_q, len_k]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在Encoder和Decoder中使用Mask的情况可能各有不同:</p><ul><li><p>在Encoder中使用Mask, 是为了将<code>encoder_input</code>中没有内容而打上PAD的部分进行Mask, 方便矩阵运算.</p></li><li><p>在Decoder中使用Mask, 可能是在Decoder的自注意力对<code>decoder_input</code> 的PAD进行Mask, 也有可能是对Encoder - Decoder自注意力时对<code>encoder_input</code>和<code>decoder_input</code>的PAD进行Mask.</p></li></ul><h3 id="Subsequent-Mask-for-Decoder"><a href="#Subsequent-Mask-for-Decoder" class="headerlink" title="Subsequent Mask for Decoder"></a>Subsequent Mask for Decoder</h3><p>该Mask是为了防止Decoder的自回归信息泄露而生的Mask, 直接生成一个上三角矩阵即可:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_attn_subsequent_mask</span><span class="token punctuation">(</span>seq<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">'''
  Build attention mask matrix for decoder when it autoregressing.

  parameters:
  seq: [batch, target_len]

  return:
  subsequent_mask: [batch, target_len, target_len] 
  '''</span>
  attn_shape <span class="token operator">=</span> <span class="token punctuation">[</span>seq<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> seq<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> seq<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># [batch, target_len, target_len]</span>
  subsequent_mask <span class="token operator">=</span> np<span class="token punctuation">.</span>triu<span class="token punctuation">(</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>attn_shape<span class="token punctuation">)</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, target_len, target_len] </span>
  subsequent_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>subsequent_mask<span class="token punctuation">)</span>

  <span class="token keyword">return</span> subsequent_mask <span class="token comment" spellcheck="true"># [batch, target_len, target_len] </span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其中, 用到了生成上三角的函数<code>np.triu</code>, 其用法为:</p><pre class="line-numbers language-python"><code class="language-python">np<span class="token punctuation">.</span>triu<span class="token punctuation">(</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">'''
array([[0., 1., 1., 1.],
       [0., 0., 1., 1.],
       [0., 0., 0., 1.]])
'''</span>
np<span class="token punctuation">.</span>triu<span class="token punctuation">(</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">'''
array([[1., 1., 1., 1.],
       [0., 1., 1., 1.],
       [0., 0., 1., 1.]])
'''</span>
np<span class="token punctuation">.</span>triu<span class="token punctuation">(</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">'''
array([[1., 1., 1., 1.],
       [1., 1., 1., 1.],
       [0., 1., 1., 1.]])
'''</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其中<code>k</code>能控制上三角的大小, 越大则上三角范围越小. 与之完全<strong>相反</strong>的函数是<code>np.tril</code>, 能够生成下三角矩阵.</p><h2 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h2><p>在Transformer中, 使用的是绝对位置编码, 用于传输给模型Self - Attention所不能传输的位置信息, 编码使用正余弦公式实现:<br>$$<br>\begin{aligned}<br>PE(pos, 2i)&amp; = \sin(pos / 10000^{\frac{2i}{d_{model}}}) \\<br>PE(pos, 2i+1)&amp; = \cos(pos / 10000^{\frac{2i}{d_{model}}})<br>\end{aligned}<br>$$<br>基于上述公式, 我们把它实现出来:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">PositionalEncoding</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">,</span> max_len<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    super<span class="token punctuation">(</span>PositionalEncoding<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span>p_drop<span class="token punctuation">)</span>

    positional_encoding <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>max_len<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [max_len, d_model]</span>
    position <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> max_len<span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [max_len, 1]</span>

    div_term <span class="token operator">=</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> 
                         <span class="token punctuation">(</span><span class="token operator">-</span>torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">10000</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> d_model<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [max_len / 2]</span>

    positional_encoding<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>position <span class="token operator">*</span> div_term<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># even</span>
    positional_encoding<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>position <span class="token operator">*</span> div_term<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># odd</span>

    <span class="token comment" spellcheck="true"># [max_len, d_model] -> [1, max_len, d_model] -> [max_len, 1, d_model]</span>
    positional_encoding <span class="token operator">=</span> positional_encoding<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token comment" spellcheck="true"># register pe to buffer and require no grads</span>
    self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">'pe'</span><span class="token punctuation">,</span> positional_encoding<span class="token punctuation">)</span>

  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># x: [seq_len, batch, d_model]</span>
    <span class="token comment" spellcheck="true"># we can add positional encoding to x directly, and ignore other dimension</span>
    x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>pe<span class="token punctuation">[</span><span class="token punctuation">:</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span>

    <span class="token keyword">return</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>实现$1/ 10000^{\frac{2i}{d_{model}}}$ 时既可以像我写出的那样使用幂指运算, 也可以直接写出.</p><p><code>register_buffer</code>能够申请一个缓冲区中的<strong>常量</strong>, 并且它不会被加入到计算图中, 也就不会参与反向传播.</p><p>更多关于<code>register</code>在<code>parameter</code>和<code>buffer</code>上的区别请见<a href="https://zhuanlan.zhihu.com/p/89442276" target="_blank" rel="noopener">Pytorch模型中的parameter与buffer</a>.</p><h2 id="Feed-Forward-Neural-Network"><a href="#Feed-Forward-Neural-Network" class="headerlink" title="Feed Forward Neural Network"></a>Feed Forward Neural Network</h2><p>在Transformer中, Encoder或者Decoder每个Block都需要用一个前馈神经网络来添加<strong>非线性</strong>:<br>$$<br>\operatorname{FFN}(x)=\operatorname{ReLU}(xW_1+b_1)W_2 + b_2<br>$$<br>注意, 这里它们都是有偏置的, 而且这两个Linear可以用两个$1\times1$ 的卷积来实现:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">FeedForwardNetwork</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">'''
  Using nn.Conv1d replace nn.Linear to implements FFN.
  '''</span>
  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    super<span class="token punctuation">(</span>FeedForwardNetwork<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment" spellcheck="true"># self.ff1 = nn.Linear(d_model, d_ff)</span>
    <span class="token comment" spellcheck="true"># self.ff2 = nn.Linear(d_ff, d_model)</span>
    self<span class="token punctuation">.</span>ff1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_ff<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>ff2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>d_ff<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>

    self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span>p_drop<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>layer_norm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>

  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># x: [batch, seq_len, d_model]</span>
    residual <span class="token operator">=</span> x
    x <span class="token operator">=</span> x<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, d_model, seq_len]</span>
    x <span class="token operator">=</span> self<span class="token punctuation">.</span>ff1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    x <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    x <span class="token operator">=</span> self<span class="token punctuation">.</span>ff2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    x <span class="token operator">=</span> x<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, seq_len, d_model]</span>

    <span class="token keyword">return</span> self<span class="token punctuation">.</span>layer_norm<span class="token punctuation">(</span>residual <span class="token operator">+</span> x<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>作为一个子层, 不要忘记Transformer中提到的Residual Connection和Layer Norm.</p><p>我选择用两个卷积代替Linear. 在<code>nn.Conv1d</code>中, 要求数据的规格为<code>[batch, x, ...]</code>, 我们是要对<code>d_model</code> 上的数据进行卷积, 所以还是需要<code>transpose</code>一下.</p><h2 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi - Head Attention"></a>Multi - Head Attention</h2><p>先说多头注意力, 因为多头注意力能够决定缩放点积注意力的输入大小. 作为一个子层, 其中的Residual Connection和Layer Norm是必须的.</p><p>多头注意力是多个不同的头来获取不同的特征, 类似于多个<strong>卷积核</strong>所达到的效果. 在计算完后通过一个Linear调整大小:<br>$$<br>\begin{aligned}<br>\operatorname{MultiHead}(Q, K, V) &amp;= \operatorname{Concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_h)W^O \\<br>\text{where } \text{head}_i &amp;= \operatorname{Attention}(QW^Q_i, KW^K_i, VW^V_i)<br>\end{aligned}<br>$$<br>多头注意力在Encoder和Decoder中的使用略有区别, 主要区别在于Mask的不同. 我们前面已经实现了两种Mask函数, 在这里会用到.</p><p>多头注意力实际上不是通过弄出很多大小相同的矩阵然后相乘来实现的, 只需要合并到一个矩阵进行计算:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n_heads<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    super<span class="token punctuation">(</span>MultiHeadAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment" spellcheck="true"># do not use more instance to implement multihead attention</span>
    <span class="token comment" spellcheck="true"># it can be complete in one matrix</span>
    self<span class="token punctuation">.</span>n_heads <span class="token operator">=</span> n_heads

    <span class="token comment" spellcheck="true"># we can't use bias because there is no bias term in formular</span>
    self<span class="token punctuation">.</span>W_Q <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_k <span class="token operator">*</span> n_heads<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>W_K <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_k <span class="token operator">*</span> n_heads<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>W_V <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_v <span class="token operator">*</span> n_heads<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_v <span class="token operator">*</span> n_heads<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>layer_norm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>

  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_Q<span class="token punctuation">,</span> input_K<span class="token punctuation">,</span> input_V<span class="token punctuation">,</span> attn_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">'''
    To make sure multihead attention can be used both in encoder and decoder, 
    we use Q, K, V respectively.
    input_Q: [batch, len_q, d_model]
    input_K: [batch, len_k, d_model]
    input_V: [batch, len_v, d_model]
    '''</span>
    residual<span class="token punctuation">,</span> batch <span class="token operator">=</span> input_Q<span class="token punctuation">,</span> input_Q<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>

    <span class="token comment" spellcheck="true"># [batch, len_q, d_model] -- matmul W_Q --> [batch, len_q, d_q * n_heads] -- view --> </span>
    <span class="token comment" spellcheck="true"># [batch, len_q, n_heads, d_k,] -- transpose --> [batch, n_heads, len_q, d_k]</span>

    Q <span class="token operator">=</span> self<span class="token punctuation">.</span>W_Q<span class="token punctuation">(</span>input_Q<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> d_k<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, n_heads, len_q, d_k]</span>
    K <span class="token operator">=</span> self<span class="token punctuation">.</span>W_K<span class="token punctuation">(</span>input_K<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> d_k<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, n_heads, len_k, d_k]</span>
    V <span class="token operator">=</span> self<span class="token punctuation">.</span>W_V<span class="token punctuation">(</span>input_V<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> d_v<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, n_heads, len_v, d_v]</span>

    attn_mask <span class="token operator">=</span> attn_mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, n_heads, seq_len, seq_len]</span>

    <span class="token comment" spellcheck="true"># prob: [batch, n_heads, len_q, d_v] attn: [batch, n_heads, len_q, len_k]</span>
    prob<span class="token punctuation">,</span> attn <span class="token operator">=</span> ScaledDotProductAttention<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>Q<span class="token punctuation">,</span> K<span class="token punctuation">,</span> V<span class="token punctuation">,</span> attn_mask<span class="token punctuation">)</span>

    prob <span class="token operator">=</span> prob<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, len_q, n_heads, d_v]</span>
    prob <span class="token operator">=</span> prob<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads <span class="token operator">*</span> d_v<span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, len_q, n_heads * d_v]</span>

    output <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>prob<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, len_q, d_model]</span>

    <span class="token keyword">return</span> self<span class="token punctuation">.</span>layer_norm<span class="token punctuation">(</span>residual <span class="token operator">+</span> output<span class="token punctuation">)</span><span class="token punctuation">,</span> attn
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>提两个非常重要的点:</p><ol><li>在拆维度时不要破坏维度原来本身的意义.</li><li>虽然新版本已经有<code>reshape</code>函数可以用了, 但是仍然不要忘记, <code>transpose</code>后如果接<code>permute</code>或者<code>view</code>必须要加<code>contiguous</code>, 这是<strong>数据真实存储连续与否</strong>的问题, 请参见<a href="https://adaning.github.io/posts/42255.html">Pytorch之张量基础操作</a>中的<strong>维度变换</strong>部分.</li></ol><h2 id="Scaled-DotProduct-Attention"><a href="#Scaled-DotProduct-Attention" class="headerlink" title="Scaled DotProduct Attention"></a>Scaled DotProduct Attention</h2><p>Tranformer中非常重要的概念, 缩放点积注意力, 公式如下:<br>$$<br>\operatorname{Attention}(Q, K, V) = \operatorname{softmax}(\frac{QK^T}{\sqrt{d_k}})V<br>$$<br>实现起来非常简单, 只需要把Q, K两个矩阵一乘, 然后再缩放, 过一次Softmax, 再和V乘下:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">ScaledDotProductAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    super<span class="token punctuation">(</span>ScaledDotProductAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> Q<span class="token punctuation">,</span> K<span class="token punctuation">,</span> V<span class="token punctuation">,</span> attn_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">'''
    Q: [batch, n_heads, len_q, d_k]
    K: [batch, n_heads, len_k, d_k]
    V: [batch, n_heads, len_v, d_v]
    attn_mask: [batch, n_heads, seq_len, seq_len]
    '''</span>
    scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>Q<span class="token punctuation">,</span> K<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>d_k<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, n_heads, len_q, len_k]</span>
    scores<span class="token punctuation">.</span>masked_fill_<span class="token punctuation">(</span>attn_mask<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1e9</span><span class="token punctuation">)</span>

    attn <span class="token operator">=</span> nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span>scores<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, n_heads, len_q, len_k]</span>
    prob <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>attn<span class="token punctuation">,</span> V<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, n_heads, len_q, d_v]</span>
    <span class="token keyword">return</span> prob<span class="token punctuation">,</span> attn<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>masked_fill_</code>能把传进来的Mask为True的地方全都填充上某个值, 这里需要用一个很大的负数来保证$e^x \rightarrow 0$, 使得其在Softmax​ 中可以被忽略.</p><h2 id="Encoder-and-Decoder"><a href="#Encoder-and-Decoder" class="headerlink" title="Encoder and Decoder"></a>Encoder and Decoder</h2><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>先写出Encoder的每个Layer, 由多头注意力和FFN组成:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">EncoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    super<span class="token punctuation">(</span>EncoderLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>encoder_self_attn <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>ffn <span class="token operator">=</span> FeedForwardNetwork<span class="token punctuation">(</span><span class="token punctuation">)</span>

  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder_input<span class="token punctuation">,</span> encoder_pad_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">'''
    encoder_input: [batch, source_len, d_model]
    encoder_pad_mask: [batch, n_heads, source_len, source_len]

    encoder_output: [batch, source_len, d_model]
    attn: [batch, n_heads, source_len, source_len]
    '''</span>
    encoder_output<span class="token punctuation">,</span> attn <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder_self_attn<span class="token punctuation">(</span>encoder_input<span class="token punctuation">,</span> encoder_input<span class="token punctuation">,</span> encoder_input<span class="token punctuation">,</span> encoder_pad_mask<span class="token punctuation">)</span>
    encoder_output <span class="token operator">=</span> self<span class="token punctuation">.</span>ffn<span class="token punctuation">(</span>encoder_output<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, source_len, d_model]</span>

    <span class="token keyword">return</span> encoder_output<span class="token punctuation">,</span> attn<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>对于给定的<code>encoder_input</code>和<code>encoder_pad_pask</code>, Encoder应该能够完成整个Block(Layer)的计算流程. 然后实现整个Encoder:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Encoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    super<span class="token punctuation">(</span>Encoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>source_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>source_vocab_size<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>positional_embedding <span class="token operator">=</span> PositionalEncoding<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>EncoderLayer<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> layer <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_layers<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder_input<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># encoder_input: [batch, source_len]</span>
    encoder_output <span class="token operator">=</span> self<span class="token punctuation">.</span>source_embedding<span class="token punctuation">(</span>encoder_input<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, source_len, d_model]</span>
    encoder_output <span class="token operator">=</span> self<span class="token punctuation">.</span>positional_embedding<span class="token punctuation">(</span>encoder_output<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, source_len, d_model]</span>

    encoder_self_attn_mask <span class="token operator">=</span> get_attn_pad_mask<span class="token punctuation">(</span>encoder_input<span class="token punctuation">,</span> encoder_input<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, source_len, source_len]</span>
    encoder_self_attns <span class="token operator">=</span> list<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>
      <span class="token comment" spellcheck="true"># encoder_output: [batch, source_len, d_model]</span>
      <span class="token comment" spellcheck="true"># encoder_self_attn: [batch, n_heads, source_len, source_len]</span>
      encoder_output<span class="token punctuation">,</span> encoder_self_attn <span class="token operator">=</span> layer<span class="token punctuation">(</span>encoder_output<span class="token punctuation">,</span> encoder_self_attn_mask<span class="token punctuation">)</span>
      encoder_self_attns<span class="token punctuation">.</span>append<span class="token punctuation">(</span>encoder_self_attn<span class="token punctuation">)</span>

    <span class="token keyword">return</span> encoder_output<span class="token punctuation">,</span> encoder_self_attns<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>对于整个Encoder, 直接将Token的Index传入Embedding中, 再添入位置编码, 之后就经过多层Transformer Encoder. 在传入Block前, 先需要计算Padding的Mask, 再将上层的输出作为下层输入依次迭代.</p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>其实实现了Encoder, Decoder的实现部分都是对应的. 先实现Decoder的Block:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">DecoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    super<span class="token punctuation">(</span>DecoderLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>decoder_self_attn <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>encoder_decoder_attn <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>ffn <span class="token operator">=</span> FeedForwardNetwork<span class="token punctuation">(</span><span class="token punctuation">)</span>

  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> decoder_input<span class="token punctuation">,</span> encoder_output<span class="token punctuation">,</span> decoder_self_mask<span class="token punctuation">,</span> decoder_encoder_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">'''
    decoder_input: [batch, target_len, d_mdoel]
    encoder_output: [batch, source_len, d_model]
    decoder_self_mask: [batch, target_len, target_len]
    decoder_encoder_mask: [batch, target_len, source_len]
    '''</span>
    <span class="token comment" spellcheck="true"># masked mutlihead attention</span>
    <span class="token comment" spellcheck="true"># Q, K, V all from decoder it self</span>
    <span class="token comment" spellcheck="true"># decoder_output: [batch, target_len, d_model]</span>
    <span class="token comment" spellcheck="true"># decoder_self_attn: [batch, n_heads, target_len, target_len]</span>
    decoder_output<span class="token punctuation">,</span> decoder_self_attn <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder_self_attn<span class="token punctuation">(</span>decoder_input<span class="token punctuation">,</span> decoder_input<span class="token punctuation">,</span> decoder_input<span class="token punctuation">,</span> decoder_self_mask<span class="token punctuation">)</span>

    <span class="token comment" spellcheck="true"># Q from decoder, K, V from encoder</span>
    <span class="token comment" spellcheck="true"># decoder_output: [batch, target_len, d_model]</span>
    <span class="token comment" spellcheck="true"># decoder_encoder_attn: [batch, n_heads, target_len, source_len]</span>
    decoder_output<span class="token punctuation">,</span> decoder_encoder_attn <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder_decoder_attn<span class="token punctuation">(</span>decoder_output<span class="token punctuation">,</span> encoder_output<span class="token punctuation">,</span> encoder_output<span class="token punctuation">,</span> decoder_encoder_mask<span class="token punctuation">)</span>
    decoder_output <span class="token operator">=</span> self<span class="token punctuation">.</span>ffn<span class="token punctuation">(</span>decoder_output<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, target_len, d_model]</span>

    <span class="token keyword">return</span> decoder_output<span class="token punctuation">,</span> decoder_self_attn<span class="token punctuation">,</span> decoder_encoder_attn<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>与Encoder相对应, 只不过因为多了一个Encoder - Decoder自注意力, 所以需要额外计算一个Encoder - Decoder的Mask. 然后写出整个Decoder:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Decoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    super<span class="token punctuation">(</span>Decoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>target_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>target_vocab_size<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>positional_embedding <span class="token operator">=</span> PositionalEncoding<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>DecoderLayer<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> layer <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_layers<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> decoder_input<span class="token punctuation">,</span> encoder_input<span class="token punctuation">,</span> encoder_output<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">'''
    decoder_input: [batch, target_len]
    encoder_input: [batch, source_len]
    encoder_output: [batch, source_len, d_model]
    '''</span>
    decoder_output <span class="token operator">=</span> self<span class="token punctuation">.</span>target_embedding<span class="token punctuation">(</span>decoder_input<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, target_len, d_model]</span>
    decoder_output <span class="token operator">=</span> self<span class="token punctuation">.</span>positional_embedding<span class="token punctuation">(</span>decoder_output<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, target_len, d_model]</span>
    decoder_self_attn_mask <span class="token operator">=</span> get_attn_pad_mask<span class="token punctuation">(</span>decoder_input<span class="token punctuation">,</span> decoder_input<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, target_len, target_len]</span>
    decoder_subsequent_mask <span class="token operator">=</span> get_attn_subsequent_mask<span class="token punctuation">(</span>decoder_input<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, target_len, target_len]</span>

    decoder_encoder_attn_mask <span class="token operator">=</span> get_attn_pad_mask<span class="token punctuation">(</span>decoder_input<span class="token punctuation">,</span> encoder_input<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, target_len, source_len]</span>

    decoder_self_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>gt<span class="token punctuation">(</span>decoder_self_attn_mask <span class="token operator">+</span> decoder_subsequent_mask<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
    decoder_self_attns<span class="token punctuation">,</span> decoder_encoder_attns <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

    <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>
      <span class="token comment" spellcheck="true"># decoder_output: [batch, target_len, d_model]</span>
      <span class="token comment" spellcheck="true"># decoder_self_attn: [batch, n_heads, target_len, target_len]</span>
      <span class="token comment" spellcheck="true"># decoder_encoder_attn: [batch, n_heads, target_len, source_len]</span>
      decoder_output<span class="token punctuation">,</span> decoder_self_attn<span class="token punctuation">,</span> decoder_encoder_attn <span class="token operator">=</span> layer<span class="token punctuation">(</span>decoder_output<span class="token punctuation">,</span> encoder_output<span class="token punctuation">,</span> decoder_self_mask<span class="token punctuation">,</span> decoder_encoder_attn_mask<span class="token punctuation">)</span>
      decoder_self_attns<span class="token punctuation">.</span>append<span class="token punctuation">(</span>decoder_self_attn<span class="token punctuation">)</span>
      decoder_encoder_attns<span class="token punctuation">.</span>append<span class="token punctuation">(</span>decoder_encoder_attn<span class="token punctuation">)</span>

    <span class="token keyword">return</span> decoder_output<span class="token punctuation">,</span> decoder_self_attns<span class="token punctuation">,</span> decoder_encoder_attns<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>和Encoder相对应, 但Decoder和Encoder使用了两个不同的Embedding. 对于Mask, 可以把自回归Mask和Padding Mask用<code>torch.gt</code>整合成一个Mask, 送入其中.</p><h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>终于到了这一步, 虽然后面还有一些小小的工作, 但现在终于能看到Transformer的<strong>全貌</strong>了:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer.jpg" style="zoom:50%"><p>里面有一个Encoder, 一个Decoder, 在Decoder端还需要加上投影层来分类:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Transformer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    super<span class="token punctuation">(</span>Transformer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

    self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> Encoder<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> Decoder<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>projection <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> target_vocab_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder_input<span class="token punctuation">,</span> decoder_input<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">'''
    encoder_input: [batch, source_len]
    decoder_input: [batch, target_len]
    '''</span>
    <span class="token comment" spellcheck="true"># encoder_output: [batch, source_len, d_model]</span>
    <span class="token comment" spellcheck="true"># encoder_attns: [n_layers, batch, n_heads, source_len, source_len]</span>
    encoder_output<span class="token punctuation">,</span> encoder_attns <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>encoder_input<span class="token punctuation">)</span>
    <span class="token comment" spellcheck="true"># decoder_output: [batch, target_len, d_model]</span>
    <span class="token comment" spellcheck="true"># decoder_self_attns: [n_layers, batch, n_heads, target_len, target_len]</span>
    <span class="token comment" spellcheck="true"># decoder_encoder_attns: [n_layers, batch, n_heads, target_len, source_len]</span>
    decoder_output<span class="token punctuation">,</span> decoder_self_attns<span class="token punctuation">,</span> decoder_encoder_attns <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>decoder_input<span class="token punctuation">,</span> encoder_input<span class="token punctuation">,</span> encoder_output<span class="token punctuation">)</span>
    decoder_logits <span class="token operator">=</span> self<span class="token punctuation">.</span>projection<span class="token punctuation">(</span>decoder_output<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, target_len, target_vocab_size]</span>

    <span class="token comment" spellcheck="true"># decoder_logits: [batch * target_len, target_vocab_size]</span>
    <span class="token keyword">return</span> decoder_logits<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> decoder_logits<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> encoder_attns<span class="token punctuation">,</span> decoder_self_attns<span class="token punctuation">,</span> decoder_encoder_attns<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>最后对logits的处理是<code>view</code>成了<code>[batch * target_len, target_vocab_size]</code>, 前面的大小并不影响我们一会用交叉熵计算损失.</p><h2 id="Input-Data"><a href="#Input-Data" class="headerlink" title="Input Data"></a>Input Data</h2><p>输入数据没什么好说的, 为了方便直接采用了硬编码的方式构造<code>word2index</code>, 这样我们的输入序列都被转换为了Token的index输入到Embedding层中, 自动转化为嵌入在低维空间的稠密向量:</p><p>Decoder的输入构造过程采用了<strong>Teaching Forcing</strong>, 保证了训练过程是可以保持<strong>并行</strong>的.</p><pre class="line-numbers language-python"><code class="language-python">sentences <span class="token operator">=</span> <span class="token punctuation">[</span>
        <span class="token comment" spellcheck="true"># enc_input           dec_input         dec_output</span>
        <span class="token punctuation">[</span><span class="token string">'ich mochte ein bier P'</span><span class="token punctuation">,</span> <span class="token string">'S i want a beer .'</span><span class="token punctuation">,</span> <span class="token string">'i want a beer . E'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token string">'ich mochte ein cola P'</span><span class="token punctuation">,</span> <span class="token string">'S i want a coke .'</span><span class="token punctuation">,</span> <span class="token string">'i want a coke . E'</span><span class="token punctuation">]</span>
<span class="token punctuation">]</span>

<span class="token comment" spellcheck="true"># Padding Should be Zero</span>
source_vocab <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'P'</span> <span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'ich'</span> <span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'mochte'</span> <span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'ein'</span> <span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'bier'</span> <span class="token punctuation">:</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token string">'cola'</span> <span class="token punctuation">:</span> <span class="token number">5</span><span class="token punctuation">}</span>
source_vocab_size <span class="token operator">=</span> len<span class="token punctuation">(</span>source_vocab<span class="token punctuation">)</span>

target_vocab <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'P'</span> <span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'i'</span> <span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'want'</span> <span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'a'</span> <span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'beer'</span> <span class="token punctuation">:</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token string">'coke'</span> <span class="token punctuation">:</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token string">'S'</span> <span class="token punctuation">:</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token string">'E'</span> <span class="token punctuation">:</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token string">'.'</span> <span class="token punctuation">:</span> <span class="token number">8</span><span class="token punctuation">}</span>
idx2word <span class="token operator">=</span> <span class="token punctuation">{</span>i<span class="token punctuation">:</span> w <span class="token keyword">for</span> i<span class="token punctuation">,</span> w <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>target_vocab<span class="token punctuation">)</span><span class="token punctuation">}</span>
target_vocab_size <span class="token operator">=</span> len<span class="token punctuation">(</span>target_vocab<span class="token punctuation">)</span>
source_len <span class="token operator">=</span> <span class="token number">5</span> <span class="token comment" spellcheck="true"># max length of input sequence</span>
target_len <span class="token operator">=</span> <span class="token number">6</span>

<span class="token keyword">def</span> <span class="token function">make_data</span><span class="token punctuation">(</span>sentences<span class="token punctuation">)</span><span class="token punctuation">:</span>
  encoder_inputs<span class="token punctuation">,</span> decoder_inputs<span class="token punctuation">,</span> decoder_outputs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
  <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    encoder_input <span class="token operator">=</span> <span class="token punctuation">[</span>source_vocab<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token keyword">for</span> word <span class="token keyword">in</span> sentences<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
    decoder_input <span class="token operator">=</span> <span class="token punctuation">[</span>target_vocab<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token keyword">for</span> word <span class="token keyword">in</span> sentences<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
    decoder_output <span class="token operator">=</span> <span class="token punctuation">[</span>target_vocab<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token keyword">for</span> word <span class="token keyword">in</span> sentences<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
    encoder_inputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>encoder_input<span class="token punctuation">)</span>
    decoder_inputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>decoder_input<span class="token punctuation">)</span>
    decoder_outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>decoder_output<span class="token punctuation">)</span>

  <span class="token keyword">return</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>encoder_inputs<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>decoder_inputs<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>decoder_outputs<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>数据量非常的少, 所以等会的训练会根本不充分.</p><h2 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h2><p>制作一个Seq2Seq的数据集, 只需要按照Index返回Encoder的输出, Decoder的输入, Decoder的输出(label)就好:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Seq2SeqDataset</span><span class="token punctuation">(</span>Data<span class="token punctuation">.</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>

  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder_input<span class="token punctuation">,</span> decoder_input<span class="token punctuation">,</span> decoder_output<span class="token punctuation">)</span><span class="token punctuation">:</span>
    super<span class="token punctuation">(</span>Seq2SeqDataset<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>encoder_input <span class="token operator">=</span> encoder_input
    self<span class="token punctuation">.</span>decoder_input <span class="token operator">=</span> decoder_input
    self<span class="token punctuation">.</span>decoder_output <span class="token operator">=</span> decoder_output

  <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> self<span class="token punctuation">.</span>encoder_input<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

  <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> self<span class="token punctuation">.</span>encoder_input<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>decoder_input<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>decoder_output<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>对训练所需的所有东西进行定义:</p><pre class="line-numbers language-python"><code class="language-python">batch_size <span class="token operator">=</span> <span class="token number">64</span>
epochs <span class="token operator">=</span> <span class="token number">64</span>
lr <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span>

device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">'cpu'</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> Transformer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span>ignore_index<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>
encoder_inputs<span class="token punctuation">,</span> decoder_inputs<span class="token punctuation">,</span> decoder_outputs <span class="token operator">=</span> make_data<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span>
dataset <span class="token operator">=</span> Seq2SeqDataset<span class="token punctuation">(</span>encoder_inputs<span class="token punctuation">,</span> decoder_inputs<span class="token punctuation">,</span> decoder_outputs<span class="token punctuation">)</span>
data_loader <span class="token operator">=</span> Data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里有个<code>criterion = nn.CrossEntropyLoss(ignore_index=0)</code>, 其中<code>ignore_index=0</code>指的是PAD在计算交叉熵时不应该被包括进去(前面提到过PAD所对应的Index是0).</p><p>我们从定义好的数据集中取出数据到<code>device</code>, 然后用torch三件套:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">'''
  encoder_input: [batch, source_len]
  decoder_input: [batch, target_len]
  decoder_ouput: [batch, target_len]
  '''</span>
  <span class="token keyword">for</span> encoder_input<span class="token punctuation">,</span> decoder_input<span class="token punctuation">,</span> decoder_output <span class="token keyword">in</span> data_loader<span class="token punctuation">:</span>
    encoder_input <span class="token operator">=</span> encoder_input<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    decoder_input <span class="token operator">=</span> decoder_input<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    decoder_output <span class="token operator">=</span> decoder_output<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>

    output<span class="token punctuation">,</span> encoder_attns<span class="token punctuation">,</span> decoder_attns<span class="token punctuation">,</span> decoder_encoder_attns <span class="token operator">=</span> model<span class="token punctuation">(</span>encoder_input<span class="token punctuation">,</span> decoder_input<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output<span class="token punctuation">,</span> decoder_output<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Epoch:'</span><span class="token punctuation">,</span> <span class="token string">'%04d'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'loss ='</span><span class="token punctuation">,</span> <span class="token string">'{:.6f}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>loss<span class="token punctuation">)</span><span class="token punctuation">)</span>

    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Attention-Visualization"><a href="#Attention-Visualization" class="headerlink" title="Attention Visualization"></a>Attention Visualization</h2><p>这回有了自己造的Transformer, 经过了<strong>根本不完全的训练: )</strong>, 我们可以把它的Attention矩阵画出来看看:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> seaborn <span class="token keyword">as</span> sns
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token triple-quoted-string string">'''
batch 1:
[[1, 2, 3, 5, 0],
[1, 2, 3, 4, 0]]
'''</span>
temp_batch <span class="token operator">=</span> <span class="token number">0</span>
n_layers <span class="token operator">=</span> <span class="token number">4</span>
plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span>n_heads <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">,</span> n_layers <span class="token operator">*</span> <span class="token number">3</span> <span class="token operator">+</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># encoder_attns: [n_layers, batch, n_heads, source_len, source_len]</span>
i <span class="token operator">=</span> <span class="token number">0</span>
tokens <span class="token operator">=</span> sentences<span class="token punctuation">[</span>temp_batch<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> layer <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">for</span> head <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_heads<span class="token punctuation">)</span><span class="token punctuation">:</span>
    i <span class="token operator">+=</span> <span class="token number">1</span>
    plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span>n_layers<span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> i<span class="token punctuation">)</span>

    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Layer:{}, Head:{}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>layer<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> head<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> i <span class="token operator">%</span> n_heads <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
      cbar<span class="token operator">=</span><span class="token boolean">True</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
      cbar<span class="token operator">=</span><span class="token boolean">False</span>
    sns<span class="token punctuation">.</span>heatmap<span class="token punctuation">(</span>encoder_attns<span class="token punctuation">[</span>layer<span class="token punctuation">]</span><span class="token punctuation">[</span>temp_batch<span class="token punctuation">]</span><span class="token punctuation">[</span>head<span class="token punctuation">]</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cmap<span class="token operator">=</span><span class="token string">'YlGnBu'</span><span class="token punctuation">,</span> 
            xticklabels<span class="token operator">=</span>tokens<span class="token punctuation">,</span> yticklabels<span class="token operator">=</span>tokens<span class="token punctuation">,</span> cbar<span class="token operator">=</span>cbar<span class="token punctuation">,</span> vmin<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> vmax<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    plt<span class="token punctuation">.</span>xticks<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>yticks<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>最后两行<code>plt.xticks</code>和<code>plt.yticks</code>纯粹是为了<strong>方便注释掉</strong>, 才又写在了外面.</p><p><strong>不要对结果太在意</strong>, 因为<strong>训练是根本不完整的</strong>, 数据也才只有两条. 我只是想画出来看看每个头都大致学到了什么:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/pytorchtransformer1.jpg" alt=""></p><p>最右侧是Padding, 这一列的权重都被当做是0来计算. 在浅一些的层确实学到了不同Token对不同部分的权重. 再深一些的层基本都没有得到训练, 因为数据实在太少了.</p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/63679.html">https://ADAning.github.io/posts/63679.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/Transformer/"><span class="chip bg-color">Transformer</span> </a><a href="/tags/Pytorch/"><span class="chip bg-color">Pytorch</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/13629.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/3.jpg" class="responsive-img" alt="深度可分离卷积与分组卷积"> <span class="card-title">深度可分离卷积与分组卷积</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: CNN: 详见卷积神经网络小结. 本文着重介绍深度可分离卷积和分组卷积两种操作. 深度可分离卷积深度可分离卷积(Depthwise Separable Convolution)应用在MobileNet和Xceptio</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-11-26 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/CNN/"><span class="chip bg-color">CNN</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/52897.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/19.jpg" class="responsive-img" alt="KEPLER: Knowledge Embedding and Pre-trained Language Representation"> <span class="card-title">KEPLER: Knowledge Embedding and Pre-trained Language Representation</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: BERT(详见ELMo, GPT, BERT) KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representat</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-11-21 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/" class="post-category">知识图谱</a></span></div></div><div class="card-action article-tags"><a href="/tags/KGE/"><span class="chip bg-color">KGE</span> </a><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/BERT/"><span class="chip bg-color">BERT</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">390.5k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:andaning@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>