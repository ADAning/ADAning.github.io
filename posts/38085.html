<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="卷积神经网络发展史, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>卷积神经网络发展史 | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:.6"></i> <span>时间轴</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li><li><a href="/markdown"><i class="fab fa-markdown" style="margin-top:-20px;zoom:.6"></i> <span>Markdown</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li><li><a href="/markdown" style="margin-left:75px"><i class="fa fab fa-markdown" style="position:absolute;left:50px"></i> <span>Markdown</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/2.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">卷积神经网络发展史</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/CNN/"><span class="chip bg-color">CNN</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2020-09-07</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2022-03-27</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 4.7k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 16 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><h2 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h2><p>LeNet可以说是CNN的开山鼻祖之一了, 虽然它不是CNN的起点, 但是可以称为CNN兴起的标志. 它由图灵奖得主LeCun Yann在1998年的<a href="https://ieeexplore.ieee.org/document/726791?reload=true&arnumber=726791" target="_blank" rel="noopener">Gradient-based learning applied to document recognition</a>提出.</p><p>CNN最早的模型框架就是由LeNet给出的, 即卷积+池化+FC层的基本结构.</p><p>诸如局部感知, 权值共享, 池化等操作都是从这里开始的, 并深远的影响了以后的卷积神经网络发展.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lenet.jpg" style="zoom:67%"><p>对当时的深度神经网络(也就是多层感知机)来说冲击非常大. 相较于MLP来说, LeNet所采用的的<strong>权值共享</strong>和<strong>局部连接</strong>大幅度降低了模型参数数量, 并对于手写数字识别提升到更高的准确率. 并且LeNet还设计了<strong>MaxPool</strong>通过下采样提取特征. 当时采取的激活函数是Tanh, 因为它关于原点对称, 比Sigmoid<a href="https://liam.page/2018/04/17/zero-centered-active-function/" target="_blank" rel="noopener">收敛快</a>.</p><p>可惜当时LeNet受限于时代背景, 直到后来GPU运算的普及和2009年ImageNet这样大规模的数据集的出现, 才被人发现它的价值.</p><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>2012年另一个具有划时代意义的模型AlexNet横空出世. AlexNet在2012年ImageNet竞赛中以超过第二名10.9个百分点的绝对优势, 一举夺冠, 引起了相当大的轰动. 此前深度学习沉寂了很久, 但自AlexNet诞生后, 所有的ImageNet冠军都是用CNN来做的.</p><p>AlexNet的论文是<a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" target="_blank" rel="noopener">ImageNet Classification with Deep Convolutional Neural Networks</a>.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/alexnet.jpg" style="zoom:67%"><p>AlexNet的特点:</p><ol><li>相较于LeNet, AlexNet的网络深度更<strong>深</strong>.</li><li>在每个卷积层后首次使用了<strong>ReLU</strong>作为激活函数.</li><li>添加了<strong>Local Response Normalization</strong>(LRN, 局部响应归一化) 层提高准确率, 但似乎2015年的VGG中却提到LRN没什么用.</li><li>设计和使用了<strong>Dropout</strong>, 来减轻模型过拟合程度.</li><li>使用了<strong>数据增强</strong>, 对训练数据施以裁剪, 旋转等.</li><li>基于当时的算力限制, AlexNet将图像分割为上下两块<strong>分别训练</strong>, 然后用FC层合并在一起.</li></ol><p>但是AlexNet的卷积核很大, 训练起来非常困难, 并且反向传播时最靠近输入端的卷积核更难利用梯度来更新自身的值.</p><h2 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h2><p>VGGNet是由牛津大学计算机视觉组和Google DeepMind公司的研究员一起研发, 在<a href="https://arxiv.org/abs/1409.1556v6" target="_blank" rel="noopener">Very Deep Convolutional Networks for Large-Scale Image Recognition</a>中提出的深度卷积神经网络, 取得了ILSVRC2014比赛分类项目的第二名, 而第一名是同年提出的GoogLeNet.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vgg16.png" style="zoom:33%"><p>VGG使用了更小的卷积核, 首次提出用<strong>多个小卷积核代替一个大卷积核</strong>的观点. 比如用两个3x3卷积代替一个5x5卷积, 用三个3x3卷积代替一个7x7卷积, 这样能<strong>增加非线性映射</strong>, 保证了模型的感知域是相同的, 但却减小了模型参数量.</p><p>同时, VGG也指出了在AlexNet中使用的局部响应归一化可能没有增益, 因为在实验中并没有给VGG带来性能提升.</p><p>除此外, VGG构建了16-19层的稳定卷积神经网络, 证明了<strong>网络深度的增加可以带来性能提升</strong>.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vgg.jpg" style="zoom:67%"><h2 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h2><p>GoogLeNet也称为<strong>Inception</strong>, 其第一个版本在与VGG出世的同一年ImageNet中取得了冠军.</p><p>在VGGNet中发现, 在加宽和加深一个网络时, 模型效果会得到提升. 但是与之对应的就是参数数量上的增加, 和对梯度消失和弥散的不可控, 这就导致反向传播的效果差, 也意味着过拟合风险增加. 所以在加宽和加深模型的同时尽可能的<strong>减少参数</strong>, 肯定能够带来性能的提升.</p><p>其实上述所指的方法就是将全连接层替换为<strong>更加稀疏</strong>的网络结构. 因为大多数硬件是针对稠密数据计算的, 对于稀疏数据的计算没有优化, 所以稠密数据和稀疏数据的计算效果是相似的. 应该找到一种方法实现同时利用网络内的稀疏性, 又能利用密集矩阵的高性能计算. 根据文献人们知道, 将<strong>稀疏矩阵聚类为密集的子矩阵</strong>可以提高计算性能, 这就成为了GoogLeNet的核心思想: <strong>怎样用密集成分来近似最优的局部稀疏结构</strong>.</p><h3 id="Inception-v1"><a href="#Inception-v1" class="headerlink" title="Inception v1"></a>Inception v1</h3><p>稀疏连接有两种方法, 其一在空间上的局部连接, 也就是CNN的方法, 另一是在特征维度上将稀疏连接进行处理, 也就是在通道维度上处理. GoogLeNet v1通过设计密集结构来近似一个稀疏的CNN. 相关论文<a href="https://arxiv.org/abs/1409.4842" target="_blank" rel="noopener">Going deeper with convolutions</a>. 对于相同尺寸更多数量的卷积核, 提取出的特征更加稀疏, 而更少数量的卷积核提取的特征更加稠密. 同时使用<strong>不同尺寸</strong>的卷积核, 分别提取不同尺度的特征, 但保持总的卷积核数量不变, 这样提取出的特征密度更大, 并且不同尺度之间具有<strong>弱相关性</strong>, 这在现在的CNN中早已被广泛运用, 也称<strong>多尺度卷积</strong>. 最早的Inception模块如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetv1-1.jpg" style="zoom:67%"><p>多尺度卷积时在图像周围使用<strong>补0</strong>的方式来维持不同尺度的卷积特征图尺寸相同, 即TF中CNN的SAME Padding方式.</p><p>上述的Inception块容易导致运算量很大, 因为最后不同尺度的卷积核和池化是concat在一起的, 就有可能导致并行堆叠产生的维度越来越大, 使计算更困难, 所以Inception采用了更多的<strong>1x1卷积</strong>来减少维度和参数. 这种1x1卷积除了减小参数量, 还增加了非线性拟合能力.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetv1-2.jpg" style="zoom:67%"><p>Inception采用了大量的Inception块结构进行堆叠, 并另外增加了两个辅助的softmax分支, 意在<strong>避免梯度消失</strong>和为输出做<strong>辅助分类</strong>, 将loss加权后加到最终分类结果中, 文中认为辅助分类器只在训练末期提高准确率. 在最后实际测试时, 这两个辅助的分支会被去掉, 去掉后并不会导致模型性能有明显的降低. 作者认为辅助分类器起到的作用更像是<strong>正则化</strong>.</p><p>Inception v1已经尽可能的抛弃掉FC层, 只是为了方便大家对模型进行FineTune才在最后加入的FC.</p><p>Inception v1完整结构图如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetv1.png" style="zoom:50%"><p>最终卷积层后采用<strong>全局池化</strong>, 而非FC层, 全局池化有助于减少参数量.</p><h3 id="Inception-v2-Inception-v3"><a href="#Inception-v2-Inception-v3" class="headerlink" title="Inception v2/Inception v3"></a>Inception v2/Inception v3</h3><p>凭借Inception v1的优秀表现, 2015年GoogLeNet团队又对其挖掘改进, 在<a href="https://arxiv.org/abs/1512.00567v3" target="_blank" rel="noopener">Rethinking the Inception Architecture for Computer Vision</a>提出了Inception v1的升级版本Inception v2/v3.</p><p>在论文开头, 作者提出了4个网络结构设计原则:</p><blockquote><ol><li>Avoid representational bottlenecks, especially early in the network.</li><li>Higher dimensional representations are easier to process locally within a network.</li><li>Spatial aggregation can be done over lower dimensional embeddings without much or any loss in representational power.</li><li>Balance the width and depth of the network.</li></ol></blockquote><ol><li>避免信息表征的瓶颈, 尤其是在网络早期. (<strong>不要过早压缩和降维原来的信息</strong>)</li><li>更高维的表征能更容易的在网络中局部处理. (增加每层的卷积核数量能获得更低耦合度的特征)</li><li>空间聚合在低维嵌入时能在不损失太多表征能力的情况下完成. (可以先用1x1对输入特征降维再执行大卷积核卷积, 如果对输入特征直接池化, 会损失很多信息)</li><li>网络的深度和宽度要平衡.</li></ol><p>Inception v2基于VGG对用多个小卷积核代替大卷积核的思想, 将Inception v1中设计的Inception块进行了优化, 将5x5的卷积核替换为2个3x3的卷积核.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetv2-1.jpg" style="zoom:50%"><p>有没有能压缩更多参数的方法呢? 确实有, 通过<strong>非对称卷积</strong>, 每个nxn的卷积核都可以继续由1个1xn卷积核和1个nx1卷积核代替(Figure 6), 这样比原来参数更少, 也能实现更加高效的降维. 在Inception v2中, 设定n=7. 不要忘记论文开头的第一个原则, 在<strong>网络早期使用这种操作效果</strong>并不好. 除去降维外, 基于第二个准则, 设计了Figure 7 这种用来对特征提升维度的Inception模块. 提升后的高维特征更有益于模型的理解和处理, 也更加稀疏. 这种结构只在最后粗糙的8x8输入使用.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetv2-2.jpg" style="zoom:50%"><p>除此外, 还用并行结构对池化做了改进. 如果先池化后卷积可以减少, 直接进行池化, 可以减少运算量, 但会引入特征表示瓶颈. 为了遵循论文提出的第一个准则, 为了避免特征表示瓶颈, 先卷积后池化, 但这样运算量就会很大.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetv2-3.jpg" style="zoom:33%"><p>作者找到了一种两全其美的方法, 即卷积和池化同时进行, 再将它们的结果concat起来, 这样即避免了特征表示瓶颈, 也没有引入很多的计算量.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetv2-4.jpg" style="zoom:50%"><p>在<a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">Batch Normalization: Accelerating Deep Network Training b y Reducing Internal Covariate Shift</a> 中已经提到过<strong>Batch Normalization</strong>, 能够对网络模块之间达到<strong>解耦</strong>的效果, 加快收敛速度. 顺带改进了Inception v2.</p><p>相较于Inception v2, Inception v3只做了少量改动, 在先前基础上还进行了如下更改:</p><ol><li><p>使用<strong>RMSProp</strong>做优化器.</p></li><li><p>使用<strong>标签平滑</strong>防止过拟合.</p><blockquote><p>标签平滑可以看做是一种添加到损失公式中的一种正规化组件, 因为独热编码后的标签只有0和1, 可能有些过于绝对, 标签平滑提供了一种手段来使其中的0也能分配到一些数值. 我下面给出代码, 便能告诉你它的实现方式.</p></blockquote><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># input_tensor is a tensor in tensorflow</span>
<span class="token keyword">def</span> <span class="token function">label_smoothing</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> epsilon<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    classes <span class="token operator">=</span> input_tensor<span class="token punctuation">.</span>get_shape<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>as_list<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># compute the number of classes</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> epsilon<span class="token punctuation">)</span> <span class="token operator">*</span> input_tensor <span class="token operator">+</span> <span class="token punctuation">(</span>epsilon <span class="token operator">/</span> classes<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>label_smoothing<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>label_smoothing<span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># label smoothing(a): [0.03333334, 0.03333334, 0.93333334]</span>
<span class="token comment" spellcheck="true"># label smoothing(b): [0.05 0.95]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>辅助分类器使用了<strong>Batch Norm,</strong> 加快模型收敛速度.</p></li><li><p>7x7卷积拆成了1x7和7x1卷积的<strong>叠加</strong>, 同时输入图像从224x224变为<strong>299x299</strong>.</p></li></ol><p>Inception v2/v3结构如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetv3.jpg" style="zoom:67%"><h3 id="Inception-v4"><a href="#Inception-v4" class="headerlink" title="Inception v4"></a>Inception v4</h3><p>Inception v4比较保守, 对原来的版本进行了梳理, Inception v4在2016年的<a href="https://arxiv.org/abs/1602.07261" target="_blank" rel="noopener">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</a>中提出. 该论文中不但提出了Inception v4, 还借鉴了ResNet的思想, 研究了若干种Inception与残差连接之间的结合方式, 做了相当多的实验.</p><p>首先看Inception v4的大体结构(左)和主干部分(右):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetv4.jpg" style="zoom:25%"><p>Inception模块都有了相应改动, 相比v3添加了平均池化和1x1卷积, 下图从左到右分别为Inception-A, Inception-B, Inception-C:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetv4-1.jpg" alt=""></p><p>还有两种降维结构, 下图分别为35x35到17x17的Reduction-A和17x17到8x8的Reduction-B:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetv4-2.jpg" style="zoom:33%"><p>多次使用1x1卷积进一步降低了参数数量, 并且设计上更加<strong>简单一致</strong>, 抛去了Inception v3中不必要的部分, 比如辅助分类器等结构.</p><h3 id="Inception-ResNet"><a href="#Inception-ResNet" class="headerlink" title="Inception-ResNet"></a>Inception-ResNet</h3><p>Inception-ResNet结合了ResNet的思想, 与Inception v4出自同一篇论文. Inception-ResNet在Inception原有的基础上添加了残差链接, 作者研究了残差连接的作用, 指出残差连接并非能明显提升模型精度, 而是会加快训练的<strong>收敛速度</strong>. 在此基础上提出了两个版本的Inception-ResNet.</p><p>无论是Inception-ResNetv1还是Inception-ResNetv2都遵循同一个架构(左), 但Inception-ResNetv1采用了图示主干(右), 而Inception-ResNetv2却沿用了Inception v4的主干:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetres.jpg" style="zoom:67%"><h4 id="Inception-ResNetv1"><a href="#Inception-ResNetv1" class="headerlink" title="Inception-ResNetv1"></a>Inception-ResNetv1</h4><p>Inception-ResNetv1采用的Inception块如下, 下图从左到右分别为Inception-ResNet-A, Inception-ResNet-B, Inception-ResNet-C:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetresv1-1.jpg" style="zoom:50%"><p>35x35到17x17的Reduction与Inception v4相同, 17x17到8x8是新设计的结构:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetresv1-2.jpg" style="zoom:50%"><p>Inception-ResNetv1与Inception v3计算复杂度相近.</p><h4 id="Inception-ResNetv2"><a href="#Inception-ResNetv2" class="headerlink" title="Inception-ResNetv2"></a>Inception-ResNetv2</h4><p>Inception-ResNetv2沿用了Inception v4的主干, Reduction结构与Inception-ResNetv1完全相同.</p><p>只有三种j调节过参数的Inception-Res块. 其实v1和v2的不同只在于<strong>参数的不同和主干Stem的不同</strong>.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetresv2-1.jpg" style="zoom:50%"><p>Inception-ResNetv2与Inception v4计算复杂度相近.</p><h4 id="残差连接的问题"><a href="#残差连接的问题" class="headerlink" title="残差连接的问题"></a>残差连接的问题</h4><p>在引入残差连接后, 卷积核数量如果太多, 会导致训练不稳定, 甚至会出现网络训练休眠, 在池化层之前输出就为0. 对此引入一个<strong>放缩因子</strong>, 通过乘以一个非常小的系数再相加, 这样使得网络重新激活. 一般scale取0.1 ~ 0.3之间的值, 引入后不会降低精度还能使得网络更加稳定.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetres-scaling.jpg" style="zoom:50%"><p>当然图中的Inception可以替换为任意的子网络.</p><h3 id="Xception"><a href="#Xception" class="headerlink" title="Xception"></a>Xception</h3><p>2016年, Google利用深度可分离卷积对Inception v3进行了改良, 在<a href="https://arxiv.org/abs/1610.02357" target="_blank" rel="noopener">Xception: Deep Learning with Depthwise Separable Convolution</a> 中提出Xception模型(起名应该是模仿XGboost, 也是Extreme Inception的意思).</p><blockquote><p>关于深度可分离卷积可以单独开一篇文章了, 为了保证Inception系列的完整性, 先挖个坑.</p></blockquote><h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><blockquote><p>在《卷积神经网络小结》一文中, 在残差块部分有对残差网络的描述.</p></blockquote><p>随着网络深度的加深, 模型训练变得越来越复杂, 即使是使用Batch Norm或ReLU也无法解决梯度爆炸和梯度消失的现象, 仍然无法把网络做的足够深. 2015年何凯明在<a href="https://arxiv.org/abs/1512.03385v1" target="_blank" rel="noopener">Deep Residual Learning for Image Recognition</a> 中提出了带有跳跃连接的神经网络ResNet, 在2015年的ILSVRC中夺得冠军.</p><p>人们发现并不是网络越深性能越好, 例如下图中56层的模型无论是在训练集还是测试集的误差都比20层的模型要高得多, 到20层后的模型再向上堆叠效果反而会变差.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/resnet1.jpg" style="zoom:33%"><p>在反向传播中, 神经网络层数越多, 信息损失的就越多, 十分有可能发生梯度爆炸和梯度消失. 对于一个准确率近乎饱和网络来说, 再在这个网络的基础上添加新的结构是会使性能变差的, 上述问题也称为<strong>退化问题</strong>. 除非添加的神经层实现的映射是<strong>恒等映射</strong> , 即f(x) = x, 这样就能使网络在加深的同时还没有增加误差, 解决了模型性能和模型深度之间的悖论. 也就是说如果某层已经达到了趋近于饱和的性能, 那么接下来的学习目标就变为学习恒等映射. 以保证后面的其他层不会因为冗余层而导致精度下降.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/resnet2.jpg" style="zoom:50%"><p>从图中能很明显的看出, 这种恒等映射思想和<strong>LSTM</strong>中的思想极其相似, 当时何凯明也是基于LSTM得到的启发. 二者都尝试通过<strong>借鉴邻近的短期信息</strong>来减缓深层模型带来的梯度消失或爆炸的问题, 使得邻近的特征得以更长的传播. 而加法与乘法不同, 加法的梯度变化取决于信息之间的<strong>独立叠加</strong>, 而非乘法一样体现出输入信息之间的<strong>相互控制</strong>.</p><p>“残差”二字来源于目标值H(x)与冗余层前的输入值x的差值, 即后面的学习目标应该使残差目标F(x) = H(x) - x 逼近与0. 这种跳跃连接也称为<strong>Skip connections</strong>, 这种结构有良好的数学性质, 在进行反向传播时, 恒等映射的导数为1, 不会造成导数丢失.</p><p>当然, 上述内容都不能违背一个基本假设: <strong>对于一个有能力学习到恒等映射的深层网络, 它的性能绝对不会它的近乎饱和的浅层网络性能差</strong>.</p><p>ResNet结构与VGG19的对比图如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/resnet.png" style="zoom:33%"><p>在最右侧, 有两种Skip connection的实现:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/resnet3.jpg" style="zoom:50%"><p>左侧对应的是实线的实现方式, 右侧对应的是虚线的实现方式. 虚线的残差块中采用了1x1卷积来降低参数数量. 对于更深层的网络必须采用右侧的实现方式.</p><p>在论文中, 作者给出了不同层数的ResNet设计参数:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/resnet4.jpg" style="zoom:50%"><p>ResNet确实也做到了使得网络足够深, 甚至能达到152层. 也解决了网络越深越难训练的问题. 但是冗余层过多会带来更多的<strong>计算问题</strong>. 并导致特征<strong>过于冗余</strong>.</p><h2 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h2><p>为了解决ResNet在特征上的冗余问题, 在2016年<a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="noopener">Densely Connected Convolutional Networks</a>中沿用了ResNet和Highway Network的思想, 提出了DenseNet. 在Skip connections的基础上实现了<strong>特征复用</strong>. 作者提出了Dense Block作为新的网络结构. 作者认为通过更多的连接能够保证层与层之间的信息能最大程度的保留.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/densenet.jpg" style="zoom:50%"><p>Dense Block 相对于Residual Block来说更为<strong>激进和密集</strong>, 将同一个块中所有的层进行互联, 即每层都会接收前面所有层的输出作为额外输入, 与ResNet不同的地方在于, 连接并非相加, 而是直接<strong>concat</strong>, 这样就必须在块与块之间添加减小特征维数的结构.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/densenet1.jpg" alt=""></p><p>这种更密集的连接提供了更高的特征利用率, 在同等性能下减少了参数.</p><p>与ResNet相同, 在论文中作者也给出了不同深度的DenseNet设计参数:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/densenet2.jpg" style="zoom:50%"><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文的内容也都是浅尝辄止, 能够快速掌握近些年CNN的发展方向, 想深入了解还是需要精读论文. 由于CV方向应用落地快, 近些年CNN的研究也是突飞猛进, 提出了很多新CNN的结构, 这些新的结构并没有被写入.</p><p>下图出自<a href="https://arxiv.org/abs/1810.00736" target="_blank" rel="noopener">Benchmark Analysis of Representative Deep Neural Network Architectures</a>.</p><p>在这张图中, 圆圈越大, 模型参数越多, 圆圈越靠上, Top1准确率越高, 圆圈越靠右, 计算量越大. 相对位置越靠左上角, 模型的综合素质越好.</p><p>通过这种可视化, 可以更方便的对比模型之间的差距, 也方便梳理CNN的发展历史.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/cnnsummary.jpg" style="zoom:67%"><p>除文中提到的模型外, 在图中值得说一说的其他模型:</p><ul><li><strong>SqueeeNet</strong>: 2016年提出, 虽然准确率不高, 但大幅减少了模型参数, 减少了对算力和硬件的需求.</li><li><strong>MobileNet</strong>: 2017年提出, 在维持其准确率的同时, 保证了移动端的使用.</li><li><strong>NASNet</strong>: 2017年提出, 使AI能够在特殊任务上自行优化网络结构.</li><li><strong>SENet</strong>: 2017年提出, 通过网络反馈对不同的Feature Map分配权重.</li></ul></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">AnNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/38085.html">https://ADAning.github.io/posts/38085.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">AnNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/CNN/"><span class="chip bg-color">CNN</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/6744.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/3.jpg" class="responsive-img" alt="Transformer精讲"> <span class="card-title">Transformer精讲</span></div></a><div class="card-content article-content"><div class="summary block-with-text">2020.10.05: 更新训练技巧. 2020.09.27: 更新Masked Multi - Head Attention理解. 2021.06.08: 更新Teacher Forcing. TransformerTransform</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-09-21 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/Transformer/"><span class="chip bg-color">Transformer</span> </a><a href="/tags/Attention/"><span class="chip bg-color">Attention</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/35661.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/14.jpg" class="responsive-img" alt="别再对类别变量独热编码"> <span class="card-title">别再对类别变量独热编码</span></div></a><div class="card-content article-content"><div class="summary block-with-text">不要再对类别变量独热编码本文参考了Stop One-Hot Encoding Your Categorical Variables, 并对其内容在加以自身理解的情况下进行翻译. 独热编码对不同的类别变量就需要用到独热编码, 独热编码是将类别</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-09-04 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="post-category">机器学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"><span class="chip bg-color">特征工程</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">AnNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">340.9k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:695439722@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>