<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="Whisper: Robust Speech Recognition via Large-Scale Weak Supervision, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>Whisper: Robust Speech Recognition via Large-Scale Weak Supervision | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/21.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">Whisper: Robust Speech Recognition via Large-Scale Weak Supervision</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/Audio/"><span class="chip bg-color">Audio</span> </a><a href="/tags/ASR/"><span class="chip bg-color">ASR</span></a></div></div><div class="col s5 right-align"></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2025-01-14</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2025-01-14</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 3.1k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 11 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><h1 id="Robust-Speech-Recognition-via-Large-Scale-Weak-Supervision"><a href="#Robust-Speech-Recognition-via-Large-Scale-Weak-Supervision" class="headerlink" title="Robust Speech Recognition via Large-Scale Weak Supervision"></a>Robust Speech Recognition via Large-Scale Weak Supervision</h1><p>本文是论文<a href="https://dl.acm.org/doi/abs/10.5555/3618408.3619590" target="_blank" rel="noopener">Robust Speech Recognition via Large-Scale Weak Supervisions</a> 的阅读笔记和个人理解. 论文来自<strong>ICML 2023</strong>. 个人推荐看<a href="https://arxiv.org/abs/2212.04356" target="_blank" rel="noopener">Arxiv版本</a>的, 因为在ICML的版本中似乎阉割了一些内容.</p><h2 id="Whisper"><a href="#Whisper" class="headerlink" title="Whisper"></a>Whisper</h2><p>Whisper是OpenAI推出的语音处理模型, 使用清理后680k小时的<strong>大规模语音数据</strong>(弱监督)训练, 在诸多语音下无需微调, 性能超过或近似人类表现.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/whisper1.png" style="zoom:25%"><h3 id="Data-Processing"><a href="#Data-Processing" class="headerlink" title="Data Processing"></a>Data Processing</h3><p>Whisper没有做很重的预处理和标准化. 数据均来自互联网, 来自各种环境, 各种录音环境, 各种说话人, 各种语言.</p><p>但从互联网上获取的数据质量太低了, 作者采用了一些方法来提高数据质量:</p><ul><li>现有许多Transcripts都来自于现成的ASR系统, 作者基于一些启发式方法来检测机器生成的Transcripts, 并将它们过滤掉.</li><li>使用Audio Language Detector, 如果语音和转录语言不匹配, 则滤去该数据. Transcript是英文的除外, 会被当做<code>X -&gt; en</code> 的数据用于Speech Translation. 并在此时用模糊去重减少数据集中的重复.</li><li>将Audio与Transcripts按照30s的切片间隔拆分为若干个切片. 并将其中没有语音的Segment用于Voice Activity Detection.</li><li>在训练初始模型后, 汇总各数据源以及其模型表现, 并根据数据源和性能的组合来手动检查过滤数据集.</li><li>最后, 在Training Set和Validation Set, 基于Transcripts做去重.</li></ul><h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><p>模型层面, Whisper是一个常标准的Encoder - Decoder Transformer.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/whisper2.png" style="zoom:45%"><p>在输入到Encoder之前, 所有的音频都重采样16kHz, 输入音频数据归一化到<code>[-1, 1]</code> 之间近乎0均值, 并用80维的对数梅尔谱滤波器提取梅尔谱特征.</p><p>所有的Transformer Block都是<code>pre-norm</code>.</p><ul><li><strong>Encoder</strong>: Audio Encoder. Encoder侧由两个<code>Conv1d</code> 和<code>Gelu</code> 将[梅尔频谱](../Basic Knowledge/Audio Concept.md#梅尔频谱)转换为Audio Segment Representation. 转换后再加上正余弦位置编码.</li><li><strong>Decoder</strong>: Text Decoder. Decoder侧输入的是离散Token, 添加可学习的位置编码. 由于预期输出的Token不止英文, 是一个Multilingual Generation Task, 所以作者直接在GPT - 2的词表基础上扩充了其他语言的Token.</li></ul><p>单从模型角度来看, Whisper还是非常简单的.</p><h3 id="Multitask-Format"><a href="#Multitask-Format" class="headerlink" title="Multitask Format"></a>Multitask Format</h3><p>一个完整的语音处理系统应包括许多任务, 例如<strong>Transcription</strong>, <strong>Translation</strong>, <strong>Voice Activity Detection</strong>, <strong>Alignment</strong>, <strong>Language Identification</strong>等:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/whisper3.png" style="zoom:50%"><p>这些任务形式都可以被Decoder当做Input Token而形式统一. 尤其是Transcription还可以因<strong>上下文历史</strong>当做条件送入Decoder而在Long - form Transcription中受益(对话历史可以消歧, 生成更流畅的结果等等).</p><p>Decoder的Autoregressive Token的生成流程如下(其实就是一个<strong>完整的系统</strong>):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/whisper4.png" style="zoom:40%"><p>结合示意图来看:</p><ol><li>首先, 需要<code>&lt;|startoftranscript|&gt;</code> 声明Transcription要开始了, 类似于Language Model中的<code>&lt;sos&gt;</code>.</li><li>接着, 判断Audio Segment中是否有语音(Voice activity detection):<ul><li>如果有语音, 输出语音的语言类型(Language identification).</li><li>如果没有语音, 则需要输出<code>&lt;|nospeech|&gt;</code>. 直接结束转录.</li></ul></li><li>然后, 判断任务类型. 即<code>&lt;|transcribe|&gt;</code> 与 <code>&lt;|translate|&gt;</code>.</li><li>声明是否需要Time stamps:<ul><li>如果需要Time stamps, 则生成<code>&lt;起始时间戳&gt; + &lt;转录文本&gt; + &lt;结束时间戳&gt;</code>. Time stamps量化到最近的20ms内(Time - aligned Transcription).</li><li>如果不需要, 生成<code>&lt;|notimestamps|&gt;</code>, 后续只需要生成Transcription后的文本.</li></ul></li><li>最后, 结束转录时输出<code>&lt;|endoftranscript|&gt;</code>, 类似于Language Model中的<code>&lt;eos&gt;</code>.</li></ol><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的模型参数设置和实验设置, 以及数据集的相关信息请参考原论文.</p><h3 id="Zero-shot-amp-Evaluation-Metrics"><a href="#Zero-shot-amp-Evaluation-Metrics" class="headerlink" title="Zero - shot &amp; Evaluation Metrics"></a>Zero - shot &amp; Evaluation Metrics</h3><p>Whisper的预期是做一个强大的语音处理系统, 下文中所有<strong>实验的评估方式</strong>都是<strong>Zero - Shot</strong>的.</p><p>ASR中一般使用<strong>WER</strong> 作为评估指标. 但WER也可能因格式对齐之间的差异, 导致较低的WER. 这种问题在Whisper这种Zero - Shot模型的设置下尤为显著.</p><p>因此, Whisper采用了在计算WER的一种<strong>Text Normalization</strong>方法来解决这个问题, 主要是包括一些标点的去处, 语气词的去除, 还有数字的规范化等. 细节详见原论文, <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/whisper/english_normalizer.py" target="_blank" rel="noopener">transformers上已经有了实现</a>.</p><p>Whisper中使用的Text Normalization相比于Fair Speech的Normalizer有更好的效果:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/whisper5.png" style="zoom:33%"><h3 id="English-Speech-Recognition"><a href="#English-Speech-Recognition" class="headerlink" title="English Speech Recognition"></a>English Speech Recognition</h3><p>按照前人提出的”<a href="https://proceedings.neurips.cc/paper/2020/file/d8330f857a17c53d217014ee776bfd50-Paper.pdf" target="_blank" rel="noopener">Effective Robustness</a>“, 作者做出了Whisper与其他有监督ASR模型在LibriSpeech和其他12个English Academic ASR数据集上性能表现:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/whisper6.png" style="zoom:40%"><blockquote><p>Whisper上的不同坐标点代表不同规模与配置的Whisper.</p></blockquote><p>Whisper的弱监督后训练出来的表现与人类表现出的模式更相似, 在分布内外的不同数据集上具有更好的泛化能力, 表现也更加稳定. 而有监督ASR模型仅在分部内表现是良好的, 在分布外表现和分部内具有更大的差异.</p><p>与wav2vec 2.0在各类数据集上的Zero Shot比较结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/whisper7.png" style="zoom:40%"><p>尽管它们在LibriSpeech Clean上具有相同的WER, 但Whisper在其他数据集上的表现远超wav2vec, 这也佐证了研究分布外鲁棒性的重要性.</p><h3 id="Multi-lingual-Speech-Recognition"><a href="#Multi-lingual-Speech-Recognition" class="headerlink" title="Multi - lingual Speech Recognition"></a>Multi - lingual Speech Recognition</h3><p>在Multilingual LibriSpeech, VoxPopuli两个多语言的ASR数据集上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/whisper8.png" style="zoom:40%"><p>Whisper在MLS上表现非常好, VoxPopuli上表现不佳. 作者解释为其他模型将VoxPopuli作为无监督数据来源, 并且具有更多的有监督数据.</p><p>此外, 作者做出了在Fleurs上<strong>各语言有监督预训练数据量</strong>与<strong>各语言ASR</strong>之间的性能表现相关图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/whisper9.png" style="zoom:40%"><p>数据越多性能越好, 大约是每增加16倍数据WER就会减少一半. 大多数语言都符合作者发现的相关性. 少量语言例如中文(ZH), 韩语(KO), 希伯来语(HE)等偏离该规律可能是因为特殊的语系或者是数据质量, 分词之类导致的.</p><p>英语看起来也有点偏离, 可能是因为英语的边际效用导致受益递减.</p><blockquote><p>看起来它们都具有自己独特的文字, 也就意味着这些语言的Token不与英文共享, 确实不太容易从Multilingual Knowledge中受益.</p></blockquote><h3 id="Translation"><a href="#Translation" class="headerlink" title="Translation"></a>Translation</h3><p>作者在CoVoST2的子集上, 按照High, Mid, Low-resource, 测试了<code>X -&gt; En</code> 各模型的表现(指标是BLEU), 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/whisper10.png" style="zoom:40%"><p>Whisper在Zero - Shot下达到了SOTA, 在高资源情况下也能达到媲美Finetune的效果.</p><p>对不同语言进行更细致的分析, 将Fleurs从语言识别转为<code>X -&gt; en</code> 的翻译数据集, 以英文的Transcription当做Reference, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/whisper11.png" style="zoom:40%"><p>增加数据确实可以让模型效果更好, 但是也有非常多的异常值.</p><p>作者认为是Fleurs中存在很多语言识别错误, 例如将英语错分为威尔士语(CY), 虽然”威尔士语”有9k小时的数据, 但是BLEU却只有13. 大多的威尔士语音频只不过是带有英文字幕的英文音频. 因此它被错误的划分到Translation而不是Transcription当中.</p><blockquote><p>应该是这部分实验没做好, 所以没放在ICML的正式论文当中.</p></blockquote><h3 id="Language-Identification"><a href="#Language-Identification" class="headerlink" title="Language Identification"></a>Language Identification</h3><p>Whisper在Fleurs数据集上做的Language Identification, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/whisper12.png" style="zoom:40%"><p>Whisper的Zero - Shot结果却是无法与有监督的方法媲美. 作者认为一部分原因是Fleurs上的其中20多种语言Whisper完全没有见过. 如果仅看重叠的82种见过的语言, Whisper可以做到80.3%的Acc.</p><h3 id="Long-form-Transcription"><a href="#Long-form-Transcription" class="headerlink" title="Long - form Transcription"></a>Long - form Transcription</h3><h4 id="Strategies-for-Reliable-Long-form-Transcription"><a href="#Strategies-for-Reliable-Long-form-Transcription" class="headerlink" title="Strategies for Reliable Long - form Transcription"></a>Strategies for Reliable Long - form Transcription</h4><p>Whisper在训练的时候, 固定了上下文窗口长度为30s. 作者设计了多种启发式的方法来<strong>暂时缓解</strong>长文本转录中单个窗口中预测不准确对后续其他窗口产生负面影响的方法.</p><p>这套方法中包括:</p><ol><li>用Beam为5的<strong>Beam Search</strong>预测对数概率当做Score Function, 以此减少Greedy Decoding的重复问题.</li><li>以0作为起始的Temperature(即直接选取概率最大的Token), 当Token的平均概率分布低于-1或者文本的gzip压缩率高于2.4时, 以0.2为增长的步长增加Temperature. 当Temperature低于0.5时, 以前一个窗口的Transcription作为条件预测下一个窗口可以提高性能. 这种对Temperature的控制被称为”<strong>Temperature Fallback</strong>“.</li><li>将<code>&lt;nospeech&gt;</code> Token的阈值设为0.6, 且用对数概率阈值为-1能够提高Whisper检测语音活动的能力.</li><li>将Initial Timestamps设置为0到1之间能避免前几个词作为输入的错误模式.</li></ol><p>每一种方案都可以带来一定的收益, 但在各种数据集之间的提升并不是均匀的:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/whisper13.png" style="zoom:40%"><p>作者也提到, 上述方法实际上并没有办法完全消除错误预测带来的危害, 只是当做临时解决方案使用.</p><h4 id="Comparison-with-Other-ASR-Models"><a href="#Comparison-with-Other-ASR-Models" class="headerlink" title="Comparison with Other ASR Models"></a>Comparison with Other ASR Models</h4><p>在7个Long - form Transcription Dataset上和其他ASR模型对比表现如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/whisper14.png" style="zoom:25%"><p>Whisper的总体表现是相对稳定的, 而且发挥的非常好.</p><h4 id="Comparison-with-Human-Performance"><a href="#Comparison-with-Human-Performance" class="headerlink" title="Comparison with Human Performance"></a>Comparison with Human Performance</h4><p>由于数据集中的模糊或错标, WER很难说明数据集的改进空间. 作者从Kincaid46中选了25个录音, 找了5个专业的转录员来做转录, 其中一个计算机辅助(E), 剩余四个完全人工. 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/whisper15.png" style="zoom:33%"><p>Whisper仍然存在8.81的WER, 但是它在长格式转录的表现已经是目前ASR系统中最接近人类水平的了, 与纯人工相比差距不到1%.</p><h3 id="Robustness-to-Additive-Noise"><a href="#Robustness-to-Additive-Noise" class="headerlink" title="Robustness to Additive Noise"></a>Robustness to Additive Noise</h3><p>进一步测量Whisper的鲁棒性, 作者向测试数据添加一定的<strong>白噪声</strong>和<strong>酒吧</strong>中的噪声.</p><blockquote><p>酒吧中的噪声被认为是一种更自然更契合现实场景的噪声, 确实更符合ASR应用的真实情况.</p><p>让我联想到平时看过的一些做不同类型白噪音的Up主, 这些噪声更具多元化, 不知道目前有没有相关的研究?</p></blockquote><p>与其他Finetune过或预训练过的模型之间相比, 表现如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/whisper16.png" style="zoom:40%"><p>明显的, Whisper相较于其他ASR模型具有更好的抗噪能力.</p><blockquote><p>比较有趣的是, Whisper在添加了一点点噪声的情况下表现还会比不加噪声好一些, 很有可能说明Whisper训练预料中本身就是附带一部分噪声的, 人工添加的噪声使得分布与预训练时更相似.</p></blockquote><h2 id="Analysis-and-Ablations"><a href="#Analysis-and-Ablations" class="headerlink" title="Analysis and Ablations"></a>Analysis and Ablations</h2><h3 id="Dataset-Scaling"><a href="#Dataset-Scaling" class="headerlink" title="Dataset Scaling"></a>Dataset Scaling</h3><p>Whisper在ASR, MSR, Translation上随语音数据小时数变化如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/whisper17.png" style="zoom:40%"><p>数据越多越好, 数据确实是影响模型性能最关键的要素之一.</p><h3 id="Model-Scaling"><a href="#Model-Scaling" class="headerlink" title="Model Scaling"></a>Model Scaling</h3><p><strong>WER</strong>随模型Scaling变化表现如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/whisper18.png" style="zoom:33%"><p>越大越好. 但是英语似乎快摸到底了.</p><h3 id="Multitask-and-Multilingual-Transfer"><a href="#Multitask-and-Multilingual-Transfer" class="headerlink" title="Multitask and Multilingual Transfer"></a>Multitask and Multilingual Transfer</h3><p>在不同训练量下, 多任务多语言模型和只训英语的模型在英语语音识别数据集上的表现如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/whisper19.png" style="zoom:40%"><p>多语言多任务混合模型在训练量达到一定标准后, 拥有比只用英语数据训练模型更好的上限. 但在量达到之前, 还是会因为多语言多任务而加大模型的学习难度, 从而不利于英语任务的表现.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Whisper作为非常有代表性, 且成功的大大大工程, 已经在ASR领域被广泛运用, 技术已经算是非常成熟.</p><p>Whisper作为一个跨模态模型, 采用了便于扩展的Encoder - Decoder架构, 由于任务要求输出的是文本, 所以Whisper也可以从多任务统一为Next Token Prediction中受益, 语言之间的关联性也可以让Whisper在各类语言的Zero Shot都有不俗的表现.</p><p>在Whisper论文中进行的各类实验, 也为语音领域未来大大大工程的模型提供了指导. 虽然模型本身并不复杂(这也是当今的趋势), 但不得不从数据清洗, Text Normalization上看出OpenAI对工程问题的分析与处理还是相当细致的.</p><blockquote><p>此外, Whisper非常有OpenAI的风格. 其实也能从<strong>CLIP</strong>(Vision-Language)和Whisper(Audio-Language)中看出OpenAI在迈向各种模态大大大的野心.</p></blockquote><h2 id="Recommanded"><a href="#Recommanded" class="headerlink" title="Recommanded"></a>Recommanded</h2><p>最后, 根据<a href="https://zhuanlan.zhihu.com/p/692375653" target="_blank" rel="noopener">ASR神器：Whisper使用指南</a>扩展中提到的, 推荐两篇论文:</p><ul><li>加速: <a href="https://arxiv.org/abs/2311.00430" target="_blank" rel="noopener">Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling</a></li><li>幻觉: <a href="https://arxiv.org/abs/2303.00747" target="_blank" rel="noopener">WhisperX: Time-Accurate Speech Transcription of Long-Form Audio</a></li></ul><p>也推荐阅读OpenAI源码中的<a href="https://github.com/openai/whisper/blob/517a43ecd132a2089d85f4ebc044728a71d49f6e/whisper/transcribe.py#L184>" target="_blank" rel="noopener">Temperature Fallback</a>.</p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/63265.html">https://ADAning.github.io/posts/63265.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/Audio/"><span class="chip bg-color">Audio</span> </a><a href="/tags/ASR/"><span class="chip bg-color">ASR</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/22620.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/0.jpg" class="responsive-img" alt="EVA-GAN: Enhanced Various Audio Generation via Scalable Generative Adversarial Networks"> <span class="card-title">EVA-GAN: Enhanced Various Audio Generation via Scalable Generative Adversarial Networks</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: HiFi - GAN: HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis. EVA-</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2025-01-17 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/Audio/"><span class="chip bg-color">Audio</span> </a><a href="/tags/SVS/"><span class="chip bg-color">SVS</span> </a><a href="/tags/Vocoder/"><span class="chip bg-color">Vocoder</span> </a><a href="/tags/TTS/"><span class="chip bg-color">TTS</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/43190.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/13.jpg" class="responsive-img" alt="HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis"> <span class="card-title">HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis</span></div></a><div class="card-content article-content"><div class="summary block-with-text">HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis本文是论文HiFi-GAN: Generative Adve</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2025-01-03 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/Audio/"><span class="chip bg-color">Audio</span> </a><a href="/tags/SVS/"><span class="chip bg-color">SVS</span> </a><a href="/tags/Vocoder/"><span class="chip bg-color">Vocoder</span> </a><a href="/tags/TTS/"><span class="chip bg-color">TTS</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">399.6k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:andaning@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>