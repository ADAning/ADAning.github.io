<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="Pytorch学习: 张量进阶操作, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>Pytorch学习: 张量进阶操作 | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:.6"></i> <span>时间轴</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li><li><a href="/markdown"><i class="fab fa-markdown" style="margin-top:-20px;zoom:.6"></i> <span>Markdown</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li><li><a href="/markdown" style="margin-left:75px"><i class="fa fab fa-markdown" style="position:absolute;left:50px"></i> <span>Markdown</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/20.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">Pytorch学习: 张量进阶操作</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/%E7%BC%96%E7%A8%8B/"><span class="chip bg-color">编程</span> </a><a href="/tags/Pytorch/"><span class="chip bg-color">Pytorch</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2020-10-03</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2022-10-04</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 3.4k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 16 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p><strong>2020.10.03</strong>: 因torch版本更新, 对gather描述进行了修正.</p><p><strong>2021.03.11</strong>: 更新了对gather的描述.</p></blockquote><h1 id="Pytorch学习-张量进阶操作"><a href="#Pytorch学习-张量进阶操作" class="headerlink" title="Pytorch学习: 张量进阶操作"></a>Pytorch学习: 张量进阶操作</h1><p>整理内容顺序来自龙龙老师的<code>&lt;深度学习与PyTorch入门实战教程&gt;</code>, 根据个人所需情况进行删减或扩充. 如果想要自己创建新的模块, 这些操作都是基本功, 需要掌握扎实.</p><h2 id="拼接与拆分"><a href="#拼接与拆分" class="headerlink" title="拼接与拆分"></a>拼接与拆分</h2><h3 id="cat"><a href="#cat" class="headerlink" title="cat"></a>cat</h3><p><code>torch.cat(*tensor, dim)</code>能在指定的维度上将tensor拼接:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'b.shape:'</span><span class="token punctuation">,</span> b<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a concat b:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>a<span class="token punctuation">,</span> b<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
a.shape: torch.Size([2, 3])
b.shape: torch.Size([5, 3])
a concat b: torch.Size([7, 3])
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>当然, concat之前必须保证除了cat的维度的shape不同外, 其他维度的shape均相同.</p><h3 id="stack"><a href="#stack" class="headerlink" title="stack"></a>stack</h3><p>与<code>torch.cat()</code>不同, <code>torch.stack()</code>是将tensor堆叠在一个<strong>新的维度</strong>上, 即创建一个新的维度:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'b.shape:'</span><span class="token punctuation">,</span> b<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a concat b:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>a<span class="token punctuation">,</span> b<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a stack b:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>a<span class="token punctuation">,</span> b<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
a.shape: torch.Size([5, 3])
b.shape: torch.Size([5, 3])
a concat b: torch.Size([10, 3])
a stack b: torch.Size([2, 5, 3])
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>两个stack的tensor必须保持完全一致的维度.</p><h3 id="split"><a href="#split" class="headerlink" title="split"></a>split</h3><p><code>torch.split()</code>既可以按照指定dim的长度来拆分, 也可以按照类似步长的拆法来拆分.</p><p>当传入的参数是<code>list</code>时, 按照列表中指定的长度拆分:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span>
c <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> b<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'c.shape:'</span><span class="token punctuation">,</span> c<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
s <span class="token operator">=</span> c<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> i<span class="token punctuation">,</span> n <span class="token keyword">in</span> zip<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'aa'</span><span class="token punctuation">,</span> <span class="token string">'bb'</span><span class="token punctuation">,</span> <span class="token string">'cc'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> s<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'{}.shape:{}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>i<span class="token punctuation">,</span> n<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
c.shape: torch.Size([3, 32, 8])
aa.shape:torch.Size([1, 32, 8])
bb.shape:torch.Size([1, 32, 8])
cc.shape:torch.Size([1, 32, 8])
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这样能将c分别拆分为长度为1, 1, 1的tensor.</p><p>当传入的参数是<code>int</code>时, 意在使指定dim上每个tensor应该具有多长, 也可以理解为<strong>步长</strong>:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span>
c <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> b<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'c.shape:'</span><span class="token punctuation">,</span> c<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># s = c.split([1, 1, 1], dim=0)</span>
s <span class="token operator">=</span> c<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> i<span class="token punctuation">,</span> n <span class="token keyword">in</span> zip<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'aa'</span><span class="token punctuation">,</span> <span class="token string">'bb'</span><span class="token punctuation">,</span> <span class="token string">'cc'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> s<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'{}.shape:{}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>i<span class="token punctuation">,</span> n<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
c.shape: torch.Size([3, 32, 8])
aa.shape:torch.Size([2, 32, 8])
bb.shape:torch.Size([1, 32, 8])
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>虽然在dim=0上tensor长度为3, 但是还是能够拆分成2个tensor. 如果传入的整数是3, 则只会得到<code>aa.shape:torch.Size([3, 32, 8])</code>, 即每个tensor应该在dim=0上有3个元素.</p><h3 id="chunk"><a href="#chunk" class="headerlink" title="chunk"></a>chunk</h3><p>我认为<code>torch.chunk()</code>是为了和<code>torch.split()</code>同参数而<strong>防止歧义</strong>的区分函数.</p><p><code>torch.chunk()</code>中需要传入的就是需要分多少个tensor了, 还是上面那个例子, 体会区别:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span>
c <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> b<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'c.shape:'</span><span class="token punctuation">,</span> c<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># s = c.split([1, 1, 1], dim=0)</span>
s <span class="token operator">=</span> c<span class="token punctuation">.</span>chunk<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> i<span class="token punctuation">,</span> n <span class="token keyword">in</span> zip<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'aa'</span><span class="token punctuation">,</span> <span class="token string">'bb'</span><span class="token punctuation">,</span> <span class="token string">'cc'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> s<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'{}.shape:{}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>i<span class="token punctuation">,</span> n<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
c.shape: torch.Size([3, 32, 8])
aa.shape:torch.Size([1, 32, 8])
bb.shape:torch.Size([1, 32, 8])
cc.shape:torch.Size([1, 32, 8])
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="数学运算"><a href="#数学运算" class="headerlink" title="数学运算"></a>数学运算</h2><p>大多数操作与python中的基本类型操作区别不大, pytorch使用了<strong>运算符重载</strong>使得它们在tensor上保持相同的含义. pytorch也提供了函数为tensor做运算. 大部分函数仍然和numpy中的格式一致.</p><h3 id="加减乘除"><a href="#加减乘除" class="headerlink" title="加减乘除"></a>加减乘除</h3><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a + b和add:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>all<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>a <span class="token operator">+</span> b<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>add<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a - b和sub:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>all<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>a <span class="token operator">-</span> b<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>sub<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a * b和mul:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>all<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>a <span class="token operator">*</span> b<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a + b和div:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>all<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>a <span class="token operator">/</span> b<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>div<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
a + b和add: tensor(1, dtype=torch.uint8)
a - b和sub: tensor(1, dtype=torch.uint8)
a * b和mul: tensor(1, dtype=torch.uint8)
a + b和div: tensor(1, dtype=torch.uint8)
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>当然上面的例子中说的乘法不是矩阵乘法, 而是点乘(element - wise).</p><h3 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h3><p>如果要实现矩阵乘有三种方式:</p><ul><li><code>torch.mm()</code><strong>仅限于2d - tensor</strong>.</li><li><code>torch.matmul()</code>推荐使用.</li><li><code>@</code>是重载的运算符.</li></ul><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'mm in 2d:\n'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'matmul:\n'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'@:\n'</span><span class="token punctuation">,</span> a @ b<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
mm in 2d:
 tensor([[0.8500, 0.8500],
        [1.0964, 1.0964]])
matmul:
 tensor([[0.8500, 0.8500],
        [1.0964, 1.0964]])
@:
 tensor([[0.8500, 0.8500],
        [1.0964, 1.0964]])
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>记住, 如果tensor的维度比2大, 则默认在<strong>最后两个维度</strong>上进行运算, 也可以理解为对多个矩阵<strong>并行</strong>做矩阵乘法.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'matmul(a, b).shape:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># matmul(a, b).shape: torch.Size([4, 3, 28, 28])</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="指数和对数"><a href="#指数和对数" class="headerlink" title="指数和对数"></a>指数和对数</h3><p>既可以沿用python中的<code>**</code>做幂计算, 也可以使用<code>Tensor.pow()</code>.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.pow(2):\n'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>pow<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a ** 2:\n'</span><span class="token punctuation">,</span> a <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a ** 2.sqrt():\n'</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>a <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a ** 2.rsqrt():\n'</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>a <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>rsqrt<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a ** 2 ** 0.5:\n'</span><span class="token punctuation">,</span> a <span class="token operator">**</span> <span class="token number">2</span> <span class="token operator">**</span> <span class="token number">0.5</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
a.pow(2):
 tensor([[9., 9.],
        [9., 9.]])
a ** 2:
 tensor([[9., 9.],
        [9., 9.]])
a ** 2.sqrt():
 tensor([[3., 3.],
        [3., 3.]])
a ** 2.rsqrt():
 tensor([[0.3333, 0.3333],
        [0.3333, 0.3333]])
a ** 2 ** 0.5:
 tensor([[4.7288, 4.7288],
        [4.7288, 4.7288]])
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>Tensor.rsqrt()</code>代表的是求平方根后的倒数.</p><p>对数指数也是一样的用法:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>a<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'exp:'</span><span class="token punctuation">,</span> b<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'log:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
exp: tensor([[2.7183, 2.7183, 2.7183],
        [2.7183, 2.7183, 2.7183]])
log: tensor([[0., 0., 0.],
        [0., 0., 0.]])
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="近似值"><a href="#近似值" class="headerlink" title="近似值"></a>近似值</h3><p>近似值会在有小数点进行取舍时用到.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">3.14</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'a.floor()'</span><span class="token punctuation">,</span> <span class="token string">'a.ceil()'</span><span class="token punctuation">,</span> <span class="token string">'a.trunc()'</span><span class="token punctuation">,</span> <span class="token string">'a.frac()'</span><span class="token punctuation">,</span> <span class="token string">'a.round()'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token string">':'</span><span class="token punctuation">,</span> eval<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
a.floor(): tensor(3.)
a.ceil(): tensor(4.)
a.trunc(): tensor(3.)
a.frac(): tensor(0.1400)
a.round(): tensor(3.)
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><code>floor</code>: 下取整.</li><li><code>ceil</code>: 上取整.</li><li><code>trunc</code>: 只要整数部分.</li><li><code>frac</code>: 只要小数部分.</li><li><code>round</code>: 四舍五入.</li></ul><h3 id="裁剪"><a href="#裁剪" class="headerlink" title="裁剪"></a>裁剪</h3><p>这个操作在<strong>梯度裁剪</strong>非常常用. 当发生梯度爆炸或消失时, 使用梯度裁剪能将梯度控制在可控范围内.</p><p>在pytorch中,<code>Tensor.clamp(min, max)</code>函数作用等价于numpy的<code>np.clip()</code>.</p><pre class="line-numbers language-python"><code class="language-python">grad <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">15</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'max:'</span><span class="token punctuation">,</span> grad<span class="token punctuation">.</span>max<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'min:'</span><span class="token punctuation">,</span> grad<span class="token punctuation">.</span>min<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'grad:\n'</span><span class="token punctuation">,</span> grad<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'clamp(10):\n'</span><span class="token punctuation">,</span> grad<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'clamp(0, 10):\n'</span><span class="token punctuation">,</span> grad<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
max: tensor(13.1130)
min: tensor(1.6760)
grad:
 tensor([[10.3501, 13.1130,  1.6760],
        [ 3.1330,  7.5342,  4.7226]])
clamp(10):
 tensor([[10.3501, 13.1130, 10.0000],
        [10.0000, 10.0000, 10.0000]])
clamp(0, 10):
 tensor([[10.0000, 10.0000,  1.6760],
        [ 3.1330,  7.5342,  4.7226]])
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="属性统计"><a href="#属性统计" class="headerlink" title="属性统计"></a>属性统计</h2><h3 id="norm"><a href="#norm" class="headerlink" title="norm"></a>norm</h3><p>这里的norm指的不是标准化的那个normalization, 而是指的范数. 具体对范数的定义, 在这里就不再给出了, 请自己查询.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> a<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
c <span class="token operator">=</span> a<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.norm(1):'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'b.norm(1):'</span><span class="token punctuation">,</span> b<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'c.norm(1):'</span><span class="token punctuation">,</span> c<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.norm(2):'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'b.norm(2):'</span><span class="token punctuation">,</span> b<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'c.norm(2):'</span><span class="token punctuation">,</span> c<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
a.norm(1): tensor(8.)
b.norm(1): tensor(8.)
c.norm(1): tensor(8.)
a.norm(2): tensor(2.8284)
b.norm(2): tensor(2.8284)
c.norm(2): tensor(2.8284)
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这样得到的结果都是标量, 也可以按照dim来求范数:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> a<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
c <span class="token operator">=</span> a<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'dim 1:'</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'b.norm(1):'</span><span class="token punctuation">,</span> b<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'c.norm(1):'</span><span class="token punctuation">,</span> c<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'b.norm(2):'</span><span class="token punctuation">,</span> b<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'c.norm(2):'</span><span class="token punctuation">,</span> c<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
dim 1:
b.norm(1): tensor([4., 4.])
c.norm(1): tensor([[2., 2.],
        [2., 2.]])
b.norm(2): tensor([2., 2.])
c.norm(2): tensor([[1.4142, 1.4142],
        [1.4142, 1.4142]])
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="mean-sum-min-max-prod"><a href="#mean-sum-min-max-prod" class="headerlink" title="mean / sum / min / max / prod"></a>mean / sum / min / max / prod</h3><p>这一系列都是统计操作, 不是很难理解.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'a.min()'</span><span class="token punctuation">,</span> <span class="token string">'a.max()'</span><span class="token punctuation">,</span> <span class="token string">'a.mean()'</span><span class="token punctuation">,</span> <span class="token string">'a.prod()'</span><span class="token punctuation">,</span> <span class="token string">'a.sum()'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token string">':'</span><span class="token punctuation">,</span> eval<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
a.min(): tensor(0.)
a.max(): tensor(7.)
a.mean(): tensor(3.5000)
a.prod(): tensor(0.)
a.sum(): tensor(28.)
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="argmin-argmax"><a href="#argmin-argmax" class="headerlink" title="argmin / argmax"></a>argmin / argmax</h3><p>在求min和max时经常有一种操作, 找到一个tensor中最大或最小的元素并返回其<strong>索引</strong>, <code>Tensor.argmin()</code>和<code>Tensor.argmax()</code>就能实现这个功能. 不加参数默认为返回整个tensor中最大或最小的元素索引, 加dim后为沿着该维度切分tensor, 找到每个tensor最大或最小的元素索引.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.argmin():'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>argmin<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.argmin(dim=1):'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>argmin<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.argmax():'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.argmax(dim=1):'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
a.argmin(): tensor(2)
a.argmin(dim=1): tensor([2, 2])
a.argmax(): tensor(4)
a.argmax(dim=1): tensor([0, 1])
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="dim-keepdim"><a href="#dim-keepdim" class="headerlink" title="dim / keepdim"></a>dim / keepdim</h3><p><code>dim</code>和<code>keepdim</code>是作为<strong>参数</strong>放在前面所说的函数中的, 对于<code>dim</code>我们已经接触很多次了, 理解为沿着该维度进行某种操作. <code>keepdim</code>指的是在函数做完操作后还要不要维持原来的维度, 如:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.argmin(dim=1):'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.argmin(dim=1, keepdim=True):\n'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.argmin(dim=1).shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.argmin(dim=1, keepdim=True).shape:\n'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
a.argmin(dim=1): tensor([7, 2, 8, 7])
a.argmin(dim=1, keepdim=True):
 tensor([[7],
        [2],
        [8],
        [7]])
a.argmin(dim=1).shape: torch.Size([4])
a.argmin(dim=1, keepdim=True).shape:
 torch.Size([4, 1])
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在保存维度后, 结果的shape是<code>torch.Size([4, 1])</code>, 否则会被自动消去, 即<code>torch.Size([4])</code>.</p><h3 id="topk-kthvalue"><a href="#topk-kthvalue" class="headerlink" title="topk / kthvalue"></a>topk / kthvalue</h3><p>Top - k也是很常用的操作, 函数能返回最大的前k个值的相关信息. 在pytorch中, <code>Tensor.topk(k)</code>能返回最值和它们的索引.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'top3 in dim 1:\n'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>topk<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
top3 in dim 1:
torch.return_types.topk(
values=tensor([[0.9354, 0.8272, 0.8214],
        [0.9918, 0.8757, 0.8410],
        [0.9744, 0.8817, 0.8365],
        [0.9985, 0.8475, 0.8181]]),
indices=tensor([[3, 6, 0],
        [6, 0, 8],
        [7, 0, 9],
        [5, 4, 6]]))
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>因为输入的tensor大小为(4, 10), 并绑定dim=1, 所以返回了4条top3的value和index.</p><p>通过<code>largest</code>参数来控制选择最大值还是最小值, 当其为<code>True</code>时选择最大的k个值, <code>False</code>时选择最小的k个值.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'top-3 in dim 1:\n'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>topk<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> largest<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token string">""</span>"
top<span class="token number">-3</span> <span class="token keyword">in</span> dim <span class="token number">1</span><span class="token punctuation">:</span>
torch<span class="token punctuation">.</span>return_types<span class="token punctuation">.</span>topk<span class="token punctuation">(</span>
values<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.0362</span><span class="token punctuation">,</span> <span class="token number">0.0593</span><span class="token punctuation">,</span> <span class="token number">0.1323</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.1024</span><span class="token punctuation">,</span> <span class="token number">0.1237</span><span class="token punctuation">,</span> <span class="token number">0.1397</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.0160</span><span class="token punctuation">,</span> <span class="token number">0.1430</span><span class="token punctuation">,</span> <span class="token number">0.2003</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.2674</span><span class="token punctuation">,</span> <span class="token number">0.2686</span><span class="token punctuation">,</span> <span class="token number">0.4065</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
indices<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果你只是想要从小到大排列第k个值, 那么<code>Tensor.kthvalue</code>能满足你的需求.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a:'</span><span class="token punctuation">,</span> a<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.kthvalue(2):'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>kthvalue<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">"""
a: tensor([[0.2830, 0.2628, 0.2188, 0.9593, 0.6418],
        [0.8727, 0.2504, 0.8656, 0.3067, 0.6215],
        [0.0977, 0.7201, 0.1081, 0.2605, 0.7691],
        [0.4776, 0.9503, 0.9577, 0.4100, 0.6476]])
a.kthvalue(2): torch.return_types.kthvalue(
values=tensor([0.2628, 0.3067, 0.1081, 0.4776]),
indices=tensor([1, 3, 2, 0]))
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="compare"><a href="#compare" class="headerlink" title="compare"></a>compare</h3><p>和python中的比较大小运算符一样, 有<code>&gt;</code>, <code>&gt;=</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>!=</code>, <code>==</code>. 它们都能比较tensor之间的大小关系, 它们也都有相应的缩写函数. 这里不详细说了, 非要用函数再从网上查就可以了.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a <span class="token operator">></span> <span class="token number">0.5</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
tensor([[ 0.1539, -0.9071, -1.6426],
        [ 0.7671, -1.7312, -0.8053]])
tensor([[0, 0, 0],
        [1, 0, 0]], dtype=torch.uint8)
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>有一特殊函数<code>torch.eq(a, b)</code>和<code>torch.equal(a, b)</code>的返回值是不一样的, 前者返回Booltensor, 后者返回一个逻辑值, 只有全部相同时才为True, 否则为False.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>a<span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>equal<span class="token punctuation">(</span>a<span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
tensor([[1, 1, 1],
        [1, 1, 1]], dtype=torch.uint8)
True
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="高阶操作"><a href="#高阶操作" class="headerlink" title="高阶操作"></a>高阶操作</h2><h3 id="where"><a href="#where" class="headerlink" title="where"></a>where</h3><p>where的用法是<code>torch.where(condition, x, y)</code>, 返回对应位置上符合condition的tensor. 如果符合condition, 则对应元素为x, 否则为y.</p><pre class="line-numbers language-python"><code class="language-python">condition <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>condition<span class="token punctuation">)</span>
a <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>condition <span class="token operator">></span> <span class="token number">0.5</span><span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
tensor([[0.0559, 0.1984],
        [0.9894, 0.2738]])
tensor([[1., 1.],
        [0., 1.]])
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="gather"><a href="#gather" class="headerlink" title="gather"></a>gather</h3><p><code>torch.gather(input, dim, index)</code>的作用是将输入tensor按照指定的dim和index<strong>重新组合</strong>, <strong>类似于查表</strong>, 返回一个新的tensor. 它能<strong>收集特定维度的指定位置的数值</strong>. 这种操作常用于将概率转化为具体的类. 理解起来比较抽象.</p><pre class="line-numbers language-python"><code class="language-python">prob <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
index <span class="token operator">=</span> prob<span class="token punctuation">.</span>topk<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'index:'</span><span class="token punctuation">,</span> index<span class="token punctuation">)</span>
label <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">100</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'取到的classes:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>gather<span class="token punctuation">(</span>label<span class="token punctuation">.</span>expand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> index<span class="token operator">=</span>index<span class="token punctuation">.</span>long<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
index: tensor([[5, 3, 2],
        [0, 6, 7],
        [1, 4, 5],
        [2, 9, 7]])
取到的classes: tensor([[105, 103, 102],
        [100, 106, 107],
        [101, 104, 105],
        [102, 109, 107]])
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>index</code>传入的数据类型必须是<code>torch.LongTensor</code>.</p><p>再说的通俗一点, <code>index</code>的作用就像一个<strong>Mask</strong>一样, 存储的是<strong>指定dim上的位置索引</strong>, 再看一个更简单的例子:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
index_1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
index_2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>gather<span class="token punctuation">(</span>a<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> index<span class="token operator">=</span>index_1<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>gather<span class="token punctuation">(</span>a<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> index<span class="token operator">=</span>index_2<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
tensor([[1., 2., 3.],
        [4., 5., 6.]])
tensor([[1., 5., 6.],
        [1., 5., 3.]])
tensor([[1., 2.],
        [6., 4.]])
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果还没明白, 参照官网给的3d - tensor解释结合起来:</p><pre class="line-numbers language-python"><code class="language-python">out<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> input<span class="token punctuation">[</span>index<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span>k<span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># if dim == 0</span>
out<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> input<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>index<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">[</span>k<span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># if dim == 1</span>
out<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> input<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span>index<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># if dim == 2</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>以官网的角度说一下上面那个例子. 假设gather后的向量名为<code>new_tensor</code>, 我们可以手动模拟这个过程和构造出gather相同的结果.</p><p><strong>dim=0</strong>时, 对于a来说, <strong>第0维</strong>位置index是通过<strong>查表</strong>获得的.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token triple-quoted-string string">"""
out[i][j] = input[index[i][j]][j] if dim == 0

a = torch.Tensor([[1,2,3],[4,5,6]])
index_1 = torch.LongTensor([[0,1,1],[0,1,0]])
"""</span>
new_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>
    <span class="token punctuation">[</span>a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span>a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>new_tensor<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
tensor([[1., 5., 6.],
        [1., 5., 3.]])
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>dim=1</strong>时, 对于a来说, <strong>第1维</strong>位置index是通过<strong>查表</strong>获得的.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token triple-quoted-string string">"""
out[i][j] = input[i][index[i][j]] if dim == 1

a = torch.Tensor([[1,2,3],[4,5,6]])
index_1 = torch.LongTensor([[0,1,1],[0,1,0]])
"""</span>
new_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>
    <span class="token punctuation">[</span>a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span>a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>new_tensor<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
tensor([[1., 2.],
        [6., 4.]])
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>index能遍历到沿dim上所有的元素, 最起码应该<strong>保证index和输入在除去指定的dim外其他dim上shape相同</strong>.</p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/1216.html">https://ADAning.github.io/posts/1216.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/%E7%BC%96%E7%A8%8B/"><span class="chip bg-color">编程</span> </a><a href="/tags/Pytorch/"><span class="chip bg-color">Pytorch</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/3996.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/17.jpg" class="responsive-img" alt="ELMo, GPT, BERT"> <span class="card-title">ELMo, GPT, BERT</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文的前置知识: RNN Transformer Language Model ELMo, GPT, BERT本文是对ELMo, GPT, BERT三个模型的结构介绍以及个人理解, 多图预警. Introduction由于NLP领域</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-10-04 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"><span class="chip bg-color">词向量</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/42255.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/17.jpg" class="responsive-img" alt="Pytorch学习: 张量基础操作"> <span class="card-title">Pytorch学习: 张量基础操作</span></div></a><div class="card-content article-content"><div class="summary block-with-text">Pytorch学习: 张量基础操作整理内容顺序来自龙龙老师的&lt;深度学习与PyTorch入门实战教程&gt;, 根据个人所需情况进行删减或扩充. 如果想要自己创建新的模块, 这些操作都是基本功, 需要掌握扎实. 张量数据类型下表摘自Py</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-10-02 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/%E7%BC%96%E7%A8%8B/"><span class="chip bg-color">编程</span> </a><a href="/tags/Pytorch/"><span class="chip bg-color">Pytorch</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">368.5k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:695439722@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>