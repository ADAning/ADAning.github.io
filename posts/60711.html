<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>ALBERT: A Lite BERT for Self-supervised Learning of Language Representations | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/3.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/BERT/"><span class="chip bg-color">BERT</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2021-06-29</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2022-03-27</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 2.3k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 8 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p>本文前置知识:</p><ul><li>BERT: 详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>.</li></ul></blockquote><h1 id="ALBERT-A-Lite-BERT-for-Self-supervised-Learning-of-Language-Representations"><a href="#ALBERT-A-Lite-BERT-for-Self-supervised-Learning-of-Language-Representations" class="headerlink" title="ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"></a>ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</h1><p>本文是论文<a href="http://arxiv.org/abs/1909.11942" target="_blank" rel="noopener">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a>的阅读笔记和个人理解. 最近忙着毕业季, 赶巧眼病又发作了, 就拖更了几天.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>现有的NLP模型都太大了, <strong>计算资源短缺</strong>已经成为越来越显著的问题.</p><p>一般来说, 参数到达一定数量后增加参数只能带来轻微的性能提升, 模型参数过多后还容易出现过拟合, 反而导致性能下降. 作者尝试使用<strong>减少参数</strong>的多种方法, 构造一个轻量级的BERT, 能逼近原BERT的效果.</p><h2 id="ALBERT"><a href="#ALBERT" class="headerlink" title="ALBERT"></a>ALBERT</h2><p><strong>ALBERT</strong>(<strong>A</strong> <strong>L</strong>ite <strong>BERT</strong>)尝试使用三种主要的手段来节省额外的参数开销.</p><h3 id="Factorized-Embedding-Parameterization"><a href="#Factorized-Embedding-Parameterization" class="headerlink" title="Factorized Embedding Parameterization"></a>Factorized Embedding Parameterization</h3><p>在BERT系列模型架构中, Token的Embedding大小$E$ 和Encoder的Hidden Layer大小$H$ 是完全绑定的, 即$E \equiv H$, 独热编码会直接通过Embedding转换到大小为$H$ 的维度.</p><p>从<strong>建模角度</strong>来说, Word Embedding更多强调<strong>上下文无关</strong>的表示, 即Token本身在无语境时最多出现的意思, 而Hidden Layer Embedding更多强调<strong>上下文相关</strong>的表示. 但实际上我们应该希望上下文相关的部分能使用更多参数, 即$H \gg E$, 这样能最大化BERT中的参数的使用效率.</p><p>从<strong>实践角度</strong>来说, NLP经常需要非常大的字典大小$V$, 如果$E \equiv H$, 增大$H$ 的同时必然增大$E$, $H$ 和$E$ 绑在一起的思路就不太实用.</p><p>因此, ALBERT将这一部分拆分为两步, 把独热编码直接转换到$H$ 的过程拆分为两个步骤, <strong>先映射到低维嵌入空间$E$, 然后再投影到隐层大小</strong>$H$. 这样就将Embedding这部分的参数大小从$O(V \times H)$ 缩少到$O(V \times E + E \times h)$. 当$E\ll H$ 时能减少很多参数.</p><blockquote><p>例如, 词表大小$V=30000$, 在BERT中, $E=H=768$, 不考虑位置编码, Embedding的总参数为$30000 \times 768$. 在ALBERT中, $E \ne H$, 假设$E=128, H=768$, Embedding总参数为$30000 \times 128 + 128 \times 768$, 确实有减少.</p></blockquote><h3 id="Cross-Layer-Parameter-Sharing"><a href="#Cross-Layer-Parameter-Sharing" class="headerlink" title="Cross - Layer Parameter Sharing"></a>Cross - Layer Parameter Sharing</h3><p>因为BERT是Transformer Encoder堆叠起来的, 假如只用训练一组共用参数, 然后让所有层都使用这一组参数, 岂不是能大幅减少参数? 确实, ALBERT利用这种方法减少了相当多的参数量.</p><p>但即使是多层参数复用, 也有多种复用方法. 每个Transformer Encoder由FFN和Multi Head Attention组成, 所以就有<strong>仅复用Attention</strong>, <strong>仅复用FFN</strong>, <strong>直接复用Encoder</strong>三种复用方式, 作者在后文的实验中探索了这三种方式的效果.</p><p>下图为每一层中同一个Token的表示的L2距离和余弦相似度:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/albert1.jpg" style="zoom:50%"><p>ALBERT比BERT的曲线要平滑得多, 这说明共享参数有助于稳定网络参数.</p><blockquote><p>跨层的参数共享, 应该是ALBERT中减少参数最有效的方法, 对于共享参数对性能产生的负面效果, 在训练阶段使用一些Trick能弥补一些.</p></blockquote><h3 id="Inter-Sentence-Coherence-Loss"><a href="#Inter-Sentence-Coherence-Loss" class="headerlink" title="Inter - Sentence Coherence Loss"></a>Inter - Sentence Coherence Loss</h3><p>作者指出, NSP任务是可以偷懒的. NSP任务将同一个文档中的两个连续句子的句子对作为正例, 将不同文档的两个句子的句子对作为负例. 因此, NSP任务实际上可以划分为Topic Prediction和Coherence Prediction两个任务:</p><ul><li><strong>Topic</strong> Prediction: 预测前后两个句子的主题是否相同.</li><li><strong>Coherence</strong> Prediction: 预测前后两个句子是否是真正连续的.</li></ul><p>Topic Prediction的难度远小于Coherence Prediction, 所以NSP不一定能真正有益于模型在下游任务上的表现, 也符合其他研究人员的结论.</p><p>ALBERT也废除了NSP任务, 但从NSP的Coherence Prediction角度出发, 设计了<strong>SOP</strong>(<strong>S</strong>entence <strong>O</strong>rder <strong>P</strong>rediction)任务. SOP任务保留<strong>Coherence Prediction</strong>, 将同一个文档中顺序正确的两个连续句子的句子对作为正例, 将它们<strong>交换顺序</strong>后的句子对作为负例, 这样就消除了Topic Prediction的目标.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文, 作者将ALBERT在实验中所用到的几个设置与BERT进行了对比:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/albert2.jpg" style="zoom:50%"><blockquote><p>ALBERT - xxlarge的层数是12而非24, 说明此时宽的模型比深的模型效果要更好, 可能层数已经达到极限.</p><p>4096的HIdden Size与BERT - large相比已经相当大了.</p></blockquote><p>与BERT不同的是, ALBERT在90%的情况下都使用最大的文本输入长度, 仅有10%的概率使用比最大文本长度更小的输入.</p><p>并且, 作者借鉴了<a href="https://adaning.github.io/posts/34019.html#toc-heading-3">SpanBERT</a>中的<strong>N - Gram Masking</strong>, 即每次生成Mask都有$p(n)$ 的概率生成长度为$n$ 的Mask:</p><p>$$<br>p(n)=\frac{1 / n}{\sum_{k=1}^{N} 1 / k}<br>$$</p><p>作者设置最大长度$n=3$.</p><h3 id="Overall-Comparision-between-BERT-and-ALBERT"><a href="#Overall-Comparision-between-BERT-and-ALBERT" class="headerlink" title="Overall Comparision between BERT and ALBERT"></a>Overall Comparision between BERT and ALBERT</h3><p>作者将BERT的各类配置与ALBERT的各类配置在<strong>相同训练量</strong>的多个任务上做了实验, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/albert3.jpg" style="zoom:50%"><p>ALBERT - xxlarge应该是打榜用的, 性能超过了BERT - large. ALBERT - large的性能已经接近于BERT - large.</p><p>在作者设置的ALBERT配置中, 同等型号的ALBERT比BERT的速度要快一些, 但同等型号的ALBERT性能却要比BERT差许多. 只有<strong>跨配置</strong>比较, 才能保证性能相似, 但<strong>跨配置的ALBERT在推理速度上不占优势</strong>.</p><h3 id="Factorized-Embedding-Parameterization-1"><a href="#Factorized-Embedding-Parameterization-1" class="headerlink" title="Factorized Embedding Parameterization"></a>Factorized Embedding Parameterization</h3><p>作者调整了ALBERT - base的$E$ 大小, 分别对比了共享参数和不共享参数的情况.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/albert4.jpg" style="zoom:50%"><p>从结果来看, 在复用Encoder的情况下, $E=128$ 似乎是一个比较好的解, $E &gt; 128$ 性能开始退化.</p><h3 id="Cross-Layer-Parameter-Sharing-1"><a href="#Cross-Layer-Parameter-Sharing-1" class="headerlink" title="Cross - Layer Parameter Sharing"></a>Cross - Layer Parameter Sharing</h3><p>针对仅复用Attention, 仅复用FFN, 直接复用Encoder三种情况, 作者在各类下游任务上做了实验, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/albert5.jpg" style="zoom:50%"><p>不共享参数的情况自然是性能最好的, 共享FFN似乎比较容易掉点.共享Attention影响没有那么大. 作者坚持复用Encoder的策略.</p><h3 id="Sentence-Order-Prediction-SOP"><a href="#Sentence-Order-Prediction-SOP" class="headerlink" title="Sentence Order Prediction (SOP)"></a>Sentence Order Prediction (SOP)</h3><p>作者比较了不使用额外任务, NSP, SOP三者之间对ALBERT - base的影响, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/albert6.jpg" style="zoom:50%"><p>使用NSP确实会损害模型性能, 使用SOP确实会让模型涨点, 但是SST - 2的性能和不使用额外任务相当.</p><h3 id="Train-for-the-Same-Amount-of-Time"><a href="#Train-for-the-Same-Amount-of-Time" class="headerlink" title="Train for the Same Amount of Time"></a>Train for the Same Amount of Time</h3><p>在前面的实验中, ALBERT - xxlarge比BERT - large的速度要慢许多. 通常情况下, 更长的训练时间会有更好的性能, 这可能导致二者比较的不公平.</p><p>作者在此不再控制二者训练量相同, 而是将BERT和ALBERT拉到了相同训练时间下比较, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/albert12.jpg" style="zoom:50%"><p>在同训练时间下, ALBERT要优于BERT, 即ALBERT训练较为高效.</p><h3 id="Additional-Training-Data-and-Dropout-Effects"><a href="#Additional-Training-Data-and-Dropout-Effects" class="headerlink" title="Additional Training Data and Dropout Effects"></a>Additional Training Data and Dropout Effects</h3><p>RoBERTa和XLNet比BERT所使用的数据要多得多, 作者尝试将额外的训练数据添加到训练中, 前后对比结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/albert7.jpg" style="zoom:50%"><p>除去SQuAD, 其他下游任务的性能略有提升, 这是因为除去维基百科的数据集, 其他数据相对于SQuAD来说是一种噪声, 即Out of Domain, 也许维基百科数据集已经能比较有针对性的解决SQuAD问题了.</p><p>即使是训练了1M Step, ALBERT也没有找到局部最优, 所以作者在ALBERT上去掉了Dropout, 使得性能有进一步的提升:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/albert9.jpg" style="zoom:50%"><blockquote><p>其实很好理解为什么去掉Dropout后模型能继续训练下去, 模型本身都没法达到过拟合, 何谈防止过拟合? 在模型没能找到局部最优时, 加入正则化手段自然而然会损害训练.</p></blockquote><p>上面二者在训练过程中对ACC的影响如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/albert8.jpg" style="zoom:50%"><p>添加额外数据和移除Dropout后, 验证集ACC有显著提升.</p><h3 id="Current-SOTA-and-NLU-Tasks"><a href="#Current-SOTA-and-NLU-Tasks" class="headerlink" title="Current SOTA and NLU Tasks"></a>Current SOTA and NLU Tasks</h3><p>作者把流行的Baseline放到一起在GLUE上做了对比, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/albert10.jpg" style="zoom:50%"><p>ALEBRT(1M) 代表ALBERT在该任务上Train了1M个Step, 与RoBERTa训练量相同.</p><p>在SQuAD和RACE上的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/albert11.jpg" style="zoom:50%"><p>ALBERT仍然是SOTA.</p><p>附录中还有一些对加强ALBERT宽度和深度的实验, 结果都表明当深度或宽度到达一定阈值后, 性能不再能继续增加, 甚至有时会出现退化.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>ALBERT的主要贡献是减了参数, 而不是计算量. 换句话说, 只能让BERT跑起来, 但推理阶段跑的快不快就不管了. 这似乎显得ALBERT有些鸡肋了, 因为减少模型参数后它的精度仍然会受到影响, 如果继续增大深度运算量又是个门槛($H$ 比较大, 而且层数多了, 肯定会增大运算量), 这点大家好像吐槽比较多.</p><p>ALBERT能够与BERT媲美的本质可能是$H$ 增大所带来增益要超过缩小参数带来的负面效应.</p><p>ALBERT的改进点都比较偏向于工程, <del>比起论文它更像一篇炼丹报告…,</del> 看实验结果感觉应该是到ALBERT所使用的压缩方法的性能顶峰了.</p><blockquote><p>模型压缩的目的从来都不是使得小模型的效果好过大模型, 而是利用某种方式, 使得模型的参数量或计算量减少, 同时<strong>不会带来明显的性能下降</strong>.</p></blockquote></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/60711.html">https://ADAning.github.io/posts/60711.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/BERT/"><span class="chip bg-color">BERT</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/39586.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/11.jpg" class="responsive-img" alt="知识蒸馏: Distilling the Knowledge in a Neural Network"> <span class="card-title">知识蒸馏: Distilling the Knowledge in a Neural Network</span></div></a><div class="card-content article-content"><div class="summary block-with-text">Distilling the Knowledge in a Neural Network本文是论文Distilling the Knowledge in a Neural Network的阅读笔记和个人理解. Basic Idea现有机器学</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2021-07-03 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/KD/"><span class="chip bg-color">KD</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/14266.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/19.jpg" class="responsive-img" alt="UniLM: Unified Language Model Pre-training for Natural Language Understanding and Generation"> <span class="card-title">UniLM: Unified Language Model Pre-training for Natural Language Understanding and Generation</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: BERT: 详见ELMo, GPT, BERT. UniLM: Unified Language Model Pre-training for Natural Language Understanding and G</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2021-06-18 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/BERT/"><span class="chip bg-color">BERT</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">396.1k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:andaning@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>