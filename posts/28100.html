<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="CoLAKE: Contextualized Language and Knowledge Embedding, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>CoLAKE: Contextualized Language and Knowledge Embedding | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/10.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">CoLAKE: Contextualized Language and Knowledge Embedding</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/KGE/"><span class="chip bg-color">KGE</span> </a><a href="/tags/BERT/"><span class="chip bg-color">BERT</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/" class="post-category">知识图谱</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2020-10-29</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2022-03-27</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 3.8k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 14 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><h1 id="CoLAKE-Contextualized-Language-and-Knowledge-Embedding"><a href="#CoLAKE-Contextualized-Language-and-Knowledge-Embedding" class="headerlink" title="CoLAKE: Contextualized Language and Knowledge Embedding"></a>CoLAKE: Contextualized Language and Knowledge Embedding</h1><blockquote><p>本文前置知识:</p><ul><li>BERT</li><li>Self - Attention</li></ul><p><strong>2020.11.11</strong>: 想通了CoLAKE在训练时最关键的部分.</p><p><strong>2020.11.22</strong>: 在读完KEPLER后, 重温一遍CoLAKE, 更新实验部分.</p></blockquote><p>本文是论文<a href="http://arxiv.org/abs/2010.00309" target="_blank" rel="noopener">CoLAKE: Contextualized Language and Knowledge Embedding</a>的阅读笔记和个人理解. 这篇论文的很多工作与<strong>KEPLER</strong>相似, 建议先阅读<a href="https://adaning.github.io/posts/52897.html">我对KEPLER的讲解</a>, 再来看CoLAKE.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>这两年关于KGE的PTM很火爆, 论文开头便指出了现在将知识注入的PTM的劣势, 这些现存的嵌入一般都是空洞的, 静态的, 不灵活的. 这些PTM都有一些共同的<strong>短板</strong>:</p><ul><li>实体嵌入是<strong>单独</strong>训练的, 然后再使用到PTM中, 导致<strong>知识嵌入</strong>和<strong>语言嵌入</strong>不是同时嵌入的, 即没有真正做到<strong>联合嵌入</strong>.</li><li>在做实体嵌入时, 很少能全部的捕捉到丰富的<strong>上下文</strong>信息, 导致模型的性能被预训练的实体嵌入所<strong>限制</strong>.</li><li>预训练的实体嵌入是<strong>静态</strong>的, 当知识图谱发生轻微变化时(例如添加一个新的实体), 需要<strong>重新训练</strong>.</li></ul><p>基于上述缺点, 作者提出了CoLAKE, 这是一种能够根据<strong>上下文</strong>, 实现<strong>语言</strong>和<strong>知识</strong>的<strong>联合嵌入</strong>的<strong>Masked Language Model</strong>.</p><h2 id="CoLAKE"><a href="#CoLAKE" class="headerlink" title="CoLAKE"></a>CoLAKE</h2><p>CoLAKE(<strong>Co</strong>ntextualized <strong>L</strong>anguage <strong>a</strong>nd <strong>K</strong>nowledge <strong>E</strong>mbedding), CoLAKE能根据知识上下文和语言上下文来<strong>动态</strong>的表示实体. 对于每个实体, 将该实体与知识相<strong>关联</strong>的部分作为子图, 视为该实体的上下文. CoLAKE能动态访问不同的常识, 根据<strong>背景知识</strong>来更好的帮助模型理解实体在上下文中的含义, 而并非只关注实体本身.</p><blockquote><p>题外话:</p><p>其实我第一次听CoLAKE这个名字是在前两天举办的YSSNLP上, 邱锡鹏老师在预训练模型的演讲里提的. 我当时感觉哈利波特那个图(下面第一张图就是)好像在哪见过, 后来确实在邱老师9月份演讲的PDF里找到了, 只不过没标模型的名字而已.</p><p>当时还有听众提出了一个问题: 外部知识应该如何引入影响来语义的呢?</p><p>邱老师虽然没给出具体的方案, 但指出了大致的一条思路: Token的表示可能会根据外部知识的<strong>影响</strong>或<strong>变化</strong>, 从而改变它的表示, 这样能获得更<strong>精确</strong>的语义表示.</p><p>读完这篇10月份发的论文才明白, 其实邱老师说的是CoLAKE.</p></blockquote><h3 id="Word-Knowledge-Graph"><a href="#Word-Knowledge-Graph" class="headerlink" title="Word - Knowledge Graph"></a>Word - Knowledge Graph</h3><p>WK Graph(Word - Knowledge Graph)是为了处理<strong>异构</strong>的<strong>语言</strong>和<strong>知识图谱</strong>而引入的, 这种结构能将它们<strong>统一</strong>在同一个数据结构下. 知识在知识图谱中的存储方式是<strong>三元组</strong>, 而语言的存储方式一般是<strong>无结构化的文本数据</strong>. Transformer的<strong>自注意力</strong>机制可以看做是基于单词的<strong>全连接图</strong>, 所以图是表示知识和语言更<strong>通用</strong>的结构.</p><h4 id="WK-Graph-Example"><a href="#WK-Graph-Example" class="headerlink" title="WK Graph Example"></a>WK Graph Example</h4><p>下面来举一个例子体会WK Graph的作用, 在本节先关注这张图片的左侧:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/colake1.jpg" style="zoom:67%"><p>像ERNIE, KnowBERT之类的模型的实体嵌入和语言嵌入是<strong>半上下文联合</strong>的, 而CoLAKE是<strong>全上下文联合</strong>嵌入. 那么模型在CoLAKE中如何理解下面这两个句子呢?</p><blockquote><ol><li>Harry Potter points his wnad at Lord Voldemort.</li><li>“You have Lily’s hazel eyes”, he told Harry Potter.</li></ol></blockquote><p>CoLAKE能在KG中搜索<code>Harry Potter</code>相关的知识, 得到<code>(Harry Potter, enemy of, Lord Voldemort)</code>和<code>(Harry Potter, mother, Lily Poter)</code>这两个三元组在图中的表示. 然后用前者来帮助理解句子1, 后者帮助理解句子2. 这样就使得知识能够在不同的上下文中更灵活的运用.</p><p>右侧说明了Word - Knowledge Graph的结构. <strong>内圈</strong>是原文中多个单词<strong>全连接</strong>形成的<strong>Word Graph</strong>, 而<strong>外圈</strong>是经过知识扩展过的<strong>Knowledge Subgraph</strong>. 二者以相同的实体为桥接处, 将Word Graph和Knowledge Subraph<strong>拼接</strong>就形成了Word - Knowledge Graph.</p><h4 id="Graph-Construction"><a href="#Graph-Construction" class="headerlink" title="Graph Construction"></a>Graph Construction</h4><p>本节主要说明WK Graph是如何生成的.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/colake2.jpg" style="zoom:67%"><p>生成图的方式其实没有想象中的复杂. 我们先通过实体连接器将KG中能被找到的实体作为<strong>锚点</strong>(Anchor Nodes), 然后将基于KG延伸出的Subgraph在这个锚点的基础上进行<strong>扩展</strong>. 锚点在图中用红色的虚线外轮廓标识. 延伸的过程有利于将Knowledge Subgraph和Word Graph的实体嵌入进同一个空间, 这也就是一直在强调的<strong>联合嵌入</strong>.</p><p>在Word Graph中有三种类型的节点, 分别是Word Nodes, Entity Nodes, Relation Nodes, 也就分别对应着图中黄色, 蓝色, 绿色的节点.</p><p>注意观察图中的节点编号, 在锚点的Knowledge Subgraph中, 锚点连接的实体和关系节点的编号是<strong>依赖于锚点</strong>的, 这种方式也称为<strong>Soft - Position Index</strong>, 在之后还会提到.</p><p>CoLAKE只使用了与锚点相邻的<strong>15</strong>个随机关系和实体作为Subgraph, 然后并入WK Graph中.</p><h3 id="CoLAKE-Architecture"><a href="#CoLAKE-Architecture" class="headerlink" title="CoLAKE Architecture"></a>CoLAKE Architecture</h3><p>CoLAKE的整个结构基于BERT. 所以使用的Basic Blcok是Transformer Encoder. 如果对BERT不了解的建议参考我以前写的<code>&lt;ELMo, GPT, BERT&gt;</code>, 对Transformer不理解的请参照<code>&lt;Transformer精讲&gt;</code>, 看完后理解起来会好一些.</p><p>话不多说, 直接看结构图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/colake3.jpg" style="zoom:67%"><p>请观察CoLAKE与BERT的<strong>不同点</strong>. 并且这幅图画的非常严谨, 在右侧的Knowledge Graph并不是全连接的, 左侧的Word Graph是全连接的, Token Embedding的颜色与之前Word Graph颜色也是相对应的.</p><h4 id="Embedding-Layer"><a href="#Embedding-Layer" class="headerlink" title="Embedding Layer"></a>Embedding Layer</h4><p>CoLAKE对BERT的Embedding做了改动. 在BERT中采用的Encoding是Token Encoding, <strong>Segment Encoding</strong>, Position Encoding. 而CoLAKE中使用的是Token Encoding, <strong>Type Encoding</strong>, Position Encoding.</p><h5 id="Token-Embedding"><a href="#Token-Embedding" class="headerlink" title="Token Embedding"></a>Token Embedding</h5><p>对于Token的Embedding不必多说, CoLAKE与BERT都还是使用<strong>查表</strong>的方式, 但CoLAKE是单词, 实体, 关系<strong>分别查找</strong>:</p><ul><li>对于单词的Embedding, CoLAKE使用了<strong>BPE</strong>, 能减小词表, 这已经是一种常见的Subword技巧.</li><li>对于实体和关系, 我们分别建两张表, 直接学习实体和关系的<strong>独立</strong>表示.</li></ul><h5 id="Type-Encoding"><a href="#Type-Encoding" class="headerlink" title="Type Encoding"></a>Type Encoding</h5><p>CoLAKE不涉及到Segment的问题, 所以将Segment Encoding替换成了Type Encoding. 在Word Graph中有三种类型的节点, 分别是Word Nodes, Entity Nodes, Relation Nodes. 所以要对这三种类型<strong>分别</strong>加以编码.</p><h5 id="Position-Encoding"><a href="#Position-Encoding" class="headerlink" title="Position Encoding"></a>Position Encoding</h5><p>与BERT相比, CoLAKE多了Knowledge Subgraph, 所以使用了<strong>Soft - Position Index</strong>的方式对Knowledge Subgraph中的节点进行位置编码.</p><blockquote><p>Soft - Position Index: 允许<strong>重复</strong>的Position出现, 将以锚点作为头实体的三元组记为$&lt;h, r, t&gt;$, 若锚点头实体在句子中的位置为$x$, 则关系$r$ 位置记为$x+1$, 尾实体位置记为$x+2$. 这样能够保证三元组的<strong>位置连续</strong>.</p></blockquote><h4 id="Masked-Transformer-Encoder"><a href="#Masked-Transformer-Encoder" class="headerlink" title="Masked Transformer Encoder"></a>Masked Transformer Encoder</h4><p>对于图中的节点矩阵$\mathbf{X}$ , 附加<strong>Mask</strong>的注意力机制如下:<br>$$<br>\begin{aligned}<br>\mathbf{Q}, \mathbf{K}, \mathbf{V} &amp;=\mathbf{X} \mathbf{W}^{Q}, \mathbf{X} \mathbf{W}^{K}, \mathbf{X} \mathbf{W}^{V} \\<br>\mathbf{A} &amp;=\frac{\mathbf{Q} \mathbf{K}^{\top}}{\sqrt{d_{k}}} \\<br>\operatorname{Attn}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) &amp;=\operatorname{Softmax}(\mathbf{A}+\mathbf{M}) \mathbf{V}<br>\end{aligned}<br>$$<br>Mask矩阵$\mathbf{M}$ 可以通过以下方式求得:<br>$$<br>\mathbf{M}_{i j}=\left\{\begin{array}{ll}<br>\quad 0 &amp; \text { if } x_{i} \text { and } x_{j} \text { are connected } \\<br>-\inf &amp; \text { if } x_{i} \text { and } x_{j} \text { are disconnected}<br>\end{array}\right.<br>$$</p><blockquote><p>认真看一下, 这个Mask的作用可不是Transformer Decoder中的Mask, 它是取决于<strong>节点矩阵</strong>的, 只是为了能让节点只能看到<strong>邻近一跳</strong>的信息.</p></blockquote><h3 id="Pre-Training-Objective"><a href="#Pre-Training-Objective" class="headerlink" title="Pre - Training Objective"></a>Pre - Training Objective</h3><p>CoLAKE也是一个Masked Language Model, 所以它也是通过对句子中的Token随机Mask, 然后从词表中基于上下文将Mask的Token预测出来. 将BERT的Mask模式迁移到图中, 只是将Mask Token变为Mask Node, 我们仍然是对15%的Node进行随机选中, 但选中后的操作<strong>略有不同</strong>, 下面将BERT和CoLAKE做个对比:</p><table><thead><tr><th>概率</th><th>BERT</th><th>CoLAKE</th></tr></thead><tbody><tr><td>80%</td><td>将被选中的Token替换为<code>[Mask]</code></td><td>将被选中的Node替换为<code>[Mask]</code></td></tr><tr><td>10%</td><td>将被选中的Token替换为<strong>任意词</strong></td><td>将被选中的Node替换为<strong>与原节点同类的节点</strong></td></tr><tr><td>10%</td><td>不做任何替换</td><td>不做任何替换</td></tr></tbody></table><p>在WK Graph中有三大类节点, 被Mask后所对应的意义是不同的:</p><ul><li><p>Masking Word Nodes: 与BERT的情况是一致的, 但CoLAKE在预测Word时除了依赖上下文还能依赖<strong>知识</strong>做出预测, 因为锚点与Word是全连接的关系, 而锚点受到知识的影响.</p></li><li><p>Masking Entity Nodes: 如果被Mask的实体是<strong>锚点</strong>, 那么则依靠它的上下文进行预测. 如果不是锚点, 那么CoLAKE的目标就是KGE.</p></li><li><p>Masking Relation Nodes: 如果Mask的是两锚点之间的关系, 那么目标就与<strong>关系抽取</strong>一致. 否则, 目标就是预测两实体之间的关系, 这与传统的KGE方法相似.</p></li></ul><p>然而, 在预训练时预测被Mask的锚点可能比较简单, 模型很容易就能用外部知识而不是依赖上下文完成这个人物, 因此在预训练时会<strong>丢掉50%</strong>的锚点邻居.</p><blockquote><p>在Mask做训练时, 从Knowledge Embedding的角度来看待, CoLAKE的方式有点像<strong>多任务训练</strong>. 因为Mask了不同属性的节点会导致CoLAKE所利用的信息不同, 执行任务的初始条件也不同.</p><p>我开始对Mask掉锚点后仍然能够利用知识库中的内容表示疑惑, 我开始认为在不知道锚点的情况下, 知识库中所存在的知识应该是不能成功进行链接的. 后来我从另一个角度出发, 作为人类, <strong>我们常已知了句子中的上下文和锚点相关的知识(除去锚点本身), 我们完全可以根据其他与锚点相关的属性和实体来猜出锚点是什么</strong>, 这样就完全说得通了.</p></blockquote><h3 id="Model-Training"><a href="#Model-Training" class="headerlink" title="Model Training"></a>Model Training</h3><p>CoLAKE通过对三种不同类型的节点进行训练, 采用交叉熵作为损失函数. 但是因为知识图谱中的实体数量实在是太庞大了, 所以实现CoLAKE有两个问题:</p><ul><li><p>实体数量较大, 训练输入时几乎不可能在GPU上维护一个entity embedding矩阵, 作者给出的解决方案是把entity embedding放在CPU上, 把模型的其他部分放在多张GPU上, GPU上的训练进程从CPU中读出entity embedding同时向CPU写入对应的梯度, CPU上的进程根据收集的梯度对entity embedding进行异步地更新.</p></li><li><p>实体数量过于庞大导致Softmax十分耗时, 所以简单的采用<strong>负采样</strong>解决这个问题.</p></li></ul><p>该部分来自<a href="https://zhuanlan.zhihu.com/p/263012775" target="_blank" rel="noopener">CoLAKE: 同时预训文本和知识</a>. 问题1隐射出当前KG和NLP发展可能还是受算力的制约, 相较与CV, NLP需要更多的计算来表达信息.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="DataSet-and-Implementation-Details"><a href="#DataSet-and-Implementation-Details" class="headerlink" title="DataSet and Implementation Details"></a>DataSet and Implementation Details</h3><p>CoLAKE主要用了两个数据集:</p><ul><li>English Wikipedia.</li><li>Wikidata5M(出自<strong>KEPLER</strong>).</li></ul><p>CoLAKE使用英文的维基百科作为预训练数据, 使用了Hugging Face的Transformer来实现, 和RoBERTa一样使用BPE, 直接使用了<strong>RoBERTa</strong>的权重初始化, 在1e-4的学习率下只训练了一轮.</p><h3 id="Knowledge-Driven-Tasks"><a href="#Knowledge-Driven-Tasks" class="headerlink" title="Knowledge Driven Tasks"></a>Knowledge Driven Tasks</h3><p>在<strong>知识驱动</strong>型任务上, 作者主要将CoLAKE与同样的注入知识的PLM进行<strong>横向对比</strong>, 其次与不注入知识的RoBERTa和BERT<strong>纵向对比</strong>, CoLAKE表现相当不错:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/colake5.jpg" style="zoom:25%"><p>左侧Open Entity对应的任务为实体分类, 右侧FewRel对应的任务为关系抽取.</p><h3 id="Knowledge-Probing"><a href="#Knowledge-Probing" class="headerlink" title="Knowledge Probing"></a>Knowledge Probing</h3><p>LAMA(<strong>LA</strong>nguage <strong>M</strong>odel <strong>A</strong>nalysis) probe任务的目的是定性的测量模型到底存储了多少<strong>常识</strong>. 作者希望通过LAMA来观察CoLAKE对知识的掌握程度, 为公平起见, 作者还对所有模型只使用词汇之间有交集的部分.通过对Mask掉的知识进行预测, 取得它们的P@1, 实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/colake6.jpg" style="zoom:50%"><p>BERT的表现倒是很亮眼. 作者认为K - Adapter比CoLAKE在两个数据集上效果好的原因是基于RoBERTa的LARGE版本, 并且使用了LAMA的一部分数据进行训练. CoLAKE总体而言的表现比RoBERTa要好非常多.</p><blockquote><p>BERT比使用更多数据的RoBERTa效果好太多了, 这与提出LAMA的论文<a href="https://arxiv.org/abs/1909.01066" target="_blank" rel="noopener">Language Models as Knowledge Bases?</a>结论一致.</p></blockquote><h3 id="Language-Understanding-Tasks"><a href="#Language-Understanding-Tasks" class="headerlink" title="Language Understanding Tasks"></a>Language Understanding Tasks</h3><p>目前的许多研究表明, 注入知识后, PLM的NLU能力可能会<strong>退化</strong>. 作者在GLUE常用数据集上对RoBERTa, KEPLER, CoLAKE做了实验:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/colake7.jpg" style="zoom:50%"><p>从实验结果来看, CoLAKE并没有发生很严重NLU能力退化, 与KEPLER相比, 真的只是比RoBERTa差一点点, KEPLER就退化的比较严重.</p><blockquote><p>从这个实验来看, NLU能力是和<strong>语言知识</strong>相关的, 如果给模型灌输一些<strong>世界知识</strong>, 原来存储语言知识的部分可能会被<strong>干扰</strong>, 可能是引入了许多的噪声, 我们或许还没有很好的掌握如何运用这些知识.</p><p>我认为将KEPLER和CoLAKE放在一起对比, 一定是CoLAKE的WK Graph起到了某种作用, 能够保留模型对语言模型的理解能力. 沿着这个思路, <strong>图</strong>或<strong>网</strong>的结构是非常重要的. 如果对CoLAKE的WK Graph进一步改进, 或许能够提升模型的NLU能力.</p></blockquote><h3 id="Word-Knowledge-Graph-Completion"><a href="#Word-Knowledge-Graph-Completion" class="headerlink" title="Word - Knowledge Graph Completion"></a>Word - Knowledge Graph Completion</h3><p>因为CoLAKE加入了 Word - Knowledge Graph的结构, 它本质上已经变为了一个预训练好的GNN. 作者希望利用这个特性, 来测试CoLAKE对结构和语义特征的建模能力. 只要在FewRel上做关系抽取, 就能使其对关系进行补全.</p><p>与KEPLER相同, 这里作者给出了两种设置:</p><ul><li><strong>Transductive setting</strong>: 对于每个样本, 两个实体$h, t$, 和它们的关系$r$, 可能分别在训练, 验证, 还是测试中出现过. 但它们的整体表示三元组$(h, r, t)$ 却没有在训练数据中出现.</li><li><strong>Inductive setting</strong>: 对于每个样本, 至少有一个实体在训练阶段是模型没见过的. 这更考验模型的推断能力. 即将两个实体中至少一个在训练阶段没见过的实体进行Mask, 然后利用其邻居节点来预测该节点的表示.</li></ul><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/colake4.jpg" style="zoom:50%"><p>实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/colake8.jpg" style="zoom:25%"><p>CoLAKE无论在Transductive Setting还是Inductive Setting上都比Baseline强大非常多.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>CoLAKE有如下特点(或说主要贡献):</p><ul><li>它是一个<strong>Masked Language Model</strong>, 并能将上下文知识表示和上下文语言表示<strong>联合嵌入</strong>.</li><li>由于<strong>Word - Knowledge Graph</strong>的存在, 它能够轻松地将<strong>异构</strong>的知识信息和语言信息<strong>融合</strong>.</li><li>因为它本质上是一个预训练的<strong>图神经网络</strong>, 所以具有结构感知能力, 并且易于<strong>扩展</strong>.</li></ul><p>CoLAKE真的是和我理想中将<strong>外部知识注入PTM</strong>的方式非常相似了.</p><p>CoLAKE提供了一个统一的数据结构. 这样非常有利于将来其他形式的数据注入到其中, 当然注入的方式可能是一个值得研究的问题.</p><p>顺带一提, 从这些Knowledge Enriched PTM来看, BERT非常受大家的青睐, 说明BERT与XLNet相比更为<strong>简洁</strong>, 容易被大家所理解和接受.</p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/28100.html">https://ADAning.github.io/posts/28100.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/KGE/"><span class="chip bg-color">KGE</span> </a><a href="/tags/BERT/"><span class="chip bg-color">BERT</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/30287.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/12.jpg" class="responsive-img" alt="InteractE: Improving Convolution-based KGE by Increasing Feature Interactions"> <span class="card-title">InteractE: Improving Convolution-based KGE by Increasing Feature Interactions</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: ConvE Depth - wise Convolution 2020.11.14: 对实验进行部分补充. InteractE: Improving Convolution-based Knowledge Graph</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-11-06 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/" class="post-category">知识图谱</a></span></div></div><div class="card-action article-tags"><a href="/tags/KGE/"><span class="chip bg-color">KGE</span> </a><a href="/tags/%E5%BE%AA%E7%8E%AF%E5%8D%B7%E7%A7%AF/"><span class="chip bg-color">循环卷积</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/59193.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/20.jpg" class="responsive-img" alt="AcrE: Atrous Convolution and Residual Embedding"> <span class="card-title">AcrE: Atrous Convolution and Residual Embedding</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: 膨胀卷积(空洞卷积) 残差连接 Knowledge Graph Embedding with Atrous Convolution and Residual Learning本文是论文Knowledge Graph</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-10-27 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/" class="post-category">知识图谱</a></span></div></div><div class="card-action article-tags"><a href="/tags/KGE/"><span class="chip bg-color">KGE</span> </a><a href="/tags/%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF/"><span class="chip bg-color">空洞卷积</span> </a><a href="/tags/%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5/"><span class="chip bg-color">残差连接</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">390.5k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:andaning@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>