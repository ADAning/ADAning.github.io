<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="Large Model并行优化, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>Large Model并行优化 | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/16.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">Large Model并行优化</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/"><span class="chip bg-color">并行计算</span> </a><a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"><span class="chip bg-color">分布式</span> </a><a href="/tags/ZeRO/"><span class="chip bg-color">ZeRO</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2023-06-01</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2023-06-01</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 3.6k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 13 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><h1 id="Large-Model并行优化"><a href="#Large-Model并行优化" class="headerlink" title="Large Model并行优化"></a>Large Model并行优化</h1><h2 id="为什么要并行优化"><a href="#为什么要并行优化" class="headerlink" title="为什么要并行优化?"></a>为什么要并行优化?</h2><p><strong>大就是好</strong>, 虽然丛2019年人们的认识普遍就是大就是好, 这个概念在当今依然没有被改变, 只是有了更深刻的认识.</p><p>所以, 为什么要并行?</p><ul><li>虽然大就是好, 模型太大<strong>显存</strong>吃不消(空间).</li><li>虽然大就是好, 但是模型太大<strong>速度</strong>也吃不消(时间).</li></ul><p>目标即<strong>难点</strong>:</p><ul><li>大模型训练时, 中间过程保存的<strong>变量</strong>及<strong>参数</strong>成为负担.</li><li>大模型训练时, <strong>通信开销</strong>不可忽视.</li></ul><p>当今经典分布式并行优化方式主要有三种:</p><ul><li>流水线并行(Pipeline Parallelism).</li><li>数据并行(Data Parallelism).</li><li>张量并行(Tensor Parallelism).</li></ul><h2 id="数据并行-Data-Parallelism"><a href="#数据并行-Data-Parallelism" class="headerlink" title="数据并行 Data Parallelism"></a>数据并行 Data Parallelism</h2><blockquote><p>请参考<a href="https://zhuanlan.zhihu.com/p/617133971" target="_blank" rel="noopener">图解大模型训练之：数据并行上篇(DP, DDP与ZeRO) - 知乎</a>.</p></blockquote><p>顾名思义, 数据并行(DP)是直接在Batch的维度上进行划分, 将多个Batch拆分到多个节点上进行计算.</p><h3 id="参数服务器-Parameter-Server"><a href="#参数服务器-Parameter-Server" class="headerlink" title="参数服务器 Parameter Server"></a>参数服务器 Parameter Server</h3><p>数据并行最经典的例子是参数服务器(Server), 会在每个节点(Wokrer)上都存储同一份模型, 然后将Batch下放到每个不同的Wokrer上, 完成Forward和Backward, 最后将每个Wokrer算完的梯度回传到一个参数服务器上, 由参数服务器聚合各节点的梯度, 再将聚合后的梯度 / 新的模型参数 <strong>广播</strong>到各个Wokrer上:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lmparallel1.png" style="zoom:67%"><p>各个计算节点(Wokrer) 将梯度上传到参数服务器之后, 参数服务器可能会有两种实现:</p><ol><li>PS计算平均梯度(或加权梯度), 并代替各Worker完成模型参数更新, 之后将参数下放到各计算节点中.</li><li>PS代替仅仅计算平均梯度(或加权梯度), 但不更新模型参数, 而是将计算完的梯度下放到各个Worker当中, 由各个节点自主更新各节点上的模型参数.</li></ol><p>而聚合梯度外加下放梯度这个过程, 被称为”<strong>AllReduce</strong>“.</p><p>由于计算体系内的带宽各不同, 主要考虑AllReduce的开销, 不同的参数服务器聚合方式可能会产生不同的耗时.</p><p>数据并行在每个Worker上都存放了一份模型参数, 所以其实造成了大量<strong>冗余</strong>, 并且Server需要向每个Worker都传输一份梯度 / 模型参数.</p><p>所以, 每当Worker在接收参数或者梯度的时候, 一直在<strong>空转</strong>, 造成了利用率不高. 为了避免这种情况, 可以将<strong>梯度异步更新</strong>, 让Worker拿旧的模型参数来跑新的数据, 但是异步也不能太异步, 可以设定一个延迟步数来保证权重不会太久没有发生更新.</p><p>异步更新由于拿到的梯度不稳定, 会<strong>减缓收敛速度</strong>, 发散的风险也提高了.</p><h3 id="Ring-AllReduce"><a href="#Ring-AllReduce" class="headerlink" title="Ring - AllReduce"></a>Ring - AllReduce</h3><p>Ring - AllReduce, 现在<a href="https://www.youtube.com/watch?v=Cvdhwx-OBBo&ab_channel=PyTorch" target="_blank" rel="noopener">Pytorch的分布式数据并行(DDP)</a>用的就是这种实现方式, 用于多机训练场景.</p><p>DP中最大的缺点就是在AllReduce中, Server需要和其他所有的Worker通信, 这个通信过程使得每个Worker的计算通信比不高. Server有问题, 那就把所有的通信压力全部转移到Worker上, <strong>人人都是Worker</strong>, <strong>人人又都是Server</strong>.</p><p>Ring-AllReduce将该过程拆分为两个部分, <strong>Reduce - Scatter</strong>和<strong>All - Gather</strong>.</p><h4 id="Reduce-Scatter"><a href="#Reduce-Scatter" class="headerlink" title="Reduce - Scatter"></a>Reduce - Scatter</h4><p>在Reduce - Scatter中, 所有Worker都在拓扑结构上与相邻的两个Worker通信, 因此构成一个<strong>拓扑环</strong>(Ring):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lmparallel3.png" style="zoom:40%"><p>假设一共有$N$ 块GPU, 每块GPU记为$i$, 且$i = 1, 2, \dots, N$. 那么我们把每块GPU上计算得到的梯度拆分为$N$ 份, 称为 $i$ 的 $N$ 个Gradient Chunk $G_i$.</p><p>每次通信时, 每块GPU $i$ 都会将自己的某个梯度块 $G_i[i]$ 传递到拓扑环上相邻的下一块GPU $i+1$ 上, 使得下一块GPU的梯度块$G_{i+1}[i] = G_{i+1}[i] + G_i[i]$.</p><p>$i$ 同时接收拓扑环中上一块GPU $i-1$ 的某个梯度块$G_{i-1}[i-1]$, 加到自己的对应位置梯度块$G_{i}[i-1]$ 上面, 使得$i$ 的GPU的梯度块$G_{i}[i-1] = G_{i}[i-1] + G_{i-1}[i-1]$:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lmparallel4.png" style="zoom:40%"><p>如此反复, 每块GPU都会发送出上次自己接收到梯度块的位置的梯度块到下一个相邻节点, 并接收上个节点送来的梯度块:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lmparallel5.png" style="zoom:40%"><p>经过$N-1$ 次传递后, 每块GPU上都拥有了一个完整的梯度块, 这个梯度块被累加过$N$ 次, 也就是经过所有GPU运算得到的梯度之和. 即对于每块GPU $i$, 都应有$G[i]=G_{i}[i]= \sum_{j=1}^{N} G_j[i]$.</p><h4 id="All-Gather"><a href="#All-Gather" class="headerlink" title="All - Gather"></a>All - Gather</h4><p>All - Gather与Reduce - Scatter过程几乎完全一样, 只不过把累加操作变为了<strong>直接替换</strong>的操作, 将每块GPU上得到的一块”完整的梯度块”发送到拓扑环上下个相邻节点:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lmparallel6.png" style="zoom:60%"><p>同样是经过$N-1$ 次操作后, 每块GPU上便拥有了经过所有数据计算得到的完整梯度:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lmparallel7.png" style="zoom:40%"><p>此时在每块GPU上分别完成模型参数更新.</p><h4 id="通信量分析"><a href="#通信量分析" class="headerlink" title="通信量分析"></a>通信量分析</h4><p>假设模型参数大小为$\Phi$, 则梯度大小也为$\Phi$, 每个梯度块的大小为$\frac{\Phi}{N}$, 对于单块GPU来说有:</p><ul><li>Reduce - Scatter的通信量为$(N-1)\frac{\Phi}{N}$.</li><li>All - Gather的通信量也为$(N-1)\frac{\Phi}{N}$.</li></ul><p>所以单卡通信总量为$2(N-1)\frac{\Phi}{N}$, 当$N \rightarrow \infty$ 时, 全卡通信总量可以近似为$2N\Phi$. 虽然通信量与DP相同, 但Ring - AllReduce把负载均摊到了每个Worker上.</p><blockquote><p>Reduce - Scatter的本质是从通信角度把Server - Worker之间的串行通信变为了Worker之间的并行通信, 同时利用了所有GPU的计算资源.<br>之所以对每块GPU上的梯度分块就是这个原因, 如果梯度不分块, 又从环退化回了串行通信.</p></blockquote><h2 id="模型并行-Model-Parallelism"><a href="#模型并行-Model-Parallelism" class="headerlink" title="模型并行 Model Parallelism"></a>模型并行 Model Parallelism</h2><p>单卡装不下模型的时候, 最自然的想法就是把模型的各个部分拆分到每个GPU上分别做Forward和Backward, 然后最后再汇总起来:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lmparallel2.png" style="zoom:67%"><p>说起来轻巧, 怎么拆呢?</p><h3 id="流水线并行-Pipeline-Parallelism"><a href="#流水线并行-Pipeline-Parallelism" class="headerlink" title="流水线并行 Pipeline Parallelism"></a>流水线并行 Pipeline Parallelism</h3><blockquote><p>请参考<a href="https://zhuanlan.zhihu.com/p/613196255" target="_blank" rel="noopener">图解大模型训练之：流水线并行（Pipeline Parallelism）, 以Gpipe为例 - 知乎</a>.</p></blockquote><p>最简单的, 按层拆呗, 流水线并行也可以被看做是<strong>层间并行</strong>. 把模型的所有层分成多份, <strong>分别拆到每块GPU去算</strong>. 但是这样在Forward和Backward时都会有问题, 由于模型Forward是顺序串行的, 所以Forward和Backward也是顺序串行的. 即使是这样做了, 还是会存在两个问题:</p><ol><li>串行导致GPU<strong>利用率</strong>很低, 大部分时间在空转.</li><li>随着模型规模的增大, 每块GPU上每层的<strong>中间状态</strong>的显存开销也非常大. 虽然这个原因不是流水线并行本身导致的, 但它会因模型大小而削弱模型并行所带来的优势.</li></ol><p>针对上述两点, 有两种解决办法.</p><p>其中一种缓解的方法, 就是把数据并行也引入. 把所有数据再划分为若干个Batch给到GPU训练, 之前的Batch叫做Mini Batch, 那再次划分的Batch叫做<strong>Micro Batch</strong>.</p><p>在引入Micro Batch以后, 每个GPU可以直接进行<strong>流水线作业</strong>, 将自己的计算结果提交到模型下一层对应的GPU中, 然后再计算下一个Micro Batch的梯度.</p><p>另一种解决办法被称为<strong>Re - Materialization</strong>(Activation Checkpoint), <strong>直接用时间换空间</strong>. 几乎不存储中间结果, 除了每块GPU的最终输出, 其余的Activation等到Backward用到的时候直接再让模型Forward一遍就行了.</p><h3 id="张量并行-Tensor-Parallelism"><a href="#张量并行-Tensor-Parallelism" class="headerlink" title="张量并行 Tensor Parallelism"></a>张量并行 Tensor Parallelism</h3><blockquote><p>请参考<a href="https://zhuanlan.zhihu.com/p/622212228" target="_blank" rel="noopener">图解大模型训练之：张量模型并行(TP), Megatron-LM - 知乎</a>.</p></blockquote><p><a href="https://arxiv.org/abs/1909.08053" target="_blank" rel="noopener">Megatron</a>是19年遵循张量并行搞出的大模型. 张量并行并不像流水线并行一样, 拆分各层到各块GPU上, 而是对每层里面的矩阵进行拆分, 下放到每块GPU上. 也就是<strong>将模型每层操作的一部分放到不同GPU上完成</strong>, 所以张量并行也被看做为<strong>层间并行</strong>. 最基本的有<strong>按行切分</strong>和<strong>按列切分</strong>, 并且对于不同的操作, 有不同的切分方式, 在此不详细展开.</p><h2 id="ZeRO"><a href="#ZeRO" class="headerlink" title="ZeRO"></a>ZeRO</h2><blockquote><p>请参考:</p><ul><li><a href="https://zhuanlan.zhihu.com/p/618865052" target="_blank" rel="noopener">图解大模型训练之：数据并行下篇( DeepSpeed ZeRO, 零冗余优化) - 知乎</a>.</li><li><a href="https://www.bilibili.com/video/BV1tY411g7ZT" target="_blank" rel="noopener">Zero 论文精读【论文精读】_哔哩哔哩_bilibili</a>.</li></ul></blockquote><p>在数据并行(DP)和分布式数据并行(DDP)中, 都针对通信上负载不均的问题做了优化, 但是仍然没有解决<strong>爆显存</strong>的问题.</p><p><strong>微软</strong>的<a href="https://arxiv.org/abs/1910.02054" target="_blank" rel="noopener">ZeRO</a> 解决了显存上的困难. ZeRO全称为<strong>Ze</strong>ro <strong>R</strong>edundancy <strong>O</strong>ptimizer, 从名字上来看就主要是解决的显存开销, Zero Redundancy.</p><p>训练过程中的显存占用主要包含以下几个方面:</p><ul><li><strong>Model State Memory</strong>:<ul><li>参数<strong>梯度</strong>.</li><li>模型<strong>参数</strong>.</li><li><strong>优化器状态</strong>. 尤其是<strong>Adam</strong>这样的优化器, 对于每个参数需要保存Momentum和Variance, 也是大头.</li></ul></li><li><strong>Activation Memory</strong>: 在Forward之后, 通常会保存部分输入输出(Activation)的值, 来方便Backward. 当然这个保存不是必须的, 可以通过重新Forward来再次得到.</li><li><strong>Fragmented Memory</strong>: 碎片化存储空间.</li></ul><p>由于FP16的计算效率比FP32要高得多, 所以大模型往往是使用<a href="https://www.jianshu.com/p/58a68fa69332" target="_blank" rel="noopener">混合精度训练</a>的:</p><ol><li>模型参数$W$ 是FP32, Momentum和Variance也是FP32, 统称为Model States.</li><li>Forward时将FP32的Parameter新建一份FP16备份, 然后用FP16的正常做Forward和Backward. 产生的Activation全部用FP16存储.</li><li>用FP16的Gradient, 更新FP32的Model States(涉及到Loss Scaling).</li></ol><p>最终输出的模型权重应该是<strong>FP32</strong>而不是FP16.</p><p>在权重更新时, 采用FP32而不是FP16, 原因是FP16训练时的<strong>精度可能会不够</strong>, 容易炸, 特别小的数可能会直接变成0. 而且如果模型参数是FP16而不是FP32, 可能出现参数半天也不变的情况.</p><p>根据对混合精度训练的描述, 可以知道模型训练时所需的空间大小, 假设模型参数数量为$\Phi$, 假如以Bytes为单位, 需要的空间如下:</p><ol><li>FP32:<ol><li>Parameter: $4\Phi$.</li><li>Momentum: $4\Phi$.</li><li>Variance: $4\Phi$.</li></ol></li><li>FP16:<ol><li>Parameter: $2\Phi$.</li><li>Gradients: $2\Phi$.</li></ol></li></ol><p>总共$16\Phi$, 当然这个值没有包含Activation在内, 因为Activation的存在比较灵活, 所以在此暂不做考虑.</p><h3 id="ZeRO-DP"><a href="#ZeRO-DP" class="headerlink" title="ZeRO - DP"></a>ZeRO - DP</h3><p>很多States在自己的大多数时间内, 都不会被一直使用, 而是一直拿着, 直到某个被调用的一刻才会用到. ZeRO对这部分States做了优化, <strong>用到时再拿</strong>, 而不是一直在每块GPU上拿着.</p><h4 id="ZeRO-Stage-1"><a href="#ZeRO-Stage-1" class="headerlink" title="ZeRO Stage 1"></a>ZeRO Stage 1</h4><p>参考Ring - AllReduce, 每块GPU上都有完整的模型参数$W$. 对梯度做一次AllReduce($2\Phi$, 特指单卡通信量, 下同), 所有GPU都能拿到完整的梯度$G$.</p><p>在ZeRO Stage 1中, 所有<strong>Optimizer States</strong> $O$ 被平均拆到了每块GPU上. 模型参数的更新取决于梯度和Optimizer States, 但是现在Optimizer States分布在各块GPU上, 记作$O_i$, 所以只能先结合完整梯度$G$ 来更新一部分模型参数$W_i$, 然后将更新完的这部分$W_i^\prime$ 做一次All - Gather($\Phi$), 所有GPU的模型参数就都是更新完成的了.</p><h4 id="ZeRO-Stage-2"><a href="#ZeRO-Stage-2" class="headerlink" title="ZeRO Stage 2"></a>ZeRO Stage 2</h4><p>在Stage 1的基础上, 把<strong>梯度</strong>也拆分到每块GPU上. 与Ring - AllReduce相似的, 如果每块GPU的最终目标是只维护完整梯度的某一块$G_i$, 那么每块GPU不需要维护除该块以外的梯度, 这是与Ring - AllReduce最大不同的地方, 这节省大量的梯度显存占用.</p><p>对梯度做一次Reduce - Scatter($\Phi$), 每块GPU用自己维护的梯度块$G_i$ 来和部分Optimizer States $O_i$ 来更新对应的$W_i \rightarrow W_i^\prime$, 然后再仿照Stage 1的方式将自己更新好的$W^\prime$ 发送出去, 做一次All - Gather($\Phi$), 所有GPU上的参数就都更新完成了.</p><h4 id="ZeRO-Stage-3"><a href="#ZeRO-Stage-3" class="headerlink" title="ZeRO Stage 3"></a>ZeRO Stage 3</h4><p>在Stage2的基础上, <strong>模型参数</strong>也全部都拆分到每块GPU上, 每块GPU只维护自己的$W_i$. 在做Forward时, 对$W$ 做一次All - Gather($\Phi$), 做完Forward以后立即把不属于自己管理的$W$ 删除.</p><blockquote><p>这样All - Gather并不会导致峰值过高, 做Forward时也可以是<strong>分批</strong>慢慢做的.</p></blockquote><p>做Backward时, 对$W$ 做All - Gather($\Phi$), 做完以后再删除.</p><p>做完Backward以后, 对梯度$G$ 做一次Reduce - Scatter($\Phi$), 以确保自己能拿到自己应该维护的那部分梯度, 聚合后把不属于自己的梯度删除.</p><p>之后更新自己应该维护的权重$W_i$, 由于每块GPU只需要维护部分权重$W^\prime$, 所以不需要对$W^\prime$ 再重新All - Gather.</p><p>所以其实从ZeRO的Stage1 - 3, 思想都是完全一样的, 不过是分别把Optimizer States, Gradient, Model Parameters分别拆到了每块GPU上, 然后解决它们的通信问题:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lmparallel8.png" style="zoom:33%"><h4 id="ZeRO-Stage-3-VS-模型并行"><a href="#ZeRO-Stage-3-VS-模型并行" class="headerlink" title="ZeRO Stage 3 VS 模型并行"></a>ZeRO Stage 3 VS 模型并行</h4><blockquote><p>引用<a href="https://zhuanlan.zhihu.com/p/618865052" target="_blank" rel="noopener">原话</a>:<br>其实<strong>ZeRO是模型并行的形式, 数据并行的实质</strong>.<br>模型并行, 是指在Forward和Backward的过程中, 我只需要用自己维护的那块W来计算就行. 即<strong>同样的输入X, 每块GPU上各算模型的一部分, 最后通过某些方式聚合结果</strong>.<br>但对ZeRO来说, 它做Forward和Backward的时候, 是需要把各GPU上维护的W聚合起来的, 即本质上还是用完整的W进行计算. <strong>它是不同的输入X, 完整的参数W, 最终再做聚合</strong>.</p></blockquote><h3 id="ZeRO-R"><a href="#ZeRO-R" class="headerlink" title="ZeRO - R"></a>ZeRO - R</h3><p>ZeRO - R是对模型训练过程中<strong>额外产生的内容</strong>做的优化, 这个R指的就是Residual States:</p><ul><li>Partitioned Activation Checkpointing: 灵活的存储Activation.</li><li>Constant Size Buffer: 固定内存大小Buffer, 减少GPU之间的通讯次数, 当积攒足够的数据时才进行GPU通讯, 使得带宽利用率更高, 也使得存储大小已知.</li><li>Memory Defragmentation: 对碎片化存储空间重新整合成连续存储空间.</li></ul><h3 id="ZeRO-Offload"><a href="#ZeRO-Offload" class="headerlink" title="ZeRO-Offload"></a>ZeRO-Offload</h3><p>显存再不够, 实在不行只能扔<strong>CPU</strong>上了. 因此ZeRO - Offload把Update相关的不需要频繁计算的东西全部扔到了CPU上, 比如FP32的Parameter, FP32的Optimizer States, FP16的Gradient.</p><p>剩下Forward和Backward这种频繁需要的部分就全放在GPU上, 比如FP16的Parameter, FP16的Activation.</p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/8982.html">https://ADAning.github.io/posts/8982.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/"><span class="chip bg-color">并行计算</span> </a><a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"><span class="chip bg-color">分布式</span> </a><a href="/tags/ZeRO/"><span class="chip bg-color">ZeRO</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/44986.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/5.jpg" class="responsive-img" alt="Vision &amp; Language Pretrained Model 总结"> <span class="card-title">Vision &amp; Language Pretrained Model 总结</span></div></a><div class="card-content article-content"><div class="summary block-with-text">2024.4.21: 添加CoCa, 并修改对WPA的描述. 2024.4.23: 增加了BLIP-2的部分描述. Vision &amp; Language Pretraining 总结本文只是以总结的形式梳理了近期比较有代表性的VL</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2023-07-18 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/MM/"><span class="chip bg-color">MM</span> </a><a href="/tags/VLP/"><span class="chip bg-color">VLP</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/33099.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/17.jpg" class="responsive-img" alt="QIDN: Query-based Instance Discrimination Network for Relational Triple Extraction"> <span class="card-title">QIDN: Query-based Instance Discrimination Network for Relational Triple Extraction</span></div></a><div class="card-content article-content"><div class="summary block-with-text">Query-based Instance Discrimination Network for Relational Triple Extraction本文是论文Query-based Instance Discrimination Net</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2023-02-10 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/RTE/"><span class="chip bg-color">RTE</span> </a><a href="/tags/ERE/"><span class="chip bg-color">ERE</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">383.8k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:andaning@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>