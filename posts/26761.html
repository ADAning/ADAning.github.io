<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/10.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/STS/"><span class="chip bg-color">STS</span> </a><a href="/tags/Audio/"><span class="chip bg-color">Audio</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2025-05-13</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2025-05-14</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 3.9k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 16 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><h1 id="AlignSTS-Speech-to-Singing-Conversion-via-Cross-Modal-Alignment"><a href="#AlignSTS-Speech-to-Singing-Conversion-via-Cross-Modal-Alignment" class="headerlink" title="AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment"></a>AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment</h1><ul><li>论文: <a href="https://aclanthology.org/2023.findings-acl.442/" target="_blank" rel="noopener">AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment</a>, ACL 2023 Findings, Zhou Zhao组.</li><li>代码: <a href="https://github.com/RickyL-2000/AlignSTS" target="_blank" rel="noopener">GitHub - RickyL-2000/AlignSTS: Findings of ACL 2023 | AlignSTS: a speech-to-singing (STS) model based on modality disentanglement and cross-modal alignment</a>.</li></ul><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p><strong>语音到歌声转换</strong>(<strong>S</strong>peech-<strong>t</strong>o-<strong>S</strong>inging Voice Conversion, <strong>STS</strong>)旨在生成与语音记录相对应的歌声样本. 但由于<strong>无文本</strong>时, 歌声的<strong>音高轮廓</strong>(Pitch Contour)与<strong>语音内容</strong>(Speech Content)之间<strong>难以对齐</strong>而导致难以学习.</p><p>因此, 作者提出AlignSTS, 基于一种<strong>显式</strong>的<strong>跨模态对齐</strong>来建模STS任务. 作者从<strong>Rhythm</strong>入手来对齐<strong>Speech Content</strong> &amp; <strong>Singing Pitch Contour</strong>.</p><h2 id="AlignSTS"><a href="#AlignSTS" class="headerlink" title="AlignSTS"></a>AlignSTS</h2><h3 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h3><p>设语音梅尔谱为$S_{\text{sp}}$, 设歌声梅尔谱为$S_{\text{sg}}$.</p><p>AlignSTS将语音信号$S_{\text{sp}}$ 建模为内容 Content $\boldsymbol{c}_{\text{sp}}$, 音色 Timbre Identity $\boldsymbol{i}_{\text{sp}}$, 音高 Pitch $\boldsymbol{f}_{\text{sp}}$, 和节奏 Rhythm $\boldsymbol{r}_{\text{sp}}$ 几种信号的融合:</p><p>$$<br>S_{\text{sp}}=g\left(\boldsymbol{c}_{\text{sp}}, \boldsymbol{i}_{\text{sp}}, \boldsymbol{f}_{\text{sp}}, \boldsymbol{r}_{\text{sp}}\right)<br>$$</p><p>其中$g(\cdot)$ 为多模态融合函数.</p><p><strong>STS</strong>(Speech to Singing)的任务目标是训练一个网络$f_\theta$, 在保持说话人<strong>音色</strong> $\boldsymbol{i}_{\text{sp}}$ 和<strong>内容</strong> $\boldsymbol{c}_{\text{sp}}$ <strong>不变</strong>的情况下, 生成Speech Pitch $\boldsymbol{f}_{\text{sp}}$ 转换为Singing Pitch $\boldsymbol{f}_{\text{sg}}$ 后的歌声频谱$\widehat{S_{\text{sg}}}$:</p><p>$$<br>\widehat{S_{\text{sg}}}=f_{\theta}\left(\boldsymbol{c}_{\text{sp}}, \boldsymbol{i}_{\text{sp}}, \boldsymbol{f}_{\text{sg}}, \boldsymbol{r}_{\text{sg}}\left(\boldsymbol{c}_{\text{sp}}, \boldsymbol{f}_{\text{sg}}\right)\right)<br>$$</p><p>在上式中, 由于Singing Rhythm $\boldsymbol{r}_{\text{sg}}$ 暗含<strong>时间信息</strong>, 所以应该根据Speech Content $\boldsymbol{c}_{\text{sp}}$ 与Singing Pitch $\boldsymbol{f}_{\text{sg}}$ 而变化.</p><h3 id="Method-Overview"><a href="#Method-Overview" class="headerlink" title="Method Overview"></a>Method Overview</h3><p>AlignSTS将语音与歌声视为一系列唱法信息的融合, 进一步可以将每种<strong>唱法</strong>(Variance)信息看做是不同的模态.</p><p>在STS中, Pitch和Rhythm为主要的转换模态, 它们能够被并行解耦, 但是转换过程需要仔细设计Variance的合成逻辑.</p><p>作者认为, 从表面看, Phoneme Sequence和Pitch Contour似乎没什么关联, 但实际上人类在创作的时候极有可能利用二者之间的对齐关系进行旋律创作:</p><ul><li>根据歌词(Lyrics)和旋律(Melody), 来确定合适的音素(Phoneme), 音符(Note)的起始时间和结束时间.</li><li>根据Phoneme Sequence是非常有可能依赖Rhythm来排列的, 与Melody结合得到歌声.</li></ul><p>因此, AlignSTS基于上述假设, 做出如下设计:</p><ul><li>将Speech Signal分解为一系列解耦的<strong>Variance Information</strong>.</li><li>根据Speech到Singing的转换过程中发生改变的<strong>Pitch Contour</strong>($\boldsymbol{f}_{\text{sp}} \rightarrow \boldsymbol{f}_{\text{sg}}$)来调整Singing Rhythm $\boldsymbol{r}_{\text{sg}}$.</li><li>根据调整后的Rhythm重新与Speech Content对齐, 然后进行多模态融合来整合Variance Information.</li></ul><p>AlignSTS主要框架图如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/alignsts1.png" style="zoom:33%"><p>AlignSTS主要有四个模块: <strong>Encoder</strong>, <strong>Rhythm Adapter</strong>, <strong>Cross-Modal Aligner</strong>, <strong>Mel-Decoder</strong>. Encoder和Mel-Decoder不是本文的重点. 图中虚线代表仅在训练时使用, 推理阶段不使用.</p><p>就像之前提到的, AlignSTS以<strong>Rhythm</strong>为切入点来改善Pitch和Content之间的对齐关系.</p><ol><li><strong>Rhythm Adapter</strong> (c): 用于填补Pitch与Content之间的Gap, 对Rhythm Representation采用了<strong>VQ</strong>的设计.</li><li><strong>Cross-Modal Aligner</strong> (b): 用于重新对齐Inference时预测出的Rhythm与输入的Speech Content Feature, 以此重新指导歌声合成.</li></ol><h3 id="Information-Perturbation"><a href="#Information-Perturbation" class="headerlink" title="Information Perturbation"></a>Information Perturbation</h3><p>语音样本和歌声样本都可以被分解为Content, Pitch, Rhythm, Timbre等唱法特征.</p><p>每种特征需要分别以不同的方式解耦抽取出来:</p><ul><li><strong>Linguistic Content</strong>: 用<a href="https://huggingface.co/facebook/wav2vec2-base-960h" target="_blank" rel="noopener">wave2vec 2.0</a>来提取Speech Content.</li><li><strong>Pitch</strong>: 提取基频轮廓F0作为音高信息. 基频轮廓被量化为256个值(即256个类).</li><li><strong>Timbre</strong>: 使用开源的说话人识别API <a href="https://github.com/resemble-ai/Resemblyzer" target="_blank" rel="noopener">Resemblyzer</a>抽取音色信息, Resemblyzer可以根据声音生成一个256维向量.</li><li><strong>Rhythm</strong>: Rhythm可以控制每个Phoneme的<strong>Duration</strong>和<strong>Speed</strong>, 所以它能提供Duration和Content. 但是从模式上来说, 歌声比语音的强度普遍波动更大, 所以作者用处理后的歌声样本的时域能量轮廓(Energy Contour) $\boldsymbol{e}_t$ (每个时刻的值为各频域的L2范数)来表征Rhythm.</li></ul><p>具体的, 将Energy Contour使用标准正态分布标准化, 并且用Sigmoid函数$\sigma(\cdot)$ 将Energy Contour进行归一化:</p><p>$$<br>\boldsymbol{r}_t=\sigma\left(\beta\times\frac{\boldsymbol{e}_t-\operatorname{mean}(\boldsymbol{e})}{\operatorname{std}(\boldsymbol{e})+\epsilon}\right)<br>$$</p><p>其中, $\epsilon$ 来避免零除. $\beta$ 为一个超参, 用于控制标准化程度. $\boldsymbol{r}_t$ 为得到的Rhythm Representation.</p><p>通过这种方式, 作者解耦出了Rhythm, 同时保留了Duration. 例如, 在歌曲中的一句”I’m a big big girl”中有:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/alignsts2.png" style="zoom:67%"><p>明显的, Rhythm Representation(黄线)的Duration是要更加清晰的, 对于Phoneme的区分要更友好.</p><blockquote><p>似乎不同音素转换之间的过渡也被构造的表示抹掉了… 但作者最重要的想法应该就是<strong>只保留Duration</strong>作为已解耦的Rhythm, 不同音素之间的过渡方式通过其他模块或者用其他唱法信息来补充学习.</p></blockquote><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>对于Singing F0 $\boldsymbol{f}_{\text{sg}}$, Rhythm $\boldsymbol{r}_{\text{sg}}$, Speech Content $\boldsymbol{c}_{\text{sp}}$, 分别用$\mathrm{E}_{\mathrm{P}}$, $\mathrm{E}_{\mathrm{R}}$, $\mathrm{E}_{\mathrm{C}}$ 来编码, Encoder都是两层1D Conv Layer的堆叠:</p><p>$$<br>\boldsymbol{x}_{\mathrm{P}}=\mathrm{E}_{\mathrm{P}}\left(\boldsymbol{f}_{0}\right), \boldsymbol{x}_{\mathrm{R}}=\mathrm{E}_{\mathrm{R}}(\boldsymbol{r}), \boldsymbol{x}_{\mathrm{C}}=\mathrm{E}_{\mathrm{C}}(\boldsymbol{x})<br>$$</p><h3 id="Rhythm-Modality-Modeling"><a href="#Rhythm-Modality-Modeling" class="headerlink" title="Rhythm Modality Modeling"></a>Rhythm Modality Modeling</h3><p>歌声信号中的Rhythm可以被视为是一系列<strong>离散</strong>的时间动态状态, 如Attack, Sustain, Transition, Silence等.</p><p>Rhythm在AlignSTS用<strong>VQ Module</strong>来提取, 从而获得一个离散的Rhythm表示$e \in {\mathbb{R}^{K \times D}}$, $K$ 为每个Latent Embedding $e_k \in \mathbb{R}^{D}, k \in 1, 2, \dots, K$ 的维度.</p><p>使用<strong>VQ-VAE</strong>中的<strong>Commitment Loss</strong>来优化离散的Rhythm Embedding:</p><p>$$<br>\mathcal{L}_{\text{C}}=\left\Vert z_e(x)-\operatorname{sg}[e]\right\Vert _2^2<br>$$</p><p>其中, $z_e(\cdot)$ 为VQ Module, $\text{sg}$ 为停止梯度运算符. Commitment Loss仅让VQ Module得到的Rhythm Embedding $z_e(x)$ 与Codebook中的Embedding $e$ 靠近.</p><p>为了让Singing Rhythm $\boldsymbol{r}_{\text{sg}}$ 条件依赖于Speech Content $\boldsymbol{c}_{\text{sp}}$, 可以采用Cross-Attention来实现. 具体的, 以Pitch $X_P$ 为Query, Content $X_C$ 为Key和Value做CA, 得到的是融合时需要的<strong>Target Content Feature</strong>:<br>$$<br>\begin{align}<br>\text{Attention}(Q, K, V)&amp;=\text{Attention}\left(X_{P}, X_ {C}, X_{C}\right) \\<br>&amp;=\operatorname{Softmax}\left(\frac{X_{P} X^{T}_{C}}{\sqrt{d}}\right) X_{C} \\<br>\end{align}<br>$$</p><p>$X_P, X_C$ 会先被投影到Query, Key, Value的表示空间中.</p><h4 id="Rhythm-Predictor"><a href="#Rhythm-Predictor" class="headerlink" title="Rhythm Predictor"></a>Rhythm Predictor</h4><p>但是在Inference时没有歌声的能量轮廓, Rhythm此时是不可用的, 所以VQ Module只能用于Training. 因此, 需要一个<strong>Rhythm Predictor</strong>来预测Rhythm Class.</p><p>作者将Rhythm Predictor设计为一些Cross-Attention和卷积层的堆叠.</p><p>Rhythm是离散的, 用交叉熵来训练Rhythm Predictor:</p><p>$$<br>\mathcal{L}_{\text{R}}=-\frac{1}{T}\sum_{t=1}^{T}\sum_{c=1}^{K} y_{t, c}\log\left(\hat{y}_{t, c}\right)<br>$$</p><p>其中, $T$ 为总共的时间帧数量, $y_{t,c}$ 为VQ Module生成的Rhythm Embedding的下标索引, 即当$t$ 时刻的Rhythm Class $c$ 为$k_t$ 时, $y_{t, c}=1$. $\hat{y}_{t, c}$ 为预测出的Rhythm Indices.</p><p>所以Rhythm Adapter大概就是下面这个样子:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/alignsts3.png" style="zoom:50%"><ul><li>训练时已知歌声的Rhythm, 得到Rhythm Encoder $\mathrm{E}_{\mathrm{R}}$ 对Rhythm编码后的表示, 然后用VQ Module得到Rhythm Indices, 以此更新Rhythm Codebook Embeddings.</li><li>在推理的时候, Rhythm Predictor以Pitch Encoder $\mathrm{E}_{\mathrm{P}}$ 对Pitch编码后的表示为Query, Content Encoder $\mathrm{E}_{\mathrm{C}}$ 对Content编码后的表示为Key, Value, 做完Cross-Attention后再用Conv Stack预测出Rhythm Indices.</li></ul><h3 id="Cross-Modal-Alignment"><a href="#Cross-Modal-Alignment" class="headerlink" title="Cross-Modal Alignment"></a>Cross-Modal Alignment</h3><p>在通过VQ得到了Rhythm Sequence之后, 还需要Cross-Modal Aligner来重新对齐Rhythm和Content, 得到融合时的<strong>Rhythm Feature</strong>.</p><p>Cross-Modal Aligner采用两层Cross-Attention Layer来重对齐Rhythm和Content:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/alignsts4.png" style="zoom:50%"><p>用Rhythm Embeddings $e$ 作为Query, 并<strong>再次</strong>以Content Encoder $\mathrm{E}_{\mathrm{C}}$ 对Content编码后的表示为Key, Value, 实现这种<strong>Soft-Selection</strong>.</p><blockquote><p>所以实际上Rhythm Predictor里的CA和Cross-Modal Aligner的CA都是以Content为Key, Value, 只不过是分别以Pitch和VQ过后的Rhythm Embedding为Query.</p></blockquote><p>当所有的唱法信息都被重新对齐后, 终于集齐了这几种特征, 可以准备召唤神龙了:</p><ul><li><strong>Pitch</strong>: 直接通过Pitch Encoder $\mathrm{E}_{\mathrm{P}}\left(\boldsymbol{f}_{0}\right)$ 得到.</li><li><strong>Content</strong>: 在Rhythm Predictor中, 以Pitch为Query得到. 因此与Pitch具有相同的Shape.</li><li><strong>Rhythm</strong>: 在Cross-Modal Alignment中, 以VQ后的Rhythm Embedding为Query得到. 而Rhythm Embedding在VQ前与上面的Content同Shape, 所以Rhythm也与Pitch同Shape.</li><li><strong>Timbre</strong>: 对于一段音频仅只有一个向量, 没有时间长度的维度.</li></ul><p>除了Timbre, 剩下的三个的时间长度都是一样的. Timbre可以直接通过广播与其他三者相加, 从而完成最后的跨模态融合.</p><blockquote><p>这部分原文里写的挺乱的, 没写到底是用哪个Content做融合, 看了下代码才对得上. 而且代码里配置文件里面还写了个<code>q_rhythm_bridge: False</code>, 也就是没加VQ以后的Rhythm Embedding… 有点难绷, 反正大概意思就是几个特征加一下, 咱们在这也不纠结了.</p></blockquote><p>Rhythm的解耦建模明确了Word / Phoneme的数量与它们的顺序, 正常情况下, Content和Rhythm应该是按顺序一一对应的, 所以Attention Matrix应该偏向于集中在主对角线附近. 所以, 作者希望进一步采用两种令注意力学到主对角线附近的约束来建模这种先验.</p><h4 id="Attention-and-Alignment-Regulation"><a href="#Attention-and-Alignment-Regulation" class="headerlink" title="Attention and Alignment Regulation"></a>Attention and Alignment Regulation</h4><h5 id="Attention-Windowing"><a href="#Attention-Windowing" class="headerlink" title="Attention Windowing"></a>Attention Windowing</h5><p>对于每个时间步$t$ 对应的作为Query的中间位置$p_t$, Key / Value的序列$\hat{\boldsymbol{x}}$ 被限制在一个窗口大小为$2w$ 的窗口内:</p><p>$$<br>\hat{\boldsymbol{x}}=\left[\boldsymbol{x}_{p_t-w}, \ldots, \boldsymbol{x}_{p_t+w}\right]<br>$$</p><p>对于在窗口外的元素, Attention Matrix被填充以$-10^8$ 来限制它们被给予Attention.</p><blockquote><p>作者设定的Window Size是一个长度百分比, 即每个窗口占序列中的百分比.</p></blockquote><h5 id="Guided-Attention-Loss"><a href="#Guided-Attention-Loss" class="headerlink" title="Guided Attention Loss"></a>Guided Attention Loss</h5><p>为了进一步限制Attention Matrix近似对角阵, 作者用Loss加以约束引导模型学习:</p><p>$$<br>\mathcal{L}_{\text{attn}}=\frac{1}{TN}\sum_{t=1}^{T}\sum_{n=1}^{N}\alpha_{t,n}w_{t,n}, \quad \text{where} \quad w_{t, n}=1-\exp\left(-\frac{\left(\frac{n}{N}-\frac{t}{T}\right)^2}{2g^2}\right)<br>$$</p><p>其中$\mathrm{T}, \mathrm{N}$ 分别为Query, Key的长度, $w_{t, n}$ 为约束后的权重分布, $g$ 为控制集中在对角线附近的超参, 通常设为0.1. 因此, 当$at, n$ 离对角线很远时, 则Key项对应的Loss系数会更大, 会被给予更大的惩罚.</p><h3 id="Mel-Decoder"><a href="#Mel-Decoder" class="headerlink" title="Mel-Decoder"></a>Mel-Decoder</h3><p>作者采用一个<strong>Diffusion Model</strong>, <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3547855" target="_blank" rel="noopener">ProDiff</a>的Teacher Model作为Mel-Decoder来完成更好的合成. 对Decoder施加两个Reconstruction Loss:</p><ul><li>MAE Loss: $\mathcal{L}_{\text{MAE}}=\left\Vert \boldsymbol{x}_\theta\left(\alpha_t \boldsymbol{x}_0+\sqrt{1-\alpha_t^2}\boldsymbol{\epsilon}\right)-\boldsymbol{x}_0\right\Vert$.</li><li>SSIM Loss: $\mathcal{L}_{\text{SSIM}}=1-\text{ SSIM}\left(\boldsymbol{x}_{\theta}\left(\alpha_{t} \boldsymbol{x}_{0}+\sqrt{1-\alpha_{t}^{2}}\boldsymbol{\epsilon}\right), \boldsymbol{x}_{0}\right)$.</li></ul><h3 id="Training-and-Inference"><a href="#Training-and-Inference" class="headerlink" title="Training and Inference"></a>Training and Inference</h3><p>Loss主要由5个项组成:</p><ul><li>VQ里面的Commitment Loss $\mathcal{L}_{\text{C}}$.</li><li>Rhythm Predictor的分类损失 $\mathcal{L}_{\text{R}}$.</li><li>Cross-Modal Alignment的Guided Loss $\mathcal{L}_{\text{attn}}$.</li><li>MAE Loss $\mathcal{L}_{\text{MAE}}$.</li><li>SSIM Loss $\mathcal{L}_{\text{SSIM}}$.</li></ul><p>在训练和推理时:</p><ul><li>在训练阶段, Cross-Modal Aligner直接用VQ Module生成的Rhythm Embedding作为输入, 同时把对应的索引给Rhythm Predictor用CE来训练.</li><li>在推理阶段, VQ Module采用Rhythm Predictor预测得到的Rhythm Indices再从Codebook中查表得到的Embedding作为Rhythm Embedding.</li></ul><blockquote><p>我粗看代码里VQ Module在训练的时候还用了<strong>EMA</strong>, 来保证Codebook更新不会过快, 否则会影响其他依赖于Rhythm Embedding的模块参数更新.</p></blockquote><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><blockquote><p>详细的实验设置和超参数设置请参照原论文.</p></blockquote><h3 id="Dataset-amp-Evaluation"><a href="#Dataset-amp-Evaluation" class="headerlink" title="Dataset &amp; Evaluation"></a>Dataset &amp; Evaluation</h3><h4 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h4><p>数据集采用<strong>PopBuTFy</strong>, 这是一个歌声美化(<strong>S</strong>inging <strong>V</strong>oice <strong>B</strong>eautifying, SVB)的数据集. 作者收集并标注了一个PopBuTFy的子集的语音数据, 形成一个Speech-Singing Paired Dataset. 其中有16个歌手的152首英文歌, 大约5.5小时, 以及约3.7小时的语音记录.</p><h4 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul><li><strong>Objective Metrics</strong>:<ul><li><strong>LSD</strong>(<strong>L</strong>og-<strong>S</strong>pectral <strong>D</strong>istance): 用于衡量两个频谱之间的距离.</li><li><strong>RCA</strong>(<strong>R</strong>aw <strong>C</strong>hroma <strong>A</strong>ccuracy): 用于预测的F0与真实F0在感知上的差距.</li><li><strong>RRD</strong>(<strong>R</strong>hythm <strong>R</strong>epresentation <strong>D</strong>istance): 使用生成的梅尔谱计算得到的Rhythm与Ground Truth的Rhythm之间的欧氏距离来衡量对Rhythm的恢复能力.</li></ul></li><li><strong>Subjective Metrics</strong>:<ul><li><strong>MOS-Q</strong>: 表示合成歌声音频的整体质量.</li><li><strong>MOS-P</strong>: 表示自然度和连贯性.</li></ul></li></ul><blockquote><p>LSD衡量的是生成结果与GT之间的全局预测的好坏, 而RCA偏向于Pitch预测的好坏, RRD偏向于Rhythm预测的好坏.</p></blockquote><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><p>在数据集上主要实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/alignsts5.png" style="zoom:50%"><p>其中的AlignSTS(GAN)是将Diffusion Model替换为GAN的结果.</p><p>结果上来说, AlignSTS比其他Baseline要大幅领先, 但相较于GT还是有显著差距. 在作者提出的评估指标RRD上, AlignSTS具有较大的优势, 说明AlignSTS对Rhythm建模是相对合理的.</p><p>作者在这里提到的Zero-Shot Speech-to-Singing Conversion指的是只使用Singing Sample执行Singing-to-Singing的训练, 即对同一段Singing Sample提取Rhythm和Content, 但是推理的时候采用Speech作为输入. 从结果上发现, 即使AlignSTS做Zero-Shot, 似乎也能取得一个与其他Baseline不做Zero-Shot时相当甚至还好的效果.</p><p>下图是生成的梅尔谱可视化:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/alignsts6.png" style="zoom:50%"><p>图(a), (b)分别为表格中的第一二个Baseline, 图(c)为AlignSTS, 图(d)为GT. 从梅尔谱上来看, 前两个Baseline明显在Rhythm的编排上比AlignSTS差得多. 细节上来说, AlignSTS要优于前两个Baseline, 但也能看出AlignSTS在高频的细节上还有很多不足.</p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>消融实验上选择CMOS-Q(<strong>C</strong>omparative <strong>M</strong>ean <strong>O</strong>pinion <strong>S</strong>core of <strong>Q</strong>uality), CMOS-P(<strong>C</strong>omparative <strong>M</strong>ean <strong>O</strong>pinion <strong>S</strong>core of <strong>P</strong>rosody)作为评估指标, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/alignsts7.png" style="zoom:50%"><ol><li><strong>w/o RA</strong>: 使用线性插值来拉伸Speech Rhythm Representation代替Rhythm Adapter, 直接用于Cross-Modal Alignment中.</li><li><strong>w/o CM</strong>: 去掉Cross-Modal Alignment, 直接把Content Encoder得到的表示线性拉伸到与Pitch Contour相同的长度, 以此适应Rhythm, 然后直接把所有的Feature相加做融合.</li><li><strong>w/o F0</strong>: 直接在融合中去掉Pitch, 只将Rhythm, Content, Timbre加到一起.</li></ol><p>从消融实验结果上来看, 影响最大的是F0. Rhythm Adapter和Cross-Modal Alignment去掉后掉点情况近似.</p><h4 id="Attention-Weights-Visualization"><a href="#Attention-Weights-Visualization" class="headerlink" title="Attention Weights Visualization"></a>Attention Weights Visualization</h4><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/alignsts8.png" style="zoom:50%"><p>上图依次为:</p><ul><li><strong>(a)</strong> / <strong>(b)</strong>: 不加 / 加Alignment Regulation的Cross-Attention.</li><li><strong>(c)</strong> / <strong>(d)</strong>: AlignSTS Training / Inference时的Zero-Shot.</li></ul><p>从Visualization上能明显的看出作者采用的两种约束的训练效果. 如果不加限制, Attention Matrix直接起飞, 加完约束后明显集中在主对角线附近了.</p><p>按照前文中的实验部分, Training的Zero-Shot做的是Singing-to-Singing, 当Singing Sample同时作为Content和Rhythm输入时, Attention被约束的非常好, 近乎线性模式. 在推理阶段做的才是Speech-to-Singing, 能发现AlignSTS对于未见过的Speech Sample也仍然有一定的能力将Attention集中在主对角线附近.</p><blockquote><p>此外, 我个人认为Zero-Shot的强大泛化应该暗示了Speech &amp; Singing Pair之间的紧密联系.</p></blockquote><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>AlignSTS将STS任务建模为一系列唱法信号的转换与融合的过程, 并以<strong>离散</strong>的形式建模包含时间信息的<strong>Rhythm</strong>, 并以Rhythm为切入点来完成Speech Content和Singing Pitch之间的Cross-Modal Alignment, 也引入了一些先验来显式指导对齐的过程.</p><p>不过从5202年这个ASR相当成熟(有<strong>Whisper</strong>这种鲁棒性非常强的ASR工具)的时间点来看, 但对齐来说, Text-based Alignment也许也能做的更轻松些.</p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/26761.html">https://ADAning.github.io/posts/26761.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/STS/"><span class="chip bg-color">STS</span> </a><a href="/tags/Audio/"><span class="chip bg-color">Audio</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="far fa-dot-circle"></i>&nbsp;本篇</div><div class="card"><a href="/posts/26761.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/10.jpg" class="responsive-img" alt="AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment"> <span class="card-title">AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment</span></div></a><div class="card-content article-content"><div class="summary block-with-text">AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment 论文: AlignSTS: Speech-to-Singing Conversion via Cross-Mo</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2025-05-13 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/STS/"><span class="chip bg-color">STS</span> </a><a href="/tags/Audio/"><span class="chip bg-color">Audio</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/40650.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/6.jpg" class="responsive-img" alt="DDIM: Denoising Diffusion Implicit Models"> <span class="card-title">DDIM: Denoising Diffusion Implicit Models</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: DDPM: DDPM: Denoising Diffusion Probabilistic Model. Denoising Diffusion Implicit Models 论文: Denoising Diffu</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2025-03-21 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/Diffusion/"><span class="chip bg-color">Diffusion</span> </a><a href="/tags/DDIM/"><span class="chip bg-color">DDIM</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">420.2k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:andaning@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>