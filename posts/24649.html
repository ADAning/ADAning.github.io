<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="RoBERTa: A Robustly Optimized BERT Pretraining Approach, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>RoBERTa: A Robustly Optimized BERT Pretraining Approach | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:.6"></i> <span>时间轴</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li><li><a href="/markdown"><i class="fab fa-markdown" style="margin-top:-20px;zoom:.6"></i> <span>Markdown</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li><li><a href="/markdown" style="margin-left:75px"><i class="fa fab fa-markdown" style="position:absolute;left:50px"></i> <span>Markdown</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/medias/featureimages/1.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">RoBERTa: A Robustly Optimized BERT Pretraining Approach</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/BERT/"><span class="chip bg-color">BERT</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2020-11-18</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2022-03-17</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 1.9k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 7 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p>本文前置知识:</p><ul><li>BERT(详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>)</li></ul></blockquote><h1 id="RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach"><a href="#RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach" class="headerlink" title="RoBERTa: A Robustly Optimized BERT Pretraining Approach"></a>RoBERTa: A Robustly Optimized BERT Pretraining Approach</h1><p>本文是论文<a href="https://arxiv.org/abs/1907.11692" target="_blank" rel="noopener">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a>的阅读笔记和个人理解. RoBERTa已经被广泛的应用于各类由BERT衍生的模型参数初始化, 可以视为是完全体形态的BERT.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者在文章中指出, 训练是一个非常重要的过程, 但BERT在发布时并没有得到很好的训练, 导致其性能看起来比现在的<strong>自回归语言</strong>模型性能要略差(例如XLNet). 但实际上, 对BERT应用一些<strong>训练技巧</strong>对提升BERT性能影响是非常大的. 因此, 作者重新对BERT施加了一些训练技巧, 使得BERT的<strong>性能</strong>得到了进一步提升, 并且具有更强的<strong>鲁棒性</strong>.</p><h2 id="RoBERTa"><a href="#RoBERTa" class="headerlink" title="RoBERTa"></a>RoBERTa</h2><p>RoBERTa(<strong>R</strong>obustly <strong>o</strong>ptimized <strong>BERT</strong> <strong>a</strong>pproach). RoBERTa只是应用了更好的训练技巧, 因此整体结构是没有发生任何变化的. 如果对BERT的结构不熟悉, 建议回顾BERT的知识.</p><h3 id="Dynamic-Masking"><a href="#Dynamic-Masking" class="headerlink" title="Dynamic Masking"></a>Dynamic Masking</h3><p>作者总结出三种Mask的方法:</p><ul><li><strong>纯静态Mask</strong>: 就是BERT中使用的Mask, 在<strong>数据预处理</strong>阶段就进行, 每个Epoch所Mask同一句中的Token位置都是相同的.</li><li><strong>改进一点的静态Mask</strong>: 将每个Sentence都<strong>重复</strong>N次, 这样可能在预处理阶段能得到N种不同的Mask. 因为扩大了每个Epoch的数据量, 训练的Epoch要是原来的1/N倍.</li><li><strong>动态Mask</strong>: 每个Sentence给BERT之前<strong>动态</strong>Mask, 即生成一种新的Mask方式. 这样每个Epoch拿到的Mask基本上是不同的.</li></ul><p>从Mask的方法上来看, 动态Mask并没有引入太多的计算花费, 但是却大大提升了训练时句子的<strong>多样性</strong>. 为了证明其有效性, 作者将上述三种Mask方式的性能将BERT(Base)在SQuAD上的F1 Score, MNLI - m和SST - 2上的ACC做了比较:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/roberta1.jpg" style="zoom:33%"><p>其中reference来自XLNet中给出的BERT(Base)数据. 每种方式都采用了5轮的随机初始化.</p><p>从中能看出, 改进后的静态Mask与原版Mask性能相仿, 动态Mask要比改进后的静态Mask稍好一点. 但动态Mask又不会引入太高的时间开销, 这样的增益还是很划算的.</p><h3 id="Training-without-NSP"><a href="#Training-without-NSP" class="headerlink" title="Training without NSP"></a>Training without NSP</h3><p>在BERT中, 训练时候采用了预测Mask和NSP两种训练任务. NSP(Next Sentence Prediction)任务是它随机的将两段连续或毫不相关的文档拼在一起, 然后用<code>[CLS]</code>位置的输出预测两段文字是否来自于统一文章(或者说连续不连续).</p><p>其实在早一点的多篇论文中指出, NSP任务虽然在BERT中被假设非常重要, 但实际上NSP任务会导致BERT的<strong>退化</strong>. 越来越多的人开始<strong>质疑</strong>NSP任务的必要性. 作者为了观察各种训练方式之间的差异, 设计了如下四种训练方式:</p><ol><li><strong>Segment - Pair + NSP</strong>: 与训练BERT时的方案无异. <strong>每次输入两段来自同一文档或多个文档的内容</strong>. 内容总长度必须少于512个Token.</li><li><strong>Sentence - Pair + NSP</strong>: <strong>每次只输入两个来自同一文档或多个文档的句子</strong>. 每次输入的序列长度肯定小于512个Token, 所以用<strong>增大Batch Size</strong>的方式来让这种方式的总Token数与Segment - Pair + NSP总Token数相近.</li><li><strong>Full - Sentences</strong>: <strong>全部输入可能来自于同一文档或多个文档的连续句子</strong>, 直到填满为止. 序列长度最多512个Token. 在切换不同文档时, 在之间加上特殊的分隔符. 不采用NSP任务.</li><li><strong>Doc - Sentences</strong>: <strong>全部输入来自同一文档的句子</strong>, 即只从一篇文档中对连续句子采样, 如果文档的内容少于512个Token, 则动态增大Batch Size使得其与Full - Sentences总Token数量相近.</li></ol><p>实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/roberta2.jpg" style="zoom:33%"><p>作者得出了以下结论:</p><ol><li>观察方式1和方式2, 使用单个句子明显的损失了下游任务的性能. 可能模型并不能从句子中学习到长范围的依赖.</li><li>观察方式1和方式4, 即BERT的原始训练方式与不适用NSP的训练方式, 移除NSP能稍微增强下游任务的性能.</li></ol><p>但是由于表现最好的Doc - Sentences需要动态调整Batch Size, 作者还是采用了Full - Sentences作为后文实验方式.</p><h3 id="Bigger-bigger-and-bigger"><a href="#Bigger-bigger-and-bigger" class="headerlink" title="Bigger, bigger, and bigger"></a>Bigger, bigger, and bigger</h3><h4 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h4><p>XLNet用了126G的数据, 当时BERT训练只用了十几个G的数据, 所以对比是很不公平的. RoBERTa在数据上不能落后, 一口气用了160G的数据. 分别由以下部分组成:</p><table><thead><tr><th align="left">数据集名称</th><th align="center">大小(GB)</th><th>说明</th></tr></thead><tbody><tr><td align="left">BookCorpus</td><td align="center">16</td><td>BERT训练时候用的数据</td></tr><tr><td align="left">CC - NEWS</td><td align="center">76</td><td>过滤后的新闻类数据</td></tr><tr><td align="left">OpenWebText</td><td align="center">38</td><td>根据网友点赞数从URL中提取的帖子文本</td></tr><tr><td align="left">Stories</td><td align="center">31</td><td>故事类数据</td></tr><tr><td align="left"><strong>总计</strong></td><td align="center"><strong>161</strong></td><td></td></tr></tbody></table><p>更大量的数据对BERT提升是巨大的, 使之能够与XLNet相对公平的进行比较.</p><blockquote><p>从<strong>辩证</strong>的角度来说, 更大量的数据也会使模型的<strong>偏见</strong>和<strong>歧视性</strong>更强. 在数据上的偏见消除是非常重要的, 目前来说没有太好的解决方案.</p></blockquote><h4 id="Batch-Size"><a href="#Batch-Size" class="headerlink" title="Batch Size"></a>Batch Size</h4><p>先前有大量实验表明, 适量增大Batch Size有益于模型的收敛, 更能使训练稳定, 并有助于提高模型的性能. 更大的Batch Size也能帮助快速训练. 作者做了增大Batch Size对性能影响的实验, 保证Batch Size和Step的积不变, 即<strong>维持相同的计算开销</strong>, 实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/roberta3.jpg" style="zoom:33%"><p>适量增大Batch Size确实有助于提高模型的性能, 但考虑到更大Batch Size在<strong>训练速度</strong>上带来的优势, 作者只采用了8K的Batch Size, 而非效果最好的2K Batch Size.</p><p>本小节叙述一些无关紧要的参数调整. 除去峰值Learning Rate, 和Warmup的次数, RoBERTa延续了BERT的<strong>原始参数</strong>. 考虑到RoBERTa采用了更大的Batch Size, 所以将Adam中的Beta2从0.999 换为了0.98.</p><p>RoBERTa不会随机的将短句注入, 并且前90%的训练中不会使用缩短的序列, 只使用全长序列.</p><p>RoBERTa还采用了<strong>BPE</strong>缩小词表, 进一步提升了性能.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>作者从增大数据量和增长训练时间两个角度做了实验:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/roberta4.jpg" style="zoom:50%"><p>从逐步添加训练技巧的流程来看, RoBERTa在和BERT使用十几G数据的情况下提升非常大, 也和XLNet在用13G时的性能不分高下, 证明了RoBERTa改进的<strong>正确性</strong>. 逐渐增大训练数据量和训练时长(这里是通过Step调整), RoBERTa<strong>逐渐碾压</strong>了XLNet.</p><p>然后将RoBERta与GLUE排行榜中其他的模型也进行了实验:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/roberta5.jpg" style="zoom:50%"><p>RoBERTa作者提到, 对于GLUE有两种Fine - Tune的方式:</p><ul><li>单任务, 对每个GLUE任务分别进行Fine - Tune, 并且只用相应任务的训练数据.</li><li>多任务, 在测试集上进行比较. 但与排行榜上其他的模型不同, 其他模型对多任务进行Fine - Tune, RoBERTa只对单任务进行Fine - Tune.</li></ul><p>从实验结果中来看, RoBERTa比没训练好的BERT提升相当的大.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>作者实际上在RoBERTa中主要做了四件事:</p><ol><li>用更大的Batch Size, 更多的Data, 更长的训练时间. 就是更大.</li><li>废除NSP的训练目标, 这个非常重要.</li><li>将静态Mask换为动态Mask.</li><li>用更长序列训练(不怎么重要).</li></ol><p>严格意义上来说, RoBERTa才是BERT的完全体. 这提供给大家一个非常好的预训练基准, 而且在其他论文中也鼓励用RoBERTa而不是BERT进行比较, 因为BERT的训练是不够充分的.</p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">AnNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/24649.html">https://ADAning.github.io/posts/24649.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">AnNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/NLP/"><span class="chip bg-color">NLP</span> </a><a href="/tags/BERT/"><span class="chip bg-color">BERT</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/60645.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/medias/featureimages/22.jpg" class="responsive-img" alt="Pytorch实现: Skip-Gram"> <span class="card-title">Pytorch实现: Skip-Gram</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: Pytorch基本操作 Word2Vec Skip - Gram的Pytorch实现本文用Pytorch实现了Skip - Gram, 它是Word2Vec的其中一种. 本文实现参考PyTorch 实现 Word2V</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-11-19 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/Word2Vec/"><span class="chip bg-color">Word2Vec</span> </a><a href="/tags/Pytorch/"><span class="chip bg-color">Pytorch</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/13721.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/medias/featureimages/3.jpg" class="responsive-img" alt="Integrating Image-Based and Knowledge-Based Representation Learning"> <span class="card-title">Integrating Image-Based and Knowledge-Based Representation Learning</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: AlexNet(详见卷积神经网络发展史) Attention(详见Seq2Seq和Attention) TransE(详见TransE: Translating Embeddings for Modeling Multi</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-11-13 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/" class="post-category">知识图谱</a></span></div></div><div class="card-action article-tags"><a href="/tags/KGE/"><span class="chip bg-color">KGE</span> </a><a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"><span class="chip bg-color">多模态</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">AnNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">234.4k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:695439722@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io@master/libs/instantpage/instantpage.js" type="module"></script></body></html>