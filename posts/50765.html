<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="RoPE / RoFormer: Enhanced Transformer with Rotary Position Embedding, DaNing的博客"><meta name="description" content="DaNing的个人博客."><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>RoPE / RoFormer: Enhanced Transformer with Rotary Position Embedding | DaNing的博客</title><link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/favicon.png"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/matery.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/css/my.css"><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 4.2.1"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">DaNing的博客</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="" class="waves-effect waves-light"><i class="fas fa-tools" style="zoom:.6"></i> <span>工具</span> <i class="fas fa-chevron-down" aria-hidden="true" style="zoom:.6"></i></a><ul class="sub-nav menus_item_child"><li><a href="/shortcut"><i class="fas fa-rocket" style="margin-top:-20px;zoom:.6"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu"><i class="fas fa-wheelchair" style="margin-top:-20px;zoom:.6"></i> <span>帮你百度</span></a></li></ul></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">DaNing的博客</div><div class="logo-desc">DaNing的个人博客.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="javascript:;"><i class="fa-fw fas fa-tools"></i> 工具 <span class="m-icon"><i class="fas fa-chevron-right"></i></span></a><ul><li><a href="/shortcut" style="margin-left:75px"><i class="fa fas fa-rocket" style="position:absolute;left:50px"></i> <span>导航(待完善)</span></a></li><li><a href="/helpyoubaidu" style="margin-left:75px"><i class="fa fas fa-wheelchair" style="position:absolute;left:50px"></i> <span>帮你百度</span></a></li></ul></li></ul></div></div></nav></header><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/cryptojs/crypto-js.min.js"></script><script></script><div class="bg-cover pd-header post-cover" style="background-image:url(https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/3.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">RoPE / RoFormer: Enhanced Transformer with Rotary Position Embedding</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/RoPE/"><span class="chip bg-color">RoPE</span> </a><a href="/tags/LLM/"><span class="chip bg-color">LLM</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2025-02-12</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2025-03-23</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 5.7k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 26 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><h1 id="RoPE-RoFormer-Enhanced-Transformer-with-Rotary-Position-Embedding"><a href="#RoPE-RoFormer-Enhanced-Transformer-with-Rotary-Position-Embedding" class="headerlink" title="RoPE / RoFormer: Enhanced Transformer with Rotary Position Embedding"></a>RoPE / RoFormer: Enhanced Transformer with Rotary Position Embedding</h1><p>本文是论文 <a href="https://arxiv.org/abs/2104.09864" target="_blank" rel="noopener">RoFormer: Enhanced Transformer with Rotary Position Embedding</a> 的阅读笔记和个人理解.</p><h2 id="Recommended"><a href="#Recommended" class="headerlink" title="Recommended"></a>Recommended</h2><p>与往常不同, 在本文开头先给出推荐.</p><p>首先<strong>苏神博客看RoPE整个系列的文章</strong>(RoPE的论文就是出自这些博客的集成):</p><ul><li><a href="https://kexue.fm/archives/8130" target="_blank" rel="noopener">让研究人员绞尽脑汁的Transformer位置编码 - 科学空间|Scientific Spaces</a>.</li><li><a href="https://kexue.fm/archives/8265/" target="_blank" rel="noopener">Transformer升级之路：2、博采众长的旋转式位置编码 - 科学空间|Scientific Spaces</a>.</li><li><a href="https://kexue.fm/tag/rope/" target="_blank" rel="noopener">标签 rope 下的文章 - 科学空间|Scientific Spaces</a>.</li></ul><p>本文中穿插的内容也有少量由苏神博客中延伸.</p><p>如果想看比较简单一点的, 强烈推荐直接看抱抱脸推出的Blog<a href="https://huggingface.co/blog/zh/designing-positional-encoding" target="_blank" rel="noopener">设计位置编码</a>, 从一个比较<strong>简单易懂</strong>的角度解释了RoPE的由来.</p><p><strong>代码</strong>方面, 也推荐看看LLaMA对RoPE的实现:</p><ul><li><a href="https://github.com/meta-llama/llama3/blob/main/llama/model.py#L49-L75" target="_blank" rel="noopener">llama3/llama/model.py at main · meta-llama/llama3 · GitHub</a>.</li></ul><p>如果希望看形象一点的, 有图描述的, 推荐看:</p><ul><li><a href="https://zhuanlan.zhihu.com/p/863378538" target="_blank" rel="noopener">避开复数推导，我们还可以怎么理解RoPE？ - 猛猿的文章 - 知乎</a>.</li></ul><p>本文是对<a href="https://arxiv.org/abs/2104.09864" target="_blank" rel="noopener">RoPE论文</a>的记录.</p><h2 id="Background-and-Related-Work"><a href="#Background-and-Related-Work" class="headerlink" title="Background and Related Work"></a>Background and Related Work</h2><p><strong>位置编码</strong>的主要作用是为了<strong>保证同样的Token输入到模型中是可以区分的</strong>, 从而可以编码它们的位置关系. 一个显然的例子, “我吃饭”和”饭吃我”明显因主宾语的位置不同而具有不同的含义.</p><blockquote><p>熟悉的Position Embedding的本小节可以跳过.</p></blockquote><h3 id="Preliminary"><a href="#Preliminary" class="headerlink" title="Preliminary"></a>Preliminary</h3><p>对于长度为$N$ 的句子$\mathbb{S}_N$ , 将其中所有Token组成的Embedding序列$\mathbb{E}_N$, $\boldsymbol{x}_i \in \mathbb{R}^d$ 为$\mathbb{E}_N$ 中第$i$ 个位置上的$d$ 维Embedding.</p><p>当Transformer中的Self - Attention在<strong>忽略位置信息的情况下</strong>, 其Attention Score的计算方式如下:</p><p>$$<br>\begin{aligned}<br>\boldsymbol{q}_m &amp; =f_q\left(\boldsymbol{x}_m, m\right) \\<br>\boldsymbol{k}_n &amp; =f_k\left(\boldsymbol{x}_n, n\right) \\<br>\boldsymbol{v}_n &amp; =f_v\left(\boldsymbol{x}_n, n\right)<br>\end{aligned}<br>$$</p><p>其中$\boldsymbol{q}_m$, $\boldsymbol{k}_n, \boldsymbol{v}_n$ 分别为第$m, n$ 位置通过$f_q, f_k, f_v$ 获得的Query, Key, Value.</p><p>接着计算Scaled Dot-Product Attention:</p><p>$$<br>\begin{aligned}<br>a_{m, n} &amp; =\frac{\exp \left(\frac{\boldsymbol{q}_m^{\top} \boldsymbol{k}_n}{\sqrt{d}}\right)}{\sum_{j=1}^N \exp \left(\frac{\boldsymbol{q}_m^{\top} \boldsymbol{k}_j}{\sqrt{d}}\right)} \\<br>\mathbf{o}_m &amp; =\sum_{n=1}^N a_{m, n} \boldsymbol{v}_n<br>\end{aligned}<br>$$</p><p>$a_{m, n}$ 为$m, n$ Token之间的Attention Score, $\mathbf{o}_m$为第$m$ 个Token的计算完Self - Attention后的表示.</p><h3 id="Absolute-Position-Embedding"><a href="#Absolute-Position-Embedding" class="headerlink" title="Absolute Position Embedding"></a>Absolute Position Embedding</h3><p>最经典的$f_{t: t \in\{q, k, v\}}(\cdot)$ 的方法是直接将Embedding $\boldsymbol{x}_i$ 与位置信息$\boldsymbol{p}_i$ 直接相加, 再通过一个线性投影$\boldsymbol{W}_{t: t \in\{q, k, v\}}$ 直接得到:</p><p>$$<br>f_{t: t \in\{q, k, v\}}\left(\boldsymbol{x}_i, i\right):=\boldsymbol{W}_{t: t \in\{q, k, v\}}\left(\boldsymbol{x}_i+\boldsymbol{p}_i\right)<br>$$</p><p>而在Transformer的原论文中, 作者通过<strong>正余弦函数</strong>来生成$\boldsymbol{p}_i$:</p><p>$$<br>\begin{cases}\boldsymbol{p}_{i, 2 t} &amp; =\sin \left(k / 10000^{2 t / d}\right) \\\ \boldsymbol{p}_{i, 2 t+1} &amp; =\cos \left(k / 10000^{2 t / d}\right)\end{cases}<br>$$</p><p>$\boldsymbol{p}_{i, 2 t}, \boldsymbol{p}_{i, 2 t+1}$ 分别为$\boldsymbol{p}_i \in \mathbb{R}^d$ 的奇数维和偶数维.</p><p>当然, 在<strong>BERT</strong>, <strong>ViT</strong>等论文中也有直接使用Learnable的绝对位置编码, 直接将绝对位置设定为一个可学习的参数让网络自己学.</p><h3 id="Relative-Position-Embedding"><a href="#Relative-Position-Embedding" class="headerlink" title="Relative Position Embedding"></a>Relative Position Embedding</h3><p>由于绝对位置编码不能编码两个Token之间的相对位置关系, <a href="https://aclanthology.org/N18-2074/" target="_blank" rel="noopener">一些工作</a>中也通过相对位置的建模, 将相对位置信息作为位置编码:</p><p>$$<br>\begin{aligned}<br>f_q\left(\boldsymbol{x}_m\right)&amp;:=\boldsymbol{W}_q \boldsymbol{x}_m \\<br>f_k\left(\boldsymbol{x}_n, n\right)&amp;:=\boldsymbol{W}_k\left(\boldsymbol{x}_n+\tilde{\boldsymbol{p}}_r^k\right) \\<br>f_v\left(\boldsymbol{x}_n, n\right)&amp;:=\boldsymbol{W}_v\left(\boldsymbol{x}_n+\tilde{\boldsymbol{p}}_r^v\right)<br>\end{aligned}<br>$$</p><p>$\tilde{\boldsymbol{p}}_r^k, \tilde{\boldsymbol{p}}_r^v \in \mathbb{R}^d$ 为可学习的相对位置编码.</p><p>其中, $r$ 为$m, n$ 之间的相对位置, 用$r=\text{clip}(m-n, r_{min}, r_{max})$ 得到. <strong>这假设了超出一定距离限制的位置信息是不太重要的</strong>. 此时直接采用最远距离的相对位置编码即可.</p><p><a href="http://arxiv.org/abs/1901.02860" target="_blank" rel="noopener">一些工作(TransformerXL / XLNet)</a>直接沿用这个形式将它们按项拆分:</p><p>$$<br>\boldsymbol{q}_m^{\top} \boldsymbol{k}_n=\boldsymbol{x}_m^{\top} \boldsymbol{W}_q^{\top} \boldsymbol{W}_k \boldsymbol{x}_n+\boldsymbol{x}_m^{\top} \boldsymbol{W}_q^{\top} \boldsymbol{W}_k \boldsymbol{p}_n+\boldsymbol{p}_m^{\top} \boldsymbol{W}_q^{\top} \boldsymbol{W}_k \boldsymbol{x}_n+\boldsymbol{p}_m^{\top} \boldsymbol{W}_q^{\top} \boldsymbol{W}_k \boldsymbol{p}_n<br>$$</p><p>进一步的, 由于投影矩阵 $\boldsymbol{W}_k$ 有时候会对Content-based Key $\boldsymbol{x}_n$ 做投影, 有时候会对Position-based Key $\boldsymbol{p}_n$ 做投影, 这显然不合理对吧? 所以可以额外加一个矩阵$\widetilde{\boldsymbol{W}}_k$ 让它来编码$\boldsymbol{p}_n$:</p><p>$$<br>\boldsymbol{q}_m^{\top} \boldsymbol{k}_n=\boldsymbol{x}_m^{\top} \boldsymbol{W}_q^{\top} \boldsymbol{W}_k \boldsymbol{x}_n+\boldsymbol{x}_m^{\top} \boldsymbol{W}_q^{\top} \widetilde{\boldsymbol{W}}_k \tilde{\boldsymbol{p}}_{m-n}+\mathbf{u}^{\top} \boldsymbol{W}_q^{\top} \boldsymbol{W}_k \boldsymbol{x}_n+\mathbf{v}^{\top} \boldsymbol{W}_q^{\top} \widetilde{\boldsymbol{W}}_k \tilde{\boldsymbol{p}}_{m-n}<br>$$</p><p>这玩意看起来太麻烦了, 我们要的其实不就是一项位置信息和一项内容信息吗? 于是<a href="https://www.jmlr.org/papers/v21/20-074.html" target="_blank" rel="noopener">一些工作(T5)</a>直接将<strong>位置信息</strong>和<strong>内容信息</strong>解耦, 直接将相对位置变为一个Learnable bias term $b_{i, j}$:</p><p>$$<br>\boldsymbol{q}_m^{\top} \boldsymbol{k}_n=\boldsymbol{x}_m^{\top} \boldsymbol{W}_q^{\top} \boldsymbol{W}_k \boldsymbol{x}_n+b_{i, j}<br>$$</p><p>用两个Projection进一步分别编码$\boldsymbol{p}_m, \boldsymbol{p}_n$, 即相对位置, 似乎是更合理的:</p><p>$$<br>\boldsymbol{q}_m^{\top} \boldsymbol{k}_n=\boldsymbol{x}_m^{\top} \boldsymbol{W}_q^{\top} \boldsymbol{W}_k \boldsymbol{x}_n+\boldsymbol{p}_m^{\top} \mathbf{U}_q^{\top} \mathbf{U}_k \boldsymbol{p}_n+b_{i, j}<br>$$</p><p>也有<a href="https://arxiv.org/abs/2006.03654" target="_blank" rel="noopener">一些工作(DeBERTa)</a>基于绝对位置的形式, 认为简单的将绝对位置$\boldsymbol{p}_n, \boldsymbol{p}_m$ 替换为相对位置$\tilde{\boldsymbol{p}}_{m-n}$就可以:</p><p>$$<br>\boldsymbol{q}_m^{\top} \boldsymbol{k}_n=\boldsymbol{x}_m^{\top} \boldsymbol{W}_q^{\top} \boldsymbol{W}_k \boldsymbol{x}_n+\boldsymbol{x}_m^{\top} \boldsymbol{W}_q^{\top} \boldsymbol{W}_k \tilde{\boldsymbol{p}}_{m-n}+\tilde{\boldsymbol{p}}_{m-n}^{\top} \boldsymbol{W}_q^{\top} \boldsymbol{W}_k \boldsymbol{x}_n<br>$$</p><h2 id="RoPE"><a href="#RoPE" class="headerlink" title="RoPE"></a>RoPE</h2><p>用下图可以概括RoPE的核心思想:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rope1.png" style="zoom:33%"><p>为什么要叫<strong>Ro</strong>tary <strong>P</strong>osition <strong>E</strong>mbedding呢? 正如它的名字一样, 它是通过<strong>旋转</strong>来区分不同位置的. 上图举了一个$d=2$ 时的简单例子, 对于绝对位置$m$, RoPE将向量在极坐标系中通过旋转能够代表$m$ 的角度来表征它们的位置.</p><p>但RoPE是以表征相对位置闻名的, 因为它可以借助一些巧妙的性质, 使得计算Self-Attention的内积时能将相对位置信息注入到其中. 下面就来看看RoPE是怎么做到这一点的.</p><h3 id="Formulation"><a href="#Formulation" class="headerlink" title="Formulation"></a>Formulation</h3><p>结合前面的分析可知, 想要融入Token之间的相对位置关系, 只需要将$\boldsymbol{q}_m^{\top} \boldsymbol{k}_n$ 建模为能注入相对位置$m-n$ 的函数$g$:</p><p>$$<br>\langle f_q(\boldsymbol{x}_m, m),f_k(\boldsymbol{x}_n, n)\rangle=g(\boldsymbol{x}_m,\boldsymbol{x}_n,m-n)<br>$$</p><p>所以目标就是找到满足$g$ 性质的$f_q(\boldsymbol{x}_m, m),f_k(\boldsymbol{x}_n, n)$.</p><h3 id="Rotary-Position-Embedding"><a href="#Rotary-Position-Embedding" class="headerlink" title="Rotary Position Embedding"></a>Rotary Position Embedding</h3><h4 id="Derivation-of-RoPE-under-2D"><a href="#Derivation-of-RoPE-under-2D" class="headerlink" title="Derivation of RoPE under 2D"></a>Derivation of RoPE under 2D</h4><blockquote><p>在本节中先讨论简单的<strong>二维情况</strong>.</p></blockquote><p>SDPA(Scaled Dot-Product Attention)是两向量的<strong>内积</strong>.</p><p>如果实数运算规则不好找到函数$g$, 有没有其他的运算规则能帮助我们求解? 比如思考<strong>实数内积</strong>与<strong>复数内积</strong>有什么关系吗? 刚刚好, <strong>两向量的内积等于一个复数与另一个复数的共轭的乘积的实部</strong>.</p><p>例如, 对于任意两个实数向量$\mathbf{a} = (a_1, a_2), \mathbf{b} = (b_1, b_2)$, 它们的内积定义为:</p><p>$$<br>\langle \mathbf{a} \cdot \mathbf{b} \rangle = a_1b_1 + a_2b_2<br>$$</p><p>当我们将$\mathbf{a}, \mathbf{b}$ 看成复数时, 有复数$z_a = a_1 + a_2i$, 其共轭为$z_a^\ast = a_1 - a_2i$, 同理对于复数$z_b = b_1 + b_2i$ 有共轭$z_b^\ast = b_1 - b_2i$. 什么叫”求一个复数与另一个复数的共轭的乘积的实部”? 计算$z_a, z_b^\ast$ 之间的内积:</p><p>$$<br>\begin{aligned}<br>z_a \cdot z_b^\ast &amp;= (a_1 + a_2i)(b_1 - b_2i) \\<br>&amp;= a_1b_1 + a_1(-b_2i) + a_2i b_1 + a_2i (-b_2i) \\<br>&amp;= a_1b_1 + a_2b_2 + i(a_2b_1 - a_1b_2)<br>\end{aligned}<br>$$</p><p>实部$a_1b_1 + a_2b_2$ 恰好就是$\mathbf{a}, \mathbf{b}$ 在实数域内积的值.</p><p>好, 有了上面的性质, 下面开始今天的正题. 先往上小节说的Formulation上靠拢呗.</p><p>如果想要表征<strong>绝对位置信息</strong>, 需要使得$\boldsymbol{q}_m, \boldsymbol{k}_n$ 仅依赖于它们的绝对位置$m, n$:</p><p>$$<br>\begin{aligned}<br>\boldsymbol{q}_m &amp;= f_q(\boldsymbol{x}_q, m) \\<br>\boldsymbol{k}_n &amp;= f_k(\boldsymbol{x}_k, n)<br>\end{aligned}<br>$$</p><p>利用实数内积与复数内积的关联, 我们假设有<strong>利用复数运算的函数</strong>$g$ 能够在二者做内积的时候编码它们的相对位置关系, 使得其内积依赖于<strong>相对位置</strong>$m-n$, 以此在内积上表征二者的<strong>相对位置信息</strong>:</p><p>$$<br>\boldsymbol{q}^\intercal_m\boldsymbol{k}_n = \langle f_q(\boldsymbol{x}_m, m),f_k(\boldsymbol{x}_n, n)\rangle= g(\boldsymbol{x}_m, \boldsymbol{x}_n, n - m)<br>$$</p><p>同时, 这种编码方式还应该保证在初始状态下不编码位置信息:</p><p>$$<br>\begin{aligned}<br>\boldsymbol{q} &amp;= f_q(\boldsymbol{x}_q, 0) \\<br>\boldsymbol{k} &amp;= f_k(\boldsymbol{x}_k, 0)<br>\end{aligned}<br>$$</p><p>首先, 用<strong>复数的指数形式</strong>(都学过<a href="https://zh.wikipedia.org/wiki/极坐标系#复数" target="_blank" rel="noopener">极坐标系中表示复数</a>吧), 将$f_q(\boldsymbol{x}_m, m),f_k(\boldsymbol{x}_n, n)$ 表征为:</p><p>$$<br>\begin{aligned}<br>f_q(\boldsymbol{x}_q, m) &amp;= R_q(\boldsymbol{x}_q, m)e^{i\Theta_q(\boldsymbol{x}_q, m)} \\<br>f_k(\boldsymbol{x}_k, n) &amp;= R_k(\boldsymbol{x}_k, n)e^{i\Theta_k(\boldsymbol{x}_k, n)} \\<br>g(\boldsymbol{x}_q, \boldsymbol{x}_k, n - m) &amp;= R_g(\boldsymbol{x}_q, \boldsymbol{x}_k, n - m)e^{i\Theta_g(\boldsymbol{x}_q, \boldsymbol{x}_k, n - m)}<br>\end{aligned}<br>$$</p><p>其中, $R_f, R_g, \Theta_f, \Theta_g$ 分别为$f_{\{q, k\}}$ 和$g$ 的幅度(模)和角度. $f$ 的意思是$f_{\{q, k\}}$, 代表$q, k$.</p><p>不难发现, 代入到$\boldsymbol{q}_m, \boldsymbol{k}_n$ 中, 当二者做内积时, 得到幅度$R$ 和角度$\Theta$ 之间的关系:</p><p>$$<br>\begin{aligned}<br>R_q(\boldsymbol{x}_q, m)R_k(\boldsymbol{x}_k, n) &amp;= R_g(\boldsymbol{x}_q, \boldsymbol{x}_k, n - m) \\<br>\Theta_k(\boldsymbol{x}_k, n) - \Theta_q(\boldsymbol{x}_q, m) &amp;= \Theta_g(\boldsymbol{x}_q, \boldsymbol{x}_k, n - m)<br>\end{aligned}<br>$$</p><p>此时, 它在初始条件(看成是不表征位置信息)的情况下, 应满足:</p><p>$$<br>\begin{aligned}<br>\boldsymbol{q} &amp;= \Vert \boldsymbol{q}\Vert e^{i\theta_q} = R_q(\boldsymbol{x}_q, 0)e^{i\Theta_q(\boldsymbol{x}_q, 0)}\\<br>\boldsymbol{k} &amp; = \Vert \boldsymbol{k}\Vert e^{i\theta_k} = R_k(\boldsymbol{x}_k, 0)e^{i\Theta_k(\boldsymbol{x}_k, 0)}<br>\end{aligned}<br>$$</p><p>其中, $\Vert q \Vert , \Vert k \Vert$ 和$\theta_q, \theta_k$ 分别为$\boldsymbol{q}, \boldsymbol{k}$ 在二维平面上的幅值(模)和角度.</p><p>接着将相同的位置$m=n$ 带入到幅度$R$ 和角度$\Theta$ 的关系式中, 分别得到:</p><p>$$<br>\begin{aligned}<br>R_q(\boldsymbol{x}_q, m)R_k(\boldsymbol{x}_k, m) &amp;= R_g(\boldsymbol{x}_q, \boldsymbol{x}_k, 0) = R_q(\boldsymbol{x}_q, 0)R_k(\boldsymbol{x}_k, 0) = \Vert \boldsymbol{q}\Vert \Vert \boldsymbol{k}\Vert \\<br>\Theta_k(\boldsymbol{x}_k, m) - \Theta_q(\boldsymbol{x}_q, m) &amp;= \Theta_g(\boldsymbol{x}_q, \boldsymbol{x}_k, 0) =\Theta_k(\boldsymbol{x}_k, 0) - \Theta_q(\boldsymbol{x}_q, 0) = \theta_k - \theta_q<br>\end{aligned}<br>$$</p><p>此时, 幅值$R_f$ 的一个显然解为:</p><p>$$<br>\begin{aligned}<br>R_q(\boldsymbol{x}_q, m) &amp;= R_q(\boldsymbol{x}_q, 0) = \Vert \boldsymbol{q}\Vert \\<br>R_k(\boldsymbol{x}_k, n) &amp;= R_k(\boldsymbol{x}_k, 0) = \Vert \boldsymbol{k}\Vert \\<br>R_g(\boldsymbol{x}_q, \boldsymbol{x}_k, n - m) &amp;= R_g(\boldsymbol{x}_q, \boldsymbol{x}_k, 0) = \Vert \boldsymbol{q}\Vert \Vert \boldsymbol{k}\Vert<br>\end{aligned}<br>$$</p><p>它显然不依赖位置信息, 即满足$\boldsymbol{q} = f_q(\boldsymbol{x}_q, 0), \boldsymbol{k} = f_k(\boldsymbol{x}_k, 0)$.</p><p>那么角度$\Theta$ 的解呢?</p><p>当位置相同时, 不应该编码任何位置信息. 令$\Theta_f:=\Theta_q=\Theta_k$, 根据$\Theta_q\left(\boldsymbol{x}_q, m\right)-\theta_q=\Theta_k\left(\boldsymbol{x}_k, m\right)-\theta_k$, 此时明显两侧是相同的函数. 由此, 可以推断出$\Theta_f\left(\boldsymbol{x}_{\{q, k\}}, m\right)-\theta_{\{q, k\}}$ 是与位置$m$ 相关, 与Word Embedding $\boldsymbol{x}_{\{q, k\}}$ 无关的函数, 可设$\Theta_f\left(\boldsymbol{x}_{\{q, k\}}, m\right)-\theta_{\{q, k\}}=\phi(m)$, 得到:</p><p>$$<br>\Theta_f(\boldsymbol{x}_{\{q, k\}}, m) = \phi(m) + \theta_{\{q,k\}}<br>$$</p><p>$n=m+1$, 将上式代入到角度关系式当中, 可以得到:</p><p>$$<br>\phi(m + 1) - \phi(m) = \Theta_g(\boldsymbol{x}_q, \boldsymbol{x}_k, 1) + \theta_q - \theta_k<br>$$</p><p>上式中右侧的常数项与绝对位置$m$ 是无关的, 所以令上式右端整体为$\theta$, $\phi(m)$ 就是一个<strong>等差数列</strong>:</p><p>$$<br>\phi(m) = m\theta + \gamma<br>$$</p><p>其中$\gamma \in \mathbb{R}$ 为常数(首项), $\theta$ 非零, 绝对位置$m$ 在这个式子中也是公差.</p><p>通过对幅值$R_f$ 和角度$\Theta_f$ 的求解, 整理一下上面的式子, 可以得到:</p><p>$$<br>\begin{aligned}<br>f_q(\boldsymbol{x}_q, m) &amp;= \Vert \boldsymbol{q}\Vert e^{i(\theta_q + m\theta + \gamma)} = \boldsymbol{q} e^{i(m\theta + \gamma)} \\<br>f_k(\boldsymbol{x}_k, n) &amp;= \Vert \boldsymbol{k}\Vert e^{i(\theta_k + n\theta + \gamma)}= \boldsymbol{k} e^{i(n\theta + \gamma)}<br>\end{aligned}<br>$$</p><p>通常$\boldsymbol{q} = f_q(\boldsymbol{x}_m, 0), \boldsymbol{k} = f_k(\boldsymbol{x}_n, 0)$ 是直接由一层投影得到的:</p><p>$$<br>\begin{aligned}<br>\boldsymbol{q} = f_q(\boldsymbol{x}_m, 0) &amp;= \boldsymbol{W}_q\boldsymbol{x}_n\\<br>\boldsymbol{k} = f_k(\boldsymbol{x}_n, 0) &amp;= \boldsymbol{W}_k\boldsymbol{x}_n<br>\end{aligned}<br>$$</p><p>方便起见, 直接设定$\gamma = 0$, 因此得到最终的RoPE形式:</p><p>$$<br>\begin{aligned}<br>f_q(\boldsymbol{x}_m, m) &amp;= (\boldsymbol{W}_q\boldsymbol{x}_m)e^{im\theta} \\<br>f_k(\boldsymbol{x}_n, n) &amp;= (\boldsymbol{W}_k\boldsymbol{x}_n)e^{in\theta}<br>\end{aligned}<br>$$</p><p>所以, 我们在<strong>二维情况</strong>下, 最后找到的函数$g$ 为:</p><p>$$<br>g(\boldsymbol{x}_m, \boldsymbol{x}_n, m - n) = \operatorname{Re}[(\boldsymbol{W}_q\boldsymbol{x}_m)(\boldsymbol{W}_k\boldsymbol{x}_n)^{\ast}e^{i(m - n)\theta}]<br>$$</p><p>其中$\text{Re}(\cdot)$ 为取复数的实部, $(\boldsymbol{W}_k\boldsymbol{x}_n)^{\ast}$ 为$(\boldsymbol{W}_k\boldsymbol{x}_n)$ 的共轭复数, $\theta \in \mathbb{R}$ 为预设好的非零常数(基波长, 沿用Transformer的原论文, 经常设为10000, 底数的选择可以参考<a href="https://kexue.fm/archives/10122" target="_blank" rel="noopener">这里</a>).</p><p>进一步写出二维情况下$f_{q, k}$ 的表达式:</p><p>$$<br>\begin{aligned}<br>f_{\{q, k\}}(\boldsymbol{x}_m, m) &amp;= \left(<br>\begin{array}{cc}<br>\cos{m\theta}&amp; -\sin{m\theta} \\<br>\sin{m\theta}&amp;\cos{m\theta}<br>\end{array}<br>\right)<br>\left(<br>\begin{array}{cc}<br>W^{(11)}_{\{q, k\}} &amp; W^{(12)}_{\{q, k\}} \\<br>W^{(21)}_{\{q, k\}} &amp; W^{(22)}_{\{q, k\}}<br>\end{array}<br>\right)<br>\left(<br>\begin{array}{cc}<br>x^{(1)}_m\\<br>x^{(2)}_m<br>\end{array}<br>\right) \\<br>&amp;=\left(<br>\begin{array}{cc}<br>\cos{m\theta}&amp; -\sin{m\theta} \\<br>\sin{m\theta}&amp;\cos{m\theta}<br>\end{array}<br>\right)<br>\left(<br>\begin{array}{cc}<br>\{q, k\}^{(1)}_m\\<br>\{q, k\}^{(2)}_m<br>\end{array}<br>\right)<br>\end{aligned}<br>$$</p><p>它就是$\{\boldsymbol{q, k}\}_m$ 乘了一个<strong>旋转矩阵</strong>, 对应着空间中的向量旋转操作, 这才是<strong>RoPE</strong>(<strong>Ro</strong>tary <strong>P</strong>osition <strong>E</strong>mbedding)名字的由来.</p><h4 id="General-form"><a href="#General-form" class="headerlink" title="General form"></a>General form</h4><p>更为普遍的, 将上节得到的二维情况扩展到任意偶数维度$d$. 将RoPE应用于$\boldsymbol{x}_i \in \mathbb{R}^d$:</p><p>$$<br>f_{\{q, k\}}(\boldsymbol{x}_m, m) = \boldsymbol{R}^d_{\Theta, m}\boldsymbol{W}_{\{q, k\}}\boldsymbol{x}_m<br>$$</p><p>非常自然的思路就是将$d$ 维拆成$d/2$ 个的二维子空间, 所以$\boldsymbol{R}^d_{\Theta, m}$ 为:</p><p>$$<br>\boldsymbol{R}_{\Theta, m}^d=\left(\begin{array}{ccccccc}<br>\cos m \theta_1 &amp; -\sin m \theta_1 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\<br>\sin m \theta_1 &amp; \cos m \theta_1 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\<br>0 &amp; 0 &amp; \cos m \theta_2 &amp; -\sin m \theta_2 &amp; \cdots &amp; 0 &amp; 0 \\<br>0 &amp; 0 &amp; \sin m \theta_2 &amp; \cos m \theta_2 &amp; \cdots &amp; 0 &amp; 0 \\<br>\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\<br>0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \cos m \theta_{d / 2} &amp; -\sin m \theta_{d / 2} \\<br>0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \sin m \theta_{d / 2} &amp; \cos m \theta_{d / 2}<br>\end{array}\right)<br>$$</p><p>其中, $\Theta=\left\{\theta_i=10000^{-2(i-1) / d}, i \in[1,2, \ldots, d / 2]\right\}$. 所以$i$ 越大, $\theta_i$ 越小, 即<strong>下标越大的维度旋转速度越慢</strong>.</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/863378538" target="_blank" rel="noopener">这篇文章</a>提供了一个非常有意思的角度, 不同角速度的维度就像<strong>时针分针秒针</strong>一样, 通过不同的快慢角速度来组合表征一个位置信息.</p></blockquote><p>将RoPE直接作用于Self-Attention, 可得:</p><p>$$<br>\boldsymbol{q}_m^{\intercal}\boldsymbol{k}_n<br>=(\boldsymbol{R}^d_{\Theta, m}\boldsymbol{W}_q\boldsymbol{x}_m)^\intercal(\boldsymbol{R}^d_{\Theta, n}\boldsymbol{W}_k\boldsymbol{x}_n) =\boldsymbol{x}^\intercal\boldsymbol{W}_qR^d_{\Theta, n-m}\boldsymbol{W}_k\boldsymbol{x}_n<br>$$</p><p>其中, $\boldsymbol{R}_{\Theta, n-m}^d=\left(\boldsymbol{R}_{\Theta, m}^d\right)^{\top} \boldsymbol{R}_{\Theta, n}^d$. 由于$\boldsymbol{R}_{\Theta}^d$ 是一个<strong>正交矩阵</strong>, 所以它在编码位置信息的时候是相对稳定的.</p><p>RoPE相较于之前位置编码的最大不同, 在于它是<strong>乘性</strong>的, 而不是加性的, 因此它在编码的相对位置信息可以天然的融入到Self-Attention的内积中.</p><h3 id="Properties-of-RoPE"><a href="#Properties-of-RoPE" class="headerlink" title="Properties of RoPE"></a>Properties of RoPE</h3><p>RoPE有一些优良的性质.</p><p>首先, RoPE具有<strong>长程衰减</strong>, 和最早版本的Transformer一样, RoPE设定$\theta_i=10000^{-2i/d}$, <strong>底数$\theta$ 越大长程衰减越慢</strong>.</p><p>其次, RoPE在<strong>Linear Attention</strong>当中可以使得Self-Attention被更一般的形式重写.</p><p>例如, 常规的SDPA可以写成:</p><p>$$<br>\operatorname{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V})_m=\frac{\sum_{n=1}^{N}\operatorname{sim}(\boldsymbol{q}_m,\boldsymbol{k}_n)\boldsymbol{v}_n}{\sum_{n=1}^{N}\operatorname{sim}(\boldsymbol{q}_m, \boldsymbol{k}_n)}<br>$$</p><p>Self-Attention中采用的是$\operatorname{sim}\left(\boldsymbol{q}_m, \boldsymbol{k}_n\right)=\exp \left(\boldsymbol{q}_m^{\top} \boldsymbol{k}_n / \sqrt{d}\right)$, 它具有$\mathbb{O}(N^2)$ 的复杂度.</p><p><a href="https://kexue.fm/archives/7546/" target="_blank" rel="noopener">Linear Attention</a>的复杂度是比SDPA更低的线性复杂度, 形式如下:</p><p>$$<br>\operatorname{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V})_m=\frac{\sum_{n=1}^{N}\phi(\boldsymbol{q}_m)^{\intercal}\varphi(\boldsymbol{k}_n)\boldsymbol{v}_n}{\sum_{n=1}^{N}\phi(\boldsymbol{q}_m)^{\intercal}\varphi(\boldsymbol{k}_n)}<br>$$</p><p>其中$\phi(\cdot), \varphi(\cdot)$ 通常是非负函数, 例如$\phi(x)=\varphi(x)=\text{elu}(x)+1$, 或者$\phi\left(\boldsymbol{q}_i\right)=\operatorname{softmax}\left(\boldsymbol{q}_i\right), \varphi\left(\boldsymbol{k}_j\right)=\exp \left(\boldsymbol{k}_j\right)$.</p><p>由于RoPE的存在, 可以使得Hidden State的范数不变, 因此可以将RoPE直接乘, 从而无缝集成到Linear Attention中, 从而不增加Linear Attention的复杂度:</p><p>$$<br>\operatorname{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V})_m=\frac{\sum_{n=1}^{N}\big(\boldsymbol{R}^d_{\Theta, m}\phi(\boldsymbol{q}_m)\big)^{\intercal}\big(\boldsymbol{R}^d_{\Theta, n}\varphi(\boldsymbol{k}_n)\big)\boldsymbol{v}_n}{\sum_{n=1}^{N}\phi(\boldsymbol{q}_m)^{\intercal}\varphi(\boldsymbol{k}_n)}<br>$$</p><h3 id="Theoretical-Explanation"><a href="#Theoretical-Explanation" class="headerlink" title="Theoretical Explanation"></a>Theoretical Explanation</h3><h4 id="Computational-efficient-realization-of-rotary-matrix-multiplication"><a href="#Computational-efficient-realization-of-rotary-matrix-multiplication" class="headerlink" title="Computational efficient realization of rotary matrix multiplication"></a>Computational efficient realization of rotary matrix multiplication</h4><p>由于$\boldsymbol{R}^d_{\Theta, m}$ 是一个<strong>稀疏矩阵</strong>, 所以它的计算效率很低. 下式可以化简计算:</p><p>$$<br>\boldsymbol{R}^d_{\Theta, m}\boldsymbol{x} =<br>\begin{pmatrix}<br>x_1\\<br>x_2\\<br>x_3\\<br>x_4\\<br>\vdots\\<br>x_{d-1}\\<br>x_d<br>\end{pmatrix}<br>\otimes<br>\begin{pmatrix}<br>\cos{m\theta_1} \\<br>\cos{m\theta_1} \\<br>\cos{m\theta_2} \\<br>\cos{m\theta_2} \\<br>\vdots \\<br>\cos{m\theta_{d/2}} \\<br>\cos{m\theta_{d/2}}<br>\end{pmatrix}<br>+<br>\begin{pmatrix}<br>-x_2\\<br>x_1\\<br>-x_4\\<br>x_3\\<br>\vdots\\<br>-x_d\\<br>x_{d-1}<br>\end{pmatrix}<br>\otimes<br>\begin{pmatrix}<br>\sin{m\theta_1}\\<br>\sin{m\theta_1}\\<br>\sin{m\theta_2}\\<br>\sin{m\theta_2}\\<br>\vdots\\<br>\sin{m\theta_{d/2}}\\<br>\sin{m\theta_{d/2}}<br>\end{pmatrix}<br>$$</p><p>其中$\otimes$ 为逐元素点乘.</p><h4 id="Long-term-decay-of-RoPE"><a href="#Long-term-decay-of-RoPE" class="headerlink" title="Long-term decay of RoPE"></a>Long-term decay of RoPE</h4><p>在RoPE的作用下, 将向量$\boldsymbol{q}=\boldsymbol{W}_q\boldsymbol{x}_m, \boldsymbol{k}=\boldsymbol{W}_k\boldsymbol{x}_n$ 两维两维的分组成对, 其内积可以写成:</p><p>$$<br>(\boldsymbol{R}^d_{\Theta, m}\boldsymbol{W}_q\boldsymbol{x}_m)^\intercal(\boldsymbol{R}^d_{\Theta, n}\boldsymbol{W}_k\boldsymbol{x}_n) = \operatorname{Re}\bigg[\sum_{i=0}^{d/2-1}\boldsymbol{q}_{[2i:2i+1]}\boldsymbol{k}_{[2i:2i+1]}^{\ast}e^{i(m-n)\theta_{i}}\bigg]<br>$$</p><p>其中$\boldsymbol{q}_{[2i:2i+1]}$ 代表$\boldsymbol{q}$ 中的第$2i$ 到第$2i+1$ 个元素, $\boldsymbol{k}$ 同理.</p><p>令$h_i = \boldsymbol{q}_{[2i:2i+1]}\boldsymbol{k}_{[2i:2i+1]}^\ast, S_j = \sum_{i=0}^{j-1} e^{\text{i}(m-n)\theta_i}$, 同时设$h_{d/2}=0, S_0=0$, 由<a href="https://zh.wikipedia.org/wiki/分部求和法" target="_blank" rel="noopener">Abel变换</a>可以得到:</p><p>$$<br>\sum_{i=0}^{d/2-1}\boldsymbol{q}_{[2i:2i+1]}\boldsymbol{k}_{[2i:2i+1]}^{\ast}e^{i(m-n)\theta_{i}}=\sum_{i=0}^{d/2-1}h_i(S_{i+1}-S_{i})=-\sum_{i=0}^{d/2-1}S_{i+1}(h_{i+1}-h_i)<br>$$</p><p>因此有:</p><p>$$<br>\begin{aligned}<br>\bigg\vert\sum_{i=0}^{d/2-1}\boldsymbol{q}_{[2i:2i+1]}\boldsymbol{k}_{[2i:2i+1]}^{\ast}e^{i(m-n)\theta_{i}}\bigg\vert &amp;= \bigg\vert\sum_{i=0}^{d/2-1}S_{i+1}(h_{i+1}-h_i)\bigg\vert\\<br>&amp;\leq \sum_{i=0}^{d/2-1}\vert S_{i+1}\vert\vert(h_{i+1}-{h_i})\vert\\<br>&amp;\leq \big(\max_i\vert h_{i+1}-h_i\vert\big)\sum_{i=0}^{d/2-1}\vert S_{i+1}\vert<br>\end{aligned}<br>$$</p><p>即当相对位置$m-n$ 增大时, 在$\theta_i=10000^{-2i/d}$的设定下, $\frac{1}{d / 2} \sum_{i=1}^{d / 2}\left|S_i\right|$ 的值是逐渐减小的, 如下图所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rope2.png" style="zoom:40%"><h2 id="Experiments-and-Evaluations"><a href="#Experiments-and-Evaluations" class="headerlink" title="Experiments and Evaluations"></a>Experiments and Evaluations</h2><p>详细的实验参数设置和模型参数设置请参考原论文.</p><h3 id="Machine-Translation"><a href="#Machine-Translation" class="headerlink" title="Machine Translation"></a>Machine Translation</h3><p>作者将RoFormer与<strong>Vallina Transformer</strong>在WMT 2014 English-German(450w平行语料)上对比:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rope3.png" style="zoom:50%"><p>RoFormer略高于Vallina Transformer.</p><h3 id="Pre-training-Language-Modeling"><a href="#Pre-training-Language-Modeling" class="headerlink" title="Pre-training Language Modeling"></a>Pre-training Language Modeling</h3><p>接着, 在BookCorpus和Wikipedia这两个常用的语料上做预训练, 与<strong>BERT</strong>做对比, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rope4.png" style="zoom:50%"><p>RoFormer明显的比BERT要收敛更快一些, 而且从曲线上来看它的训练看起来更加稳定.</p><h3 id="Fine-tuning-on-GLUE-tasks"><a href="#Fine-tuning-on-GLUE-tasks" class="headerlink" title="Fine-tuning on GLUE tasks"></a>Fine-tuning on GLUE tasks</h3><p>在GLUE上做Finetune, 继续与<strong>BERT</strong>对比:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rope5.png" style="zoom:50%"><p>RoFormer在MRPC, STS-B, QQP上有显著优势, 它们都是偏语义类的任务, 在SST-2和QNLI, MNLI中表现弱于BERT.</p><h3 id="Performer-with-RoPE"><a href="#Performer-with-RoPE" class="headerlink" title="Performer with RoPE"></a>Performer with RoPE</h3><p>上文中提到了RoPE在Linear Attention中的优势, 因此测试与使用Linear Attention的Performer做个结合:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rope6.png" style="zoom:50%"><p>RoPE能够在维持原复杂度不变的条件下加速收敛.</p><h3 id="Evaluation-on-Chinese-Data"><a href="#Evaluation-on-Chinese-Data" class="headerlink" title="Evaluation on Chinese Data"></a>Evaluation on Chinese Data</h3><p>除了在英文数据上进行评估, 还要在中文数据上进行评估.</p><p>在本实验中, 作者将自己提出的<a href="https://github.com/ZhuiyiTechnology/WoBERT" target="_blank" rel="noopener">WoBERT</a>的绝对位置编码替换为RoPE:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rope7.png" style="zoom:50%"><p>并在34G的语料上进行不同参数的多阶段训练, 以适应<strong>不同的输入长度</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rope8.png" style="zoom:50%"><p>随着序列长度的增加, RoFormer总是能取得更好的性能, 说明RoFormer在长度外推设置下表现较好.</p><p>在<a href="https://arxiv.org/abs/1911.08962" target="_blank" rel="noopener">CAIL2019-SCM</a>(一个语义相似任务)上来测试RoFormer的长文本能力, 表现如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rope9.png" style="zoom:50%"><ul><li>当RoFormer输入长为512时, 能够略优于BERT和WoBERT.</li><li>当RoFormer的最大输入长度扩展到1024的时候, 显著优于BERT和WoBERT.</li></ul><blockquote><p>此外, 还能观察到在中文场景下, 词建模的WoBERT要优于字建模的BERT. 我以前也有类似的观点, 更大的优势是词建模如果用到生成类任务中能带来更快的推理速度.</p></blockquote><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>与其说RoPE简洁旦优雅, 不如说RoPE简洁甚至优雅. RoPE<strong>用绝对位置编码实现了相对位置编码</strong>, 并且能够丝滑无缝的融入到Self-Attention当中.</p><p>由于我硕士期间是做信息抽取的, 所以我印象中RoPE最早在信息抽取里面运用的非常广泛, 其中可能原因有两个:</p><ul><li>其一是因为<strong>相对位置</strong>对信息抽取的<strong>Span</strong>抽取影响比较大, 因为每个Span的都是通过<strong>起止边界位置</strong>确定的, 在Universal Information发展后期, 基本都是要带上RoPE.</li><li>另外可能是因为信息抽取抽取Span的大多都是通过一个<strong>乘积</strong>计算得到得分, <strong>与内积形式一致</strong>, 这与RoPE给Self-Attention设计的形式恰恰好是一致的, 所以RoPE表现好也不奇怪.</li></ul><p>RoPE早期在其他领域似乎没有得到特别多的关注. 但是随着<strong>PaLM</strong>, <strong>LLaMA</strong>等LLM采用了RoPE, RoPE便渐渐的在LLM里面成为一项标准的配置, 所以RoPE甚至也被人誉为<strong>LLM时代的ResNet</strong>.</p><p>在更长的上下文场景下, RoPE的续作是<a href="https://kexue.fm/archives/9708" target="_blank" rel="noopener">ReRoPE</a>, 超过了NTK-RoPE, 感兴趣的可以继续深入.</p><p>虽然论文本身不是苏神自己写的, 但最后在文末也不得不感叹, 苏神真强啊.</p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">DaNing</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://ADAning.github.io/posts/50765.html">https://ADAning.github.io/posts/50765.html</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">DaNing</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/RoPE/"><span class="chip bg-color">RoPE</span> </a><a href="/tags/LLM/"><span class="chip bg-color">LLM</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="qq,wechat,weibo" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/share/js/social-share.min.js"></script></div></div></div></div></div><style>.mvaline-card{margin:1.5rem auto}.mvaline-card .card-content{padding:20px 20px 5px 20px}</style><div class="card mvaline-card" data-aos="fade-up"><div class="comment_headling" style="font-size:20px;font-weight:700;position:relative;padding-left:20px;top:15px;padding-bottom:5px"><i class="fas fa-comments fa-fw" aria-hidden="true"></i> <span>评论</span></div><div id="mvcomments" class="card-content" style="display:grid"></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/minivaline/MiniValine.min.js"></script><script>new MiniValine({el:"#mvcomments",appId:"M2K91TQqrwW698jR08LdugNz-gzGzoHsz",appKey:"b04G08nTf4B3kqCfOOY1urvC",mode:"xCss",placeholder:"评论暂不支持Latex公式, 但支持Markdown语法.",pathname:window.location.pathname,lang:"",adminEmailMd5:"ebbfbc84f11742e41a94a4e64b1d37ab",tagMeta:["管理员","小伙伴","访客"],master:["ebbfbc84f11742e41a94a4e64b1d37ab"],friends:["b5bd5d836c7a0091aa8473e79ed4c25e","adb7d1cd192658a55c0ad22a3309cecf","3ce1e6c77b4910f1871106cb30dc62b0","cfce8dc43725cc14ffcd9fb4892d5bfc"],math:!1,md:!0,enableQQ:!0,NoRecordIP:!1,visitor:!1,maxNest:6,pageSize:12,serverURLs:"",emoticonUrl:["https://cdn.jsdelivr.net/npm/alus@latest","https://cdn.jsdelivr.net/gh/MiniValine/qq@latest","https://cdn.jsdelivr.net/gh/MiniValine/Bilibilis@latest","https://cdn.jsdelivr.net/gh/MiniValine/tieba@latest","https://cdn.jsdelivr.net/gh/MiniValine/twemoji@latest","https://cdn.jsdelivr.net/gh/MiniValine/weibo@latest"]})</script><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/posts/40650.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/6.jpg" class="responsive-img" alt="DDIM: Denoising Diffusion Implicit Models"> <span class="card-title">DDIM: Denoising Diffusion Implicit Models</span></div></a><div class="card-content article-content"><div class="summary block-with-text">本文前置知识: DDPM: DDPM: Denoising Diffusion Probabilistic Model. Denoising Diffusion Implicit Models 论文: Denoising Diffu</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2025-03-21 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/Diffusion/"><span class="chip bg-color">Diffusion</span> </a><a href="/tags/DDIM/"><span class="chip bg-color">DDIM</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/posts/21614.html"><div class="card-image"><img src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/medias/featureimages/15.jpg" class="responsive-img" alt="CLAP: Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation"> <span class="card-title">CLAP: Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation</span></div></a><div class="card-content article-content"><div class="summary block-with-text">Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation本文是论文Large-Sca</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2025-02-07 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">深度学习</a></span></div></div><div class="card-action article-tags"><a href="/tags/Audio/"><span class="chip bg-color">Audio</span></a></div></div></div></div></article></div><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{let e=$("#artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#prenext-posts").width(t)}return}})})</script></main><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">DaNing</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">420.2k</span>&nbsp;字 <span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/ADAning" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:andaning@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="https://www.zhihu.com/people/nzhu-27" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nzhu-27" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-angle-double-up"></i></a></div><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/materialize/materialize.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/masonry/masonry.pkgd.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/aos/aos.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/scrollprogress/scrollProgress.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script async src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/others/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/gh/ADAning/ADAning.github.io/libs/instantpage/instantpage.js" type="module"></script></body></html>