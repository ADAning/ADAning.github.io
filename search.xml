<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Hexo博客迁移</title>
      <link href="/posts/6682.html"/>
      <url>/posts/6682.html</url>
      
        <content type="html"><![CDATA[<h1 id="Hexo博客迁移"><a href="#Hexo博客迁移" class="headerlink" title="Hexo博客迁移"></a>Hexo博客迁移</h1><p>本文记录一次带主题的Hexo博客迁移过程, 从Win上迁移到MacOS.</p><h2 id="前置依赖"><a href="#前置依赖" class="headerlink" title="前置依赖"></a>前置依赖</h2><p>首先, 需要在MacOS上装Node.js和Git的环境, 网上有大把大把的教程, 这里就不再多说了, 自行搜索即可.</p><h2 id="必要文件"><a href="#必要文件" class="headerlink" title="必要文件"></a>必要文件</h2><p>把Windows上博客下的下列文件和文件夹暂存下来, 方便后续使用:</p><pre><code>_config.ymlpackage.jsonscaffoldssourcethemes</code></pre><h2 id="Hexo"><a href="#Hexo" class="headerlink" title="Hexo"></a>Hexo</h2><p>装完Node.js后需要安装Hexo:</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> -g hexo-cli<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>然后在你想迁移到的地方初始化Hexo, 例如我想把<code>blog</code>文件夹当做博客新位置:</p><pre class="line-numbers language-bash"><code class="language-bash">hexo init blog<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="迁移博客"><a href="#迁移博客" class="headerlink" title="迁移博客"></a>迁移博客</h2><p>切换到<code>blog</code>文件夹下, 把刚才Windows上暂存的文件全部都拿过来后, 安装相关博客依赖:</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>然后检查一下博客是否迁移成功:</p><pre class="line-numbers language-bash"><code class="language-bash">hexo g <span class="token operator">&amp;&amp;</span> hexo s<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>去浏览器看一下网址有没有问题就行, 后续使用流程和之前就没有任何区别了, 整个流程还是比较简单的.</p><h2 id="后处理"><a href="#后处理" class="headerlink" title="后处理"></a>后处理</h2><p>我迁移后出现了公式无法正常显示的问题, 才想起来之前Mathjax和Markdown语法是有冲突的, 我之前改过匹配规则, 参照<a href="https://adaning.github.io/posts/33457.html#MathJax和Markdown冲突">这里</a>重新做一遍即可.</p>]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Matery </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SDN: Synchronous Dual Network with Cross-Type Attention for Joint Entity and Relation Extraction</title>
      <link href="/posts/879.html"/>
      <url>/posts/879.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li><strong>LSTM</strong>: <a href="https://adaning.github.io/posts/60202.html">循环神经网络小结</a></li></ul></blockquote><h1 id="Synchronous-Dual-Network-with-Cross-Type-Attention-for-Joint-Entity-and-Relation-Extraction"><a href="#Synchronous-Dual-Network-with-Cross-Type-Attention-for-Joint-Entity-and-Relation-Extraction" class="headerlink" title="Synchronous Dual Network with Cross-Type Attention for Joint Entity and Relation Extraction"></a>Synchronous Dual Network with Cross-Type Attention for Joint Entity and Relation Extraction</h1><p>本文是论文<a href="https://aclanthology.org/2021.emnlp-main.219/" target="_blank" rel="noopener">Synchronous Dual Network with Cross-Type Attention for Joint Entity and Relation Extraction</a>的阅读笔记和个人理解, 论文来自<strong>EMNLP2021</strong>.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>联合抽取因NER和RE之间的复杂交互而有挑战性, 现有的方法经把二者通过一个共享的网络来解决, 丢失了<strong>实体类型</strong>和<strong>关系类型</strong>的<strong>相互依赖</strong>.</p><p>因此, 作者从<strong>多任务学习</strong>角度, 设计了一种跨类型注意力的同步对偶网络, 来充分利用实体类型和关系类型之间的联系.</p><p>下图是一个实体关系抽取的例子, 需要根据给出的句子来抽取出实体以及其对应的实体类型, 并判断实体之间所存在的关系, 以此组成三元组:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/sdn1.jpg" style="zoom: 50%;" /><h2 id="Type-Attention-LSTM"><a href="#Type-Attention-LSTM" class="headerlink" title="Type - Attention LSTM"></a>Type - Attention LSTM</h2><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/sdn3.jpg" style="zoom: 50%;" /><p>作者设计的框架TA - LSTM(<strong>T</strong>ype <strong>A</strong>ttention <strong>LSTM</strong>)是基于LSTM的, 作者先介绍了标准LSTM. 在$t$ 时刻, 输入的Token的Embedding为$\mathbf{x}_t$, 基于细胞状态$\mathbf{c}_t$ 的隐态输出$\mathbf{h}_t^c$ 计算流程为:</p><p>$$<br>\begin{aligned}<br>\left[\begin{array}{c}<br>\mathbf{i}_{t} \\<br>\mathbf{o}_{t} \\<br>\mathbf{f}_{t} \\<br>\widetilde{\mathbf{c}}_{t}<br>\end{array}\right] &amp;=\left[\begin{array}{c}<br>\sigma \\<br>\sigma \\<br>\sigma \\<br>\tanh<br>\end{array}\right]\left(\mathbf{W}\left[\mathbf{h}_{t-1} ; \mathbf{x}_{t}\right]+\mathbf{b}\right) \\<br>\mathbf{c}_{t} &amp;=\mathbf{i}_{t} \odot \widetilde{\mathbf{c}}_{t}+\left(\mathbf{1}-\mathbf{i}_{t}\right) \odot \mathbf{c}_{t-1} \\<br>\mathbf{h}_{t}^{c} &amp;=\mathbf{o}_{t} \odot \tanh \left(\mathbf{c}_{t}\right)<br>\end{aligned}<br>$$</p><p>其中$\mathbf{W}, \mathbf{b}$ 为可学习参数, $\sigma$ 为Sigmoid激活函数.</p><p>上述式子由上到下分别为: $t$ 时刻的输入门$\mathbf{i}_t$, 输出门$\mathbf{o}_t$, 遗忘门$\mathbf{f}_t$, 初始细胞状态$\tilde{\mathbf{c}}_t$, 细胞状态$\mathbf{c}_t$, 隐态输出$\mathbf{h}_t^c$(也作为上下文表示).</p><h3 id="Type-Attention-Mechanism"><a href="#Type-Attention-Mechanism" class="headerlink" title="Type - Attention Mechanism"></a>Type - Attention Mechanism</h3><blockquote><p>这部分是TA - LSTM额外添加的内容.</p></blockquote><p>在Type - Attention机制中, 对于类型$k$, 以及给定的$t$ 时刻输入$\mathbf{x}_t$ 和$t-1$ 时刻的隐态$\mathbf{h}_{t-1}$. 该类型相关的Key - Value对可以由下式计算得来:<br>$$<br>\left[\begin{array}{l}<br>\mathbf{k}_{k}^{(t)} \\<br>\mathbf{v}_{k}^{(t)}<br>\end{array}\right]=\left[\begin{array}{c}<br>\sigma \\<br>\sigma<br>\end{array}\right]\left(\mathbf{W}_{k}\left[\mathbf{h}_{t-1} ; \mathbf{x}_{t}\right]+\mathbf{b}_{k}\right)<br>$$</p><p>$k \in \left[1, \dots, m \right]$ 可以是实体类型或关系类型, 其中$\mathbf{W}_k, \mathbf{b}_k$ 为可学习参数, $\sigma$ 为Sigmoid函数.  其实跟$\mathbf{i}_t, \mathbf{o}_t, \mathbf{f}_t$ 得来的方式差不多.</p><p>这样就可以根据类型数量$m$, 得到$m$ 个类别特化的Key - Value对$\mathbf{K}^{(t)}=\left[\mathbf{k}_{1}^{(t)}, \ldots, \mathbf{k}_{m}^{(t)}\right]$ 和$\mathbf{V}^{(t)}=\left[\mathbf{v}_{1}^{(t)}, \ldots, \mathbf{v}_{m}^{(t)}\right]$.</p><p>接着把上下文表示$\mathbf{h}_{t}^c$ 视为Query, 把Attention机制应用到这上面来:<br>$$<br>\begin{aligned}<br>\mathbf{h}_{t}^{l} &amp;=\operatorname{attention}\left(\mathbf{h}_{t}^{c}, \mathbf{K}^{(t)}, \mathbf{V}^{(t)}\right)=\boldsymbol{\alpha}^{(t)} \mathbf{V}^{(t)} \\<br>\boldsymbol{\alpha}^{(t)} &amp;=\operatorname{softmax}\left(\frac{\mathbf{h}_{t}^{c} \mathbf{K}^{(t)}{ }^{\top}}{\sqrt{d_{e}}}\right)<br>\end{aligned}<br>$$</p><p>$\sqrt{d_e}$ 为Hidden state维度. </p><p>最后, TA - LSTM的$t$ 时刻上下文表示$\mathbf{h}_t^c$ 和类别表示$\mathbf{h}_t^l$ 相加得到$t$ 时刻的最终表示$\mathbf{h}_t$: </p><p>$$<br>\mathbf{h}_{t}=\mathbf{h}_{t}^{c}+\mathbf{h}_{t}^{l}<br>$$</p><h2 id="Synchronous-Dual-Network-with-Cross-Type-Attention"><a href="#Synchronous-Dual-Network-with-Cross-Type-Attention" class="headerlink" title="Synchronous Dual Network with Cross - Type Attention"></a>Synchronous Dual Network with Cross - Type Attention</h2><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/sdn2.jpg" style="zoom: 50%;" /><h3 id="Task-Definition"><a href="#Task-Definition" class="headerlink" title="Task Definition"></a>Task Definition</h3><p>在这里重新给出<strong>形式化</strong>的实体关系抽取任务定义.</p><p>对于给定的包含$n$ 个单词的句子$\mathbf{s}=[w_1, \dots, w_n]$, RTE的任务目标是抽取出句子$\mathbf{s}$ 中的关系三元组$\mathcal{T}=\left\{\left(\mathbf{e}_i, r, \mathbf{e}_j \right) \mid \mathbf{e}_i, \mathbf{e}_j \in \mathcal{E}, r \in \mathcal{R}\right\}$. $\mathbf{e}_i, \mathbf{e}_j, r$ 分别代表关系三元组的Subject, Object, Relation. </p><p>Subject, Object规定在实体集$\mathcal{E}=\left\{\mathbf{e}_i\right\}^P_{i=1}$中, 关系从预定义好的关系集$\mathcal{R}=\{\mathcal{R}_1, \dots, \mathcal{R}_m\}$中选出, $m$ 为有效关系类型数.</p><h3 id="Synchronous-Dual-Learning"><a href="#Synchronous-Dual-Learning" class="headerlink" title="Synchronous Dual Learning"></a>Synchronous Dual Learning</h3><p>接下来作者将通过Entity Type Learning和Relation Type Learning来捕获实体类型增强的表示$\mathbf{h}_t^e$, 关系类型增强的表示$\mathbf{h}_t^r$, 以增强模型对Type的感知力.</p><h4 id="Entity-Type-Learning"><a href="#Entity-Type-Learning" class="headerlink" title="Entity Type Learning"></a>Entity Type Learning</h4><p>NER作为序列标注问题, 实体类型作为标签, 例如PER, LOC, ORG等:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/sdn5.jpg" style="zoom: 50%;" /><p>若有$p$ 种实体标签, 每个实体类型都对应一个<strong>ETC</strong>(Entity Type Cell), 就有$p$ 个ETCs. </p><p>为了让模型学会实体类型预测的感知, 所以引入了<strong>Entity Type Learning</strong>作为<strong>辅助任务</strong>.</p><blockquote><p>下面的公式有些琐碎, 与原论文保持同步, 但其实<strong>并不复杂</strong>, 请耐心看完.</p></blockquote><p>从上一节可以仅用LSTM直接得到$t$ 时刻的上下文表示$\mathbf{h}_t^c$​, 用双向平均来, 即$\bar{\mathbf{h}}_{t}^{c}=\left[\left(\overrightarrow{\bar{\mathbf{h}}_{t}^{c}}+\overleftarrow{\bar{\mathbf{h}}_{t}^{c}}\right) / 2\right]$.</p><p>并由Type Attention机制得到一组与每种关系一一对应的Key - Value对 $\bar{\mathbf{K}}^{(t)}=\left[\bar{\mathbf{k}}_{1}^{(t)}, \ldots, \bar{\mathbf{k}}_{p}^{(t)}\right]$, $\bar{\mathbf{V}}^{(t)}=\left[\bar{\mathbf{v}}_{1}^{(t)}, \ldots, \bar{\mathbf{v}}_{p}^{(t)}\right]$, 其中的每个实体类型$p$ 对应的Key和Value也是由单向LSTM生成后, 将双向平均而来, 即 $\bar{\mathbf{k}}_{l}^{(t)}=\left[\left(\overrightarrow{\bar{\mathbf{k}}_{l}^{(t)}}+\overleftarrow{\bar{\mathbf{k}}_{l}^{(t)}}\right) / 2\right]$, $\bar{\mathbf{v}}_l^{(t)} = \left[\left(\overrightarrow{\bar{\mathbf{v}}_{l}^{(t)}}+\overleftarrow{\bar{\mathbf{v}}_{l}^{(t)}}\right) / 2\right],(l \in[1, \ldots, p])$.</p><p>$t$ 时刻实体类型相关的表示由两个方向拼接而成, $\mathbf{h}_{t}^{e}=\left[\overrightarrow{\mathbf{h}_{t}^{e}} \oplus \overleftarrow{\mathbf{h}_{t}^{e}}\right]$, 整个序列的实体类型表示记为$\mathbf{H}^{(e)}=\left[\mathbf{h}_{1}^{e}, \ldots, \mathbf{h}_{n}^{e}\right]$. </p><p>根据上述过程, <strong>每个时间步$t$ 都能得到不同的Type Specific Key - Value对</strong>.</p><p>然后把上下文表示$\bar{\mathbf{h}}_{t}^{c}$ 和不同实体类型的Key$\bar{\mathbf{k}}_{l}^{(c)}$做缩放点积, 得到当前时刻Token最相似的实体类型$T_l^e$:<br>$$<br>p\left(T_{l}^{e} \mid w_{t}\right)=\operatorname{softmax}\left(\frac{\overline{\mathbf{h}}_{t}^{c} \overline{\mathbf{k}}_{l}^{(t) \top}}{\sqrt{d_{e}}}\right)<br>$$</p><p>然后用极大似然优化:</p><p>$$<br>\mathcal{L}_{E T}=-\sum_{t=1}^{n} \log \left(p\left(T_{t}^{e} \mid w_{t}\right)\right)<br>$$</p><h4 id="Relation-Type-Learning"><a href="#Relation-Type-Learning" class="headerlink" title="Relation Type Learning"></a>Relation Type Learning</h4><p>同样的, 跟Entity Type Learning相类似, Relation Type Learning也是用来强化模型对类型感知的<strong>辅助任务</strong>.</p><p>因为存在<strong>相同实体对存在多种关系</strong>的情况(<strong>EPO</strong>问题), 所以用<strong>多标签</strong>来标注, 即用0, 1标签来表明实体之间的关系, 例如:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/sdn6.jpg" style="zoom: 50%;" /><p>在这里, 作者区分了Subject和Object的<strong>位置关系</strong>, 并将其纳入标签中. 设$M$ 为关系数量, 加上<strong>没有关系</strong>的情况, 共含有$2\times M + 1$ 种标签.</p><p>下内容与Entity Type Learning完全类似, 这里只是将实体类型转换为$q$ 种关系类型, 不再多加赘述.</p><p>上下文表示为$\hat{\mathbf{h}}_{t}^{c}=\left[\left(\overrightarrow{\hat{\mathbf{h}}_{t}^{c}}+\overleftarrow{\hat{\mathbf{h}}_{t}^{c}}\right) / 2\right]$, Key - Value对 $\hat{\mathbf{K}}^{(t)}=\left[\hat{\mathbf{k}}_{1}^{(t)}, \ldots, \hat{\mathbf{k}}_{q}^{(t)}\right]$,  $\hat{\mathbf{V}}^{(t)}=\left[\hat{\mathbf{v}}_{1}^{(t)}, \ldots, \hat{\mathbf{v}}_{q}^{(t)}\right]$, 由双向后求和平均得到, $\hat{\mathbf{k}}_{l}^{(t)}=\left[\left(\overrightarrow{\hat{\mathbf{k}}_{l}^{(t)}}+\overleftarrow{\hat{\mathbf{k}}_{l}^{(t)}}\right) / 2\right]$,$\left[\left(\overrightarrow{\hat{\mathbf{v}}_{l}^{(t)}}+\overleftarrow{\hat{\mathbf{v}}_{l}^{(t)}}\right) / 2\right],(l \in[1, \ldots, q])$</p><p>最后也是将双向拼接得到关系表示,  $\mathbf{h}_{t}^{r}=\left[\overrightarrow{\mathbf{h}_{t}^{r}} \oplus \overleftarrow{\mathbf{h}_{t}^{r}}\right]$, 整个句子的关系表示记为$\mathbf{H}^{(e)}=\left[\mathbf{h}_{1}^{r}, \ldots, \mathbf{h}_{n}^{r}\right]$.</p><p>因为是多标签问题, 所以用的是Sigmoid得到$w_t$ 的关系类型$T_l^r$:</p><p>$$<br>p\left(T_{l}^{r} \mid w_{t}\right)=\operatorname{sigmoid}\left(\frac{\hat{\mathbf{h}}_{t}^{c} \hat{\mathbf{k}}_{l}^{(t) \top}}{\sqrt{d_{e}}}\right)<br>$$</p><p>也是用极大似然优化:</p><p>$$<br>\mathcal{L}_{R T}=-\sum_{t=1} \sum_{r=1}^{2}<br>\left\{\log \left(p\left(T_{t}^{r} \mid w_{t}\right)\right)^{\mathbb{I}\left\{\hat{T}_{t}^{r}=1\right\}}+\log \left(1-p\left(T_{t}^{r} \mid w_{t}\right)\right)^{\mathbb{I}\left\{\hat{T}_{t}^{r}=0\right\}}\right\}<br>$$</p><h3 id="Cross-Type-Attention-Mechanism"><a href="#Cross-Type-Attention-Mechanism" class="headerlink" title="Cross - Type Attention Mechanism"></a>Cross - Type Attention Mechanism</h3><p>其实在Entity Type Learning和Relation Type Learning中的Entity Type和Relation Type使用是<strong>独立</strong>的, 所以接下来作者需要让它们彼此产生<strong>交互</strong>.</p><p>对于Entity Type Learning中得到的实体类型增强的表示$\mathbf{h}_t^e$, 以及关系类型相关的Key - Value对$\hat{\mathbf{K}}^{(t)}, \hat{\mathbf{V}}^{(t)}$, 关系 - 实体表示$\mathbf{c}_t^e$ 可以由前面讲过的Type - Attention机制得到. </p><p>与之相似的, 对于Relation Type Learning中得到的关系类型增强表示$\mathbf{h}_{t}^{r}$, 以及实体类型相关的Key - Value对$\bar{\mathbf{K}}^{(t)}, \bar{\mathbf{V}}^{(t)}$, 实体 - 关系表示$\mathbf{c}_t^r$ 也可以由Type - Attention得到. 即:<br>$$<br>\begin{aligned}<br>\mathbf{c}_{t}^{e} &amp;=\operatorname{attention}\left(\mathbf{h}_{t}^{e}, \hat{\mathbf{K}}^{(t)}, \hat{\mathbf{V}}^{(t)}\right) \\<br>\mathbf{c}_{t}^{r} &amp;=\operatorname{attention}\left(\mathbf{h}_{t}^{r}, \bar{\mathbf{K}}^{(t)}, \bar{\mathbf{V}}^{(t)}\right)<br>\end{aligned}<br>$$</p><blockquote><p>注: 该Cross Attention形式绝非首次出现, 在<strong>多模态模型</strong><a href="https://arxiv.org/abs/1908.02265" target="_blank" rel="noopener">ViLBERT</a> 中早就已经有把两种跨模态信息互相作为Query的方法. 只不过这里是将两种模态换为两种包含相关性的任务而已, 这二者十分相似.</p></blockquote><p>然后仿照TA - LSTM unit的最后, 把两种表示相加作为新的实体类型增强表示和新的关系类型增强表示:<br>$$<br>\begin{aligned}<br>\tilde{\mathbf{h}}_{t}^{e} &amp;= \mathbf{c}_t^e + \mathbf{h}_t^e \\<br>\tilde{\mathbf{h}}_{t}^{r} &amp;= \mathbf{c}_t^r + \mathbf{h}_t^r<br>\end{aligned}<br>$$<br>这也就是处理NER和RE前的<strong>最终表示形式</strong>了.</p><h3 id="Joint-Entity-and-Relation-Extraction"><a href="#Joint-Entity-and-Relation-Extraction" class="headerlink" title="Joint Entity and Relation Extraction"></a>Joint Entity and Relation Extraction</h3><p>下面的内容才是针对实体关系联合抽取的模型设计, 这部分设计的非常简单, 因为本文主要侧重点在于前面.</p><p>首先把Entity Type Learning中得到的实体表示$\tilde{\mathbf{h}}_{t}^{e}$ 和Relation Type Learning关系表示$\tilde{\mathbf{h}}_{t}^{r}$ 拼接起来, 得到一个联合表示:</p><p>$$<br>\tilde{\mathbf{h}}_{t}=\tilde{\mathbf{h}}_{t}^{e} \oplus \tilde{\mathbf{h}}_{t}^{r}<br>$$</p><h4 id="Named-Entity-Recognition"><a href="#Named-Entity-Recognition" class="headerlink" title="Named Entity Recognition"></a>Named Entity Recognition</h4><p>在NER任务中, 使用BIESO标签, 用Softmax和一层线性层搞定:<br>$$<br>y_{t}=\operatorname{softmax}\left(\mathbf{W}_{e} \tilde{\mathbf{h}}_{t}+\mathbf{b}_{e}\right)<br>$$</p><p>用极大似然优化即可:<br>$$<br>\mathcal{L}_{E}=-\sum_{t=1}^{n} \log \left(y_{t}\right)<br>$$</p><h4 id="Relation-Extraction"><a href="#Relation-Extraction" class="headerlink" title="Relation Extraction"></a>Relation Extraction</h4><p>作者Follow了前人的做法, 因为RE是一个与<strong>实体对相关</strong>的多标签任务, 所以这里做了实体对穷举, 判断Token $i$ 和Token $j$ 之间的关系$r^\prime$:</p><p>$$<br>\begin{gathered}<br>\mathbf{m}=\phi\left(\mathbf{W}_{m}\left(\tilde{\mathbf{h}}_{i} \oplus \tilde{\mathbf{h}}_{j}\right)+\mathbf{b}_{m}\right) \\<br>y_{i, j}^{r^{\prime}}=\operatorname{sigmoid}\left(\mathbf{W}_{r^{\prime}} \mathbf{m}+\mathbf{b}_{r^{\prime}}\right)<br>\end{gathered}<br>$$</p><p>其中$\mathbf{W}_{m} , \mathbf{b}_{m}, \mathbf{W}_{r^{\prime}} , \mathbf{b}_{r^{\prime}}$ 为可学习参数, $\phi$ 为ReLU.</p><p>然后用二分类交叉熵做Loss:</p><p>$$<br>\mathcal{L}_{R}=-\sum_{r^{\prime}=1}^{M} \sum_{i, j=1}^{n}\{\log \left(y_{i, j}^{r^{\prime}}\right)^{\mathbb{I}\left\{\hat{y}_{i, j}^{r^{\prime}}=1\right\}}<br>\left.+\log \left(1-y_{i, j}^{r^{\prime}}\right)^{\mathbb{I}\left\{\hat{y}_{i, j}^{\prime}=0\right\}}\right\}<br>$$</p><p>其中$\hat{y}_{i,j}^{r\prime}$ 为关系的Golden Label.</p><p>至此, SDN的模型结构已经完全确定:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/sdn4.jpg" style="zoom: 50%;" /><p>其实就是用TA - LSTM和ETC提供的信息完成辅助任务Entity Type Prediction, 再由这部分信息和Cross Attention组合, 经过简单的变换处理NER. 关系侧则完全同理, 不再叙述.</p><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>Training的Loss一共是五个, 最后加上一个L2正则化Loss:</p><p>$$<br>\mathcal{L}=\lambda^{t 1} \mathcal{L}_{E T}+\lambda^{t 2} \mathcal{L}_{R T}+\lambda^{e} \mathcal{L}_{E}+\lambda^{r} \mathcal{L}_{R}+\frac{\lambda}{2}|\Theta|^{2}<br>$$</p><p>$\lambda$ 为各个任务的权重系数, $\Theta$ 为模型参数. </p><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p>在推断时, 需要判断三元组是否正确. </p><p>对于NER抽取出的实体集$\mathcal{E}$ 中的实体, 有Subject $\mathbf{e}_{i}=\left[w_{\xi_{i}}, \ldots, w_{\zeta_{i}}\right]$, Object $e_{j}=\left[w_{\xi_{j}}, \ldots, w_{\zeta_{j}}\right]$, 关系$r$ 下的概率为$p_r$ 为:<br>$$<br>p_{r}=\frac{1}{\left|\mathbf{e}_{i}\right|} \frac{1}{\left|\mathbf{e}_{j}\right|} \sum_{f=\xi_{i}}^{\zeta_{i}} \sum_{s=\xi_{j}}^{\zeta_{j}} y_{f, s}^{r}<br>$$<br>$\left|\mathbf{e}_{i}\right|, \left|\mathbf{e}_{j}\right|$ 为$\mathbf{e}_i, \mathbf{e}_j$ 的长度, 仅当$p_r &gt; \theta$ 时三元组成立, $\theta$ 为阈值.</p><blockquote><p>这种计算方式确实比较特殊, 是<strong>将实体内所有Token对逐一求和平均</strong>来确定两实体之间是否存在指定关系.</p></blockquote><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>作者使用了RTE上最常用的两个Benchmark NYT和WebNLG, 统计信息如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/sdn7.jpg" style="zoom: 50%;" /><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><p>作者将SDN分别与多任务类, Tagging类, 生成类放在一起对比, 结果如下(应该指的是<strong>精确匹配结果</strong>):</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/sdn8.jpg" style="zoom: 50%;" /><p>SDN在F1上是最好的.</p><h3 id="Ablation-Experiments"><a href="#Ablation-Experiments" class="headerlink" title="Ablation Experiments"></a>Ablation Experiments</h3><p>文中消融实验如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/sdn9.jpg" style="zoom: 50%;" /><p>Entity Type Learning和Relation Type Learning对模型性能均有相当大提升, 而且是对NER和RE任务都有影响, 并且能观察到NER和RE任务之间的关联性很强.</p><h3 id="Analysis-of-Inference-Threshold"><a href="#Analysis-of-Inference-Threshold" class="headerlink" title="Analysis of Inference Threshold"></a>Analysis of Inference Threshold</h3><p>作者做了不同阈值$\theta$ 和模型结果之间变化图:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/sdn10.jpg" style="zoom: 50%;" /><p>作者将阈值对性能的影响归因与WebNLG和NYT之间实体长度不一.</p><h3 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h3><p>作者做了Case Study, 如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/sdn11.jpg" style="zoom: 50%;" /><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>作者从<strong>实体类型</strong>和<strong>关系类型</strong>的假设入手, 提出了一种基于TA - LSTM和Cross Type Attention的同步<strong>对偶</strong>网络, 通过强调对实体类型的感知, 关系类型的感知, 以及<strong>跨类型注意力</strong>解决了实体关系抽取问题.</p><p>但其实从文章中看出, 能把这个想法做Work是一件非常不容易的事情, 引入了额外的两个辅助任务, 除去正则外4个Loss属实难顶. 文章中的符号描述比较混乱, 尤其是两个辅助任务那部分, 但其实不复杂, </p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RTE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PFN: A Partition Filter Network for Joint Entity and Relation Extraction</title>
      <link href="/posts/27457.html"/>
      <url>/posts/27457.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>RNN: 详见<a href="https://adaning.github.io/posts/60202.html">循环神经网络小结</a>.</li></ul></blockquote><h1 id="A-Partition-Filter-Network-for-Joint-Entity-and-Relation-Extraction"><a href="#A-Partition-Filter-Network-for-Joint-Entity-and-Relation-Extraction" class="headerlink" title="A Partition Filter Network for Joint Entity and Relation Extraction"></a>A Partition Filter Network for Joint Entity and Relation Extraction</h1><p>本文是论文<a href="https://arxiv.org/abs/2108.12202" target="_blank" rel="noopener">A Partition Filter Network for Joint Entity and Relation Extraction</a>的阅读笔记和个人理解, 论文来自<strong>EMNLP 2021</strong>. 图片全部出自原论文和<a href="https://docs.google.com/presentation/d/1CiHWBdwoQexY0JgSP_JxC-QFciZBmTGo" target="_blank" rel="noopener">PPT</a>. 本文为RTE问题中, 探讨NER和RE任务间关系的系列三部曲中的第三篇.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>在先前的联合抽取任务中, 人们少有考虑NER和RE任务间的关系.要么是以<strong>Pipeline</strong>的形式做NER和RE, 导致<strong>任务间的特征交互不平衡</strong>, 要么是<strong>并行</strong>的对NER和RE任务的特征分别编码, 导致特征构建很大程度上是<strong>相互独立</strong>的. </p><p>因此, 作者希望提出一种两路交互模型, 从<strong>多任务学习</strong>视角充分挖掘NER和RE两任务之间的关系, 保证二者之间信息的<strong>均衡传递</strong>, 在此基础上完成联合抽取任务. </p><h2 id="PFN"><a href="#PFN" class="headerlink" title="PFN"></a>PFN</h2><blockquote><p>PFN由LSTM改进而来, 有LSTM基础再理解会好一些.</p></blockquote><h3 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h3><p>首先联合抽取中的两个子任务NER, RE做出定义.</p><p>对于给定的长度为$L$ 的输入序列$s=\set{w_1, \dots, w_L}$:</p><ul><li>NER的任务目标是在给定实体起始Token$w_i$ 和结束Token$w_j$ 的情况下, 抽取出标有类别的实体$e \in \mathcal{E}$ , $\left\langle w_{i}, e, w_{j}\right\rangle \in S$.</li><li>RE的任务目标是在给定Subject的起始Token $w_i$ 和Object的起始Token$w_j$ 的情况下, 抽取出二者存在的关系$r \in \mathcal{R}$, 即$\left\langle w_{i}, r, w_{j}\right\rangle \in T$, $T$ 为三元组.</li></ul><p>从形式上观察, NER和RE这两个任务都可以仅由实体的Span完成, 它们从形式上是<strong>保持一致</strong>的. 只不过NER要求的Span同属于同一个实体, 而RE的Span一个来自于Subject, 另一个来自于Object.</p><h3 id="Partition-Filter-Encoder"><a href="#Partition-Filter-Encoder" class="headerlink" title="Partition Filter Encoder"></a>Partition Filter Encoder</h3><p>PFN的Ecndoer就是被改进过的<strong>LSTM</strong>, 作者在LSTM基础上增加了<strong>Partition</strong>和<strong>Filter</strong>机制, 使得两任务之间的信息能够均衡合理的传递.</p><h4 id="Partition"><a href="#Partition" class="headerlink" title="Partition"></a>Partition</h4><p>NER和RE之间存在很强的关联, 但每个任务也同样应该有自己独立的特征, 这部分独立特征是不该被另一个任务所影响的. 划分(Partition)机制的目标就是要学习到仅与本任务相关联的特征位置.</p><blockquote><p>并不是所有特征全部共享就好, 而是<strong>只共享该共享的</strong>. 这看起来是一句废话, 但在多任务学习(Multi Task Learning, MTL) 中很常见, 也很重要. 若共享了不该共享的部分, 对每个任务的完成可能都有害.</p></blockquote><p>先从LSTM的细胞状态入手, 当前时刻$t$ 的初始细胞状态$\tilde{c_t}$ 由当前时刻输入$x_t$  和上个时刻的记忆$h_{t-1}$ 共同决定:</p><p>$$<br>\tilde{c}_{t}=\tanh \left(\operatorname{Linear}\left(\left[x_{t} ; h_{t-1}\right]\right)\right)<br>$$</p><p>这点与LSTM的输入门得到$\tilde{C}_t$ 的方式保持一致:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lstm%E8%BE%93%E5%85%A5%E9%97%A8.png" style="zoom:50%;" /><p>接着, 作者希望使用<strong>门控</strong>机制, 使得两个任务能够找到独属于自己的那部分特征划分. $\text{cummax}$ 可以完成划分的这个动作.<br>$$<br>\operatorname{cummax}(\cdot)=\operatorname{cumsum}(\operatorname{softmax}(\cdot))<br>$$<br>其中, $\text { cumsum }$ 为作者自定义的函数:<br>$$<br>\begin{gathered}<br>\text { cumsum }\left(x_{1}, x_{2}, \ldots, x_{n-1}, x_{n}\right)=\left(x_{1}, x_{1}+x_{2}, \ldots,\right. \\<br>\left.x_{1}+x_{2}+\cdots+x_{n-1}, x_{1}+x_{2}+\cdots+x_{n-1}+x_{n}\right)<br>\end{gathered}<br>$$</p><p>每个位置上的返回值为输入的对应位置元素及之前元素的和, 亦或许可以写做:<br>$$<br>\begin{gathered}<br>\text { cumsum }\left(x_{1}, x_{2}, \ldots, x_{n-1}, x_{n}\right)=\left(\sum^1_{n=1}x_i, \sum^2_{n=1}x_i,\sum^{n-1}_{n=1}x_i,\cdots, \sum^n_{n=1}x_i\right)<br>\end{gathered}<br>$$<br>$\text{cummax}$ 的输出值可以近似成一个二进制向量$(0, \cdots, 0, 1, \cdots, 1)$.  举个栗子:<br>$$<br>\begin{aligned}<br>x &amp;= (0.1, 0.1, 0.6, 0.1, 0.1) \\<br>\text{cumsum}(x) &amp;= (0.1, 0.2, 0.8, 0.9, 1.0) \\<br>&amp;\approx (0, 0, 1, 1, 1)<br>\end{aligned}<br>$$<br>NER和RE这两个任务都各有一个对输入划分的门控剪刀, 把输入分为任务<strong>相关</strong>, 任务<strong>不相关</strong>两块. 这两把剪刀分别记为$\tilde{e}$ 和$\tilde{r}$:<br>$$<br>\begin{aligned}<br>&amp;\tilde{e}=\operatorname{cummax}\left(\operatorname{Linear}\left(\left[x_{t} ; h_{t-1}\right]\right)\right) \\<br>&amp;\tilde{r}=1-\operatorname{cummax}\left(\operatorname{Linear}\left(\left[x_{t} ; h_{t-1}\right]\right)\right)<br>\end{aligned}<br>$$</p><p>该设计使得NER和RE任务都有独属于自己的部分.</p><blockquote><p>注意, 这两个$\text{Linear}$ 的参数是不一样的. 对于同一组输入, $\tilde{e}$ 和$\tilde{r}$ 可能得到一块交叉的区域, 这也就是<strong>共享区域</strong>, 该区域是由两个门控共同决定的.</p></blockquote><p>两刀切三份, 接下来就把两把剪刀切出来的结果表示出来.</p><p>这里给出的是上个时刻$t-1$ 的划分$\rho_{c_{t-1}}$, 因为在LSTM中, 后面要借助$t-1$ 时刻表达当前时刻$t$ 对应的划分:</p><p>$$<br>\begin{aligned}<br>\rho_{s, c_{t-1}} &amp;=\tilde{e}_{c_{t-1}} \circ \tilde{r}_{c_{t-1}} \\<br>\rho_{e, c_{t-1}} &amp;=\tilde{e}_{c_{t-1}}-\rho_{s, c_{t-1}} \\<br>\rho_{r, c_{t-1}} &amp;=\tilde{r}_{c_{t-1}}-\rho_{s, c_{t-1}}<br>\end{aligned}<br>$$<br>$\tilde{e}, \tilde{r}$ 的结果做逐元素点乘, 就能获取共享部分, <strong>细胞状态</strong>被切成了三份: <strong>NER区</strong>, <strong>RE区</strong>, <strong>Share区</strong>:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/pfn1.jpg" style="zoom: 50%;" /><p>对于三个式子, PPT中给出了一个例子. 若$\tilde{e}=(0, 1, 1), \tilde{r} =(1, 1, 0)$, 则有:<br>$$<br>\begin{aligned}<br>\rho_{s, c_{t-1}} &amp;= (0,1,1) \circ (1,1,0)=(0,1,0)\\<br>\rho_{e, c_{t-1}} &amp;= (0,1,1)- (0,1,0)=(0,0,1)\\<br>\rho_{r, c_{t-1}} &amp;= (1,1,0)- (0,1,0)=(1,0,0)<br>\end{aligned}<br>$$<br>对于NER, RE, Share区, 每个区域划分到的细胞状态$\rho$ 都由上个时刻的对应区域的细胞状态$c_{t-1}$ 和当前时刻对应区域的初始细胞状态$\tilde{c_t}$ 共同决定:<br>$$<br>\begin{aligned}<br>\rho_{e} &amp;=\rho_{e, c_{t-1}} \circ c_{t-1}+\rho_{e, \tilde{c}_{t}} \circ \tilde{c}_{t} \\<br>\rho_{r} &amp;=\rho_{r, c_{t-1}} \circ c_{t-1}+\rho_{r, \tilde{c}_{t}} \circ \tilde{c}_{t} \\<br>\rho_{s} &amp;=\rho_{s, c_{t-1}} \circ c_{t-1}+\rho_{s, \tilde{c}_{t}} \circ \tilde{c}_{t}<br>\end{aligned}<br>$$</p><p>$\rho$ 并不是PFN Encoder最终的细胞状态, 上述过程仅是与LSTM中的细胞状态更新相似:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lstm%E7%BB%86%E8%83%9E%E7%8A%B6%E6%80%81.png" style="zoom: 50%;" /><blockquote><p>作者认为, 把细胞状态拆为三个区域, 再将它们合起来, 和不划分时不是等价的, 会损失一部分信息, 这个机制与LSTM的遗忘门类似:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lstm%E9%81%97%E5%BF%98%E9%97%A8.png" style="zoom:50%;" /><p>这种解释还是非常巧妙的, 不需要在模型中显式设计出遗忘门.</p></blockquote><h4 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h4><p>在上个阶段Partition对输入区域完成了划分, 在本阶段将对每个任务所需特征做<strong>表征重组</strong>.</p><p>首先把上阶段得到的三部分<strong>组装</strong>在一起, 对应区域得到新的记忆$\mu$.</p><p>NER可以使用独有的细胞状态$\rho_e$ 和共享细胞状态$\rho_s$, RE可以使用独有细胞状态$\rho_r$ 和共享细胞状态$\rho_s$, 对任务无关的信息直接被滤去:<br>$$<br>\begin{aligned}<br>\mu_{e}&amp;=\rho_{e}+\rho_{s} \\\<br>\mu_{r}&amp;=\rho_{r}+\rho_{s}\\<br>\mu_{s}&amp;=\rho_{s}<br>\end{aligned}<br>$$</p><p>然后给记忆$\mu$ 激活一下子:<br>$$<br>\begin{aligned}<br>&amp;h_{e}=\tanh \left(\mu_{e}\right) \\<br>&amp;h_{r}=\tanh \left(\mu_{r}\right) \\<br>&amp;h_{s}=\tanh \left(\mu_{s}\right)<br>\end{aligned}<br>$$</p><p>$h$ 将作为特定任务的表示, 在解码过程中使用.</p><p>最后就是PFN Encoder的更新细胞状态$c_t$, 并由$c_t$ 得到当前时刻隐态$h_t$, 完成输出过程:<br>$$<br>\begin{aligned}<br>c_{t} &amp;=\operatorname{Linear}\left(\left[\mu_{e, t} ; \mu_{r, t} ; \mu_{s, t}\right]\right) \\<br>h_{t} &amp;=\tanh \left(c_{t}\right)<br>\end{aligned}<br>$$</p><p>这与LSTM的输出门相似, 但又不太一样:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lstm%E8%BE%93%E5%87%BA%E9%97%A8.png" style="zoom: 50%;" /><p>至此, PFN Encoder信息流可以由下图概括:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/pfn3.jpg" style="zoom: 50%;" /><p>只是看输入和输出, 相较于LSTM仍然没有变化, 信息流为:</p><ol><li>上时刻$t-1$ 被划分好的三份的细胞状态(NER区, RE区, Share区)记为$\rho_{c_{t-1}}$, 与当前时刻划分好的三份初始细胞状态$\rho_{\tilde{c}_t}$, 生成$t$ 时刻划分好的新细胞状态$\rho$.</li><li>新细胞状态进一步过滤得到任务特化的细胞状态$\mu$.</li><li>激活后得到任务所需的表征$h_e, h_r, h_s$.</li><li>用类似输出门的方式得到当前时刻$t$ 的最终细胞状态$c_t$, 和输出隐态$h_t$.</li></ol><h3 id="Global-Representation"><a href="#Global-Representation" class="headerlink" title="Global Representation"></a>Global Representation</h3><p>因为作者使用的是单向编码器, 只有前向(Forward)而没有反向(Backward), 所以这里作者用提取的全局特征$h_{g}$ 来代替反向编码所获取的信息:</p><p>$$<br>\begin{aligned}<br>h_{g_{e}, t}=\tanh \left(\operatorname{Linear}\left[h_{e, t} ; h_{s, t}\right]\right) \\<br>h_{g_{r}, t}=\tanh \left(\operatorname{Linear}\left[h_{r, t} ; h_{s, t}\right]\right) \\<br>h_{g_{e}}=\operatorname{maxpool}\left(h_{g_{e}, 1}, \ldots, h_{g_{e}, L}\right) \\<br>h_{g_{r}}=\operatorname{maxpool}\left(h_{g_{r}, 1}, \ldots, h_{g_{r}, L}\right)<br>\end{aligned}<br>$$</p><p>最后用了一个最大池化, 只保留整个句子中最重要的信息, 作为全局表示.</p><h3 id="Task-Units"><a href="#Task-Units" class="headerlink" title="Task Units"></a>Task Units</h3><p>Task Unit也就是Decoder, 采用朴实无华的<strong>Table Filling</strong> Decoding策略. 因为前面观察过NER和RE任务的形式一致, 均可视为<strong>二分类</strong>问题, 仅依赖于两两Token就能完成实体类别和关系类别的判定, 所以两个任务的解码单元也可以设计得一样.</p><h4 id="NER-Unit"><a href="#NER-Unit" class="headerlink" title="NER Unit"></a>NER Unit</h4><p>每个实体类型$k$ 都有一张表, 在$k$ 表中第$i$ 行第$j$ 列被填上的值为, 类型为$k$ 的实体起始Token为$w_i$, 结束Token为$w_j$ 的概率$e_{ij}^k$, 由$h_i^e, h_j^e$, 以及全局表示$h_{g_e}$ 拼接后变换得到:<br>$$<br>\begin{aligned}<br>h_{i j}^{e}&amp;=\operatorname{ELU}\left(\operatorname{Linear}\left(\left[h_{i}^{e} ; h_{j}^{e} ; h_{g_{e}}\right]\right)\right)\\<br>e_{i j}^{k} &amp;=p\left(e=\left\langle w_{i}, k, w_{j}\right\rangle \mid e \in S\right) \\<br>&amp;=\sigma\left(\operatorname{Linear}\left(h_{i j}^{e}\right)\right), \forall k \in \mathcal{E}<br>\end{aligned}<br>$$</p><h4 id="RE-Unit"><a href="#RE-Unit" class="headerlink" title="RE Unit"></a>RE Unit</h4><p>RE单元可以由NER单元如法炮制. 每个关系类型$l$ 都有一张表, 在$l$ 表中第$i$ 行第$j$ 列被填上的值为, Token为$w_i$ 的Subject和起始Token为$w_j$ 间存在关系$l$ 的概率$r_{ij}^l$ , 同样由两Token间表示$h_i^r, h_j^r$, 以及全局表示$h_{g_r}$ 拼接, 后经过激活, 变换得到:<br>$$<br>\begin{aligned}<br>h_{i j}^{r} &amp;=\operatorname{ELU}\left(\operatorname{Linear}\left(\left[h_{i}^{r} ; h_{j}^{r} ; h_{g_{r}}\right]\right)\right) \\<br>r_{i j}^{l} &amp;=p\left(r=\left\langle w_{i}, l, w_{j}\right\rangle \mid r \in T\right) \\<br>&amp;=\sigma\left(\operatorname{Linear}\left(h_{i j}^{r}\right)\right), \forall l \in \mathcal{R}<br>\end{aligned}<br>$$</p><h3 id="Training-and-Inference"><a href="#Training-and-Inference" class="headerlink" title="Training and Inference"></a>Training and Inference</h3><p>因为作者假定填表任务为二分类问题, 所以使用二分类交叉熵(BCE Loss)优化:</p><p>$$<br>\begin{aligned}<br>L_{n e r} &amp;=\sum_{\hat{e}_{i j}^{k} \in S} \operatorname{BCELoss}\left(e_{i j}^{k}, \hat{e}_{i j}^{k}\right) \\<br>L_{r e} &amp;=\sum_{\hat{r}_{i j}^{l} \in T} \mathrm{BCELoss}\left(r_{i j}^{l}, \hat{r}_{i j}^{l}\right)<br>\end{aligned}<br>$$</p><p>$\hat{e}_{i j}^{k}, \hat{r}_{i j}^{l}$ 为真实标签. 在推理时, 各实体, 关系存在的概率仅当均大于阈值$\lambda$ 时, 三元组$(s_{i, j}^k, l, o_{m, n}^{k\prime})$ 成立:<br>$$<br>e_{i j}^{k} \geq \lambda_{e} ; e_{m n}^{k^{\prime}} \geq \lambda_{e} ; r_{i m}^{l} \geq \lambda_{r}<br>$$</p><p>文中取阈值均为0.5.</p><p>PFN整体的模型框架图如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/pfn2.jpg" style="zoom: 50%;" /><p>先由Partition Filter划分过滤得到任务特化的特征, 再提取整句全局特征, 最后结合二者通过两个简单的Task Unit二分类填表, 完成联合抽取.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><p>在联合抽取常用的两个数据集NYT和WebNLG上, 效果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/pfn4.jpg" style="zoom: 50%;" /><p>性能超过了之前最强的TPLinker.</p><p>在ADE, ACE05, ACE04, SciERC上结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/pfn5.jpg" style="zoom: 50%;" /><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/pfn6.jpg" style="zoom: 50%;" /><p>PFN的性能也与强调任务间交互的<a href="https://adaning.github.io/posts/37252.html">Table - Sequence</a>和Pipeline设计模型<a href="https://adaning.github.io/posts/22256.html">PURE</a>(也是本系列前两篇文章介绍的模型)各有优劣, 整体上来说PFN要好. PURE在ACE05的NER上性能仍为最强.</p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>在SciERC上消融实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/pfn7.jpg" style="zoom: 50%;" /><h4 id="Number-of-Encoder-Layers"><a href="#Number-of-Encoder-Layers" class="headerlink" title="Number of Encoder Layers"></a>Number of Encoder Layers</h4><p>堆叠PFN层数不能带来性能提升. 个人认为多层PFN会令信息流混乱, 干扰了任务之间的平衡.</p><h4 id="Bidirection-Vs-Unidirection"><a href="#Bidirection-Vs-Unidirection" class="headerlink" title="Bidirection Vs Unidirection"></a>Bidirection Vs Unidirection</h4><p>无论是单向和还是双向, 使用全局信息都有一点点提升, 但是全局信息对单向增益更大.</p><h4 id="Encoding-Scheme"><a href="#Encoding-Scheme" class="headerlink" title="Encoding Scheme"></a>Encoding Scheme</h4><p>作者把PFN Encoder替换成两个LSTM, 观察不同编码方式的效果. Joint代表本文的做法.</p><p>在后两种实验中均把PFN Encoder替换成LSTM. Sequential代表Pipeline式的做法, 即单路交互, 先生成实体特征再生成关系特征. Parallel代表两个LSTM都分别编码, 不做交互.</p><p>从结果上来看, 平衡交互 &gt; 单路交互 &gt; 不做交互. 似乎平衡交互更有利于召回.</p><blockquote><p>其实有点不公平, 比较单独比较编码策略的时候还是都用LSTM比较好, 可以补一个只用LSTM的Joint结果.</p></blockquote><h4 id="Partition-Granularity"><a href="#Partition-Granularity" class="headerlink" title="Partition Granularity"></a>Partition Granularity</h4><p>作者探究了粗细粒度门控对性能的影响. 例如表示是300维的, 如果把300维劈成10份, 每份都共用30维相同的实体门和关系门, 称为Coarse. 如果300维不劈开, 用300维的实体门和关系门处理, 就称为Fine - grained. </p><p>结果表明细粒度更强, 其实也很好解释. 细粒度能更好的区分任务所需要的特征区间.</p><h4 id="Decoding-Strategy"><a href="#Decoding-Strategy" class="headerlink" title="Decoding Strategy"></a>Decoding Strategy</h4><p>作者把Pipeline的解码方式称为Selective Decoding, 因为关系模型只对Entity Golden Label解码, 建立在有效实体对之上的. 作者认为, 更好的解码策略是让关系模型把有效和无效实体对都考虑进去, 即Universal Decoding.</p><p>Selective是将NER Unit预测得出的有效实体对送给RE Unit解码的结果.</p><p>接下来的观点就很有意思了, 作者认为通用解码类似于<strong>对比学习</strong>, 因为其中包含了无效实体对作为<strong>负例</strong>, 所以通用解码效果更好.</p><h3 id="Effects-of-Relation-Signal-on-Entity-Recognition"><a href="#Effects-of-Relation-Signal-on-Entity-Recognition" class="headerlink" title="Effects of Relation Signal on Entity Recognition"></a>Effects of Relation Signal on Entity Recognition</h3><p>在先前的研究中, NER的实体信息对RE有帮助是大家公认的, 但RE是否对NER有益仍然有争议.</p><h4 id="Analysis-on-Entity-Prediction-of-Different-Types"><a href="#Analysis-on-Entity-Prediction-of-Different-Types" class="headerlink" title="Analysis on Entity Prediction of Different Types"></a>Analysis on Entity Prediction of Different Types</h4><p>因为在主实验结果中, PFN的表现没有很抢眼, 作者认为, 可能是ACE05中包含很多<strong>不属于任何三元组的实体</strong>. </p><p>因此作者将NER任务的结果分为<strong>三元组内</strong>的实体预测, <strong>三元组外</strong>的实体预测, 观察它们的差距:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/pfn8.jpg" style="zoom: 50%;" /><p>结果发现, 在三元组内的实体预测和在三元组外的实体预测性能有巨大差距, 在不借助关系信息时, 精度下降的很厉害. 与ACE05上NER SOTA Pipeline模型PURE对比, PFN的NER性能似乎与三元组外的实体百分比负相关, 即<strong>对于联合模型来说</strong>, <strong>三元组外的实体越多</strong>, <strong>NER性能越差</strong>. 因为在三元组内的实体和三元组外的实体预测推理时本质上是不同的, <strong>在三元组内部的实体预测可以借助关系信息</strong>, <strong>而三元组外实体预测却不能</strong>.</p><h4 id="Robustness-Test-on-Named-Entity-Recognition"><a href="#Robustness-Test-on-Named-Entity-Recognition" class="headerlink" title="Robustness Test on Named Entity Recognition"></a>Robustness Test on Named Entity Recognition</h4><p>在ACE05上的鲁棒性测试结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/pfn9.jpg" style="zoom: 50%;" /><p>相较于其他模型, PFN的鲁棒性非常好.</p><h4 id="Does-Relation-Signal-Helps-in-Predicting-Entities"><a href="#Does-Relation-Signal-Helps-in-Predicting-Entities" class="headerlink" title="Does Relation Signal Helps in Predicting Entities"></a>Does Relation Signal Helps in Predicting Entities</h4><p>与PURE中得出的结论相反, 作者认为关系信息对实体预测影响很大. </p><p>PURE中的实验是在ACE05上做的, 而忽略了ACE05上的三元组外实体的巨大影响.</p><blockquote><p>附录中还有CasRel, TPLinker, PFN三者在NYT和WebNLG上应对重叠三元组的详细表现, 结果表明PFN在处理重叠三元组的实力也很出色.</p></blockquote><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>本文从<strong>多任务学习</strong>视角, 以任务间<strong>不平衡的信息交互</strong>为出发点, 通过<strong>划分</strong>, <strong>过滤</strong>在最基础的<strong>特征区域</strong>上控制任务间的信息流通, 再结合整句的<strong>全局特征</strong>, 用两个任务特化的Task Unit用<strong>填表</strong>的方法解决联合抽取问题.</p><p>我个人认为, 这是一篇挺有想法的文章, 顺着论文的引文, 发现其灵感似乎也来自于<a href="https://openreview.net/forum?id=B1l6qiR5F7" target="_blank" rel="noopener">ON - LSTM</a>, 感兴趣的可以直接看<a href="https://spaces.ac.cn/archives/6621" target="_blank" rel="noopener">苏神博客</a>.</p><p>PFN进一步的说明了联合抽取中NER和RE任务之间是密不可分的, 并有效驳斥了PURE中的论点.</p><p>从论文本身来说, 是很优秀的一篇. 实验完备, 论据充分, 图也很好看.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RTE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PURE: A Frustratingly Easy Approach for Entity and Relation Extraction</title>
      <link href="/posts/22256.html"/>
      <url>/posts/22256.html</url>
      
        <content type="html"><![CDATA[<h1 id="A-Frustratingly-Easy-Approach-for-Entity-and-Relation-Extraction"><a href="#A-Frustratingly-Easy-Approach-for-Entity-and-Relation-Extraction" class="headerlink" title="A Frustratingly Easy Approach for Entity and Relation Extraction"></a>A Frustratingly Easy Approach for Entity and Relation Extraction</h1><p>本文是论文<a href="https://arxiv.org/abs/2010.12812" target="_blank" rel="noopener">A Frustratingly Easy Approach for Entity and Relation Extraction</a> 的阅读笔记和个人理解. 论文来自<strong>NAACL 2021</strong>. 本文为RTE问题中, 探讨NER和RE任务间关系的系列三部曲中的第二篇.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>最近的工作对NER和RE引入了联合训练, 即将NER和RE在多任务学习下一起解决. 并且联合模型经常使用同一个<strong>共享</strong>的Encoder解码, 解决问题的效果也都还不错, 这使得人们认为共享Encoder能够更好的解决这两个任务, 且认为联合模型能<strong>缓解错误误差传递</strong>的问题.</p><p>作者认为, <strong>Pipeline模型不一定真弱于联合学习模型</strong>. 因此希望构建一个简单的Pipeline模型来击败联合学习模型的SOTA, 来打破人们的固有观念.</p><h2 id="PURE"><a href="#PURE" class="headerlink" title="PURE"></a>PURE</h2><p><strong>PURE</strong>(the <strong>P</strong>rinceton <strong>U</strong>niversity <strong>R</strong>elation <strong>E</strong>xtraction system)简单的分为<strong>实体模型</strong>和<strong>关系模型</strong>两部分, 训练时完全分开训练. 因为是<strong>Pipeline</strong>, 所以关系模型使用实体的<strong>Golden Label</strong>训练.</p><p>用如下一张图可以简单概括:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/pure1.jpg" style="zoom: 50%;" /><ul><li>(a) <strong>实体模型</strong>: 根据Span的表示, 判断给定的Span的实体类型.</li><li>(b) <strong>关系模型</strong>: 根据Span对的表示, 判断给定的Span对的关系类型.</li><li>(c) <strong>批计算的关系模型</strong>: 关系模型的一种加速实现.</li></ul><h3 id="Problem-Definition"><a href="#Problem-Definition" class="headerlink" title="Problem Definition"></a>Problem Definition</h3><p>本文所采用的方法是基于Span的方法, 因此给出基于Span的视角下问题的定义和NER, RE的定义.</p><p>对于给输入句子$X$ 含有$n$ 个Token$x_1, x_2, \dots, x_n$. 令$S$ 为$X$ 中的长度为$L$ 的Span集合, 即$S=\{s_1, s_2, \dots, s_m\}$, $\mathrm{START}(i), \mathrm{END}(i)$ 分别代表$s_i$ 的起始和结束切片Token.</p><ul><li>NER: 对于预定义的实体类型集合$\mathcal{E}$, 基于Span的NER任务为判断每个Span $s_i \in S$ 的实体类型$y_e(s_i) \in \mathcal{E}$, 若Span不是实体, 则记为$y_e(s_i) = \epsilon$. 任务的最终输出为$Y_e=\set{(s_i, e): s_i \in S, e \in \mathcal{E}}$.</li><li>RE: 对于预定义的关系类型集合$\mathcal{R}$, RE任务为判断每个Span对$s_i\in S, s_j \in S$ 之间的关系类型$y_r(s_i, s_j) \in \mathcal{R}$, 若Span对之间不存在关系, 则记为$y_r(s_i, s_j) = \epsilon$. 任务最终输出为$Y_r = \set{(s_i, s_j, r): s_i, s_j \in S, r \in \mathcal{R}}$.</li></ul><h3 id="Entity-Model"><a href="#Entity-Model" class="headerlink" title="Entity Model"></a>Entity Model</h3><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/pure9.png" style="zoom: 60%;" /><p>实体模型简单的建模为基于<strong>Span</strong>的模型, 先用BERT获得输入Token $x_t$ 的上下文表示$\mathbf{x}_t$, 对于某个Span $s_i \in S$, 它的表示如下:</p><p>$$<br>\mathbf{h}_{e}\left(s_{i}\right)=\left[\mathbf{x}_{\mathrm{START}(i)} ; \mathbf{x}_{\mathrm{END}(i)} ; \phi\left(s_{i}\right)\right]<br>$$</p><p>其中, $\phi{(s_i)}$ 为Span长度的Embedding.</p><blockquote><p>Span的表示仅简单的使用了Span<strong>边界</strong>以及Span<strong>长度</strong>信息, 而非把Span内部所有信息全部囊括. 基于Span的模型经常只使用边界上的信息, 可能是假设边界蕴含的语义更强.</p></blockquote><p>在得到Span表示$\mathbf{h}_{e}\left(s_{i}\right)$ 后, 直接用一个FeedForward Network就能预测得到实体类型的概率分布:<br>$$<br>P_{e}\left(e \mid s_{i}\right)=\operatorname{softmax}\left(\mathbf{W}_{e} \mathrm{FFNN}\left(\mathbf{h}_{e}\left(s_{i}\right)\right)\right.<br>$$</p><h3 id="Relation-Model"><a href="#Relation-Model" class="headerlink" title="Relation Model"></a>Relation Model</h3><p>在实体模型得到了实体的Span $s_i, s_j$ 的表示$\mathbf{h}_e(s_i), \mathbf{h}_e(s_j)$ 后, 也直接接一个FeedForward Network就能预测Span间关系类型. </p><p>但作者认为, 实体模型得到的边界表示只能捕获每个<strong>单独实体</strong>周围的上下文信息, 没法感知到<strong>Span之间</strong>的依存信息, 而这种依存信息是对RE非常重要的.</p><p>所以, 作者在Span周围插入了独立的Text Marker指导模型获得Span间的依存关系:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/pure10.png" style="zoom: 60%;" /><p>具体来说, 对于给定的输入句子$X$ 和<strong>每个</strong>存在$e_i, e_j \in \mathcal{E} \cup \set{\epsilon}$ 的Subject - Object Span对$s_i, s_j$, 定义了四种类型的Text Marker, $\left\langle\mathrm{S}: e_{i}\right\rangle,\left\langle/ \mathrm{S}: e_{i}\right\rangle, \left\langle\mathrm{O}: e_{j}\right\rangle,\left\langle/ \mathrm{O}: e_{j}\right\rangle$, 分别插入到Subject的前后和Object的前后.</p><blockquote><p>这种Text Marker除了提供了Subject和Object的角色信息, 实体边界信息, 还提供了实体类型信息.</p></blockquote><p>即插入Text Marker后的输入序列$\widehat{X}$ 为:<br>$$<br>\begin{aligned}<br>&amp;\widehat{X}=\ldots\left\langle\operatorname{S}: e_{i}\right\rangle, x_{\operatorname{START}(i)}, \ldots, x_{\operatorname{END}(i)},\left\langle/ \mathrm{S}: e_{i}\right\rangle \\<br>&amp;\ldots\left\langle\mathrm{O}: e_{j}\right\rangle, x_{\mathrm{START}(j)}, \ldots, x_{\mathrm{END}(j)},\left\langle/ \mathrm{O}: e_{j}\right\rangle, \ldots<br>\end{aligned}<br>$$</p><blockquote><p>注意, 这种Type Marker对于每个Subject - Object Span对都是在最初输入$X$ 上进行的, <strong>每个不同的Span对会获得不同的输入</strong>$\widehat{X}$. 这引入了巨大的计算量.</p></blockquote><p>接着使用不同于实体模型的第二个Encoder捕获关系模型下Subject和Object的表示:<br>$$<br>\mathbf{h}_{r}\left(s_{i},s_{j}\right)=\left[\widehat{\mathbf{x}}_{\widehat{\operatorname{START}}(i)} ; \widehat{\mathbf{x}}_{\widehat{\operatorname{START}}(j)}\right]<br>$$</p><p>$\widehat{\operatorname{START}}(i), \widehat{\operatorname{START}}(j)$ 为$\left\langle\mathrm{S}: e_{i}\right\rangle, \left\langle\mathrm{O}: e_{j}\right\rangle$ 的在$\widehat{X}$ 的下标. </p><blockquote><p>仅使用Text Marker的<strong>起始</strong>作为Subject和Object的表示, 在这里结束位置信息仅作为区分实体是否结束的标志.</p></blockquote><p>同样的, 在表示后面用一个FeedForward Network来预测关系类型:<br>$$<br>P_{r}\left(r \mid s_{i}, s_{j}\right)=\operatorname{softmax}\left(\mathbf{W}_{r} \mathbf{h}_{r}\left(s_{i}, s_{j}\right)\right)<br>$$</p><h3 id="Cross-sentence-context"><a href="#Cross-sentence-context" class="headerlink" title="Cross - sentence context"></a>Cross - sentence context</h3><p>跨句信息对预测实体类型和关系是有帮助的. </p><p>作者简单的使用固定大小为$W$ 的<strong>滑动窗口</strong>, 来获取更长的上下文. 假设输入句子有$n$ 个单词, 则滑动窗口可以分别获得左侧和右侧的$(W - n) / 2$ 个单词作为额外上下文.</p><h3 id="Training-amp-inference"><a href="#Training-amp-inference" class="headerlink" title="Training &amp; inference"></a>Training &amp; inference</h3><p>两个模型分别用交叉熵优化即可:</p><p>$$<br>\begin{aligned}<br>&amp;\mathcal{L}_{e}=-\sum_{s_{i} \in S} \log P_{e}\left(e_{i}^{\ast} \mid s_{i}\right) \\<br>&amp;\mathcal{L}_{r}=-\sum_{s_{i}, s_{j} \in S_{G}, s_{i} \neq s_{j}} \log P_{r}\left(r_{i, j}^{\ast} \mid s_{i}, s_{j}\right)<br>\end{aligned}<br>$$</p><p>$e_i^\ast$ 为$s_i$ 的真实实体类型, $r_{i, j}^\ast$ 为$s_i, s_j$ 间的真实关系类型.</p><h3 id="Differences-from-DYGIE"><a href="#Differences-from-DYGIE" class="headerlink" title="Differences from DYGIE++"></a>Differences from DYGIE++</h3><p>作者的方法与DYGIE++非常相似, 因此在这里强调了与它的不同:</p><ol><li>作者用两个独立的Encoder分别做NER和RE任务, 而非用MTL(Multi - task Learning)的角度去看待.并且关系模型所需的实体类型信息能完全产生于实体模型.</li><li>引入了Text Marker, 在关系模型中Span对的表示不同.</li><li>使用跨句信息, 而不是使用Beam Search和图网络.</li></ol><h3 id="Efficient-Batch-Computations"><a href="#Efficient-Batch-Computations" class="headerlink" title="Efficient Batch Computations"></a>Efficient Batch Computations</h3><p>在加入Type Marker时, 每个Span对都有不同的$\widehat{X}$, 即使输入的原句子$X$ 相同, 也会因为Type Marker的位置, 类型不同而必须重新计算<strong>整句表示</strong>, 这种计算开销实在是太大了, 所以作者提出了一种加速的<strong>近似模型</strong>.</p><p>首先, 把所有Text Marker全部添加到句子的尾部, 并令Text Marker与实体的起始位置, 结束位置的Position Embedding共享:<br>$$<br>\begin{aligned}<br>&amp;\mathrm{P}\left(\left\langle\mathrm{S}: e_{i}\right\rangle\right), \mathrm{P}\left(\left\langle/ \mathrm{S}: e_{i}\right\rangle\right):=\mathrm{P}\left(x_{\mathrm{START}(i)}\right), \mathrm{P}\left(x_{\mathrm{END}(i)}\right) \\<br>&amp;\mathrm{P}\left(\left\langle\mathrm{O}: e_{j}\right\rangle\right), \mathrm{P}\left(\left\langle/ \mathrm{O}: e_{j}\right\rangle\right):=\mathrm{P}\left(x_{\mathrm{START}(j)}\right), \mathrm{P}\left(x_{\mathrm{END}(j)}\right)<br>\end{aligned}<br>$$</p><p>$P(\cdot)$ 代表取Token的位置ID.</p><p>然后, 对Attention Layer添加约束, 强制Text Token只能Pay Attention to Text, 不能对Marker分配注意力权重, 而Marker Token可以对所有Token分配注意力. </p><blockquote><p>这样就能拿到一组<strong>纯净</strong>而<strong>共享</strong>的Text表示, Text表示没有Type Marker干预.</p></blockquote><p>给出一个例子:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/pure11.png" style="zoom: 60%;" /><p>图中Text Marker均位于句子尾部, 并且<code>mor</code> 和<code>[S:Md]</code>  共享相同的Position Embedding, <code>##pa</code> 和<code>[/S:Md]</code> 共享相同的Position Embedding. </p><p>在运行RE模型的时候, 因为<strong>Text Token表示不再与Marker Token相关</strong>, 所以不用重复计算Text的表示, 直接用Marker表示可以<strong>一次性计算句子中所有的Span对之间的关系</strong>, 不同Span对之间可以放到不同Batch里一起处理.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><p>实验中使用的数据集是ACE04, ACE05, SciERC, 统计数据如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/pure3.jpg" style="zoom: 40%;" /><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><p>在上述三个数据集上的结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/pure2.jpg" style="zoom: 40%;" /><p>♣代表引入跨句, †代表引入额外数据. 结果中L = LSTM, L + E = LSTM + Elmo, Bb = BERT - base, Bl = BERT - large, SciB = SciBERT(规模与Bert - base相同), ALB = ALBERT - xxlarge - v1. Rel为实体边界准确且关系准确, Rel+为实体边界和实体类型, 关系均准确.</p><p>其中Wang and Lu是<a href="https://adaning.github.io/posts/37252.html">TSE</a> 的结果. ALBERT下的PURE比TSE性能要好一点. Wadden是DYGIE++的结果.</p><p>在单句情况下PURE达到了SOTA, 在跨句情况下更是比单句要高上一层.</p><h3 id="Batch-Computations-and-Speedup"><a href="#Batch-Computations-and-Speedup" class="headerlink" title="Batch Computations and Speedup"></a>Batch Computations and Speedup</h3><p>近似的关系模型效果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/pure4.jpg" style="zoom: 40%;" /><p>近似模型基本上掉了一个点左右, 但是耗时有明显下降.</p><h3 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h3><h4 id="Importance-of-Typed-Text-Markers"><a href="#Importance-of-Typed-Text-Markers" class="headerlink" title="Importance of Typed Text Markers"></a>Importance of Typed Text Markers</h4><p>不同类型的Type Markers也对关系模型性能有着影响, 在ACE05和SciERC上结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/pure5.jpg" style="zoom: 40%;" /><p>gold为使用Golden Entity, e2e代表端到端, 也就是训练时直接使用实体模型的结果. </p><p>上表对应了6种输入关系模型前的Span表示方法:</p><ul><li><strong>TEXT</strong>: 直接用原Text的每个实体的的起始Token Embedding.</li><li><strong>TEXTETYPE</strong>: 在TEXT基础上拼接Entity Type Embedding.</li><li><strong>MARKERS</strong>: 使用不包含实体类型的边界Text Marker的Embedding.</li><li><strong>MARKERSETYPE</strong>: 在MARKERS基础上拼接实体类型Embedding.</li><li><strong>MARKERSELOSS</strong>: 使用不包含实体类型的边界Text Marker Embedding, 并引入一个辅助Loss, 用边界表示判断实体类型.</li><li><strong>TYPEDMARKERS</strong>: 正文中所使用的方法.</li></ul><p>能看到, 不同类型的Type Markers前前后后能差出四个点来, 增益非常大. </p><h4 id="Modeling-Entity-Relation-Interactions"><a href="#Modeling-Entity-Relation-Interactions" class="headerlink" title="Modeling Entity-Relation Interactions"></a>Modeling Entity-Relation Interactions</h4><p>作者探究了共享Encoder对性能的影响, 在ACE05上结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/pure7.jpg" style="zoom: 40%;" /><p>当两个模型共享Encoder联合优化时, 性能都有下降, 解释为两个任务形式不同, 使用面向单一任务的单一Encoder比共享要专一. <strong>简单的共享Encoder对作者的方法无益</strong>.</p><blockquote><p>作者在论文脚注处写到, 某些方法的作者提到共享Encoder有提升. </p><p>论文中的用词很严谨, 一是”简单共享”, 二是”对我们模型没有好处”, 个人认为对于大多数模型共享是否有害还无法一锤定音.</p></blockquote><h4 id="Mitigating-Error-Propagation"><a href="#Mitigating-Error-Propagation" class="headerlink" title="Mitigating Error Propagation"></a>Mitigating Error Propagation</h4><p>误差的错误传播在联合抽取中一直是没有被解决的问题, 正是因为这一问题, 才提出的联合抽取模型. </p><p>作者探究了关系模型采用实体模型预测出来的实体, 而不是Golden Entity, </p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/pure6.jpg" style="zoom: 40%;" /><p>10 - way jackkniﬁng就是十折交叉, 即为把数据划分成十份, 并且每个模型都用其中9份训练, 预测最后1份, 每个模型都对应着1份的预测结果, 把它们作为关系模型的输入, 然后训练关系模型. </p><p>结果表明使用实体模型的预测结果不如使用Golden Entity训练, 解释为实体模型的预测为关系模型在训练时带来了更多的噪声, 导致性能下降. Pipeline的曝光偏差影响或许并不像我们想的那样.</p><p>如果实体模型没有抽取到Golden Entity, 那么关系模型的预测肯定是错误的, 如果假设误差错误传播存在, 召回更多实体后F1应该提升. 基于这一假设, 作者尝试召回更多实体, 把实体模型输出分数最高的40%作为关系模型的输入, 然后用Beam Search得到结果, 发现F1仍在下降. 结果并没有表明误差错误传播的问题存在, 同时召回更多实体在训练时也会引入更多的噪声.</p><blockquote><p>为什么F1下降就说明召回下降了? 因为关系模型对召回噪声的判断是不包含在Precision里面的, 但提供更多的实体对却有可能让实体模型检测不出的实体对得到关系模型的判断, 对Precision无影响, 只对召回有影响.</p></blockquote><h4 id="Effect-of-Cross-sentence-Context"><a href="#Effect-of-Cross-sentence-Context" class="headerlink" title="Effect of Cross - sentence Context"></a>Effect of Cross - sentence Context</h4><p>作者研究了滑动窗口大小$W$ 对模型性能的影响:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/pure8.jpg" style="zoom: 40%;" /><p>$W=100$ 后提升不大, 甚至对关系模型有负面影响.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>本篇论文主要指出了基于Pipeline的模型不一定要弱于已经存在的联合抽取模型. 属于”逆行者”类论文. 在印象中, Pipeline性能是要弱于joint模型的, 因为其误差错误传播问题比较大.</p><p>本文使用两个极其<strong>简单</strong>且<strong>完全没有交互</strong>的Encoder, 用基于<strong>Span</strong>的<strong>Pipeline</strong>模型达到了联合抽取模型的SOTA效果, 并指出关系模型中的效率问题, 给出了一个近似实现. 此外, 针对联合抽取任务中Pipeline模型存在的偏见做出了探究, 奇怪的是误差的错误传播在本模型中并没有得到体现. </p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RTE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Two are Better than One: Joint Entity and Relation Extraction with Table - Sequence Encoders</title>
      <link href="/posts/37252.html"/>
      <url>/posts/37252.html</url>
      
        <content type="html"><![CDATA[<h1 id="Two-are-Better-than-One-Joint-Entity-and-Relation-Extraction-with-Table-Sequence-Encoders"><a href="#Two-are-Better-than-One-Joint-Entity-and-Relation-Extraction-with-Table-Sequence-Encoders" class="headerlink" title="Two are Better than One: Joint Entity and Relation Extraction with Table - Sequence Encoders"></a>Two are Better than One: Joint Entity and Relation Extraction with Table - Sequence Encoders</h1><p>本文是论文<a href="https://arxiv.org/abs/2010.03851" target="_blank" rel="noopener">Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders</a>的阅读笔记和个人理解. 论文来自<strong>EMNLP 2020</strong>. 本文为RTE问题中, 探讨NER和RE任务间关系的系列三部曲中的第一篇.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>NER和RE是两个NLP里的基础任务, 最近<strong>联合学习</strong>的算法尝试把NER和RE<strong>同时解决</strong>, 很多算法都尝试用<strong>填表式</strong>一次性完成. 但是现有的算法大多采用的是单个Encoder, 把NER和RE放在同一个Encoder提供的空间捕获信息.</p><p>作者举了一个非常简单的例子:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/tse1.jpg" style="zoom: 50%;" /><p>上图中, NER和RE在一张表中被一次性解决:</p><ul><li>在主对角线中, 实体被BIO标注法标出, NER被完成.</li><li>在对角线两侧, 不同的颜色代表不同的关系, 而方向标明了该实体是Subject还是Object, RE被完成.</li></ul><p>作者认为, 这样的设计是不符合联合学习的设计思想的, 必然会<strong>限制</strong>联合算法的效果:</p><ul><li>存在<strong>特征困惑</strong>问题, 即<strong>One Feature for Two Task</strong>, NER和RE本来是两个不同的任务, 单个Encoder会在学习过程中产生困惑, 不能学习到同时解决NER和RE任务的特征.</li><li>没有充分使用<strong>表结构</strong>信息, 大多数的Joint Extraction论文确实构造了表结构, 但是却最后把Table Filling问题转化为一个Sequence Labeling问题去解决.</li></ul><p>基于上述问题, 作者尝试在模型底层构建两个单独学习特征的Encoder, 分别学习NER和RE的特征.</p><h2 id="TSE"><a href="#TSE" class="headerlink" title="TSE"></a>TSE</h2><p><strong>TSE</strong>(<strong>T</strong>able - <strong>S</strong>equence <strong>E</strong>ncoders)是我自己给模型起的名字. 相较于后续其他论文起的Table - Sequence, 我还是认为简写更好.</p><p>作者眼中的NER和RE任务形式为:</p><ul><li><strong>NER</strong>: NER被建模为一个序列标注问题, 其Label $\boldsymbol{y}^{\text {NER}}$ 为标准的BIO标注. </li><li><strong>RE</strong>: RE被建模为一个填表问题, 对于给出的句子$\boldsymbol{x}=\left[x_{i}\right]_{1 \leq i \leq N}$, 在标注表中标注出$\boldsymbol{y}^{\mathrm{RE}}=\left[y_{i, j}^{\mathrm{RE}}\right]_{1 \leq i, j \leq N}$. 实体$x_{i^{b}}, \ldots, x_{i^{e}}$ 与实体$x_{j^{b}}, . ., x_{j^{e}}$ 之间的关系定义为$y_{i, j}^{\mathrm{RE}}=\overrightarrow{r}$, 当两实体调换位置时, 记为$y_{j, i}^{\mathrm{RE}}=\overleftarrow{r}$. $\perp$ 代表不存在关系.</li></ul><p>TSE主要由三个部分组成: 基础表示Text Embedder, 表表示Table Encoder, 序列表示Sequence Encoder:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/tse2.jpg" style="zoom: 50%;" /><h3 id="Text-Embedder"><a href="#Text-Embedder" class="headerlink" title="Text Embedder"></a>Text Embedder</h3><p>句子中含有$N$ 个单词$\boldsymbol{x}=\left[x_{i}\right]_{1 \leq i \leq N}$, 句子中的Token表示分别由三类Embedding组成, 分别是<strong>Word Embedding</strong>, LSTM提取的<strong>Character Embedding</strong>, 以及BERT抽取的上下文相关的<strong>Contextualized Word Embedding</strong>. </p><p>句子级别的Token表示分别为$\boldsymbol{x}^w$, $\boldsymbol{x}^c$, $\boldsymbol{x}^l$. 句子表示$\boldsymbol{S}_0$如下:</p><p>$$<br>\boldsymbol{S}_{0}=\operatorname{Linear}\left(\left[\boldsymbol{x}^{c} ; \boldsymbol{x}^{w} ; \boldsymbol{x}^{\ell}\right]\right)<br>$$</p><p>即将三个Embedding拼接后直接变换, $\boldsymbol{S}_0 \in \mathbb{R}^{N \times H}$.</p><h3 id="Table-Encoder"><a href="#Table-Encoder" class="headerlink" title="Table Encoder"></a>Table Encoder</h3><p>Table Encoder专门学习<strong>表</strong>表示.</p><p>首先, 由Sequence表示构造表结构, 刚开始表是<strong>表内上下文无关</strong>的, 从第$l-1$ 层Sequence Encoder得到的Sequence表示中获取表第$i$ 行第$j$ 列所需的信息, 简单的用$\operatorname{Linear}$ 展开成一个表:</p><p>$$<br>X_{l, i, j}=\operatorname{ReLU}\left(\operatorname{Linear}\left(\left[S_{l-1, i} ; S_{l-1, j}\right]\right)\right)<br>$$</p><p>第$l$ 层Table Encoder有$\boldsymbol{X}_l \in \mathbb{R} ^{N \times N \times H}$.</p><p>接着, 采用<strong>多维RNN</strong>(MD - RNN)去迭代的融合<strong>表间</strong>信息, 也就是<strong>表内上下文相关</strong>的信息: </p><p>$$<br>T_{l, i, j}=\operatorname{GRU}\left(X_{l, i, j}, T_{l-1, i, j}, T_{l, i-1, j}, T_{l, i, j-1}\right)<br>$$</p><p>$\operatorname{GRU}$ 跨越了三个维度, 除了表的行$i$, 列$j$, 还有跨越不同Table Encoder层的$l$.</p><p>第$l$ 个Table Encoder的表中的第$i$ 行, 第$j$ 列的信息$T_{l, i, j}$ 由多个GRU捕获, 每个GRU可以捕获到不同信息:</p><p>$$<br>\begin{aligned}<br>&amp;T_{l, i, j}^{(a)}=\operatorname{GRU}^{(a)}\left(X_{l, i, j}, T_{l-1, i, j}^{(a)}, T_{l, i-1, j}^{(a)}, T_{l, i, j-1}^{(a)}\right) \\<br>&amp;T_{l, i, j}^{(c)}=\operatorname{GRU}^{(c)}\left(X_{l, i, j}, T_{l-1, i, j}^{(c)}, T_{l, i+1, j}^{(c)}, T_{l, i, j+1}^{(c)}\right) \\<br>&amp;T_{l, i, j}=\left[T_{l, i, j}^{(a)} ; T_{l, i, j}^{(c)}\right]<br>\end{aligned}<br>$$</p><p>在表结构中, RNN可以选择递归<strong>方向</strong>, 对于式子中出现的$a, b, c, d$ 分别代表作者所尝试的方向, 示意图如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/tse4.jpg" style="zoom: 50%;" /><p>作者尝试了a, c组合和a, b, c, d组合两种设置, 后来发现前后二者差别不大, 所以采用简单的前者.</p><blockquote><p>个人猜测应该是b, d<strong>信息冗余</strong>. 从a触发, 代表表行和列的顺向, 从c出发代表表行和列的逆向. 而b代表行的顺向, 列的逆向, d代表行的逆向, 列的顺向. </p><p>在我们所习惯的双向RNN中扩展到表结构上, 就是a, c的组合, 无需再重复添加b, d.</p></blockquote><h3 id="Sequence-Encoder"><a href="#Sequence-Encoder" class="headerlink" title="Sequence Encoder"></a>Sequence Encoder</h3><p>Sequence Encoder专门学习<strong>序列</strong>表示.</p><p>作者认为单纯的缩放点积注意力在本模型中不够好, 于是提出了Table - Guided Attention, 用表信息指导Sequence中的Attention.</p><p>对于给定的$\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}$, 一般的注意力可被下述式子概括:<br>$$<br>f\left(Q_{i}, K_{j}\right)=U \cdot g\left(Q_{i}, K_{j}\right)<br>$$</p><p>$f$ 能返回$Q_i$ 下$V_i$ 的权重. 其中$U$ 为可学习参数, $g$ 为计算$Q, K$ 间相关性的函数.</p><p>如下图所示:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/tse5.jpg" style="zoom: 50%;" /><p>在本模型中, 恰好$\boldsymbol{Q}= \boldsymbol{K} =\boldsymbol{V}=\boldsymbol{S}_{l-1}$, 而$\boldsymbol{T}_l$ 是由$\boldsymbol{S}_{l-1}$ 得到的, 可以视为$T_{l, i, j} = g(S_{l-1, i}, S_{l-1, j}) = g(Q_i, K_j)$. 因此有:</p><p>$$<br>f\left(Q_{i}, K_{j}\right)=U \cdot T_{l, i, j}<br>$$</p><p>作者认为这种Attention有三个好处:</p><ol><li>不用算$g(\cdot)$, 直接从Table Encoder里拿$\boldsymbol{T}_l$ 就行.</li><li>因为$\boldsymbol{T}_l$ 是表内上下文相关的, 所以更能捕捉词间不同相关性.</li><li>使得Table Encoder能参与到Sequence Encoder的学习过程中, 从而提升两个Encoder间的交互性.</li></ol><p>其余的内容和Self - Attention一样:<br>$$<br>\begin{aligned}<br>\tilde{\boldsymbol{S}}_{l} &amp;=\operatorname{LayerNorm}\left(\boldsymbol{S}_{l-1}+\operatorname{SelfAttn}\left(\boldsymbol{S}_{l-1}\right)\right) \\<br>\boldsymbol{S}_{l} &amp;=\operatorname{LayerNorm}\left(\tilde{\boldsymbol{S}}_{l}+\operatorname{FFNN}\left(\tilde{\boldsymbol{S}}_{l}\right)\right)<br>\end{aligned}<br>$$</p><p>我再回来看一眼Table Encoder和Sequence Encoder之间<strong>迭代提升</strong>的过程:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/tse2.jpg" style="zoom: 50%;" /><blockquote><p>注意, 在这个图左侧是有虚线的! 它的作用将在下一小节说明.</p></blockquote><h3 id="Exploit-Pre-trained-Attention-Weights"><a href="#Exploit-Pre-trained-Attention-Weights" class="headerlink" title="Exploit Pre - trained Attention Weights"></a>Exploit Pre - trained Attention Weights</h3><p>作者认为, BERT的潜力没有被充分发掘, 因为BERT能很好的把握住<strong>词和词</strong>之间的关系, 所以<strong>Attention Weights</strong>最好被<strong>显式</strong>的加进来, 预训练模型的潜力就被更进一步的挖掘了.</p><p>作者把所有层的所有头的Attention Weights全部堆叠起来, 记为$\boldsymbol{T}^{l} \in \mathbb{R}^{N \times N \times (L^\ell \times A^\ell)}$, $L^{\ell}$ 为Transformer堆叠的层数, $A^\ell$ 为注意力头数.</p><p>接着只要把前面Sequence Encoder公式替换即可:<br>$$<br>\displaylines{X_{l, i, j}=\operatorname{ReLU}\left(\operatorname{Linear}\left(\left[S_{l-1, i} ; S_{l-1, j}\right]\right)\right)<br> \\<br> \downarrow<br> \\<br>X_{l, i, j}=\operatorname{ReLU}\left(\operatorname{Linear}\left(\left[S_{l-1, i} ; S_{l-1, j} ; T_{i, j}^{\ell}\right]\right)\right)}<br>$$</p><p>在每层生成Table表示的时候, 就必须考虑第$i$ 个词和第$j$ 个词之间的Attention Score $T_{i, j}^\ell$.</p><p>其余部分完全不用变, 就可以直接把预训练的BERT拿过来用.</p><p>再来回顾一下模型的整个流程:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/tse3.jpg" style="zoom: 50%;" /><ol><li>由Sequence Encoding和预训练模型的Attention Score一起得到最初的Table Encoding.</li><li>Table Encoding内部用MD - RNN做表间和跨层的交互.</li><li>Table信息通过Table - Guided Attention反作用于Sequence Encoder的训练.</li></ol><p>总体来看设计还是挺精妙的.</p><h3 id="Training-and-Evaluation"><a href="#Training-and-Evaluation" class="headerlink" title="Training and Evaluation"></a>Training and Evaluation</h3><p>因为作者假定NER为序列标注问题, RE为填表问题, 所以直接拿Sequence的结果$\boldsymbol{S}_L$ 来标注NER, 填表$\boldsymbol{T}_L$ 的结果来标注RE: </p><p>$$<br>\begin{aligned}<br>P_{\theta}\left(\boldsymbol{Y}^{\mathrm{NER}}\right) &amp;=\operatorname{softmax}\left(\operatorname{Linear}\left(\boldsymbol{S}_{L}\right)\right) \\<br>P_{\theta}\left(\boldsymbol{Y}^{\mathrm{RE}}\right) &amp;=\operatorname{softmax}\left(\operatorname{Linear}\left(\boldsymbol{T}_{L}\right)\right)<br>\end{aligned}<br>$$</p><p>这二者都是分类问题, 直接用交叉熵优化:</p><p>$$<br>\begin{aligned}<br>\mathcal{L}^{\mathrm{NER}} &amp;=\sum_{i \in[1, N]}-\log P_{\theta}\left(Y_{i}^{\mathrm{NER}}=y_{i}^{\mathrm{NER}}\right) \\<br>\mathcal{L}^{\mathrm{RE}} &amp;=\sum_{i, j \in[1, N] ; i \neq j}-\log P_{\theta}\left(Y_{i, j}^{\mathrm{RE}}=y_{i, j}^{\mathrm{RE}}\right)<br>\end{aligned}<br>$$</p><p>最终目标就是联合优化二者, 即最小化$\mathcal{L}^{\text{NER}} + \mathcal{L}^{\text{RE}}$.</p><p>在推断时, 还是把NER和RE作为两个任务<strong>分别处理</strong>.</p><p>对于NER任务, 简单的有:<br>$$<br>\underset{e}{\operatorname{argmax}} P_{\theta}\left(Y_{i}^{\mathrm{NER}}=e\right)<br>$$<br>对于RE任务, 需要区分开关系所对应的Subject和Object. 对于NER抽取到的两实体的Span$\left(i^{b}, i^{e}\right), \left(j^{b}, j^{e}\right)$, 关系由下式得来:<br>$$<br>\underset{\vec{r}}{\operatorname{argmax}} \sum_{i \in\left[i^{b}, i^{e}\right], j \in\left[j^{b}, j^{e}\right]} P_{\theta}\left(Y_{i, j}^{\mathrm{RE}}=\overrightarrow{r}\right)+P_{\theta}\left(Y_{j, i}^{\mathrm{RE}}=\overleftarrow{r}\right)<br>$$</p><p>$\perp$ 代表不存在关系, 它是不存在方向的.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文. BTW, 附录里面还有作者很多小想法和实验结果, 建议阅读.</p><p>作者采用了ACE04, ACE05, CoNLL04,  ADE这几个数据集.</p><p>下文中, 作者所采用的指标为F1 Score, NER仅当实体类型和边界均匹配时才算正确, RE当实体边界和关系类型匹配时才算正确.</p><h3 id="Comparison-with-Other-Models"><a href="#Comparison-with-Other-Models" class="headerlink" title="Comparison with Other Models"></a>Comparison with Other Models</h3><p>作者将本文方法与诸多模型做对比, 结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/tse7.jpg" style="zoom: 67%;" /><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/tse8.jpg" style="zoom: 67%;" /><p>RE + 代表实体边界, 以及实体类型, 关系类型, 三者均匹配时才算正确, 比传统的RE任务要更加严格, 还包含了<strong>实体类型</strong>.</p><p>模型在上述数据集上均达到SOTA.</p><h3 id="Comparison-of-Pre-trained-Models"><a href="#Comparison-of-Pre-trained-Models" class="headerlink" title="Comparison of Pre - trained Models"></a>Comparison of Pre - trained Models</h3><p>作者通过比较不同预训练之间的差距, 来间接凸显自己方法的有效性. 在ACE05上的结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/tse9.jpg" style="zoom: 67%;" /><p>$+\boldsymbol{x}^\ell$ 代表使用了上下文相关的Word Embedding, $+\boldsymbol{T}^\ell$ 代表使用了Attention Weights.</p><p>即使是使用了不是真正上下文相关的ELMo, 性能依然能够与一些最好的模型媲美. 引入预训训练后, 性能进一步上涨, 在引入Attention Weights后涨幅比较大. </p><p>这证明了利用Attention Weight的重要性, 对NER和RE都有增益.</p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><h4 id="Bidirectional-Interaction"><a href="#Bidirectional-Interaction" class="headerlink" title="Bidirectional Interaction"></a>Bidirectional Interaction</h4><p>作者研究了NER和RE这两个任务之间的交互对性能的影响:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/tse10.jpg" style="zoom: 67%;" /><p>RE(gold) 代表在做RE时候直接使用Golden Entity.</p><p>结论如下:</p><ul><li>不使用NER Loss或者不使用RE Loss, 都会降低性能, NER和RE彼此间确实存在关联, 且有相互促进作用.</li><li>不使用Table Encoder或不使用Sequence Encoder都会对性能产生负面影响, 如果切断两个Encoder之间的交互, 效果会更差.</li><li>如果直接把Table Representation的主对角线用来做NER任务, 也就是把NER和RE统一在一个空间中解决, 效果也不错, 但达不到作者的设置. 作者认为这是因为NER和RE仍然存在潜在差异, 不能直接用RE任务的Feature来做NER. 在该基础上, 继续去掉Sequence Encoder, 效果会变差, 这表明了Sequence信息对NER的作用.</li></ul><h4 id="Encoding-Layers"><a href="#Encoding-Layers" class="headerlink" title="Encoding Layers"></a>Encoding Layers</h4><p>作者探究了Encoder层数对性能的影响, ACE05上结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/tse11.jpg" style="zoom: 67%;" /><p>堆到3层的时候性能就没有明显提升了, 如果每层的参数共享, 层数越多越好.</p><h4 id="Settings-of-MD-RNN"><a href="#Settings-of-MD-RNN" class="headerlink" title="Settings of MD - RNN"></a>Settings of MD - RNN</h4><p>作者还对不同MD - RNN设定做了探究, 在ACE05上部分结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/tse12.jpg" style="zoom: 67%;" /><p>就如在前文中提到的, 四方向的效果并不比双向好太多. 且融入跨层信息后对RE提升比较大.</p><h3 id="Attention-Visualization"><a href="#Attention-Visualization" class="headerlink" title="Attention Visualization"></a>Attention Visualization</h3><p>作者还模型做了Attention可视化:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/tse13.jpg" style="zoom: 67%;" /><p>相较于ALBERT下的Attention, 本文提出的Table - Guided Attention要更像人类的Ground Truth一些, 可视化一定程度上说明了改进的有效性.</p><h3 id="Probing-Intermediate-States"><a href="#Probing-Intermediate-States" class="headerlink" title="Probing Intermediate States"></a>Probing Intermediate States</h3><p>在ACE05中作者挑了一个Case, 然后把训练好的Linear层放在每层Encoder后面检测结果:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/tse6.jpg" style="zoom: 50%;" /><p>从下到上层数依次变深, 能看到模型逐渐对结果做<strong>修正</strong>, 作者认为更多地允许层间交互有助于捕捉困难实体和关系.</p><blockquote><p>Probing在这里起作用的原因是作者引入了跨Encoder层信息.</p></blockquote><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>在当时, 填表类的联合抽取模型已经有几年没有被提起, 此篇论文算是重新将填表式的方法抬上了桌面.</p><p>作者注意到了联合模型中存在的任务<strong>特征学习冲突</strong>问题, 并设计了一种<strong>两个独立Encoder</strong>的<strong>迭代</strong>式<strong>交互提升</strong>模型. 实验做的非常全面, 还用到了Probing等手段, 最后得到了”<strong>Two are better than One</strong>“的结论.</p><blockquote><p>非常巧合的是, 本文同期与另一篇论述Pipeline模型不一定弱于Joint模型的文章<a href="https://arxiv.org/abs/2010.12812" target="_blank" rel="noopener">A Frustratingly Easy Approach for Entity and Relation Extraction</a>得到了相似的结论, 这篇文章打算下次更新, 算作三部曲中的第二部.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RTE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking</title>
      <link href="/posts/49694.html"/>
      <url>/posts/49694.html</url>
      
        <content type="html"><![CDATA[<h1 id="TPLinker-Single-stage-Joint-Extraction-of-Entities-and-Relations-Through-Token-Pair-Linking"><a href="#TPLinker-Single-stage-Joint-Extraction-of-Entities-and-Relations-Through-Token-Pair-Linking" class="headerlink" title="TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking"></a>TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking</h1><p>本文是论文<a href="https://arxiv.org/abs/2010.13415" target="_blank" rel="noopener">TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking</a>的阅读笔记和个人理解, 论文来自COLING 2020.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>虽然人们注意到在现有的实体关系抽取工作中, 含有<strong>重叠三元组</strong>的问题(感觉是特指<a href="https://adaning.github.io/posts/27105.html">CasRel</a>), 例如:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/tplinker1.jpg" style="zoom: 50%;" /><ul><li><strong>EPO</strong>(Entity Pair Overlap): 实体对重叠是指<strong>实体对之间具有多种关系</strong>, 这些三元组共享相同的实体.</li><li><strong>SEO</strong>(Single Entity Overlap): 单实体重叠是指有<strong>单个实体被多个三元组共享</strong>.</li></ul><p>但现有<strong>基于分解</strong>的方法仍然受到<strong>曝光偏差</strong>(<strong>Exposure Bias</strong>)的困扰, 即Subject和Object的抽取存在<strong>先后顺序</strong>上的影响, 如果Subject抽取错误, 则会导致<strong>错误累积</strong>.</p><p>所以作者希望<strong>一步到位</strong>(One Stage)同时抽取Subject和Object, 来规避曝光偏差问题.</p><h2 id="TPLinker"><a href="#TPLinker" class="headerlink" title="TPLinker"></a>TPLinker</h2><h3 id="HandShaking-Tagging-Scheme"><a href="#HandShaking-Tagging-Scheme" class="headerlink" title="HandShaking Tagging Scheme"></a>HandShaking Tagging Scheme</h3><h4 id="Tagging"><a href="#Tagging" class="headerlink" title="Tagging"></a>Tagging</h4><p>TPLinker将联合抽取建模为<strong>Token Pair之间的链接问题</strong>(<strong>T</strong>oken <strong>P</strong>air <strong>Link</strong>ing Problem).</p><p>对于给定的句子中所有的Token Pair, 作者将其之间的链接考虑为:</p><ul><li><p><strong>EH-to-ET</strong>: 两Token分别为<strong>同实体</strong>的<strong>起止</strong>位置(Entity Head to Entity Tail).</p></li><li><p><strong>SH-to-OH</strong>: 两Token分别为Subject和Object的<strong>起始</strong>位置(Subject Head to Object Head).</p></li><li><p><strong>ST-to-OT</strong>: 两Token分别为Subject和Object的<strong>结束</strong>位置(Subject Tail to Object Tail).</p></li></ul><blockquote><p>简单的记<strong>头</strong>为实体的<strong>起始</strong>位置, <strong>尾</strong>为实体的<strong>结束</strong>位置.</p></blockquote><p>对于上述三种链接的情况, 作者给出了一个例子:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/tplinker2.jpg" style="zoom: 55%;" /><p>我们从图中与上述三类链接做对应:</p><ul><li><strong>EH-to-ET</strong>: 图中的<strong>紫色</strong>Tag, (<code>New</code>, <code>City</code>) 就分别是<code>New York City</code>的头尾, (<code>De</code>, <code>Blasio</code>) 分别是<code>De Blasio</code> 的头尾.</li><li><strong>SH-to-OH</strong>: 图中的<strong>红色</strong>Tag, (<code>New</code>, <code>De</code>) 分别是<strong>给定关系</strong>下<code>New York City</code> 和<code>De Blasio</code> 的头. </li><li><strong>ST-to-OT</strong>: 图中的<strong>蓝色</strong>Tag, (<code>City</code>, <code>Blasio</code>) 分别是<strong>给定关系</strong>下<code>New York City</code> 和<code>De Blasio</code> 的尾. </li></ul><p>并且, 在关系<code>major</code>中<code>New York City</code> 和<code>De Blasio</code> 分别为Subject和Object.</p><p>该建模能够解决<strong>SEO</strong>问题, 例如嵌套实体<code>New York</code> 和<code>New York City</code> 在关系<code>major</code> 中都共享Object <code>De Blasio</code>(图中未标注), 我们观察量两嵌套Subject和Object的不同Link Type包含的Token Pair:</p><table><thead><tr><th>Subject</th><th>EH-to-ET</th><th>SH-to-OH</th><th>ST-to-OT</th></tr></thead><tbody><tr><td><code>New York</code></td><td>(<code>New</code>, <code>York</code>)</td><td>(<code>New</code>, <code>De</code>)</td><td>(<code>York</code>, <code>Blasio</code>)</td></tr><tr><td><code>New York City</code></td><td>(<code>New</code>, <code>City</code>)</td><td>(<code>New</code>, <code>De</code>)</td><td>(<code>City</code>, <code>Blasio</code>)</td></tr></tbody></table><p>尽管两嵌套实体有相同的SH-to-OH, 但其他两种Link的Token Pair并不相同, 自然能区分出共享Object的两个三元组.</p><p>除此外, 作者从中有更深的思考:</p><ol><li>若按图中方式建模, 矩阵会非常<strong>稀疏</strong>. 并且因为实体的尾永远都在头的前面.</li><li>在<strong>EH-to-ET</strong>这类Link Type中, 矩阵的下三角永远都是<strong>0</strong>, 浪费了大量空间. </li><li>有趣的是, <strong>Object可能出现在Subject前面</strong>, 所以对于<strong>SH-to-OH</strong>和<strong>ST-to-OT</strong>这两种链接类型, 直接把下三角丢掉是错误的.</li></ol><blockquote><p>三种链接类型应该存在<strong>不同的表</strong>中, 这样才能得以区分, 就像在图中用<strong>颜色</strong>加以区分一样, 因为不同链接类型可能位于<strong>同一位置</strong>.</p></blockquote><p>基于上述问题, 作者巧妙的将下三角的值全部映射到上三角的对应位置, 并把原来的下三角的1记为与上三角做区分的2, 如下图所示(注意关系不再是上图的<code>major</code>, 变成了<code>born in</code>, 刚才Subject和Object的位置正好反过来, 也就是上文所述第三点的场景):</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/tplinker3.jpg" style="zoom: 55%;" /><p>这一映射是我觉得全文最绝的一点. </p><p>在下三角时, (<code>De</code>, <code>New</code>) 之间的Link Type为SH-to-OH, 但映射到上三角后, Token Pair<strong>完全颠倒</strong>了, 变成了(<code>New</code>, <code>De</code>). 同时交换Token Pair的两Token位置和Link Type的指向, 交换前后是<strong>等价</strong>的. </p><p>若想保持原来<strong>SH-to-OH</strong>原含义不变, 映射后也必须颠倒, 即<strong>OH-to-SH</strong>, 也就是<strong>反向链接</strong>, 即图中的箭头回指. ST-to-OT也同理, (<code>Blasio</code>, <code>York</code>) 映射到上三角后变为(<code>York</code>, <code>Blasio</code>), ST-to-OT变为OT-to-ST. </p><p>前文提到SEO能处理了, 那<strong>EPO</strong>呢? 直接参照<a href="https://adaning.github.io/posts/27105.html">CasRel</a>把<strong>层叠标注</strong>的思想引进过来就行了, 即对每个不同的关系都创建出一张表来分别标注, 也就是<strong>关系特化的标注</strong>, 这样就能处理同一实体对存在多关系的EPO问题. </p><p>在TPLinker中, EH-to-ET没有必要做关系特化, 因为这种链接类型处理的是Token Pair是否归属于同一实体的问题, 不涉及到跨实体之间的关系. 而SH-to-OH, ST-to-OT是不同实体之间的链接, 涉及到关系, 需要做关系特化.</p><h4 id="Decoding"><a href="#Decoding" class="headerlink" title="Decoding"></a>Decoding</h4><p>解码过程可以直接拿图来说. 将Token Pair之间的表结构<strong>展平</strong>,每张表都视为一个序列, 这样方便我们观察不同Relation特化的表的标注情况:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/tplinker4.jpg" style="zoom: 45%;" /><p>EH-to-ET分别有三个Token Pair, (<code>New</code>, <code>York</code>), (<code>New</code>, <code>City</code>), (<code>De</code>, <code>Blasio</code>), 分别对应实体<code>New York</code>, <code>New York City</code>, <code>De Blasio</code>, 这是区分是否为同实体的Tag, 必须要与其他两种Link Type结合使用.</p><p>在关系<code>major</code>中, SH-to-OH标为1的Token Pair为(<code>New</code>, <code>De</code>), 且ST-to-OT标为1的Token Pair为(<code>City</code>, <code>Blasio</code>), 二者均为同向且顺向, 故抽取出三元组(<code>New York City</code>, <code>major</code>, <code>De Blasio</code>). </p><p>别忘了2代表反向链接. 在关系<code>born in</code>中, ST-to-OT标为2的Token Pair为(<code>York</code>, <code>Blasio</code>), 结合SH-to-OH标为2的Token Pair(<code>De</code>, <code>New</code>), 二者均为同向且逆向, 故抽取出三元组(<code>De Blasio</code>, <code>born in</code>, <code>New York</code>). </p><p>在该图中, EH-to-ET需要一张表, SH-to-OH, ST-to-OT做了关系特化, 分别每个关系需要一张表, 共计$2|R| + 1$ 张表. 但是由于做了下三角舍弃的优化, 若句长为$n$, 每张表只有$\frac{n^2+n}{2}$ 个元素.</p><p>TPLinker的解码流程伪代码如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/tplinker5.jpg" style="zoom: 50%;" /><p>可以概括为:</p><ol><li>根据EH-toET的表, 检测所有存在的实体, 存入字典$D$.</li><li>对每一种关系, 根据ST-to-OT的表记录Subject和Object的尾位置存入集合$E$. 将Tag为1的顺序记下来, 及Tag为2的逆序记下来. </li><li>对每一种关系, 根据SH-to-OH的表获得Subject和Object的头位置(Tag为1, 2, 同上), 直接去$D$ 中查询对应的实体对, 查询到后去$E$ 中做匹配, 如果命中, 则抽取出三元组. </li><li>对每一种关系, 根据SH-to-OH的表, </li></ol><h3 id="Token-Pair-Representation"><a href="#Token-Pair-Representation" class="headerlink" title="Token Pair Representation"></a>Token Pair Representation</h3><p>对于长为$n$ 的句子$[w_1, w_2, \cdots, w_n]$, 首先将每个Token都用Encoder(应该是BERT之类)映射成上下文相关的低维表示$\mathbf{h_i}$, 然后按如下方式生成$(w_i, w_j)$ 之间的Token Pair表示$\mathbf{h}_{i, j}$:</p><p>$$<br>\mathbf{h}_{i, j}=\tanh \left(\mathbf{W}_{h} \cdot\left[\mathbf{h}_{\mathbf{i}} ; \mathbf{h}_{\mathbf{j}}\right]+\mathbf{b}_{h}\right), j \geq i<br>$$</p><p>其中$\mathbf{W}_h, \mathbf{b}_h$ 为可学习参数.</p><h3 id="Handshaking-Tagger"><a href="#Handshaking-Tagger" class="headerlink" title="Handshaking Tagger"></a>Handshaking Tagger</h3><p>我们只需要很简单的对Token Pair之间的表示做Softmax就好:</p><p>$$<br>\begin{aligned}<br>&amp;P\left(y_{i, j}\right)=\operatorname{Softmax}\left(\mathbf{W}_{o} \cdot \mathbf{h}_{i, j}+\mathbf{b}_{o}\right) \\<br>&amp;\operatorname{link}\left(w_{i}, w_{j}\right)=\arg \max _{l} P\left(y_{i, j}=l\right)<br>\end{aligned}<br>$$</p><p>记Token Pair $(w_i, w_j)$ 之间的链接标签为$l$, $P(y_{i, j}=l)$ 为预测值为标签的概率.</p><p>这样就可以把前面图中的所有Token Pair之间的打上Tag.</p><h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><p>损失函数为NLL Loss:</p><p>$$<br>L_{l i n k}=-\frac{1}{N} \sum_{i=1, j \geq i}^{N} \sum_{\ast \in\{E, H, T\}} \log P\left(y_{i, j}^{\ast}=\hat{l}^{\ast}\right)<br>$$</p><p>$N$ 为句子长, $\hat{l}$ 为正确Tag, $E, H, T$ 分别代表EH-to-ET, SH-toOH, ST-to-OT. </p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文.</p><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>作者所采用的数据集为常用数据集NYT和WebNLG, 统计信息如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/tplinker6.jpg" style="zoom:67%;" /><blockquote><p>下文中, NYT*, WebNLG*, 均代表部分匹配下的结果, 否则为精准匹配结果. </p></blockquote><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><p>作者将TPLinker与Tagging类, 生成类的模型, 以及其他类的模型放在一起做了对比, 结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/tplinker7.jpg" style="zoom:67%;" /><p>主要观察CasRel和TPLinker之间的性能差距. TPLinker在LSTM下与CasRel取得了相近的性能, BERT抽取出的特征对TPLinker的增益比CasRel要大得多.</p><h3 id="Analysis-on-Different-Sentence-Types"><a href="#Analysis-on-Different-Sentence-Types" class="headerlink" title="Analysis on Different Sentence Types"></a>Analysis on Different Sentence Types</h3><p>作者将实体分为不同长度, 三元组分为不同类型, 观察TPLinker的能力, 结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/tplinker8.jpg" style="zoom:67%;" /><p>从中观察到, TPLinker不太受实体长度增加的影响, 同时处理SEO和EPO问题的效果也都很不错.</p><h3 id="Analysis-on-Computational-Efficiency"><a href="#Analysis-on-Computational-Efficiency" class="headerlink" title="Analysis on Computational Efficiency"></a>Analysis on Computational Efficiency</h3><p>最后, TPLinker的内部机制相对于其他模型来说是比较复杂的, 或许从直观上来看, 这可能成为TPLinker的弱势, 所以作者还做了TPLinker和CasRel的复杂度比较:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/tplinker9.jpg" style="zoom:67%;" /><p>TPLinker的推理时间其实并没有像想象中的那么长, 也算是打消了直观上的疑虑.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>本质上, TPLinker巧妙的将联合抽取问题建模为<strong>Token Pair之间的关系抽取问题</strong>, 使得联合抽取<strong>一步到位</strong>, 不但规避了<strong>归纳偏置</strong>问题, 还能同时处理<strong>EPO</strong>, <strong>SEO</strong>问题. 效果也超越了之前SOTA的CasRel. </p><p>方法很简练, <strong>没有花里胡哨</strong>. 本篇论文有颇多值得思考的地方, 除了对抽取任务本身的考量, 能看得出来作者的工程思维很强, 在比较容易被质疑的算法复杂度部分提出了一种<strong>减少空间开销</strong>的实现, 并专门做了<strong>时间开销</strong>的比较, 这是值得学习的地方.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RTE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CasRel: A Novel Cascade Binary Tagging Framework for Relational Triple Extraction</title>
      <link href="/posts/27105.html"/>
      <url>/posts/27105.html</url>
      
        <content type="html"><![CDATA[<h1 id="A-Novel-Cascade-Binary-Tagging-Framework-for-Relational-Triple-Extraction"><a href="#A-Novel-Cascade-Binary-Tagging-Framework-for-Relational-Triple-Extraction" class="headerlink" title="A Novel Cascade Binary Tagging Framework for Relational Triple Extraction"></a>A Novel Cascade Binary Tagging Framework for Relational Triple Extraction</h1><p>本文是论文<a href="https://arxiv.org/abs/1909.03227" target="_blank" rel="noopener">A Novel Cascade Binary Tagging Framework for Relational Triple Extraction</a>的阅读笔记和个人理解, 论文来自ACL 2020.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>在面向三元组抽取设计的模型中, 少有模型能够解决多重<strong>关系三元组重叠</strong>问题, 这类三元组往往是<strong>共享实体</strong>的. 例如:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/casrel1.jpg" style="zoom: 60%;" /><p>作者认为有两种三元组重叠现象发生:</p><ul><li><strong>EPO</strong>(Entity Pair Overlap): 实体对重叠是指<strong>实体对之间具有多种关系</strong>, 这些三元组共享相同的实体.</li><li><strong>SEO</strong>(Single Entity Overlap): 单实体重叠是指有<strong>单个实体被多个三元组共享</strong>, 如例子中的<code>Washington</code>.</li></ul><p>基于重叠问题, 作者提出<strong>将关系视为句间实体的映射</strong>, 以此解决重叠问题.</p><h2 id="CasRel"><a href="#CasRel" class="headerlink" title="CasRel"></a>CasRel</h2><p>CasRel与先前的模型不同, 它希望将目标函数指定在<strong>Triple Level</strong>, 而非像先前的做法将三元组抽取的过程拆开, 并分别设立目标函数.</p><p>在数据集$D$ 中已标注的句子$x_i$ 中, 对于给定的三元组$T_j = \set{(s,r,o)}$, 作者希望最大化三元组在给定句子下的概率:</p><p>$$<br>\begin{aligned}<br>&amp; \prod_{j=1}^{|D|}\left[\prod_{(s, r, o) \in T_{j}} p\left((s, r, o) \mid x_{j}\right)\right] \\<br>=&amp; \prod_{j=1}^{|D|}\left[\prod_{s \in T_{j}} p\left(s \mid x_{j}\right) \prod_{(r, o) \in T_{j} \mid s} p\left((r, o) \mid s, x_{j}\right)\right] \\<br>=&amp; \prod_{j=1}^{|D|}\left[\prod_{s \in T_{j}} p\left(s \mid x_{j}\right) \prod_{r \in T_{j} \mid s} p_{r}\left(o \mid s, x_{j}\right) \prod_{r \in R \backslash T_{j} \mid s} p_{r}\left(o_{\varnothing} \mid s, x_{j}\right)\right]<br>\end{aligned}<br>$$</p><p>第一步到第二步是条件概率, 第二步到第三步将$r$ 分为$s$ 应该有的关系 $r \in T_{j} \mid s$ 和$s$ 一定不会存在的关系$r \in R \backslash T_{j} \mid s$ 来考虑. </p><p>所有与$s$ 可能发生的关系, 都应该在句子中找到与之对应的$o$ , 如果$s$ 不可能存在$r$, 那么必然找不到对应的$o$, 因此就要有$o_{\varnothing}$ 存在, 即”Null” Object.</p><p>这样, 如果三元组存在, 就应该可以通过Subject在Relation下找到相应的Object, 也就是所谓的”<strong>映射关系</strong>“.</p><p>作者认为这样有三点好处:</p><ol><li>Triple Level的目标函数建立与最终<strong>评估方式</strong>是相符的.</li><li>Triple Level的建模直接<strong>规避</strong>了实体如何被<strong>共享</strong>的问题.</li><li>第三步的拆解启发了模型的<strong>结构设计</strong>. 通过最大化$p\left(s \mid x_{j}\right)$训练Subject Tagger, 再在已知Subject的条件下最大化给定关系$r$ 的$p_{r}\left(o \mid s, x_{j}\right)$ 来训练Object Tagger, 关系便自然成为实体之间的映射函数.</li></ol><h3 id="BERT-Encoder"><a href="#BERT-Encoder" class="headerlink" title="BERT Encoder"></a>BERT Encoder</h3><p>对于句子$x_j$, BERT抽取它的特征. 令$\operatorname{Trans}(\mathbf{x})$ 为Transfomer Encoder Bolck, 将$N$ 层堆叠, 得到特征$\mathbf{h}_\alpha$: </p><p>$$<br>\begin{aligned}<br>\mathbf{h}_{0} &amp;=\mathbf{S} \mathbf{W}_{s}+\mathbf{W}_{p} \\<br>\mathbf{h}_{\alpha} &amp;=\operatorname{Trans}\left(\mathbf{h}_{\alpha-1}\right), \alpha \in[1, N]<br>\end{aligned}<br>$$</p><p>$\mathbf{S}$ 为句子的One Hot向量, $\mathbf{W}_s$ 为Subword的Embedding矩阵, $\mathbf{W}_p$ 为位置编码矩阵.</p><p>根据任务, 作者在这里只使用<strong>单句</strong>, 而非像BERT原文使用句子对.</p><h3 id="Cascade-Decoder"><a href="#Cascade-Decoder" class="headerlink" title="Cascade Decoder"></a>Cascade Decoder</h3><p>作者希望分两步抽取三元组:</p><ol><li>首先从句子中抽取<strong>Subject</strong>.</li><li>然后在所有可能的<strong>关系</strong>$r$ 的条件下, 看是否有与Subject相对应的<strong>Object</strong>.</li></ol><p>因此作者也设计了两相对应的模块.</p><h4 id="Subject-Tagger"><a href="#Subject-Tagger" class="headerlink" title="Subject Tagger"></a>Subject Tagger</h4><p>简单的使用两个二分类器分别识别Subject的起始和结束位置, 对每个Token的对应位置都是两个二分类问题, 这个位置是不是起始位置? 是不是结束位置?</p><p>描述如下:</p><p>$$<br>\begin{array}{r}<br>p_{i}^{\text {start_s }}=\sigma\left(\mathbf{W}_{\text {start }} \mathbf{x}_{i}+\mathbf{b}_{\text {start }}\right) \\<br>p_{i}^{\text {end_s }}=\sigma\left(\mathbf{W}_{\text {end }} \mathbf{x}_{i}+\mathbf{b}_{\text {end }}\right)<br>\end{array}<br>$$</p><p>$p_{i}^{\text {start_s }}, p_{i}^{\text {end_s }}$ 分别为输入句子的第$i$ 个Token是否为Subject起始位置的概率和结束位置的概率.</p><p>其中$\mathbf{W}_{\text{start}}, \mathbf{W}_{\text{end}}$ 为权重矩阵, $\mathbf{b}_{\text {start }}, \mathbf{b}_{\text {end }}$ 为偏置, $\sigma$ 为Sigmoid函数.</p><p>得出的概率超过先设定好的<strong>阈值</strong>时, 便标记为1, 否则为0.</p><p>对于Subject $s$, 我们最大化如下概率:</p><p>$$<br>p_{\theta}(s \mid \mathbf{x})<br>=\prod_{t \in\{\text { start_s }, \text { end_s }\}} \prod_{i=1}^{L}\left(p_{i}^{t}\right)^{\mathbf{I}\left\{y_{i}^{t}=1\right\}}\left(1-p_{i}^{t}\right)^{\mathbf{I}\left\{y_{i}^{t}=0\right\}}<br>$$</p><p>$L$ 为句长, $\mathbf{I}\set{z} = 1$ 表示如果$z$ 为真则为1, 否则为0. $y_i^{\text {start_s }}, y_i^{\text {end_s }}$ 分别为Subject的起始结束位置二值化的Tag. $\theta$ 就是前面提到的Subject Tagger的参数.</p><p>由于一定会检测到多个Subject的Start和End, 作者遵循<strong>最近匹配原则</strong>, 即将最近的Start和End做匹配, 在区间内的Token标识为一个实体.</p><p>Subject Tagger如下图所示:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/casrel7.jpg" style="zoom: 40%;" /><p>句子中的Token被BERT编码后, Subject Tagger能从中预测每个Token所对应的位置是否为Subject的起始位置, 结束位置. 在获取所有的起始结束位置后, 遵循最近匹配原则, 将最近的Start和最近的End匹配到一起, 区间内被视为一个Subject. </p><p>如图中以<code>Jackle</code>为起始, <code>Brown</code>为结束, <code>Jackle R. Brown</code>就被标识为一个Subject. <code>Washington</code>的起始和结束时同一个位置.</p><h4 id="Relation-specific-Object-Taggers"><a href="#Relation-specific-Object-Taggers" class="headerlink" title="Relation - specific Object Taggers"></a>Relation - specific Object Taggers</h4><p>关系特化的Object Tagger是一种比Subject Tagger更高阶的模块, 因为它是建立在Subject Tagger抽取出的Subject的基础之上的, 它的结构与Subject Tagger基本相同, 只是对关系做了特化处理, 即<strong>对每个关系都做给定Subject的Object抽取</strong>.</p><p>具体形式如下:<br>$$<br>\begin{array}{r}<br>p_{i}^{\text {start_o }}=\sigma\left(\mathbf{W}_{\text {start }}^{r}\left(\mathbf{x}_{i}+\mathbf{v}_{\text {sub }}^{k}\right)+\mathbf{b}_{\text {start }}^{r}\right) \<br>p_{i}^{\text {end_o }}=\sigma\left(\mathbf{W}_{\text {end }}^{r}\left(\mathbf{x}_{i}+\mathbf{v}_{s u b}^{k}\right)+\mathbf{b}_{\text {end }}^{r}\right)<br>\end{array}<br>$$</p><p>$p_{i}^{\text {start_o }}, p_{i}^{\text {end_o }}$ 分别为输入句子的第$i$ 个Token是否为Object起始位置的概率和结束位置的概率. 这里作者简单的取了第$k$ 个Subject所有Token的<strong>平均</strong>$\mathbf{v}^k_{sub}$ 作为Subject传递的信息.</p><p>其中$\mathbf{W}^r_{\text{start}}, \mathbf{W}^r_{\text{end}}$ 为关系$r$ 对应的权重矩阵, $\mathbf{b}^r_{\text {start }}, \mathbf{b}^r_{\text {end }}$ 为关系$r$ 对应的偏置.</p><p>接着对如下概率做极大似然:<br>$$<br>p_{\phi_{r}}(o \mid s, \mathbf{x})<br>=\prod_{t \in\{\text { start_o, end_o }\}} \prod_{i=1}^{L}\left(p_{i}^{t}\right)^{\mathbf{I}\left\{y_{i}^{t}=1\right\}}\left(1-p_{i}^{t}\right) ^\mathbf{\mathbf { I } \{ y _ { i } ^ { t } = 0 \}}<br>$$</p><p>$y_i^{\text {start_o }}, y_i^{\text {end_o }}$ 分别为Object的起始结束位置二值化的Tag. $\phi_r$ 为关系特化的Object Tagger的参数.</p><p>对于”Null” Object $o_{\varnothing}$, 所有的$y_i^{start\_o_{\varnothing}}, y_i^{end\_o_{\varnothing}}$ 全部为0.</p><p>Relation - specific Object Taggers如下图所示:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/casrel8.jpg" style="zoom: 40%;" /><p>CasRel会在不同的Subject之间迭代. 从Subject Tagger获取了指定的实体后, 简单的将第$k$ 个实体所在区域的Token取平均得到$\mathbf{v}^k_{sub}$, 与BERT抽取出的$\mathbf{h}_N$ 相加, 以此判断句子中所有位置是否有可能含有给定关系$r$ 的Object, 并用起始位置和结束位置来标识Object.</p><p>在图中, 假设此时$k=1$, 所选定的Subject为<code>Jackle R. Brown</code> 其可能的关系<code>Birth_place</code>中, 探测到的Object是 <code>Washington</code> 和<code>United States Of America</code>. 故抽取出三元组<code>(Jackle R. Brown, Birth_place, Washington)</code> 及<code>(Jackle R. Brown, Birth_place, United States Of America)</code>.</p><p>当$k=2$ 时, 绿色的Subject <code>Washington</code> 可能的关系<code>Capital_of</code>中, 探测到的Object为 <code>United States Of America</code>, 图中绿色的位置将会从0变为1, 以此抽取到三元组<code>(Washington, Capital_of, United States Of America)</code>.</p><h3 id="Data-Log-likelihood-Objective"><a href="#Data-Log-likelihood-Objective" class="headerlink" title="Data Log-likelihood Objective"></a>Data Log-likelihood Objective</h3><p>根据最前面的推导, 目标函数$J(\Theta)$ 为:<br>$$<br>J(\Theta) = \sum_{j=1}^{|D|}\left[\sum_{s \in T_{j}} \log p_{\theta}\left(s \mid \mathbf{x}_{j}\right)+\sum_{r \in T_{j} \mid s} \log p_{\phi_{r}}\left(o \mid s, \mathbf{x}_{j}\right)+\sum_{r \in R \backslash T_{j} \mid s} \log p_{\phi_{r}}\left(o_{\varnothing} \mid s, \mathbf{x}_{j}\right)\right]<br>$$</p><p>使用极大似然优化即可.</p><p>总览图如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/casrel2.jpg" style="zoom: 50%;" /><p>因为对于每种关系都分别检测相应的Object, 感觉就像是一堆指针堆叠起来, 所以也被称为<strong>层叠式指针标注</strong>.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的实验参数请参照原论文.</p><blockquote><p>注: 接下来的实验中BERT是<strong>没有经过预训练</strong>的, 这展示了CasRel强大的潜力.</p></blockquote><p>作者在NYL和WebNLG这两个数据集上做了实验, 数据集的详细信息如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/casrel3.jpg" style="zoom: 50%;" /><p>其中NYT规模较大, WebNLG规模较小. 它们都包含有大量的EPO和SEO三元组.</p><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><p>在这两个数据集上的结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/casrel4.jpg" style="zoom: 50%;" /><p>无论是Tagging类的方法, 还是基于图的方法, 基于Copy生成式的方法, 都比CasRel差一大截. 尤其是在重叠三元组很多的WebNLG上, CasRel展示出强大的性能.</p><h3 id="Detail-Results"><a href="#Detail-Results" class="headerlink" title="Detail Results"></a>Detail Results</h3><p>面对不同类型的三元组, 各个Baseline在NYT, WebNLG的F1结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/casrel5.jpg" style="zoom: 50%;" /><p>左, 中, 右分别为Normal Class, EPO Class, SEO Class的F1. CasRel在面对重叠三元组时, 比Baseline领先的更多. 这证明了CasRel能较好的解决SEO和EPO问题.</p><p>作者还测试了句子中三元组个数不同时, 模型预测它们时的F1, 结果如下: </p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/casrel6.jpg" style="zoom: 50%;" /><p>即使是面对句子中5个以上的三元组, 相较于Baseline, CasRel仍然能保证不受过大的影响, 这表明它有更强的鲁棒性和抽取多三元组的能力.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>在本文中, 作者将目光聚焦到<strong>三元组重叠</strong>问题上, 并提出<strong>将关系视为句间实体的映射</strong>的思想, 并通过<strong>层叠式指针标注</strong>的方法, 巧妙的解决了三元组重叠问题.</p><p>这是一篇非常有开创性意义的文章, 模型效果相较于之前的Baseline有巨大涨幅.</p><p>更为细致的考量, 可以在苏神的文章<a href="https://kexue.fm/archives/6671" target="_blank" rel="noopener">基于DGCNN和概率图的轻量级信息抽取模型</a>中看到, CasRel仅仅只是把文中的DGCNN换成了BERT, 阅读起来应该不会有问题.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RTE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Unified MRC Framework for Named Entity Recognition</title>
      <link href="/posts/53990.html"/>
      <url>/posts/53990.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>BERT: 详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>.</li></ul></blockquote><h1 id="A-Unified-MRC-Framework-for-Named-Entity-Recognition"><a href="#A-Unified-MRC-Framework-for-Named-Entity-Recognition" class="headerlink" title="A Unified MRC Framework for Named Entity Recognition"></a>A Unified MRC Framework for Named Entity Recognition</h1><p>本文是论文<a href="https://arxiv.org/abs/1910.11476" target="_blank" rel="noopener">A Unified MRC Framework for Named Entity Recognition</a>的阅读笔记和个人理解, 论文来自ACL 2020.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者认为, 基于单标签设计的模型不适合在处理Flat NER的同时, 处理<strong>嵌套NER</strong>任务, 因为一个Token仅可被分配给一个实体, 但在嵌套NER问题中, 一个Token往往隶属于多个实体.</p><p>下面有一个嵌套NER的例子:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mrc4ner1.jpg" style="zoom:40%;" /><p>这个句子中, <code>PEBP2</code> 为蛋白质, 但<code>PEBP2 site</code>是一种DNA, 实体间存在嵌套关系.</p><p>作者尝试将NER任务迁移到MRC的框架下, 同时解决<strong>Flat NER</strong>和<strong>Nested NER</strong>问题, 因为在MRC中, <strong>抽取两嵌套实体, 仅仅只是回答两个不同类别的问题</strong>.</p><h2 id="Unified-MRC-Framework-for-NER"><a href="#Unified-MRC-Framework-for-NER" class="headerlink" title="Unified MRC Framework for NER"></a>Unified MRC Framework for NER</h2><h3 id="Task-Formulation"><a href="#Task-Formulation" class="headerlink" title="Task Formulation"></a>Task Formulation</h3><h4 id="NER-Task-Formulation"><a href="#NER-Task-Formulation" class="headerlink" title="NER Task Formulation"></a>NER Task Formulation</h4><p>对于给定的长度为$n$ 的输入序列$X=\left\{x_{1}, x_{2}, \ldots, x_{n}\right\}$, 对于每个在序列中的实体, 都有一个预先定义好的标签$y \in Y$, $Y$ 为预先定义好的实体所有可能的类型.</p><h4 id="Dataset-Constraction"><a href="#Dataset-Constraction" class="headerlink" title="Dataset Constraction"></a>Dataset Constraction</h4><p>为了将NER任务迁移到MRC框架下, 作者将数据集视为$(\text{QUESTION}, \text{ANSWER}, \text{CONTEXT})$ 的三元组形式. 对于每个实体的类型$y \in Y$, 都需要一个与之对应的长度为$m$ 的自然语言问题$q_y=\set{q_1, q_2, \dots, q_m}$, 实体被标注为$x_{\text{start}, \text{end}} = \set{x_{\text{start}, }x_{\text{start+1}, }\dots, x_{\text{end}-1, }, x_{\text{end}}}$, 且同一实体的$\text{start}$ 必在$\text{end}$ 前. </p><p>$X$ 和$Y$ 在NER任务中是已存在的, 还缺少问题$\text{QUESTION}$, 需要根据标签$y$ 进一步生成问题$q_y$, 来凑齐$(q_y, x_{\text{start,end}}, X)$ 的形式.</p><h3 id="Query-Generation"><a href="#Query-Generation" class="headerlink" title="Query Generation"></a>Query Generation</h3><p>问题生成是一个非常重要的注入先验知识的过程, 作者尝试了多种生成问题的方式, 最终选择”Annotation guideline notes”, 作者认为这种方法能<strong>不产生歧义</strong>问题. 这种方法直接使用数据集构建者发配给标注者的<strong>标注提示</strong>作为问题, 更符合NLP的处理风格.</p><p>其方法例子如下表所示:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mrc4ner2.jpg" style="zoom:33%;" /><blockquote><p>感觉这玩意就是一种Prompt啊… 给模型对应类型的实体信息提示, 以获取相应类型的实体位置.</p></blockquote><h3 id="Model-Details"><a href="#Model-Details" class="headerlink" title="Model Details"></a>Model Details</h3><h4 id="Model-Backbone"><a href="#Model-Backbone" class="headerlink" title="Model Backbone"></a>Model Backbone</h4><p>现在需要把前面所说的$(q_y, x_{\text{start,end}}, X)$ 整合进MRC框架下. 作者以BERT作为模型的Backbone, 把问题$q_y$ 和句子$X$ 一并输入进去, 格式为$\left\{[\mathrm{CLS}], q_{1}, q_{2}, \ldots, q_{m},[\mathrm{SEP}], x_{1}, x_{2}, \ldots, x_{n}\right\}$. </p><p>最后可以获得BERT抽取出的句子$X$ 所对应的上下文表示$E \in \mathbb{R}^{n \times d}$. </p><blockquote><p>作者在这里直接舍弃了问题的表示, 或许在某些问题中, 问题起到增强的作用.</p></blockquote><h4 id="Span-Selection"><a href="#Span-Selection" class="headerlink" title="Span Selection"></a>Span Selection</h4><p>在MRC任务中, 问题的答案一般是基于<strong>抽取式</strong>的, 也就是直接基于问题和原文内容, 直接在原文中指出问题所在的<strong>位置</strong>. </p><p>往往在MRC中有两类抽取答案的策略:</p><ul><li>用两个<strong>$n$ 元分类器</strong>, 分别获取问题的$\text{start}, \text{end}$ 位置.</li><li>用两个<strong>二元分类器</strong>, 判断每个Token所对应的位置是否是$\text{start}$, 是否是$\text{end}$.</li></ul><p>作者在这里选择第二种, 因为第一种只能抽取出一组$\text{start}, \text{end}$, 而第二种可以抽取出多个$\text{start}, \text{end}$, 这样有更大希望命中与问题$q_y$ 对应的实体.</p><blockquote><p>这两种方法生成Label的时候都可能会有稀疏问题, 可以用Focal Loss等方式缓解.</p></blockquote><h5 id="Start-and-End-Index-Prediction"><a href="#Start-and-End-Index-Prediction" class="headerlink" title="Start and End Index Prediction"></a>Start and End Index Prediction</h5><p>对于从BERT获取来的表示$E \in \mathbb{R} ^{n \times d}$, 首先预测$n$ 个token分别是否为$\text{start}$ 的概率:</p><p>$$<br>P_{\text {start }}=\operatorname{softmax}_{\text {each row }}\left(E \cdot T_{\text {start }}\right) \in \mathbb{R}^{n \times 2}<br>$$</p><p>其中$P_{\text{start}}$ 是是否为$\text{start}$ 的概率分布, $T_{\text{start}} \in \mathbb{R} ^ {d\times 2}$ 是权重矩阵. End Index的获取方式同理, $T_{\text{end}} \in \mathbb{R} ^ {d\times 2}$ 也是权重矩阵, $P_{\text{end}}$ 为是否为$\text{end}$ 的概率分布.</p><h5 id="Start-End-Matching"><a href="#Start-End-Matching" class="headerlink" title="Start - End Matching"></a>Start - End Matching</h5><p>一般的NER任务中, 一句话都会包含<strong>多个实体</strong>. 经过了上一步的Index Prediction, 我们应该获得了多个$\text{start}$ 和多个$\text{end}$ 的概率分布. </p><p>如果只是简单的遵循<strong>最近匹配</strong>$\text{start}$ 和$\text{end}$ 的原则, 是没有办法解决<strong>实体嵌套</strong>问题的. 因此, 作者考虑一个模型来专门做$\text{start}$ 和$\text{end}$ 的匹配.</p><p>先根据我们前面求得的概率分布$P_{\text{start}}$ 和$P_{\text{end}}$ 来确定$\text{start}$ 和$\text{end}$ 的位置:<br>$$<br>\begin{aligned}<br>&amp;\hat{I}_{\text {start }}=\left\{i \mid \operatorname{argmax}\left(P_{\text {start }}^{(i)}\right)=1, i=1, \cdots, n\right\} \\<br>&amp;\hat{I}_{\text {end }}=\left\{j \mid \operatorname{argmax}\left(P_{\text {end }}^{(j)}\right)=1, j=1, \cdots, n\right\}<br>\end{aligned}<br>$$</p><p>其中${}^{(i)}$ 代表矩阵的第$i$ 行.</p><p>对于任意的$i_{\text{start}} \in \hat{I}_{\text{start}}$ 和$i_{\text{end}} \in \hat{I}_{\text{end}}$, 都直接用一个神经网络接一个Sigmoid做二分类判断是否匹配:<br>$$<br>P_{i_{\text {start }}, j_{\text {end }}}=\operatorname{sigmoid}\left(m \cdot \operatorname{concat}\left(E_{i_{\text {start }}}, E_{j_{\text {end }}}\right)\right)<br>$$</p><p>$m \in \mathbb{R} ^{1 \times 2d}$ 为可学习权重.</p><h4 id="Train-and-Test"><a href="#Train-and-Test" class="headerlink" title="Train and Test"></a>Train and Test</h4><p>在训练时, 首先要计算选择起点和终点的损失:</p><p>$$<br>\begin{aligned}<br>\mathcal{L}_{\text {start }} &amp;=\mathrm{CE}\left(P_{\text {start}}, Y_{\text {start }}\right) \\<br>\mathcal{L}_{\text {end }} &amp;=\mathrm{CE}\left(P_{\text {end}}, Y_{\text {end }}\right)<br>\end{aligned}<br>$$</p><p>然后是匹配模型的Span匹配损失, 也是用交叉熵来衡量:</p><p>$$<br>\mathcal{L}_{\text {span }}=\operatorname{CE}\left(P_{\text {start}, \text { end }}, Y_{\text {start, end }}\right)<br>$$</p><p>最后联合优化这些损失:</p><p>$$<br>\mathcal{L}=\alpha \mathcal{L}_{\text {start }}+\beta \mathcal{L}_{\text {end }}+\gamma \mathcal{L}_{\text {span }}<br>$$<br>$\alpha, \beta, \gamma \in [0, 1]$, 均为超参, 来控制三个损失对任务总目标的贡献. </p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文.</p><blockquote><p>原文在实验部分的写作采用的是QA的方式, 这种方式很利于集中读者的注意力.</p></blockquote><h3 id="Nested-NER"><a href="#Nested-NER" class="headerlink" title="Nested NER"></a>Nested NER</h3><p>对于嵌套NER任务, 实验在英文数据集ACE 2004, ACE 2005, GENIA, KBP2017上进行, 结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mrc4ner3.jpg" style="zoom: 67%;" /><p>性能涨幅较大.</p><h3 id="Flat-NER"><a href="#Flat-NER" class="headerlink" title="Flat NER"></a>Flat NER</h3><p>对于Flat NER任务, 在英文数据集CoNLL 2003, OntoNotes 5.0, 中文数据集MSRA, OntoNotes 4.0 上进行, 实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mrc4ner4.jpg" style="zoom: 67%;" /><p>在Flat NER任务上也有一些提升, 结合嵌套NER任务, 此框架确实能很好的处理这两种情况.</p><p>BERT - Tagger就是把原始BERT拿来做NER任务, 后续其他实验也用这个Baseline做了对比,</p><h3 id="Ablation-studies"><a href="#Ablation-studies" class="headerlink" title="Ablation studies"></a>Ablation studies</h3><h4 id="Improvement-from-MRC-or-from-BERT"><a href="#Improvement-from-MRC-or-from-BERT" class="headerlink" title="Improvement from MRC or from BERT"></a>Improvement from MRC or from BERT</h4><p>作者将预训练的BERT和其他没有预训练的Baseline放在一起做了个比较:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mrc4ner5.jpg" style="zoom: 50%;" /><p>BiDAF和QAnet都是基于MRC方法的模型, 没有经过预训练, 但仍然比LSTM要强, 这证明了本框架的有效性. 再将预训练的BERT与没预训练的模型相比, BERT的性能要优于其他模型.</p><p>作者还做了一次Case Study, 将BERT的Attention Matrix展示了出来:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mrc4ner7.jpg" style="zoom: 67%;" /><p>输入句子中的<code>Flevland</code> 与问题中的<code>geographical</code>, <code>cites</code>, <code>state</code> 相关性都比较大.</p><h4 id="How-to-Construct-Queries"><a href="#How-to-Construct-Queries" class="headerlink" title="How to Construct Queries"></a>How to Construct Queries</h4><p>问题构建的方式十分重要, 作者尝试了多种构建的问题的方法, 以Tag <code>ORG</code> 为例:</p><ol><li><strong>Position index of labels</strong>: 问题直接由Tag的索引构成, 例如”one”, “two”, “three”.</li><li><strong>Keyword</strong>: 使用Tag描述中的关键词, 如”organization”.</li><li><strong>Rule - based template filling</strong>: 用模板生成问题, 例如”which organization is mentioned in the text”.</li><li><strong>Wikipedia</strong>: 使用维基百科中对Tag的定义, 例如”an organization is an entity comprising multiple people, such as an institution or an association”.</li><li><strong>Synonyms</strong>: 使用牛津词典中找到的Tag的同义词, 例如”association”.</li><li><strong>Keyword + Synonyms</strong>: 将关键词和同义词拼接起来.</li><li><strong>Annotation guideline notes</strong>: 本文的方法, 例如”find organizations including companies, agencies and institutions”.</li></ol><p>上述方法在English OntoNotes 5.0 上的结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mrc4ner6.jpg" style="zoom: 60%;" /><p>只使用索引相当于添加噪声, 反而有负影响. 其他方法都能带来一定程度的信息提示作用, 作者所使用的方法效果最佳.</p><h4 id="Zero-shot-Evaluation-on-Unseen-Labels"><a href="#Zero-shot-Evaluation-on-Unseen-Labels" class="headerlink" title="Zero - shot Evaluation on Unseen Labels"></a>Zero - shot Evaluation on Unseen Labels</h4><p>为测试模型的Zero - shot能力, 作者在CoNLL 2003上训练, 在OntoNotes 5.0上做了测试, 结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mrc4ner8.jpg" style="zoom: 50%;" /><p>BERT - tagger在迁移后所剩的F1仅为31.87%, 与BERT - MRC相差甚远, 证明此框架有一定泛化能力.</p><h4 id="Size-of-Training-Data"><a href="#Size-of-Training-Data" class="headerlink" title="Size of Training Data"></a>Size of Training Data</h4><p>为证明Query中注入先验知识的重要性, 作者在OntoNotes 4.0上做了使用不同数据对模型影响的实验:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mrc4ner9.jpg" style="zoom: 67%;" /><p>BERT - MRC在使用50%数据时已经能达到BERT - Tagger使用100%数据的性能. </p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>作者从<strong>抽取式MRC</strong>的角度, 使用<strong>边界模型</strong>, <strong>同时解决</strong>了<strong>Flat NER</strong>和<strong>Nested NER</strong>问题, 并在实验部分强调了Query注入先验知识对模型的影响.</p><p>但伴随这种Tagging框架的<strong>稀疏问题</strong>也不能忽视, 只能用一些小Trick去缓解, 但不能从根本上解决问题.</p><blockquote><p>虽然本文是一篇面向NER的论文, 但实际上, 在RE与NER相结合的任务, <strong>关系三元组抽取</strong>(Relational Triple Extraction, RTE)中, 有相当深远的影响.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NER </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HypE: 知识图谱超图补全</title>
      <link href="/posts/21119.html"/>
      <url>/posts/21119.html</url>
      
        <content type="html"><![CDATA[<h1 id="Knowledge-Hypergraphs-Prediction-Beyond-Binary-Relations"><a href="#Knowledge-Hypergraphs-Prediction-Beyond-Binary-Relations" class="headerlink" title="Knowledge Hypergraphs: Prediction Beyond Binary Relations"></a>Knowledge Hypergraphs: Prediction Beyond Binary Relations</h1><p>本文是论<a href="https://www.aminer.cn/pub/5ef96b048806af6ef2772134?conf=ijcai2020?f=zh" target="_blank" rel="noopener">Knowledge Hypergraphs: Prediction Beyond Binary Relations</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>现有的知识图谱均以三元组的形式被存储, 即默认地将所有的关系以二元的形式表示. 但事实上, 在广为人知的FreeBase中, 有<strong>61%</strong>的关系都是<strong>非二元</strong>关系. </p><p>虽然现在有将多元关系转化为二元关系表示的方法, 但作者认为现有的KGE方法通过二元转换后做链接预测的效果都不是很好, 因为<strong>所有关系都被强假设为二元</strong>.</p><p>例如下图中描述的<code>flies_between</code>关系:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/hype1.jpg" style="zoom:50%;" /><p>被现有的多元关系转二元关系技术分解为如下形式:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/hype2.jpg" style="zoom:50%;" /><p>左侧转换并没有损失信息, 但添加了三种额外的实体. </p><p>右侧转换可能存在歧义, 转换后以二元关系的形式存在, <code>Air Canada</code>, <code>Montreal</code>, <code>Los Angeles</code>, 之间存在关系<code>flies_between</code>是合理的, 但单独看<code>Air Canada</code>, <code>New York</code>, <code>Los Angeles</code>也存在关系<code>flies_between</code>. 参考原图, <code>Air Canada</code>不可能从<code>New York</code>飞到<code>Los Angeles</code>, 但</p><p>因此, 作者希望提出基于<strong>超图</strong>的<strong>多元关系</strong>Knowledge Embedding方法.</p><h2 id="Hypergraph"><a href="#Hypergraph" class="headerlink" title="Hypergraph"></a>Hypergraph</h2><p>首先来介绍一下超图.</p><p>超图被描述为$H=(X, E)$, $X$ 是一个有限集合, 其中的元素被称为节点或顶点, $E$ 为$X$​ 的非空子集, 被称为超边或连接.</p><p>下面展示一个例子:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/hypergraph.png" style="zoom:67%;" /><blockquote><p>图片出自<a href="https://zh.wikipedia.org/wiki/%E8%B6%85%E5%9B%BE" target="_blank" rel="noopener">维基百科</a>.</p></blockquote><p>在该超图中, 有:<br>$$<br>\begin{aligned}<br>X&amp;=\{v_1, v_2, v_3, v_4, v_5, v_6, v_7\} \\\ \\<br>E&amp;=\{e_1, e_2, e_3, e_4\}\\<br>&amp;=\{\{v_1, v_2, v_3\}, \{v_2, v_3\}, \{v_3, v_5, v_6\}, \{v_4\}\}<br>\end{aligned}<br>$$<br>对于我们所熟知的图而言, 边只能与两个顶点相连. 在超图的体系下, 边称之为超边, 能够与任意个数的顶点相连, 普通图只是在超图中的一种每条边节点数量均为2的特殊情况, 因此, 超图理论与普通图是兼容的.</p><p>而Hypergraph本身结构的<strong>高度灵活</strong>性, 对于相同的现实生活中的场景, 可能有不同的描述方法和建模方法.</p><h3 id="Knowledge-HyperGraph-Completion"><a href="#Knowledge-HyperGraph-Completion" class="headerlink" title="Knowledge HyperGraph Completion"></a>Knowledge HyperGraph Completion</h3><p>如何把Knowledge Graph Completion扩展到超图中? </p><p>现实世界的实体集为$\mathcal{E}$​​, 有限关系集为$\mathcal{R}$​​, 区别在于超图中的元组不再是三元组, 而是可以容纳更多实体的元组$\tau = r(e_1, e_2, \dots, e_k), r\in \mathcal{R}, e_i \in \mathcal{E}$​​​. ​</p><p>在超图中, 每条超边就是一个$\tau$​​​, $|r|$​​ 为关系$r$​​​​ 所对应的<strong>实体数量</strong>, 可以设置为固定的.</p><p>我们所构建的知识超图应该是真实世界元组$\tau$ 的一个<strong>子集</strong>$\tau ^ \prime \subseteq \tau$, Knowledge HyperGraph Completion的目标就是利用$\tau ^ \prime$ 找到缺失的真实知识$\tau \backslash \tau^\prime$​.</p><h2 id="HypE"><a href="#HypE" class="headerlink" title="HypE"></a>HypE</h2><p>作者先定义了$\odot(\cdot)$ 为向量的逐元素点乘求和(其实就是向量<strong>内积</strong>):</p><p>$$<br>\odot\left(\mathbf{v}_{\mathbf{1}}, \mathbf{v}_{\mathbf{2}}, \ldots, \mathbf{v}_{\mathbf{k}}\right)=\sum_{i=1}^{\ell} \mathbf{v}_{\mathbf{1}}{ }^{(i)} \mathbf{v}_{\mathbf{2}}{ }^{(i)} \ldots \mathbf{v}_{\mathbf{k}}^{(i)}<br>$$</p><p>$\mathbf{v}_j^{(i)}$ 为$\mathbf{v}_j$ 的第$i$ 个元素.</p><h3 id="HSimplE"><a href="#HSimplE" class="headerlink" title="HSimplE"></a>HSimplE</h3><p>在SimplE中, 通过学习到实体$e$​ 分别在头实体和尾实体的两个不同位置的嵌入$\mathbf{e}^{(1)},  \mathbf{e}^{(2)}$​, 和关系及其逆关系的嵌入$\mathbf{r}^{(1)}, \mathbf{r}^{(2)}$​ 来求得三元组是否为真的得分:</p><p>$$<br>\phi\left(r\left(e_{1}, e_{2}\right)\right)=\odot\left(\mathbf{r}^{(\mathbf{1})}, \mathbf{e}_{\mathbf{1}}^{(\mathbf{1})}, \mathbf{e}_{\mathbf{2}}^{(\mathbf{2})}\right)+\odot\left(\mathbf{r}^{(\mathbf{2})}, \mathbf{e}_{\mathbf{2}}^{(\mathbf{1})}, \mathbf{e}_{\mathbf{1}}^{(\mathbf{2})}\right)<br>$$</p><p>作者认为, SimplE的核心在于将<strong>三元组形式</strong>中实体可能位于任何位置的表示方式都考虑到了, 作者在这种启发下将SimplE扩展到了超图上.</p><p>在<strong>超图</strong>中, 超边能连接任意数量的顶点, 我们不可能将每个实体在每个位置上都单独学习一个Embedding, 所以我们应该用一个单独的向量来代替, 而不是像SimplE一样使用多个向量. </p><blockquote><p>作者认为, 只使用单个$\mathbf{e}$ 能够被视为$\alpha$ 个不同位置出现的实体嵌入的拼接, 例如$\mathbf{e}=\operatorname{concat}(\mathbf{e}^{(\mathbf{1})} + \mathbf{e}^{(\mathbf{2})})$​.</p><p>我认为只要<strong>保证同实体在不同位置上的表示足够不同</strong>, 就能使用单个$\mathbf{e}$.</p></blockquote><p>作者使用了一个迂回的方法, 既然不能学习所有位置, 就简单的通过<strong>平移操作</strong>来改变不同位置上同一实体的表示. 每个实体的表示因位置不同而产生变化, 那么在元组$\tau$ 中, 打分函数为:</p><p>$$<br>\begin{aligned}<br>\phi\left(r\left(e_{i}, e_{j}, \ldots, e_{k}\right)\right) &amp;=\odot\left(\mathbf{r}, \mathbf{e}_{\mathbf{i}}, \operatorname{shift}\left(\mathbf{e}_{\mathbf{j}},<br>\operatorname{len}\left(\mathbf{e}_{\mathbf{j}}\right)\cdot\frac{1} {\alpha}\right), \ldots,\right.<br>\left.\left.\operatorname{shift}\left(\mathbf{e}_{\mathbf{k}}, \operatorname{len}\left(\mathbf{e}_{\mathbf{k}}\right) \cdot\frac{(\alpha-1)} { \alpha}\right)\right)\right)<br>\end{aligned}<br>$$</p><p>其中$\alpha=|r|$​ , $\text{shift}(\mathbf{v}, x)$​ 代表将$\mathbf{v}$​ 向左平移$x$​​​ 个单位, 并将多余的部分补到右侧(参考<a href="https://github.com/baharefatemi/HypE" target="_blank" rel="noopener">源码</a>).</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/hype3.jpg" style="zoom:50%;" /><h3 id="HypE-1"><a href="#HypE-1" class="headerlink" title="HypE"></a>HypE</h3><p> HSimplE的平移操作有点太简单了, 把HSimplE的Shift操作换成了<strong>卷积</strong>就是HypE.</p><p>对于实体$e$ 在关系$r$ 中可能存在的每个位置$i$, 都有卷积核$\omega$​ 对实体的Embedding $\mathbf{e}$​​  提取特征:<br>$$<br>f(\mathbf{e}, i)=\operatorname{concat}\left(\mathbf{e} \ast \omega_{\mathrm{i} 1}, \ldots, \mathbf{e} \ast \omega_{\mathrm{in}}\right) \mathrm{P}<br>$$<br>其中$P$ 为投影矩阵. $n$ 为卷积核个数, 即在位置$i$ 上有$n$​​​ 个不同的卷积核提取特征, 再将它们拼接到一起, 最后投影回某个维度:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/hype4.jpg" style="zoom:50%;"/><p>和HSimplE相似, 把关系嵌入, 不同位置上的不同实体嵌入在一起点乘, 作为元组得分:</p><p>$$<br>\phi\left(r\left(e_{1}, \ldots, e_{|r|}\right)\right)=\odot\left(\mathbf{r}, f\left(\mathbf{e}_{\mathbf{1}}, 1\right), \ldots, f\left(\mathbf{e}_{|\mathbf{r}|},|r|\right)\right)<br>$$</p><p>即:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/hype5.jpg" style="zoom: 50%;" /><p>作者认为, 使用位置特化的卷积核好处有二:</p><ol><li>由于实体Embedding的位置信息是由卷积核额外添加进去的, 而非包含于实体Embedding本身, 所以更利于实体Embedding变得与位置无关.</li><li>位置和实体的分离使得HypE能作用于任意数量实体的Knowledge base, 这额外给予了HypE更多的鲁棒性, 即使遇到从未见过的实体, 也能有点用处.</li></ol><h3 id="Objective-Function-and-Training"><a href="#Objective-Function-and-Training" class="headerlink" title="Objective Function and Training"></a>Objective Function and Training</h3><p>HSimplE和HypE都使用小批量梯度下降训练.</p><p>负例的生成是由TransE负例生成的思路扩展到超图而来, 对于每个元组, 生成$N|r|$ 个负例, $N$ 为负样本生成率(超参).</p><p>损失函数采用交叉熵(实际上应该是温度为1的InfoNCE, 但原文写的CE, 这里暂时尊重一下原文), 最大化正例元组的打分, 最小化负例元组的打分:</p><p>$$<br>\mathcal{L}(\mathbf{r}, \mathbf{e})=\sum_{x^{\prime} \in \tau_{\text {train }}^{\prime}}-\log \left(\frac{e^{\phi\left(x^{\prime}\right)}}{e^{\phi\left(x^{\prime}\right)}+\sum_{x \in T_{n e g}\left(x^{\prime}\right)} e^{\phi(x)}}\right)<br>$$</p><p>$x$ 为所有训练集, 验证集, 测试集元组$\tau^\prime$ 的某个元组, $\tau ^ \prime_\text{train}$ 为训练集, $T_{neg}(x^\prime)$​ 为生成的负例. </p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文.</p><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>JF - 17K是前人提出的数据集, 但没有验证集, 作者随机选了20%作为验证集.</p><p>此外, 作者从FreeBase中创建了新的数据集FB - AUTO, M - FB15K. 详细的数据集创建请参考原论文附录部分. 它们的统计信息如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/hype7.jpg" style="zoom:50%;" /><h3 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h3><p>因为在超图补全上的模型很少, 所以需要抓一些之前的方法扩展到超图情况:</p><ul><li>对于处理二元关系的方法, 作者将其扩展到更多实体的情况, 如r - SimplE, m - DistMult, m - CP. </li><li>也有一些不用扩展的方法, 能直接处理更多实体, 如m - TransH.</li></ul><h3 id="Knowledge-HyperGraph-Completion-Results"><a href="#Knowledge-HyperGraph-Completion-Results" class="headerlink" title="Knowledge HyperGraph Completion Results"></a>Knowledge HyperGraph Completion Results</h3><p>HypE在这几个超图补全数据集上结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/hype6.jpg" style="zoom:50%;" /><p>HypE相较于其他方法有显著提升.</p><p>因为位置特化的卷积核能将实体本身的信息和位置信息解耦, 所以作者认为HypE应该具有更高的<strong>参数利用率</strong>. 作者做了MRR随Embedding Dimensionss变化的曲线, 结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/hype8.jpg" style="zoom: 67%;" /><p>HypE在200维度以下时, 是完全优于Baseline HSimplE的.</p><p>HypE使用了位置特化的卷积核, 应该能处理更多实体出现在不同位置上的情况, 作者创建一个<strong>缺失位置</strong>的测试集, 其中的每个元组至少有一个实体在某个位置没有出现过, 这种情况更具挑战性. 与其他方法对比结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/hype9.jpg" style="zoom: 67%;" /><p>其他方法在缺失位置信息的情况下表现很差, 相较来说HypE确实能处理更多这种挑战性的情况, 也表明HypE确实受益于位置和实体信息解耦的设计.</p><h3 id="Knowledge-Graph-Completion-Results"><a href="#Knowledge-Graph-Completion-Results" class="headerlink" title="Knowledge Graph Completion Results"></a>Knowledge Graph Completion Results</h3><p>在现有的三元组形式数据集WN18和FB15k上结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/hype10.jpg" style="zoom:50%;" /><p>HSimplE取得了相当好的表现, HypE也与其相似. </p><blockquote><p>由于这两个数据集上包含大量的逆关系导致Data Leakage存在, 使用WN18RR和FB15k-237更有说服力. 但这个实验说明面向超图的任务优化有希望提升三元组形式的数据集链接预测性能.</p></blockquote><h3 id="Ablation-Study-on-Different-Arities"><a href="#Ablation-Study-on-Different-Arities" class="headerlink" title="Ablation Study on Different Arities"></a>Ablation Study on Different Arities</h3><p>作者还在JF17K上将超边设置为不同的实体数量, 结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/hype11.jpg" style="zoom:50%;" /><p>超边实体数量比较多时, HSimplE表现相较于HypE来说并不好, 超边实体数量较少时, HSimplE效果比HypE稍好. 总体上来说作者提出的两种方法效果略好.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>在知识超图上的研究还比较少, 本文提出了在超图架构下的两个新数据集, 并给出了将处理三元组KGE问题迁移到超图框架下的方法.</p><p>其实与某些图游走方法的想法有一些相似之处, 显式指明多实体之间存在的关系, 而非只考虑两实体, 这些额外的附加信息可以帮助推理. 但图游走类算法更加随意, 在知识超图上的游走仅限于同关系下的实体集.</p><p>总感觉这篇论文有哪块没理解, 尤其是附录, 有机会回看.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch实现: VAE</title>
      <link href="/posts/9047.html"/>
      <url>/posts/9047.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>VAE基本原理: 详见<a href="https://adaning.github.io/posts/53598.html">变分自编码器入门</a>.</li></ul></blockquote><p>本文是VAE的Pytorch版本实现, 并在末尾做了VAE的生成可视化.</p><p>本文的代码已经放到了Colab上, 打开设置GPU就可以复现(需要科学上网).</p><p><a href="https://colab.research.google.com/drive/1ZhmA2XxJ3oZC7A-U2mpUdB2eZZLz5NfW?usp=sharing" target="_blank" rel="noopener"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="右键我在COLAB中打开!"></a></p><p>如果你不能科学上网, 应该看不到<code>Open in Colab</code>的图标.</p><h2 id="Preparing"><a href="#Preparing" class="headerlink" title="Preparing"></a>Preparing</h2><p>先导包:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">from</span> torch <span class="token keyword">import</span> optim<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> transforms<span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> MNIST<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h2><p>CNN在MNIST上有过于明显的优势, 我们只采用纯DNN来做Auto Encoder.</p><p>随手搞一个网络结构出来就行:</p><ul><li>输入层维度: <code>input_dim = 784</code>.</li><li>过渡层维度: <code>inter_dim = 256</code>.</li><li>隐变量维度: <code>latent_dim = 2</code>, 方便后续可视化.</li></ul><pre class="line-numbers language-python"><code class="language-python">latent_dim <span class="token operator">=</span> <span class="token number">2</span>input_dim <span class="token operator">=</span> <span class="token number">28</span> <span class="token operator">*</span> <span class="token number">28</span>inter_dim <span class="token operator">=</span> <span class="token number">256</span><span class="token keyword">class</span> <span class="token class-name">VAE</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_dim<span class="token operator">=</span>input_dim<span class="token punctuation">,</span> inter_dim<span class="token operator">=</span>inter_dim<span class="token punctuation">,</span> latent_dim<span class="token operator">=</span>latent_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>VAE<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        elf<span class="token punctuation">.</span>encoder <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> inter_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>inter_dim<span class="token punctuation">,</span> latent_dim <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span>  nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>latent_dim<span class="token punctuation">,</span> inter_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>inter_dim<span class="token punctuation">,</span> input_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">reparameterise</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> mu<span class="token punctuation">,</span> logvar<span class="token punctuation">)</span><span class="token punctuation">:</span>        epsilon <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn_like<span class="token punctuation">(</span>mu<span class="token punctuation">)</span>        <span class="token keyword">return</span> mu <span class="token operator">+</span> epsilon <span class="token operator">*</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>logvar <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        org_size <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>        batch <span class="token operator">=</span> org_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        h <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        mu<span class="token punctuation">,</span> logvar <span class="token operator">=</span> h<span class="token punctuation">.</span>chunk<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        z <span class="token operator">=</span> self<span class="token punctuation">.</span>reparameterise<span class="token punctuation">(</span>mu<span class="token punctuation">,</span> logvar<span class="token punctuation">)</span>        recon_x <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>z<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>size<span class="token operator">=</span>org_size<span class="token punctuation">)</span>        <span class="token keyword">return</span> recon_x<span class="token punctuation">,</span> mu<span class="token punctuation">,</span> logvar<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>说一点细节: </p><ul><li><p>Encoder和Decoder用<code>nn.Sequential</code>的形式写, 方便后续直接使用decoder.</p></li><li><p>$p(Z\mid X_k)$ 的均值$\mu$ 和方差$\sigma^2$ 的形式上可以拆成两个小的DNN得出, 这里用一个DNN得出, 然后通过<code>torch.chunk</code>函数将均值和方差分开, 实际上是和前者等价的.</p></li><li><p>Encoder末尾千万别像网上某些例子在再接一个ReLU. 在优化过程中, 我们的隐变量$Z$ 是要逐渐趋向于$\mathcal{N}(0, I)$ 的, 如果非要加个ReLU的话, 本身假设的隐变量维度就很小, 小于0的隐变量直接就没了… Decoder在解码时直接就会因为信息不足而崩掉.</p></li><li><p>我们在这里拟合的是$\log \sigma^2$ 而不是$\sigma^2$, 所以重参数方差的表示法是<code>torch.exp(logvar / 2)</code>.</p></li></ul><h2 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h2><p>VAE的损失由<strong>重构损失</strong>和<strong>KL损失</strong>组成.</p><p>KL散度就不再推导了, 直接放结果:<br>$$<br>KL\Big(N(\mu,\sigma^2)\Big\Vert N(0,1)\Big)=\frac{1}{2}\Big(-\log \sigma^2+\mu^2+\sigma^2-1\Big)<br>$$<br>VAE的目标是<strong>最小化</strong>$Z$ 和$N(0, 1)$ 之间的KL散度, 代码只需要照着写就行了:</p><pre class="line-numbers language-python"><code class="language-python">kl_loss <span class="token operator">=</span> <span class="token keyword">lambda</span> mu<span class="token punctuation">,</span> logvar<span class="token punctuation">:</span> <span class="token operator">-</span><span class="token number">0.5</span> <span class="token operator">*</span> torch<span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> logvar <span class="token operator">-</span> mu<span class="token punctuation">.</span>pow<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">-</span> logvar<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>recon_loss <span class="token operator">=</span> <span class="token keyword">lambda</span> recon_x<span class="token punctuation">,</span> x<span class="token punctuation">:</span> F<span class="token punctuation">.</span>binary_cross_entropy<span class="token punctuation">(</span>recon_x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> size_average<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>因为MNIST是<strong>黑白二值图像</strong>, 所以的Decoder就可以用Sigmoid后的值当做灰度, 重构损失直接就用<strong>BCE</strong>了, 用MSE做重构损失尚可. 但如果是三通道图像或者是灰度图像, 还是必须使用MSE做重构损失.</p><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>先定义好训练的<code>epoch</code>和<code>batch_size</code>, 优化器随便选一个世界上最好的优化器<code>Adam(lr=1e-3)</code>:</p><pre class="line-numbers language-python"><code class="language-python">epochs <span class="token operator">=</span> <span class="token number">100</span>batch_size <span class="token operator">=</span> <span class="token number">128</span>transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>data_train <span class="token operator">=</span> MNIST<span class="token punctuation">(</span><span class="token string">'MNIST_DATA/'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>data_valid <span class="token operator">=</span> MNIST<span class="token punctuation">(</span><span class="token string">'MNIST_DATA/'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>train_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>data_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>test_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>data_valid<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">'cpu'</span><span class="token punctuation">)</span>model <span class="token operator">=</span> VAE<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> inter_dim<span class="token punctuation">,</span> latent_dim<span class="token punctuation">)</span>model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>训练的代码就不详细说了, 和一般的训练过程并无二异, 每次测试时最好把损失的两项都打印出来观察一下:</p><pre class="line-numbers language-python"><code class="language-python">best_loss <span class="token operator">=</span> <span class="token number">1e9</span>best_epoch <span class="token operator">=</span> <span class="token number">0</span>valid_losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>train_losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"Epoch {epoch}"</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>    train_loss <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>    train_num <span class="token operator">=</span> len<span class="token punctuation">(</span>train_loader<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span>    <span class="token keyword">for</span> idx<span class="token punctuation">,</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span> _<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>        batch <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        recon_x<span class="token punctuation">,</span> mu<span class="token punctuation">,</span> logvar <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        recon <span class="token operator">=</span> recon_loss<span class="token punctuation">(</span>recon_x<span class="token punctuation">,</span> x<span class="token punctuation">)</span>        kl <span class="token operator">=</span> kl_loss<span class="token punctuation">(</span>mu<span class="token punctuation">,</span> logvar<span class="token punctuation">)</span>        loss <span class="token operator">=</span> recon <span class="token operator">+</span> kl        train_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss <span class="token operator">=</span> loss <span class="token operator">/</span> batch        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> idx <span class="token operator">%</span> <span class="token number">100</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"Training loss {loss: .3f} \t Recon {recon / batch: .3f} \t KL {kl / batch: .3f} in Step {idx}"</span><span class="token punctuation">)</span>    train_losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>train_loss <span class="token operator">/</span> train_num<span class="token punctuation">)</span>    valid_loss <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>    valid_recon <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>    valid_kl <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>    valid_num <span class="token operator">=</span> len<span class="token punctuation">(</span>test_loader<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span>    model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> idx<span class="token punctuation">,</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span> _<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>test_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>            x <span class="token operator">=</span> x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>            recon_x<span class="token punctuation">,</span> mu<span class="token punctuation">,</span> logvar <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>            recon <span class="token operator">=</span> recon_loss<span class="token punctuation">(</span>recon_x<span class="token punctuation">,</span> x<span class="token punctuation">)</span>            kl <span class="token operator">=</span> kl_loss<span class="token punctuation">(</span>mu<span class="token punctuation">,</span> logvar<span class="token punctuation">)</span>            loss <span class="token operator">=</span> recon <span class="token operator">+</span> kl            valid_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            valid_kl <span class="token operator">+=</span> kl<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            valid_recon <span class="token operator">+=</span> recon<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        valid_losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>valid_loss <span class="token operator">/</span> valid_num<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"Valid loss {valid_loss / valid_num: .3f} \t Recon {valid_recon / valid_num: .3f} \t KL {valid_kl / valid_num: .3f} in epoch {epoch}"</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> valid_loss <span class="token operator">&lt;</span> best_loss<span class="token punctuation">:</span>            best_loss <span class="token operator">=</span> valid_loss            best_epoch <span class="token operator">=</span> epoch            torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'best_model_mnist'</span><span class="token punctuation">)</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Model saved"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>觉得Loss位数保留太多的可以自己设置.</p><p>下面画出训练过程中训练集和验证集上的损失曲线:</p><pre class="line-numbers language-python"><code class="language-python">plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>train_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Train'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>valid_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Valid'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Learning Curve'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>训练曲线如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/vae11.png" style="zoom: 67%;" /><p>基本上Valid Loss稳定了(其实还有下降空间). 同时要保存在验证集上结果最好的模型, 因为等会还要用最好的模型做生成.</p><h2 id="Visualization"><a href="#Visualization" class="headerlink" title="Visualization"></a>Visualization</h2><p>再导俩包:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> scipy<span class="token punctuation">.</span>stats <span class="token keyword">import</span> norm<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><code>norm</code>可以在隐变量的区域内按照正态分布采样.</p><pre class="line-numbers language-python"><code class="language-python">state <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'best_model_mnist'</span><span class="token punctuation">)</span>model <span class="token operator">=</span> VAE<span class="token punctuation">(</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>state<span class="token punctuation">)</span>n <span class="token operator">=</span> <span class="token number">20</span>digit_size <span class="token operator">=</span> <span class="token number">28</span>grid_x <span class="token operator">=</span> norm<span class="token punctuation">.</span>ppf<span class="token punctuation">(</span>np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0.05</span><span class="token punctuation">,</span> <span class="token number">0.95</span><span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>grid_y <span class="token operator">=</span> norm<span class="token punctuation">.</span>ppf<span class="token punctuation">(</span>np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0.05</span><span class="token punctuation">,</span> <span class="token number">0.95</span><span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>figure <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>digit_size <span class="token operator">*</span> n<span class="token punctuation">,</span> digit_size <span class="token operator">*</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">for</span> i<span class="token punctuation">,</span> yi <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>grid_y<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> j<span class="token punctuation">,</span> xi <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>grid_x<span class="token punctuation">)</span><span class="token punctuation">:</span>        t <span class="token operator">=</span> <span class="token punctuation">[</span>xi<span class="token punctuation">,</span> yi<span class="token punctuation">]</span>        z_sampled <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span>t<span class="token punctuation">)</span>        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            decode <span class="token operator">=</span> model<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>z_sampled<span class="token punctuation">)</span>            digit <span class="token operator">=</span> decode<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token punctuation">(</span>digit_size<span class="token punctuation">,</span> digit_size<span class="token punctuation">)</span><span class="token punctuation">)</span>            figure<span class="token punctuation">[</span>                i <span class="token operator">*</span> digit_size<span class="token punctuation">:</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> digit_size<span class="token punctuation">,</span>                j <span class="token operator">*</span> digit_size<span class="token punctuation">:</span> <span class="token punctuation">(</span>j <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> digit_size            <span class="token punctuation">]</span> <span class="token operator">=</span> digitplt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>figure<span class="token punctuation">,</span> cmap<span class="token operator">=</span><span class="token string">"Greys_r"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xticks<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>yticks<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>axis<span class="token punctuation">(</span><span class="token string">'off'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>采样到$Z$ 后再给Decoder解码, 生成的结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/vae12.png" style="zoom: 80%;" />对于MNIST这样简单的数据集, 隐变量的某些区域已经能完成生成任务. 并且可以从图中观察到随着隐变量的变化对生成结果产生的影响.<p>从图中能够很明确的看到手写数字种类的过渡, 例如长的比较像的1, 9, 7, 都带圆弧的8, 3, 5, 再到6, 0. 但是VAE生成的内容有点点糊, 在MNSIT上影响不大, 但扩展到三通道数据时, 这个问题会变得更为显著.</p><h2 id="Pokemon"><a href="#Pokemon" class="headerlink" title="Pokemon!"></a>Pokemon!</h2><p><del>每个人都想做从零开始的宝可梦训练大师!</del> 在李宏毅老师的课程中层提到过用VAE生成神奇宝贝的事情. 下面就来尝试下. 数据集下载<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Pokemon_creation/image.rar" target="_blank" rel="noopener">点我</a>, 原始数据大小为(3, 40, 40). 本节代码没有放到Colab上, 与在MNIST上的过程大同小异, 感兴趣可以自己尝试.</p><p>这次的VAE就该用CNN了, DNN有点力不从心.</p><p>下述代码不做过多的解读, 结果也不是太好, 大家就当看个乐子.</p><p>导包:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> os<span class="token keyword">import</span> random<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> torch <span class="token keyword">import</span> optim<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader<span class="token punctuation">,</span> Dataset<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> transforms<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这次涉及到建立和读取图像数据集, 所以额外导了一些包.</p><p>然后建立图像数据集, 因为是无监督数据集, 所以比较简单:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Pokemon</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> root<span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Pokemon<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>root <span class="token operator">=</span> root        self<span class="token punctuation">.</span>image_path <span class="token operator">=</span> <span class="token punctuation">[</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>root<span class="token punctuation">,</span> x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> os<span class="token punctuation">.</span>listdir<span class="token punctuation">(</span>root<span class="token punctuation">)</span><span class="token punctuation">]</span>        random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>self<span class="token punctuation">.</span>image_path<span class="token punctuation">)</span>        <span class="token keyword">if</span> transform <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>transform <span class="token operator">=</span> transform        <span class="token keyword">if</span> train<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>images <span class="token operator">=</span> self<span class="token punctuation">.</span>image_path<span class="token punctuation">[</span><span class="token punctuation">:</span> int<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">8</span> <span class="token operator">*</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>image_path<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>images <span class="token operator">=</span> self<span class="token punctuation">.</span>image_path<span class="token punctuation">[</span>int<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">8</span> <span class="token operator">*</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>image_path<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token punctuation">]</span>    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>images<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>self<span class="token punctuation">.</span>images<span class="token punctuation">[</span>item<span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>数据集中实际上存储的是图像文件的路径, 在需要使用的时候再读出来, 我们将这一Pipeline集成在<code>transform</code>中.</p><p>接着定义CNN下的VAE:</p><pre class="line-numbers language-python"><code class="language-python">latent_dim <span class="token operator">=</span> <span class="token number">32</span>inter_dim <span class="token operator">=</span> <span class="token number">128</span>mid_dim <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>mid_num <span class="token operator">=</span> <span class="token number">1</span><span class="token keyword">for</span> i <span class="token keyword">in</span> mid_dim<span class="token punctuation">:</span>    mid_num <span class="token operator">*=</span> i<span class="token keyword">class</span> <span class="token class-name">ConvVAE</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> latent<span class="token operator">=</span>latent_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>ConvVAE<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>mid_num<span class="token punctuation">,</span> inter_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>inter_dim<span class="token punctuation">,</span> latent <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fcr2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>latent<span class="token punctuation">,</span> inter_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fcr1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>inter_dim<span class="token punctuation">,</span> mid_num<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">reparameterise</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> mu<span class="token punctuation">,</span> logvar<span class="token punctuation">)</span><span class="token punctuation">:</span>        epsilon <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn_like<span class="token punctuation">(</span>mu<span class="token punctuation">)</span>        <span class="token keyword">return</span> mu <span class="token operator">+</span> epsilon <span class="token operator">*</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>logvar <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        batch <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        h <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        mu<span class="token punctuation">,</span> logvar <span class="token operator">=</span> h<span class="token punctuation">.</span>chunk<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        z <span class="token operator">=</span> self<span class="token punctuation">.</span>reparameterise<span class="token punctuation">(</span>mu<span class="token punctuation">,</span> logvar<span class="token punctuation">)</span>        decode <span class="token operator">=</span> self<span class="token punctuation">.</span>fcr2<span class="token punctuation">(</span>z<span class="token punctuation">)</span>        decode <span class="token operator">=</span> self<span class="token punctuation">.</span>fcr1<span class="token punctuation">(</span>decode<span class="token punctuation">)</span>        recon_x <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>decode<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token operator">*</span>mid_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> recon_x<span class="token punctuation">,</span> mu<span class="token punctuation">,</span> logvar<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>结构很随意, 主要是为了满足输入和解码后的大小相同.</p><p>定义Loss, 仍然是重构损失和KL损失:</p><pre><code>kl_loss = lambda mu, logvar: -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())recon_loss = lambda recon_x, x: F.mse_loss(recon_x, x, size_average=False)</code></pre><p>重构损失使用MSE, 不能再使用BCE了, 因为RGB图像的数值不是二值的.</p><p>接下来是训练前的一些定义:</p><pre class="line-numbers language-python"><code class="language-python">epochs <span class="token operator">=</span> <span class="token number">2000</span>batch_size <span class="token operator">=</span> <span class="token number">512</span>best_loss <span class="token operator">=</span> <span class="token number">1e9</span>best_epoch <span class="token operator">=</span> <span class="token number">0</span>valid_losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>train_losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>    <span class="token keyword">lambda</span> x<span class="token punctuation">:</span> Image<span class="token punctuation">.</span>open<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>convert<span class="token punctuation">(</span><span class="token string">'RGB'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">]</span><span class="token punctuation">)</span>pokemon_train <span class="token operator">=</span> Pokemon<span class="token punctuation">(</span><span class="token string">'./pokemon/'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>pokemon_valid <span class="token operator">=</span> Pokemon<span class="token punctuation">(</span><span class="token string">'./pokemon/'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>train_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>pokemon_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>test_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>pokemon_valid<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">'cpu'</span><span class="token punctuation">)</span>model <span class="token operator">=</span> ConvVAE<span class="token punctuation">(</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>transform</code>将图像从路径中读取出来, 并通过<code>transforms.ToTensor</code>转换为<strong>0, 1之间</strong>的RGB值.</p><p>然后就开始训练, 和在MNIST上的代码相同:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"Epoch {epoch}"</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>    train_loss <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>    train_num <span class="token operator">=</span> len<span class="token punctuation">(</span>train_loader<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span>    <span class="token keyword">for</span> idx<span class="token punctuation">,</span> x <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>        batch <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        recon_x<span class="token punctuation">,</span> mu<span class="token punctuation">,</span> logvar <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        recon <span class="token operator">=</span> recon_loss<span class="token punctuation">(</span>recon_x<span class="token punctuation">,</span> x<span class="token punctuation">)</span>        kl <span class="token operator">=</span> kl_loss<span class="token punctuation">(</span>mu<span class="token punctuation">,</span> logvar<span class="token punctuation">)</span>        loss <span class="token operator">=</span> recon <span class="token operator">+</span> kl        train_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss <span class="token operator">=</span> loss <span class="token operator">/</span> batch        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> idx <span class="token operator">%</span> <span class="token number">100</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"Training loss {loss: .3f} \t Recon {recon / batch: .3f} \t KL {kl / batch: .3f} in Step {idx}"</span><span class="token punctuation">)</span>    train_losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>train_loss <span class="token operator">/</span> train_num<span class="token punctuation">)</span>    valid_loss <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>    valid_recon <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>    valid_kl <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>    valid_num <span class="token operator">=</span> len<span class="token punctuation">(</span>test_loader<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span>    model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> idx<span class="token punctuation">,</span> x <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>test_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>            x <span class="token operator">=</span> x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>            recon_x<span class="token punctuation">,</span> mu<span class="token punctuation">,</span> logvar <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>            recon <span class="token operator">=</span> recon_loss<span class="token punctuation">(</span>recon_x<span class="token punctuation">,</span> x<span class="token punctuation">)</span>            kl <span class="token operator">=</span> kl_loss<span class="token punctuation">(</span>mu<span class="token punctuation">,</span> logvar<span class="token punctuation">)</span>            loss <span class="token operator">=</span> recon <span class="token operator">+</span> kl            valid_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            valid_kl <span class="token operator">+=</span> kl<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            valid_recon <span class="token operator">+=</span> recon<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        valid_losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>valid_loss <span class="token operator">/</span> valid_num<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>            f<span class="token string">"Valid loss {valid_loss / valid_num: .3f} \t Recon {valid_recon / valid_num: .3f} \t KL {valid_kl / valid_num: .3f} in epoch {epoch}"</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> valid_loss <span class="token operator">&lt;</span> best_loss<span class="token punctuation">:</span>            best_loss <span class="token operator">=</span> valid_loss            best_epoch <span class="token operator">=</span> epoch            torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'best_model_pokemon'</span><span class="token punctuation">)</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Model saved"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>训练完VAE后, 对VAE学到的生成能力进行探索. 继续导入:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> scipy<span class="token punctuation">.</span>stats <span class="token keyword">import</span> norm<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>因为这次隐变量维度<code>latent_dim = 32</code>, 不能再一次性的将所有维度采样看VAE的生成结果. 因此, 我打算选定一个维度和其他维度组合, 观察两两组合的维度产生的效果. 为了让结果更多变些, 我打算直接让其他隐变量也随机改变:</p><pre class="line-numbers language-python"><code class="language-python">state <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'best_model_pokemon'</span><span class="token punctuation">)</span>model <span class="token operator">=</span> ConvVAE<span class="token punctuation">(</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>state<span class="token punctuation">)</span>n <span class="token operator">=</span> <span class="token number">10</span>image_size <span class="token operator">=</span> <span class="token number">40</span>grid_x <span class="token operator">=</span> norm<span class="token punctuation">.</span>ppf<span class="token punctuation">(</span>np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0.05</span><span class="token punctuation">,</span> <span class="token number">0.95</span><span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>grid_y <span class="token operator">=</span> norm<span class="token punctuation">.</span>ppf<span class="token punctuation">(</span>np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0.05</span><span class="token punctuation">,</span> <span class="token number">0.95</span><span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>selected <span class="token operator">=</span> <span class="token number">21</span>coll <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>selected<span class="token punctuation">,</span> i<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>latent_dim<span class="token punctuation">)</span> <span class="token keyword">if</span> i <span class="token operator">!=</span> selected<span class="token punctuation">]</span><span class="token keyword">for</span> idx<span class="token punctuation">,</span> <span class="token punctuation">(</span>p<span class="token punctuation">,</span> q<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>coll<span class="token punctuation">)</span><span class="token punctuation">:</span>    figure <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> image_size <span class="token operator">*</span> n<span class="token punctuation">,</span> image_size <span class="token operator">*</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> yi <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>grid_y<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> j<span class="token punctuation">,</span> xi <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>grid_x<span class="token punctuation">)</span><span class="token punctuation">:</span>            t <span class="token operator">=</span> <span class="token punctuation">[</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>latent_dim<span class="token punctuation">)</span><span class="token punctuation">]</span>            t<span class="token punctuation">[</span>p<span class="token punctuation">]</span><span class="token punctuation">,</span> t<span class="token punctuation">[</span>q<span class="token punctuation">]</span> <span class="token operator">=</span> xi<span class="token punctuation">,</span> yi            z_sampled <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span>t<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>            <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                decode <span class="token operator">=</span> model<span class="token punctuation">.</span>fcr1<span class="token punctuation">(</span>model<span class="token punctuation">.</span>fcr2<span class="token punctuation">(</span>z_sampled<span class="token punctuation">)</span><span class="token punctuation">)</span>                decode <span class="token operator">=</span> decode<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">*</span>mid_dim<span class="token punctuation">)</span>                decode <span class="token operator">=</span> model<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>decode<span class="token punctuation">)</span>                decode <span class="token operator">=</span> decode<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>                figure<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span>                i <span class="token operator">*</span> image_size<span class="token punctuation">:</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> image_size<span class="token punctuation">,</span>                j <span class="token operator">*</span> image_size<span class="token punctuation">:</span> <span class="token punctuation">(</span>j <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> image_size                <span class="token punctuation">]</span> <span class="token operator">=</span> decode    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"X: {}, Y: {}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>p<span class="token punctuation">,</span> q<span class="token punctuation">)</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>xticks<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>yticks<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>axis<span class="token punctuation">(</span><span class="token string">'off'</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>figure<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>生成效果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/vae13.png" style="zoom: 150%;" /><p>就是生成结果太糊了, 但能看出来左上角这玩意的轮廓明显像沼跃鱼没进化时候的水跃鱼:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/vae15.png" style="zoom:33%;" /><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/vae14.png" style="zoom:150%;" /><p>右上角和左上角还有左下角可能生成的像个什么东西… 总体来说生成的效果不是很好, 非常糊, 看着感觉跟发育未完全的胚胎似的. </p><p>因为隐变量维数实在是太多了, 或许我们可以尝试更好点的办法, 找某一个神奇宝贝作为<strong>基准</strong>, 由编码器编码后得到一个现成的均值和方差, 然后再对某两个维度进行调整, 生成的结果会更贴近选定的神奇宝贝一些, 也就是使生成的结果更加合理一些.</p><p>只需要在生成隐变量时不再随机:</p><pre class="line-numbers language-python"><code class="language-python">image_path <span class="token operator">=</span> <span class="token string">'./pokemon/025MS.png'</span><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    base <span class="token operator">=</span> transform<span class="token punctuation">(</span>image_path<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> model<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>base<span class="token punctuation">)</span>    x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> model<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    h <span class="token operator">=</span> model<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    mu<span class="token punctuation">,</span> logvar <span class="token operator">=</span> h<span class="token punctuation">.</span>chunk<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>    z <span class="token operator">=</span> model<span class="token punctuation">.</span>reparameterise<span class="token punctuation">(</span>mu<span class="token punctuation">,</span> logvar<span class="token punctuation">)</span>    z <span class="token operator">=</span> z<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token keyword">for</span> idx<span class="token punctuation">,</span> <span class="token punctuation">(</span>p<span class="token punctuation">,</span> q<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>coll<span class="token punctuation">)</span><span class="token punctuation">:</span>    figure <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> image_size <span class="token operator">*</span> n<span class="token punctuation">,</span> image_size <span class="token operator">*</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> yi <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>grid_y<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> j<span class="token punctuation">,</span> xi <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>grid_x<span class="token punctuation">)</span><span class="token punctuation">:</span>            z<span class="token punctuation">[</span>p<span class="token punctuation">]</span><span class="token punctuation">,</span> z<span class="token punctuation">[</span>q<span class="token punctuation">]</span> <span class="token operator">=</span> xi<span class="token punctuation">,</span> yi            z_sampled <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span>z<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>'<span class="token comment" spellcheck="true"># ......</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>下面相同的部分被我省略掉了. 我们选定皮卡丘作为基准, 生成结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/vae16.png" style="zoom:150%;" /><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/vae17.png" style="zoom:150%;" /><p>有时可以维持住皮卡丘的基本形状. 但随着某些隐变量的变化, 逐渐变得混沌, 甚至换了一个物种:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/vae18.png" style="zoom:150%;" /><p>还是有点看不清, 如果裁剪图片到(3, 20, 20)效果可能会好一点, 重新搭建一种更小尺寸的VAE模型:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">ConvVAE</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> latent<span class="token operator">=</span>latent_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>ConvVAE<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>mid_num<span class="token punctuation">,</span> inter_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>inter_dim<span class="token punctuation">,</span> latent <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fcr2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>latent<span class="token punctuation">,</span> inter_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fcr1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>inter_dim<span class="token punctuation">,</span> mid_num<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>然后向<code>transform</code>中添加裁剪:</p><pre class="line-numbers language-python"><code class="language-python">transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>    <span class="token keyword">lambda</span> x<span class="token punctuation">:</span> Image<span class="token punctuation">.</span>open<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>convert<span class="token punctuation">(</span><span class="token string">'RGB'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    transforms<span class="token punctuation">.</span>CenterCrop<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>将<code>image_size</code>设置为图像裁剪后的大小20, 其余代码全部不用动. 重新Train完模型, 我们依旧选择皮卡丘作为基准, 继续生成:</p><p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/vae19.png" alt=""></p><p>开始也能维持住皮卡丘的基本样貌, 但多了一丝混沌的气息. 随着其他隐变量的变化, 皮卡丘长得越来越像其他的生物:</p><p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/vae20.png" alt=""></p><p>继续演变, 甚至变成了右下角的某种东西:</p><p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/vae21.png" alt=""></p><p>感兴趣的可以自己Train一个模型, 自己探索一下. 结果不太好可能是我搭的模型有点太随意了…</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> VAE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>变分自编码器入门</title>
      <link href="/posts/53598.html"/>
      <url>/posts/53598.html</url>
      
        <content type="html"><![CDATA[<h1 id="变分自编码器入门"><a href="#变分自编码器入门" class="headerlink" title="变分自编码器入门"></a>变分自编码器入门</h1><p>变分自动编码器(<strong>VAE</strong>, <strong>V</strong>ariational <strong>A</strong>uto - <strong>E</strong>ncoder)是一种基于自编码器结构的<strong>深度生成模型</strong>.</p><p>本文对VAE更深层次的数学原理没有探讨, 一般概率基础即可放心食用, 更深层次的数学原理在文末深入阅读处给出.  </p><p>VAE与GAN有非常紧密的关系, GAN之后找个机会细说(先挖坑).</p><h2 id="自编码器"><a href="#自编码器" class="headerlink" title="自编码器"></a>自编码器</h2><blockquote><p>本节图片全部出自<a href="https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798" target="_blank" rel="noopener">Applied Deep Learning - Part 3: Autoencoders</a>.</p></blockquote><p>在介绍VAE之前, 必须要简要介绍一下<strong>自编码器</strong>(<strong>AE</strong>, <strong>A</strong>uto - <strong>E</strong>ncoder). </p><p>自编码器是一种先把输入数据压缩为某种编码, 后仅通过该编码<strong>重构</strong>出原始输入的结构. 从描述来看, AE是一种<strong>无监督</strong>方法.</p><p>AE的结构非常明确, 需要有一个<strong>压缩编码</strong>的Encoder和就一个相应<strong>解码重构</strong>的Decoder:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/vae1.png" style="zoom: 50%;" /><p>Encoder能够将给定的输入$X$ 映射为<strong>编码</strong>$Z$, 即$Z=g(X)$, Decoder能将编码$Z$ 映射回与原输入相似的$\hat{X}$, 即$\hat{X}=f(Z)$. $Z$ 也被称为<strong>隐变量</strong>, 其维度必须是远远小于$X$ 的, 否则就达不到压缩编码的目的. </p><p>如果Decoder能仅依赖Encoder生成的编码$Z$ 尽可能好的还原输入数据, 那么就说明$Z$ 中真的存在某种能表征原始输入$X$ 的信息, 甚至$Z$ 的每一维都可能对应着某个输入数据变化的具体含义, 例如人脸的笑容, 褶皱, 皮肤颜色等属性.</p><p>对于压缩编码和解码重构的结构, 使用普通的神经网络, 只需要让神经元的个逐渐减少到编码的维度, 再由编码维度逐渐增大到原输入维度:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/vae2.png" style="zoom: 50%;" /><blockquote><p>Encoder和Decoder的不一定是完全对称的, 甚至也不一定是同质的.</p></blockquote><p>我们希望Auto Encoder所重构的输入$\hat{X}$ 和真正输入$X$ 的差距越小越好, 所以通常使用均方误差(MSE)来作为AE的损失函数, 即$\mathcal{L}_{MSE}=\Vert X - \hat{X} \Vert ^ 2$.</p><p>AE有一种常见的变形, 称为<strong>去噪自编码器</strong>(Denoising Auto - Encoder). 这种AE在原始输入数据的基础上添加了<strong>噪声</strong>, 然后再将其送给AE, 并要求Decoder还原出不带噪声的输入数据. 这就要求Encoder和Decoder具有更强大的能力:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/vae3.png" style="zoom: 50%;" /><p>AE不是我们今天的重点, 就不再展开说了.</p><p>仔细想想, Auto Encoder虽然是可以分成Encoder和Decoder两个部分, 但实际上Encoder和Decoder是没法作为两个组件单独使用的, 它们必须配套使用.</p><p>例如我们想用单独使用Decoder做生成, 我们只能把随机生成的向量输入到Decoder中, 强行让Decoder解码出一个极少概率有用的内容, 效果一定不会很好. 因为在训练时, Decoder获得的编码全部是来自于Encoder的, 而我们直接随机采样得到的向量与Encoder压根没有关联, 让Decoder解码出有效的结果是不可能的.</p><h2 id="变分自编码器"><a href="#变分自编码器" class="headerlink" title="变分自编码器"></a>变分自编码器</h2><blockquote><p>本节图片出自<a href="https://www.jeremyjordan.me/variational-autoencoders/" target="_blank" rel="noopener">Variational autoencoders.</a>, 内容讲解参考苏神的博客.</p></blockquote><p><strong>变分自编码器</strong>(<strong>VAE</strong>, <strong>V</strong>ariational <strong>A</strong>uto - <strong>E</strong>ncoder)从概率的角度描述隐空间与输入样本.</p><h3 id="隐变量-概率分布式"><a href="#隐变量-概率分布式" class="headerlink" title="隐变量 - 概率分布式"></a>隐变量 - 概率分布式</h3><p>理想形态下的生成模型可以被描述为$X = g(Z)$. 由于没法直接知道$p(X)$, 我们得引入隐变量$Z$ 来求:<br>$$<br>p(X) = \sum_Z p(X\mid Z) p(Z)<br>$$</p><p>如果我们能把输入样本$X$ 编码得到的$Z$ 控制在我们已知的某个分布中, 那么我们就可以从隐变量的分布中采样, 解码得到生成内容$\hat{X}$, 也算不错.</p><p>在这样的想法下, 将样本的隐变量建模为<strong>概率分布</strong>, 而非像AE一样把隐变量看做是离散的值:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/vae5.png" style="zoom: 67%;" /><p>AE将样本编码为离散的点, 而VAE将样本编码为概率分布, 直接点就是给隐变量<strong>添加噪声</strong>.</p><p>那么在Decoder解码时, 从隐变量中<strong>随机采样</strong>, 得到采样后的向量作为Decoder的输入:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/vae4.png" style="zoom: 67%;" /><p>沿着这个思路, 如果假设$p(Z) \sim \mathcal{N}(\mu, \sigma^2)$, 可以从其中采样得到$Z_1, Z_2 , \dots, Z_n$, 然后由生成模型得到$\hat{X}_1 = g(Z_1),\hat{X}_2 = g(Z_2),\dots,\hat{X}_n = g(Z_n)$, 但我们根本没法度量生成的结果$\{\hat{X}_1,\hat{X}_2,\dots,\hat{X}_n\} $ 和样本数据$\{X_1,X_2,\dots,X_n\}$ 之间的差异, 因为我们压根不知道$Z_k, X_k, \hat{X}_k$ 之间的<strong>对应关系</strong>.</p><p>没有$X, \hat{X}$ 分布的表达式, 我们就没有办法通过对齐二者分布的方法来优化模型.</p><p>所以, 我们应该在给定真实样本$X_k$ 的情况下, 假设存在分布$p(Z \mid X_k) \sim \mathcal{N}(\mu, \sigma^2)$. Decoder就可以把$p(Z \mid X_k)$ 中采样得到的$Z_k$ 还原为$X_k$, 这样保证$Z_k, X_k, \hat{X}_k$ 之间可以对应. </p><p>尽管分布内采样到的隐变量的值<strong>不完全相同</strong>, 但都应该重建回相同的输出, 这也就是把<strong>样本编码为概率分布</strong>的真正含义:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/vae6.png" style="zoom: 67%;" /><p><strong>每一个样本都对应着一个自己专属的正态分布</strong>, 样本之间必定存在重合, 当采样到两个样本叠加的区域时, 解码的内容会变得介于二者之间. 按照AE中的假设, 隐变量的每维都可能有具体的含义. 若是如此, 在概率分布视角下的隐变量就可以等距采样, 通过观察控制生成的内容. 例如:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/vae10.jpg" style="zoom: 67%;" /><p>从满月到半月等距采样, 应该能观察到由满月逐渐变到半月的所有月相.</p><blockquote><p>本图出自<a href="https://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017/Lecture/GAN%20(v3).pdf" target="_blank" rel="noopener">李宏毅老师课程配套Slide</a>.</p></blockquote><p>那每个分布的均值和方差要怎么求出来呢? 没什么好方法, 用神经网络来直接拟合样本对应的正态分布的均值$\mu$ 和方差$\sigma^2$ 吧:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/vae7.png" style="zoom: 67%;" /><blockquote><p>但我们实际上拟合的是$\log \sigma^2$, 因为$\sigma^2$ 非负, 想要让变为负数需要加激活函数处理, 而$\log \sigma^2$ 可以直接在不加激活函数的情况下变为负值.</p></blockquote><h3 id="KL散度-防止神经网络偷懒"><a href="#KL散度-防止神经网络偷懒" class="headerlink" title="KL散度 - 防止神经网络偷懒"></a>KL散度 - 防止神经网络偷懒</h3><p>神经网络一看拟合$\mu, \sigma^2$ 的任务, 心想: “采样得到的$Z$ 是包含噪声的, 重构起来多难啊, 我直接让方差$\sigma^2$ 学出个0来, 我学一个点的拟合肯定比学一个分布的拟合简单, 美滋滋”. </p><p>神经网络很容易过拟合, 把方差学成0, 那就坏了. 如果把样本重新映射回一个点, 那么VAE就直接退化回了AE. 所以我们还是希望$Z$ 是含有噪音的(方差不为0)的分布.</p><p>对于我们之前的假设$p(Z \mid X) \sim \mathcal{N}(\mu, \sigma^2)$ 要更严格, 不但要约束$\sigma^2 \neq 0$, 还要令方差不能太大, 也不能太小.</p><p>但如果假设$p(Z \mid X) \sim \mathcal{N}(0, I)$, 还保证了模型的<strong>生成能力</strong>:</p><p>$$<br>\begin{aligned}<br>p(Z) = &amp; \sum_X p(Z \mid X) p(X) \\<br> = &amp; \sum_x \mathcal{N}(0, I) p(X)\\<br> = &amp; \mathcal{N}(0, I) \sum_X p(X) \\<br> = &amp; \mathcal{N}(0, I)<br> \end{aligned}<br>$$</p><p>在该条件下$p(Z) \sim \mathcal{N}(0, I)$, 当脱离Encoder, 即不依靠输入样本$X$ 时, 我们可以直接从$\mathcal{N}(0, I)$ 中采样来生成可靠的结果.</p><p>我们直接使用KL散度来约束$p(Z \mid X)$, 令其服从标准正态分布.</p><blockquote><p><strong>KL散度</strong>(也称为<strong>相对熵</strong>)常用于度量两个分布之间的差异性, 假设$P$ 为样本真实分布, $Q$ 为模型预测的分布, 根据KL散度有:<br>$$<br>D_{\mathrm{KL}}(P \| Q)=\mathbb{E}_{\mathrm{x} \sim P}\left[\log \frac{P(x)}{Q(x)}\right]=\mathbb{E}_{\mathrm{x} \sim P}[\log P(x)-\log Q(x)]<br>$$<br>当$P, Q$ 越接近时, $D_{\mathrm{KL}}(P \| Q)$ 就越小, 当$P, Q$ 分布完全相同时, $D_{\mathrm{KL}}(P \| Q)$ 为0.</p><p>KL散度还有两个性质:</p><ol><li>非负: KL散度是非负的.</li><li>不对称: 通常情况下, $D_{\mathrm{KL}}(P \| Q) \neq D_{\mathrm{KL}}(Q \| P)$, KL散度并不是真正意义上的距离.</li></ol></blockquote><p>求解过程如下:</p><p>$$<br>\begin{aligned}<br>&amp;KL\Big(N(\mu,\sigma^2)\Big\Vert N(0,1)\Big)\\<br>=&amp;\int \frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2} \left(\log \frac{e^{-(x-\mu)^2/2\sigma^2}/\sqrt{2\pi\sigma^2}}{e^{-x^2/2}/\sqrt{2\pi}}\right)dx\\\<br>=&amp;\int \frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2} \log \left\{\frac{1}{\sqrt{\sigma^2}}\exp\left\{\frac{1}{2}\big[x^2-(x-\mu)^2/\sigma^2\big]\right\} \right\}dx\\\<br>=&amp;\frac{1}{2}\int \frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2} \Big[-\log \sigma^2+x^2-(x-\mu)^2/\sigma^2 \Big] dx \\<br>=&amp;\frac{1}{2}(-\log\sigma^2+\mu^2+\sigma^2-1)<br>\end{aligned}<br>$$</p><p>求解时, 需要<strong>最小化</strong>KL散度.</p><p>VAE常用的损失函数为:<br>$$<br>\begin{aligned}<br>\mathcal{L} = &amp; \mathcal{L}_\mathrm{Recon} + \mathcal{L}_\mathrm{KL} \\<br>= &amp; \mathcal{D}(\hat{X}_k,X_k)^2 + KL\Big(N(\mu,\sigma^2)\Big\Vert N(0,1)\Big)<br>\end{aligned}<br>$$<br>即重构损失和KL散度两部分.</p><h3 id="梯度断裂-重参数"><a href="#梯度断裂-重参数" class="headerlink" title="梯度断裂 - 重参数"></a>梯度断裂 - 重参数</h3><p>我们想要用梯度下降来优化$p(Z \mid X_k)$ 的均值$\mu$ 和方差$\sigma $, 但”采样”这个操作是<strong>不可导</strong>的, VAE利用<strong>重参数化技巧</strong>(Reparameterization Trick)使得梯度不因采样而断裂.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/vae8.png" style="zoom: 67%;" /><blockquote><p>图片来自<a href="https://www.jeremyjordan.me/variational-autoencoders/" target="_blank" rel="noopener">Variational autoencoders.</a></p></blockquote><p>原理很简单, $Z$ 的导数可以写成:<br>$$<br>\begin{aligned}&amp;\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(z-\mu)^2}{2\sigma^2}\right)dz \\<br>=&amp; \frac{1}{\sqrt{2\pi}}\exp\left[-\frac{1}{2}\left(\frac{z-\mu}{\sigma}\right)^2\right]d\left(\frac{z-\mu}{\sigma}\right)<br>\end{aligned}<br>$$<br>说明$(z - \mu) / \sigma^2 \sim \mathcal{N}(0, I)$, 从$\mathcal{N}(\mu, \sigma^2)$ 中采样, 就等价与从标准正态分布$\mathcal{N}(0, I)$ 中采样出一个$\epsilon$, 然后再通过$Z= \mu + \epsilon \times \sigma$ 缩放回去. 采样的导致梯度断裂的锅就丢给了$\epsilon$ 这个无关变量, 使得$\mu, \sigma^2$ 可以重新参与到梯度下降中优化:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/vae9.png" style="zoom: 67%;" /><h2 id="深入阅读及参考资料来源"><a href="#深入阅读及参考资料来源" class="headerlink" title="深入阅读及参考资料来源"></a>深入阅读及参考资料来源</h2><ul><li><p>视频推荐: </p><ol><li>强推李宏毅: <a href="https://www.bilibili.com/video/BV1Wv411h7kN" target="_blank" rel="noopener">李宏毅2021春机器学习课程</a> 的<a href="https://www.bilibili.com/video/BV1Wv411h7kN?p=44" target="_blank" rel="noopener">p44</a> 和<a href="https://www.bilibili.com/video/BV1Wv411h7kN?p=45" target="_blank" rel="noopener">p45</a>, 以及其<a href="https://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017/Lecture/GAN%20(v3).pdf" target="_blank" rel="noopener">配套的Slide</a></li><li>通俗篇: <a href="https://www.bilibili.com/video/BV1hf4y1r7C7" target="_blank" rel="noopener">变分自动编码器(Variational AutoEncoder, VAE)，直观理解、数学推导与最新应用</a></li><li>硬核篇: [[论文简析]VAE: Auto-encoding Variational Bayes[1312.6114]](<a href="https://www.bilibili.com/video/BV1q64y1y7J2" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1q64y1y7J2</a>)</li></ol></li><li><p>文章推荐 - 苏神系列博客: </p><ol><li><a href="https://spaces.ac.cn/archives/5253" target="_blank" rel="noopener">变分自编码器（一）：原来是这么一回事</a></li><li><a href="https://kexue.fm/archives/5343" target="_blank" rel="noopener">变分自编码器（二）：从贝叶斯观点出发</a></li><li><a href="https://kexue.fm/archives/5383" target="_blank" rel="noopener">变分自编码器（三）：这样做为什么能成？</a></li><li><a href="https://kexue.fm/archives/7725" target="_blank" rel="noopener">变分自编码器（六）：从几何视角来理解VAE的尝试</a></li></ol></li><li><p>外文文章推荐:</p><ol><li>VAE原论文: <a href="https://arxiv.org/abs/1312.6114" target="_blank" rel="noopener">Auto-Encoding Variational Bayes</a></li><li>关于重参数: <a href="http://gregorygundersen.com/blog/2018/04/29/reparameterization/" target="_blank" rel="noopener">The Reparameterization Trick</a></li></ol></li></ul><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>VAE是一个有严格数学推导的模型, 但在学的时候千万不要被”变分”二字给唬住了, 变分二字只是来源于VAE推导过程中所使用的KL散度.</p><p>实际上, VAE由于把样本编码为概率分布, 生成的实际上是训练样本之间的<strong>平均</strong>, 在样本的<strong>分布叠加</strong>后(高斯混合模型), VAE记录了一个样本到另一个样本之间的<strong>演化过程</strong>. 这也就是为什么VAE生成的结果会存在<strong>模糊</strong>的问题, 但仍然不妨碍它成为最强大的深度生成模型之一.</p><p>强烈推荐看苏神的博客, 苏神的见解要深刻得多.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> VAE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>知识蒸馏: Distilling the Knowledge in a Neural Network</title>
      <link href="/posts/39586.html"/>
      <url>/posts/39586.html</url>
      
        <content type="html"><![CDATA[<h1 id="Distilling-the-Knowledge-in-a-Neural-Network"><a href="#Distilling-the-Knowledge-in-a-Neural-Network" class="headerlink" title="Distilling the Knowledge in a Neural Network"></a>Distilling the Knowledge in a Neural Network</h1><p>本文是论文<a href="https://arxiv.org/abs/1503.02531" target="_blank" rel="noopener">Distilling the Knowledge in a Neural Network</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>现有机器学习中, 任何算法都可以用Ensemble的方法来提升性能, 但这样做会花费昂贵的计算资源, 并且不利于部署到真实场景中.</p><p>作者尝试提出一种<strong>把大模型知识尽可能的压缩进单个小模型</strong>的方法.</p><h2 id="Distillation"><a href="#Distillation" class="headerlink" title="Distillation"></a>Distillation</h2><p>人们希望得到的模型并不是在单一的数据集上拟合完美, 而是要求模型具有强大的<strong>泛化能力</strong>. 在某个问题的某个具体数据及上, 通常训练出的模型与真实问题会存在偏差,  存在一点点过拟合.</p><p>那么对于一个有能力的大模型, 就有希望直接利用大模型的知识, 训练一个具有更强泛化能力的小模型, 让小模型直接学习大模型的泛化能力.</p><blockquote><p>从泛化能力的角度来考虑, 知识蒸馏非常像一种<strong>正则化</strong>手段.</p></blockquote><p>训练时经常所采用的标签是独热编码, Softmax会刻意放大Logits之间的差距. 这使得模型输出的类别概率在某一类是非常大的(文中也称为<strong>Hard Target</strong>), 其他类别的概率都非常小. </p><p>但不同类别之间的<strong>相对概率</strong>仍然很重要, 例如猫的图片可能与狗有一定相似, 它一定比和苹果的相似性要低. 这种类别概率差异仍然可能存在着一些隐含的知识, 但它会被Softmax所抹除掉, 所以需要一些手段把这种知识传授给小模型.</p><p>一种可以尝试的方法是把大模型(<strong>Teacher</strong>)的预测结果和大模型的知识作为小模型(<strong>Student</strong>)的Target, 即将处理过后的大模型Logits作为Label或Label的一部分训练小模型.</p><p>既然是Softmax抹除了不同类别之间的差异, 那么可以对Softmax改动, 弱化其对隐含知识的影响.</p><p>假设神经网络在没有经过Softmax前的<strong>Logits</strong>记为$z_i$, 我们可以添加”<strong>温度</strong>“$T$ 来弱化影响, 记结果为$q_i$:</p><p>$$<br>q_{i}=\frac{\exp \left(z_{i}\right)}{\sum_{j} \exp \left(z_{j}\right)} \quad \rightarrow \quad q_{i}=\frac{\exp \left(z_{i} / T\right)}{\sum_{j} \exp \left(z_{j} / T\right)}<br>$$<br>$T=1$ 时, 就是正常的Softmax. 当$T &gt; 1$ 时, 原来的Softmax将变得<strong>更加软化</strong>, 不同类别之间的差距将被放大, Teacher中的知识得到一定保留. 因为标签变得软化了, 所以熵更大, 也保存了更多的信息.</p><p>如果这个式子不够直观体现出它的作用, 我做出了不同$T$ 对Target $q_i$ 的影响变化曲线图:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kd6.png" style="zoom:80%;" /><p>随着$T$ 的增大, 生成的Soft Target之间的差距会越来越小, 变得<strong>Softer</strong>.</p><p>Student将使用Hard Target和Soft Target共同训练自己, Teacher软化后的知识将作为损失函数的一部分调节Student的参数:<br>$$<br>\mathcal{L} = \mathcal{L}_{hard} + \lambda \mathcal{L}_{soft}<br>$$</p><p>$\lambda$ 为超参, 用于调节Teacher Soft Target的影响占比.</p><blockquote><p>在训练时, 必须保证Teacher和Student的温度一致, 当训练完成后, Student预测不再使用$T$, 或者说训练完成后的推断设置$T=1$.</p><p>同时, 由于温度$T$ 的影响, 梯度均缩小了$T^2$ 倍(详见下一小节最后), 所以在设置$\lambda$ 时, 需要让其尽可能大一些, 或者乘$T^2$ 倍, 才能保证两种损失的贡献度相同.</p></blockquote><h3 id="Matching-Logits-is-a-Special-Case-of-Distillation"><a href="#Matching-Logits-is-a-Special-Case-of-Distillation" class="headerlink" title="Matching Logits is a Special Case of Distillation"></a>Matching Logits is a Special Case of Distillation</h3><p>作者下面证明了直接让Student学Teacher的Logits只是蒸馏的一种特殊情况.</p><blockquote><p>下面涉及到更详细的推导过程在末尾引文连接已附上.</p></blockquote><p>假设我们处理的问题所采用的损失函数是交叉熵$C$, 梯度为$\frac{\partial C}{\partial z_i}$, Teacher模型的Logits为$v_i$, 以及其对应的概率为$p_i$, 则有:</p><p>$$<br>\frac{\partial C}{\partial z_{i}}=\frac{1}{T}\left(q_{i}-p_{i}\right)=\frac{1}{T}\left(\frac{e^{z_{i} / T}}{\sum_{j} e^{z_{j} / T}}-\frac{e^{v_{i} / T}}{\sum_{j} e^{v_{j} / T}}\right)<br>$$</p><p>当$T$ 相较于Logits充分大的时候, 可以使用泰勒展开, 有$e^{x/T}\approx1+x/T$:</p><p>$$<br>\frac{\partial C}{\partial z_{i}} \approx \frac{1}{T}\left(\frac{1+z_{i} / T}{N+\sum_{j} z_{j} / T}-\frac{1+v_{i} / T}{N+\sum_{j} v_{j} / T}\right)<br>$$</p><p>当对Logits做了零均值假设后, 有$\sum_jz_j=\sum_jv_j=0$, 结合上式有:</p><p>$$<br>\frac{\partial C}{\partial z_{i}} \approx \frac{1}{N T^{2}}\left(z_{i}-v_{i}\right)<br>$$</p><p>因此, 在较高的温度$T$ 设置下, 蒸馏等价于最小化$\frac{1}{2}(z_i - v_i)^2$, 也就是直接把Teacher和Student的Logits匹配, 所以匹配Logits是一种蒸馏的特殊情况.</p><p>当温度较低时, 对负样本的关注就比较少, 可能滤去关键信息, 但实际上这<strong>有利有弊</strong>. 有些负样本的Logits应该是非常小的负值, 这种极小的负值在高温时的作用会被放大, 作为强大的噪声影响Student. 在低温时, 这种噪声将被滤去.</p><p>所以温度的选取一般依赖于经验, 不要太高也不要太低.</p><blockquote><p>分母上有$T^2$, 所以在知识蒸馏时, $\mathcal{L}_{soft}$ 的影响被缩小了$T^2$, 所以需要在设置损失项时平衡回来.</p></blockquote><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="MNIST"><a href="#MNIST" class="headerlink" title="MNIST"></a>MNIST</h3><p>作者将知识蒸馏应用于小规模数据集MNIST, Teacher训练了一个两隐层的DNN并使用Dropout和Weight Constraints, Student网络也是两隐层的DNN但神经元个数比Teacher少, 不使用正则化手段. </p><p>作者尝试了几种不同的小模型设置, 在合适的温度下取得了与Teacher相近的表现.</p><h3 id="Speech-Recognition"><a href="#Speech-Recognition" class="headerlink" title="Speech Recognition"></a>Speech Recognition</h3><p>语音识别中, 当时比较流行的做法是用HMM, 并按照如下目标优化模型参数$\theta$:<br>$$<br>\boldsymbol{\theta}=\arg \max _{\boldsymbol{\theta}^{\prime}} P\left(h_{t} \mid \mathbf{s}_{t} ; \boldsymbol{\theta}^{\prime}\right)<br>$$<br>其中$\mathbf{s_t}$ 为$t$ 时刻的结果, $h_t$ 为$t$ 时刻的HMM隐态.</p><p>结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kd1.jpg" style="zoom: 50%;" /><p>蒸馏一个小模型出来后的结果比单独Train一个大模型的效果要好.</p><h3 id="JFT"><a href="#JFT" class="headerlink" title="JFT"></a>JFT</h3><p>JFT是一个比前面二者大得多的图像分类数据集, 这个数据集有1亿张图片, 15000个类别. </p><p>作者训练了一个通用模型和若干个专家模型, 作为没有使用知识蒸馏时的Baseline.</p><p>对若干种通用模型经常<strong>易混淆</strong>的类做一个聚类(也有可能有些类不被归纳进专家模型, 这就需要通用模型自己处理), 记为$S^m$, 作为多个JFT的子集, 将不同子集的数据交给不同的专家模型$m$ 预测, 下面是作者展示出的子集示例:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kd2.jpg" style="zoom: 50%;" /><p>对于输入的图片$\mathbf{x}$, 得到分类结果需要两步:</p><ol><li><strong>粗分</strong>: 对于每个测试数据, 由通用模型得到$n$ 个最有可能的类别, 记这些类为$k$.</li><li><strong>细分</strong>: 按照$k$, $S^m$ 的非空交集找到相应的专家模型$A_k$ , 让专家模型进行预测.</li></ol><p><strong>专家模型极易过拟合</strong>. 为了防止过拟合, 专家模型所采用的一半数据来自指定类别, 剩下一半来自全数据集, 其他类别被全部设置为一个单独的”Dustbin”类.</p><p>然后最小化所有类别的概率分布$\mathbf{q}$ 和通用模型, 专家模型得到的概率分布的KL散度.</p><blockquote><p><strong>KL散度</strong>(也称为<strong>相对熵</strong>)常用于度量两个分布之间的差异性, 假设$P$ 为样本真实分布, $Q$ 为模型预测的分布, 根据KL散度有:<br>$$<br>D_{\mathrm{KL}}(P \| Q)=\mathbb{E}_{\mathrm{x} \sim P}\left[\log \frac{P(x)}{Q(x)}\right]=\mathbb{E}_{\mathrm{x} \sim P}[\log P(x)-\log Q(x)]<br>$$<br>当$P, Q$ 越接近时, $D_{\mathrm{KL}}(P \| Q)$ 就越小, 当$P, Q$ 分布完全相同时, $D_{\mathrm{KL}}(P \| Q)$ 为0.</p><p>KL散度还有两个性质:</p><ol><li>非负: KL散度是非负的.</li><li>不对称: 通常情况下, $D_{\mathrm{KL}}(P \| Q) \neq D_{\mathrm{KL}}(Q \| P)$, KL散度并不是真正意义上的距离.</li></ol></blockquote><p>记通用模型得到的概率分布为$\mathbf{p}^g$, 专家模型得到的概率分布为$\mathbf{p}^m$, 总体分布$\mathbf{q}$ 和模型预测得到概率分布的KL散度计算方式如下:</p><p>$$<br>K L\left(\mathbf{p}^{g}, \mathbf{q}\right)+\sum_{m \in A_{k}} K L\left(\mathbf{p}^{m}, \mathbf{q}\right)<br>$$</p><p>总体来说, 最好的结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kd3.jpg" style="zoom: 50%;" /><p>当时JFT的Baseline是CNN. </p><p>逐渐增大专家模型的数量, 结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kd4.jpg" style="zoom: 50%;" /><p>随专家数量提升, 相对提升逐渐增大.</p><p>前面说过, 专家模型极易过拟合, 如果使用3%的数据的Hard Target训练专家模型, 它更有可能过拟合, 并且在附加早停的情况下非常早就停止了.</p><p>但如果使用知识蒸馏, 把Hard Target用Soft Target代替, 仅用3%的数据训练专家模型, 不但不会很早早停, 而且还能保留专家模型的泛化能力, 效果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kd5.jpg" style="zoom: 50%;" /><p>使用Soft Target效果要好于同样使用3%的数据的Baseline的训练效果, 并且与使用全部数据的Baseline效果相近.</p><h2 id="Recommended"><a href="#Recommended" class="headerlink" title="Recommended"></a>Recommended</h2><ul><li>论文原文: <a href="https://arxiv.org/abs/1503.02531" target="_blank" rel="noopener">Distilling the Knowledge in a Neural Network</a></li><li>比较全面的文章: <a href="https://zhuanlan.zhihu.com/p/102038521" target="_blank" rel="noopener">【经典简读】知识蒸馏(Knowledge Distillation) 经典之作</a></li><li>BERT相关:<a href="https://zhuanlan.zhihu.com/p/71986772" target="_blank" rel="noopener">深度神经网络模型蒸馏Distillation</a></li><li>很详细的视频(需翻墙): <a href="https://www.youtube.com/watch?v=xKPvt3GQ1j8&ab_channel=peakqi" target="_blank" rel="noopener">Hinton：distilling knowledge in a neural network</a></li><li>数学推导(推荐看看): <a href="https://zhuanlan.zhihu.com/p/385374430" target="_blank" rel="noopener">Distilling the Knowledge in a Neural Network 理论推导</a></li></ul><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>知识蒸馏是一种将大模型的隐含知识通过某种手段提取出来, 提炼传授给小模型的<strong>模型压缩</strong>方法.</p><p>该论文发表自2014年, 当时深度学习的模型还没有发展到像现在这样的超大规模, Hinton能提出这种具有工程意义并且值得挖掘的新方向相当有远见.</p><blockquote><p>蒸馏为何有效, 人们还没有彻底摸清其中的作用原理.</p><p>甚至单个模型的<strong>自蒸馏</strong>也是有效的… 这点非常诡异, 为什么模型单单依靠样本本身却无法达到自蒸馏后的效果? 样本之间隐含的差异居然需要自己产生的产物重新喂给自己才能吸收(反刍)?</p><p>读完本论文后, 自然会产生进一步的想法. 直接把Logits蒸给小模型效果如何? 能蒸Logits为什么不直接蒸Feature呢? 要是蒸Feature也不够直接的话把参数蒸给小模型是不是也可以?  这些想法确实都可以, 或多或少都有效果.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
      <link href="/posts/60711.html"/>
      <url>/posts/60711.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>BERT: 详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>.</li></ul></blockquote><h1 id="ALBERT-A-Lite-BERT-for-Self-supervised-Learning-of-Language-Representations"><a href="#ALBERT-A-Lite-BERT-for-Self-supervised-Learning-of-Language-Representations" class="headerlink" title="ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"></a>ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</h1><p>本文是论文<a href="http://arxiv.org/abs/1909.11942" target="_blank" rel="noopener">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a>的阅读笔记和个人理解. 最近忙着毕业季, 赶巧眼病又发作了, 就拖更了几天.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>现有的NLP模型都太大了, <strong>计算资源短缺</strong>已经成为越来越显著的问题.</p><p>一般来说, 参数到达一定数量后增加参数只能带来轻微的性能提升, 模型参数过多后还容易出现过拟合, 反而导致性能下降. 作者尝试使用<strong>减少参数</strong>的多种方法, 构造一个轻量级的BERT, 能逼近原BERT的效果.</p><h2 id="ALBERT"><a href="#ALBERT" class="headerlink" title="ALBERT"></a>ALBERT</h2><p><strong>ALBERT</strong>(<strong>A</strong> <strong>L</strong>ite <strong>BERT</strong>)尝试使用三种主要的手段来节省额外的参数开销.</p><h3 id="Factorized-Embedding-Parameterization"><a href="#Factorized-Embedding-Parameterization" class="headerlink" title="Factorized Embedding Parameterization"></a>Factorized Embedding Parameterization</h3><p>在BERT系列模型架构中, Token的Embedding大小$E$ 和Encoder的Hidden Layer大小$H$ 是完全绑定的, 即$E \equiv H$, 独热编码会直接通过Embedding转换到大小为$H$ 的维度.</p><p>从<strong>建模角度</strong>来说, Word Embedding更多强调<strong>上下文无关</strong>的表示, 即Token本身在无语境时最多出现的意思, 而Hidden Layer Embedding更多强调<strong>上下文相关</strong>的表示. 但实际上我们应该希望上下文相关的部分能使用更多参数, 即$H \gg E$, 这样能最大化BERT中的参数的使用效率. </p><p>从<strong>实践角度</strong>来说, NLP经常需要非常大的字典大小$V$, 如果$E \equiv H$, 增大$H$ 的同时必然增大$E$, $H$ 和$E$ 绑在一起的思路就不太实用.</p><p>因此, ALBERT将这一部分拆分为两步, 把独热编码直接转换到$H$ 的过程拆分为两个步骤, <strong>先映射到低维嵌入空间$E$, 然后再投影到隐层大小</strong>$H$. 这样就将Embedding这部分的参数大小从$O(V \times H)$ 缩少到$O(V \times E + E \times h)$. 当$E\ll H$ 时能减少很多参数.</p><blockquote><p>例如, 词表大小$V=30000$, 在BERT中, $E=H=768$, 不考虑位置编码, Embedding的总参数为$30000 \times 768$. 在ALBERT中, $E \ne H$, 假设$E=128, H=768$, Embedding总参数为$30000 \times 128 + 128 \times 768$, 确实有减少.</p></blockquote><h3 id="Cross-Layer-Parameter-Sharing"><a href="#Cross-Layer-Parameter-Sharing" class="headerlink" title="Cross - Layer Parameter Sharing"></a>Cross - Layer Parameter Sharing</h3><p>因为BERT是Transformer Encoder堆叠起来的, 假如只用训练一组共用参数, 然后让所有层都使用这一组参数, 岂不是能大幅减少参数? 确实, ALBERT利用这种方法减少了相当多的参数量.</p><p>但即使是多层参数复用, 也有多种复用方法. 每个Transformer Encoder由FFN和Multi Head Attention组成, 所以就有<strong>仅复用Attention</strong>, <strong>仅复用FFN</strong>, <strong>直接复用Encoder</strong>三种复用方式, 作者在后文的实验中探索了这三种方式的效果.</p><p>下图为每一层中同一个Token的表示的L2距离和余弦相似度:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/albert1.jpg" style="zoom:50%;" /><p>ALBERT比BERT的曲线要平滑得多, 这说明共享参数有助于稳定网络参数.</p><blockquote><p>跨层的参数共享, 应该是ALBERT中减少参数最有效的方法, 对于共享参数对性能产生的负面效果, 在训练阶段使用一些Trick能弥补一些.</p></blockquote><h3 id="Inter-Sentence-Coherence-Loss"><a href="#Inter-Sentence-Coherence-Loss" class="headerlink" title="Inter - Sentence Coherence Loss"></a>Inter - Sentence Coherence Loss</h3><p>作者指出, NSP任务是可以偷懒的. NSP任务将同一个文档中的两个连续句子的句子对作为正例, 将不同文档的两个句子的句子对作为负例. 因此, NSP任务实际上可以划分为Topic Prediction和Coherence Prediction两个任务:</p><ul><li><strong>Topic</strong> Prediction: 预测前后两个句子的主题是否相同.</li><li><strong>Coherence</strong> Prediction: 预测前后两个句子是否是真正连续的.</li></ul><p>Topic Prediction的难度远小于Coherence Prediction, 所以NSP不一定能真正有益于模型在下游任务上的表现, 也符合其他研究人员的结论.</p><p>ALBERT也废除了NSP任务, 但从NSP的Coherence Prediction角度出发, 设计了<strong>SOP</strong>(<strong>S</strong>entence <strong>O</strong>rder <strong>P</strong>rediction)任务. SOP任务保留<strong>Coherence Prediction</strong>, 将同一个文档中顺序正确的两个连续句子的句子对作为正例, 将它们<strong>交换顺序</strong>后的句子对作为负例, 这样就消除了Topic Prediction的目标.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文, 作者将ALBERT在实验中所用到的几个设置与BERT进行了对比:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/albert2.jpg" style="zoom:50%;" /><blockquote><p>ALBERT - xxlarge的层数是12而非24, 说明此时宽的模型比深的模型效果要更好, 可能层数已经达到极限.</p><p>4096的HIdden Size与BERT - large相比已经相当大了.</p></blockquote><p>与BERT不同的是, ALBERT在90%的情况下都使用最大的文本输入长度, 仅有10%的概率使用比最大文本长度更小的输入.</p><p>并且, 作者借鉴了<a href="https://adaning.github.io/posts/34019.html#toc-heading-3">SpanBERT</a>中的<strong>N - Gram Masking</strong>, 即每次生成Mask都有$p(n)$ 的概率生成长度为$n$ 的Mask:</p><p>$$<br>p(n)=\frac{1 / n}{\sum_{k=1}^{N} 1 / k}<br>$$</p><p>作者设置最大长度$n=3$.</p><h3 id="Overall-Comparision-between-BERT-and-ALBERT"><a href="#Overall-Comparision-between-BERT-and-ALBERT" class="headerlink" title="Overall Comparision between BERT and ALBERT"></a>Overall Comparision between BERT and ALBERT</h3><p>作者将BERT的各类配置与ALBERT的各类配置在<strong>相同训练量</strong>的多个任务上做了实验, 结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/albert3.jpg" style="zoom:50%;" /><p>ALBERT - xxlarge应该是打榜用的, 性能超过了BERT - large. ALBERT - large的性能已经接近于BERT - large.</p><p>在作者设置的ALBERT配置中, 同等型号的ALBERT比BERT的速度要快一些, 但同等型号的ALBERT性能却要比BERT差许多. 只有<strong>跨配置</strong>比较, 才能保证性能相似, 但<strong>跨配置的ALBERT在推理速度上不占优势</strong>.</p><h3 id="Factorized-Embedding-Parameterization-1"><a href="#Factorized-Embedding-Parameterization-1" class="headerlink" title="Factorized Embedding Parameterization"></a>Factorized Embedding Parameterization</h3><p>作者调整了ALBERT - base的$E$ 大小, 分别对比了共享参数和不共享参数的情况.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/albert4.jpg" style="zoom:50%;" /><p>从结果来看, 在复用Encoder的情况下, $E=128$ 似乎是一个比较好的解, $E &gt; 128$ 性能开始退化.</p><h3 id="Cross-Layer-Parameter-Sharing-1"><a href="#Cross-Layer-Parameter-Sharing-1" class="headerlink" title="Cross - Layer Parameter Sharing"></a>Cross - Layer Parameter Sharing</h3><p>针对仅复用Attention, 仅复用FFN, 直接复用Encoder三种情况, 作者在各类下游任务上做了实验, 结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/albert5.jpg" style="zoom:50%;" /><p>不共享参数的情况自然是性能最好的, 共享FFN似乎比较容易掉点.共享Attention影响没有那么大. 作者坚持复用Encoder的策略.</p><h3 id="Sentence-Order-Prediction-SOP"><a href="#Sentence-Order-Prediction-SOP" class="headerlink" title="Sentence Order Prediction (SOP)"></a>Sentence Order Prediction (SOP)</h3><p>作者比较了不使用额外任务, NSP, SOP三者之间对ALBERT - base的影响, 结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/albert6.jpg" style="zoom:50%;" /><p>使用NSP确实会损害模型性能, 使用SOP确实会让模型涨点, 但是SST - 2的性能和不使用额外任务相当.</p><h3 id="Train-for-the-Same-Amount-of-Time"><a href="#Train-for-the-Same-Amount-of-Time" class="headerlink" title="Train for the Same Amount of Time"></a>Train for the Same Amount of Time</h3><p>在前面的实验中, ALBERT - xxlarge比BERT - large的速度要慢许多. 通常情况下, 更长的训练时间会有更好的性能, 这可能导致二者比较的不公平.</p><p>作者在此不再控制二者训练量相同, 而是将BERT和ALBERT拉到了相同训练时间下比较, 结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/albert12.jpg" style="zoom:50%;" /><p>在同训练时间下, ALBERT要优于BERT, 即ALBERT训练较为高效.</p><h3 id="Additional-Training-Data-and-Dropout-Effects"><a href="#Additional-Training-Data-and-Dropout-Effects" class="headerlink" title="Additional Training Data and Dropout Effects"></a>Additional Training Data and Dropout Effects</h3><p>RoBERTa和XLNet比BERT所使用的数据要多得多, 作者尝试将额外的训练数据添加到训练中, 前后对比结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/albert7.jpg" style="zoom:50%;" /><p>除去SQuAD, 其他下游任务的性能略有提升, 这是因为除去维基百科的数据集, 其他数据相对于SQuAD来说是一种噪声, 即Out of Domain, 也许维基百科数据集已经能比较有针对性的解决SQuAD问题了.</p><p>即使是训练了1M Step, ALBERT也没有找到局部最优, 所以作者在ALBERT上去掉了Dropout, 使得性能有进一步的提升:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/albert9.jpg" style="zoom:50%;" /><blockquote><p>其实很好理解为什么去掉Dropout后模型能继续训练下去, 模型本身都没法达到过拟合, 何谈防止过拟合? 在模型没能找到局部最优时, 加入正则化手段自然而然会损害训练.</p></blockquote><p>上面二者在训练过程中对ACC的影响如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/albert8.jpg" style="zoom:50%;" /><p>添加额外数据和移除Dropout后, 验证集ACC有显著提升.</p><h3 id="Current-SOTA-and-NLU-Tasks"><a href="#Current-SOTA-and-NLU-Tasks" class="headerlink" title="Current SOTA and NLU Tasks"></a>Current SOTA and NLU Tasks</h3><p>作者把流行的Baseline放到一起在GLUE上做了对比, 结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/albert10.jpg" style="zoom:50%;" /><p>ALEBRT(1M) 代表ALBERT在该任务上Train了1M个Step, 与RoBERTa训练量相同.</p><p>在SQuAD和RACE上的结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/albert11.jpg" style="zoom:50%;" /><p>ALBERT仍然是SOTA.</p><p>附录中还有一些对加强ALBERT宽度和深度的实验, 结果都表明当深度或宽度到达一定阈值后, 性能不再能继续增加, 甚至有时会出现退化.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>ALBERT的主要贡献是减了参数, 而不是计算量. 换句话说, 只能让BERT跑起来, 但推理阶段跑的快不快就不管了. 这似乎显得ALBERT有些鸡肋了, 因为减少模型参数后它的精度仍然会受到影响, 如果继续增大深度运算量又是个门槛($H$ 比较大, 而且层数多了, 肯定会增大运算量), 这点大家好像吐槽比较多.</p><p>ALBERT能够与BERT媲美的本质可能是$H$ 增大所带来增益要超过缩小参数带来的负面效应.</p><p>ALBERT的改进点都比较偏向于工程, <del>比起论文它更像一篇炼丹报告…,</del> 看实验结果感觉应该是到ALBERT所使用的压缩方法的性能顶峰了.</p><blockquote><p>模型压缩的目的从来都不是使得小模型的效果好过大模型, 而是利用某种方式, 使得模型的参数量或计算量减少, 同时<strong>不会带来明显的性能下降</strong>.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> BERT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>UniLM: Unified Language Model Pre-training for Natural Language Understanding and Generation</title>
      <link href="/posts/14266.html"/>
      <url>/posts/14266.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>BERT: 详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>.</li></ul></blockquote><h1 id="UniLM-Unified-Language-Model-Pre-training-for-Natural-Language-Understanding-and-Generation"><a href="#UniLM-Unified-Language-Model-Pre-training-for-Natural-Language-Understanding-and-Generation" class="headerlink" title="UniLM: Unified Language Model Pre-training for Natural Language Understanding and Generation"></a>UniLM: Unified Language Model Pre-training for Natural Language Understanding and Generation</h1><p>本文是论文<a href="https://arxiv.org/abs/1905.03197" target="_blank" rel="noopener">Unified Language Model Pre-training for Natural Language Understanding and Generation</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者认为, 当前流行的语言模型有三大类, 分别是以<strong>单向</strong>(左向右或右向左)的语言模型ELMo, GPT, 以<strong>双向</strong>为代表的语言模型BERT, 以及由各类Encoder和Decoder组合使用的<strong>Seq2Seq</strong>类模型:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/unilm1.jpg" style="zoom:50%;" /><p>单向, 双向, Seq2Seq类的模型的优缺点各不相同, 所擅长的下游任务也不同, 对语言的编码方式更是不同:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/unilm2.jpg" style="zoom:50%;" /><p>尽管这三类模型各有千秋, 但从来没有人尝试把上述三类模型<strong>统一</strong>到同一个模型中:</p><p>因此, 作者提出<strong>UniLM</strong>(<strong>Uni</strong>fied pre-trained <strong>L</strong>anguage <strong>M</strong>odel), 将上述三大类模型统一到同一个语言模型中, 共享同一组参数.</p><h2 id="UniLM"><a href="#UniLM" class="headerlink" title="UniLM"></a>UniLM</h2><h3 id="Input-Representation"><a href="#Input-Representation" class="headerlink" title="Input Representation"></a>Input Representation</h3><p>对于输入的序列$x$, 无论是作为单向语言模型的文本段, 还是对于双向语言模型训练时所使用的文本对, 都在输入文本段前添加起始Token<code>[SOS]</code>, 添加在文本段结束时添加结束Token<code>[EOS]</code>.</p><p>此外, <code>[EOS]</code> 不光作为NLU任务中的边界标记, 还作为NLG任务中的生成结束标记.</p><p>分词时采用WordPiece.</p><h3 id="Backbone-Network-Multi-Layer-Transformer"><a href="#Backbone-Network-Multi-Layer-Transformer" class="headerlink" title="Backbone Network: Multi - Layer Transformer"></a>Backbone Network: Multi - Layer Transformer</h3><blockquote><p>本部分即Transformer Encoder.</p></blockquote><p>对输入的Token表示$\left\{\mathbf{x}_{i}\right\}_{i=1}^{|x|}$, 输入第0层Transformer中获得初始上下文表示$\mathbf{H}^{0}=\left[\mathbf{x}_{1}, \cdots, \mathbf{x}_{|x|}\right]$, 经过$L$层Transformer堆叠$\mathbf{H}^{l}=\text { Transformer }_{l}\left(\mathbf{H}^{l-1}\right), l \in[1, L]$, 获得最终表示$\mathbf{H}^{l}=\left[\mathbf{h}_{1}^{l}, \cdots, \mathbf{h}_{|x|}^{l}\right]$. </p><p>每层Transformer层中的自注意力头输出$\mathbf{A}_l$ 可以被表示为:</p><p>$$<br>\begin{aligned}<br>\mathbf{Q} &amp;=\mathbf{H}^{l-1} \mathbf{W}_{l}^{Q}, \quad \mathbf{K}=\mathbf{H}^{l-1} \mathbf{W}_{l}^{K}, \quad \mathbf{V}=\mathbf{H}^{l-1} \mathbf{W}_{l}^{V} \\<br>\mathbf{M}_{i j} &amp;=\left\{\begin{array}{ll}<br>0, &amp; \text { allow to attend } \\<br>-\infty, &amp; \text { prevent from attending }<br>\end{array}\right.\\<br>\mathbf{A}_{l} &amp;=\operatorname{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^{\top}}{\sqrt{d_{k}}}+\mathbf{M}\right) \mathbf{V}_{l}<br>\end{aligned}<br>$$</p><p>其中, $\mathbf{W}_l^Q, \mathbf{W}_l^K, \mathbf{W}_l^V$ 为线性投影矩阵, $\mathbf{M}$ 为Mask矩阵, 决定Token能够给予哪些部分注意力.</p><p>UniLM使用不同的Mask矩阵$\mathbf{M}$ 来决定模型在接下来的运算中能对哪些Token给予注意力. 所以UniLM能直接通过更改$\mathbf{M}$ 在使用同一组参数的条件下改变模型的运行模式, 非常灵活.</p><h3 id="Pre-Training-Objectives"><a href="#Pre-Training-Objectives" class="headerlink" title="Pre - Training Objectives"></a>Pre - Training Objectives</h3><p>在上一节说过, 双向语言模型, 单向语言模型, 亦或许是Seq2Seq架构下的语言模型, 都能够通过修改Self - Attention Mask的形式实现:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/unilm3.jpg" style="zoom:50%;" /><p>UniLM仍然采用BERT类似的<strong>完形填空</strong>式作为目标, 采用<strong>最小化交叉熵</strong>的方式来优化模型参数.</p><p>在预训练, 作者介绍了单向, 双向, Seq2Seq三类目标在UniLM中的实现方式(其实加上NSP是四类, 但NSP仅在双向语言模型目标中使用):</p><ul><li><p><strong>Unidirectional LM</strong> - (上图中间): 对于单向语言目标, 不对输入分段, 只输入文本段$\text{S}_1$. 在左到右或右到左的单一方向是时序可见的, 因此只需要将$\mathbf{M}$ 的上三角或下三角设置为$-\infty$, 其余位置置零即可.</p></li><li><p><strong>Bidirectional LM</strong>- (上图顶端): 对于双向语言目标, 使用了NSP任务, 输入文本段$\text{S}_1, \text{S}_2$, 与BERT训练方式保持一致. Token之间是全部互相可见的, 因此$\mathbf{M}=\mathbf{0}$.</p></li><li><p><strong>Sequence-to-Sequence LM</strong> - (上图底部): 对于Seq2Seq目标, 输入源序列$\text{S}_1$ 和目标序列$\text{S}_2$, 并在 Encoder对应的$\text{S}_1$ 应该是双向可见的, 故$\mathbf{M}$ 的左侧为$\mathbf{0}$. 在Decoder对应的$\text{S}_2$, 上下文只应该单向可见, 即$\mathbf{M}$ 右侧$\text{S}_2$ 的上三角为$-\infty$, 其余为$\mathbf{0}$. </p><p>例如, 在预训练阶段, 对于Seq2Seq任务的输入$\text{S}_1=[t_1, t_2]$, 输出$\text{S}_2=[t_3, t_4, t_5]$, 将以<code>[SOS]</code>, $t_1$, $t_2$, <code>[EOS]</code>, $t_3$, $t_4$, $t_5$, <code>[EOS]</code> 的形式输入到模型中. $t_2$ 只能看见<code>[SOS]</code>, $t_1$, $t_2$, <code>[EOS]</code>, 而$t_4$ 可以看到<code>[SOS]</code>, $t_1$, $t_2$, <code>[EOS]</code>, $t_3$, $t_4$.</p><blockquote><p>在打Mask时, 如果Mask掉的部分为$\text{S}_1$ 的Token, 则有助于模型学到双向Encoder, 若Mask掉的部分为$\text{S}_2$ 的Token, 则有助于让模型学到单向Decoder. 另外, 由于<code>[EOS]</code> 是可以被Mask掉的, 在预测时可以教会模型何时该结束生成.</p><p>仔细一想其实挺巧妙的, 因为这样并不需要明确的在模型中划分出Encoder和Decoder的位置. 也就意味着整个模型的所有区域都有可能被当做是Encoder或者Decoder.</p></blockquote></li></ul><p>在UniLM实际训练过程中, 总的Loss为三者的<strong>加和</strong>. 并且对各类训练目标分配时间是<strong>均匀</strong>的, 即有$\frac{1}{3}$ 时间采用双向语言训练目标, $\frac{1}{3}$ 时间采用Seq2Seq训练目标, 从左到右和从右到左的单向语言训练目标各有$\frac{1}{6}$ 时间 . 因为只是对Mask做出了调整, 因此<strong>与BERT完全兼容</strong>, UniLM参数直接使用BERT - LARGE初始化.</p><blockquote><p>这三种类型的训练难度可能不同, 这里将它们平均考虑应该是出于探索, 不带来过多的麻烦.</p></blockquote><h3 id="Fine-Tuning-on-Downstream-NLU-and-NLG-Tasks"><a href="#Fine-Tuning-on-Downstream-NLU-and-NLG-Tasks" class="headerlink" title="Fine - Tuning on Downstream NLU and NLG Tasks"></a>Fine - Tuning on Downstream NLU and NLG Tasks</h3><ul><li>对NLU任务, 采用<code>[SOS]</code> 处的输出作为整个输入的表示, 记为$\mathbf{h}_1^L$, 可以由此计算文本在分类任务上的概率$\operatorname{softmax}\left(\mathbf{h}_{1}^{L} \mathbf{W}^{C}\right)$, $C$ 为类别数, 最大化类别标签的概率即可, 该部分与BERT一致.</li><li>对NLG任务, 令输入的源序列$\text{S}_1$, 目标序列$\text{S}_2$, 以<code>[SOS]</code>, $\text{S}_1$, <code>[EOS]</code>, $\text{S}_2$, <code>[EOS]</code> 的形式输入. 训练目标为最大化在给定上下文条件下, 被Mask部分原来内容的概率. 但在精调阶段, 只对$\text{S}_2$ 中的内容打Mask.</li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的实验参数设置请参考原论文.</p><h3 id="Abstractive-Summarization"><a href="#Abstractive-Summarization" class="headerlink" title="Abstractive Summarization"></a>Abstractive Summarization</h3><p>在CNN / DailyMail上的抽取式摘要和生成式摘要结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/unilm4.jpg" style="zoom:50%;" /><p>Gigaword上生成式摘要结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/unilm5.jpg" style="zoom:50%;" /><p>UniLM要比Baseline有小幅提升, 并几乎全面领先.</p><h3 id="Question-Answering-QA"><a href="#Question-Answering-QA" class="headerlink" title="Question Answering (QA)"></a>Question Answering (QA)</h3><h4 id="Extractive-QA"><a href="#Extractive-QA" class="headerlink" title="Extractive QA"></a>Extractive QA</h4><p>抽取式QA的在SQuAD上的结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/unilm6.jpg" style="zoom:50%;" /><p>在CoQA上结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/unilm7.jpg" style="zoom:50%;" /><h4 id="Generative-QA"><a href="#Generative-QA" class="headerlink" title="Generative QA"></a>Generative QA</h4><p>生成式QA在CoQA上结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/unilm8.jpg" style="zoom:50%;" /><p>相较于指针生成网络, 在生成式QA上有非常明显的进步.</p><h3 id="Question-Generation"><a href="#Question-Generation" class="headerlink" title="Question Generation"></a>Question Generation</h3><p>SQuAD上问题生成结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/unilm9.jpg" style="zoom:50%;" /><p>作者还使用UniLM所生成的问题让QA中的模型结果涨了四个点:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/unilm10.jpg" style="zoom:50%;" /><p>据作者所述, 这应该是一种<strong>数据增强</strong>方法, 用生成的500w个可回答的问题, 以及经过更改后不可回答的400w个问题, 反喂给QA的UniLM微调少量Epoch, 结果有提升.</p><h3 id="Response-Generation"><a href="#Response-Generation" class="headerlink" title="Response Generation"></a>Response Generation</h3><p>对话生成结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/unilm11.jpg" style="zoom:50%;" /><p>与Baseline相比, 若假设数据集所采取的评价指标是有效的, UniLM已经超过人类表现, 甚至更加精简.</p><h3 id="GLUE-Benchmark"><a href="#GLUE-Benchmark" class="headerlink" title="GLUE Benchmark"></a>GLUE Benchmark</h3><p>在GLUE上的结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/unilm12.jpg" style="zoom:50%;" /><p>UniLM达到了新SOTA.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>同是出自MSRA的论文, UniLM与<a href="https://adaning.github.io/posts/46395.html">MASS</a>所尝试的思路是完全相反的.</p><p>如果说MASS是将BERT搬到了Seq2Seq上, 那么UniLM则是将Seq2Seq搬入了BERT体系, 用Mask来实现Seq2Seq的思路还是挺巧妙的, 个人感觉UniLM比MASS要有趣一些. 从结果上来看, UniLM将各类任务推向了新SOTA.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> BERT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MASS: Masked Sequence to Sequence Pre - training for Language Generation</title>
      <link href="/posts/46395.html"/>
      <url>/posts/46395.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识;</p><ul><li>BERT: 详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>.</li><li>Transformer: 详见<a href="https://adaning.github.io/posts/6744.html">Transformer精讲</a>.</li></ul></blockquote><h1 id="MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation"><a href="#MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation" class="headerlink" title="MASS: Masked Sequence to Sequence Pre-training for Language Generation"></a>MASS: Masked Sequence to Sequence Pre-training for Language Generation</h1><p>本文是论文<a href="https://arxiv.org/abs/1905.02450" target="_blank" rel="noopener">MASS: Masked Sequence to Sequence Pre-training for Language Generation</a>的阅读笔记和个人理解. 最近一个月在忙毕设相关, 所以有些论文看完还没有做笔记, 本文属于对这段时间没有写笔记的论文填坑之一.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>在Pre - training, Fine - tuning下的BERT是没有办法处理生成问题的, 而自回归的结构能较好的处理序列生成问题, 所以作者尝试继续将效果比较好的BERT重新迁移回Transformer的<strong>Seq2Seq</strong>框架下, 并能与<strong>Encoder - Decoder</strong>相兼容.</p><blockquote><p>该模型与<a href="https://adaning.github.io/posts/1394.html">BART</a>属于相同出发点的模型, 建议也学习一下BART.</p></blockquote><h2 id="MASS"><a href="#MASS" class="headerlink" title="MASS"></a>MASS</h2><h3 id="Sequence-to-Sequence-Learning"><a href="#Sequence-to-Sequence-Learning" class="headerlink" title="Sequence to Sequence Learning"></a>Sequence to Sequence Learning</h3><p>先来回顾一下标准的Seq2Seq训练过程. </p><p>在序列生成任务中, 输入输出为句子对$(x, y)\in(\mathcal{X},\mathcal{Y})$, $x$ 包含$m$ 个Token的源句子, 即$x=(x_1, x_2,\dots, x_m)$, $y$ 为包含$n$ 个Token的目标句子, 即$y=(y_1, y_2,\dots, y_n)$.</p><p>在Seq2Seq问题下, 目标为最大化模型参数$\theta$ 下的条件概率$P(y \mid x;\theta)$, 根据极大似然, 并结合语言模型中自回归的特性, 条件概率由链式法则能够被拆分, 目标函数为:<br>$$<br>\begin{aligned}<br>L(\theta ;(\mathcal{X}, \mathcal{Y}))&amp;=\sum_{(x, y) \in(\mathcal{X}, \mathcal{Y})} \log P(y \mid x ; \theta)\\<br>&amp;=\sum_{(x, y) \in(\mathcal{X}, \mathcal{Y})} \log \prod_{t=1}^{n} P\left(y_{t} \mid y_{&lt;t}, x ; \theta\right)<br>\end{aligned}<br>$$<br>其中, $y_{&lt;t}$ 为$t$ 时刻前自回归模型的输出.</p><p>一种最普遍的训练法为将Seq2Seq中Encoder的每个时刻隐态输出全部拿到, 然后用Attention机制在Decoder端对隐态输出加权解码.</p><h3 id="Masked-Sequence-to-Sequence-Pre-Training"><a href="#Masked-Sequence-to-Sequence-Pre-Training" class="headerlink" title="Masked Sequence to Sequence Pre - Training"></a>Masked Sequence to Sequence Pre - Training</h3><p>在<strong>MASS</strong>(<strong>MA</strong>sked <strong>S</strong>equence to <strong>S</strong>equence Pre - training for Language Generation)中, 不再采用句子对的方法训练模型, 而是采用<strong>去噪编码器</strong>的方式训练模型. </p><p>在Encoder端, 对于给定的Token数量为$m$ 的单句$x \in \mathcal{X}$, 假设<strong>连续</strong>地将位置$u \to v,(0&lt;u&lt;v&lt;m)$ 上的Token打上Mask, 打上Mask后的部分记为$x^{\backslash u:v}$, 打Mask前的实际内容被记为$x^{u:v}$. 那么$k=v-u+1$ 就为一共打上Mask的数量. </p><p>在Decoder端, Decoder的目标为重构加噪的文本, 即预测出被Mask掉的内容, 由于<strong>Teacher Forcing</strong>, 训练时的Decoder的输入为移位后的$x^{u:v}$, 其余全为Mask. </p><p>标准Seq2Seq的目标函数在引入Mask后就应该被改写为:</p><p>$$<br>\begin{aligned}<br>L(\theta ; \mathcal{X}) &amp;=\frac{1}{|\mathcal{X}|} \Sigma_{x \in \mathcal{X}} \log P\left(x^{u: v} \mid x^{\backslash u: v} ; \theta\right) \\<br>&amp;=\frac{1}{|\mathcal{X}|} \Sigma_{x \in \mathcal{X}} \log \prod_{t=u}^{v} P\left(x_{t}^{u: v} \mid x_{&lt;t}^{u: v}, x^{\backslash u: v} ; \theta\right)<br>\end{aligned}<br>$$</p><p>对于上述过程, 举出一个例子:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mass1.jpg" style="zoom: 50%;" /><p>其中<code>_</code>代表<code>[MASK]</code>.</p><p>我们将长度为$m=8$ 的句子中$x_3, x_4, x_5, x_6$ 连续地打上Mask, 在Decoder端, 除去Encoder中被Mask掉的$x_3, x_4, x_5$, 其余输入全部为Mask. </p><blockquote><p>对于为何Decoder处将不需要被预测的Token都变成Mask, 作者的解释是这样能更好利用Encoder的编码, 结合Attention, 使得Encoder和Decoder更好的联合训练.</p></blockquote><p>连续Mask的长度$k$ 为超参数. 作者巧妙地利用不同$k$ 的取值将BERT和GPT统一在了同一个框架中:</p><ul><li><p>BERT:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mass2.jpg" style="zoom: 50%;" /><p>当$k=1$ 时, Decoder没有输入任何额外信息, 单纯利用Encoder的信息预测出被Mask掉的Token. 从双向利用上下文的角度来说, BERT是MASS的一种特殊情况.</p></li><li><p>GPT:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mass12.jpg" style="zoom: 50%;" /><p>当$k=m$ 时, MASS直接就变成了自回归式的生成模型, 需要预测所有的Token. Encoder没有给Decoder任何信息.</p></li><li><p>MASS:</p><p>MASS便是介于BERT和GPT二者之间的, $k\in(1, m)$.</p></li></ul><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mass3.jpg" style="zoom: 50%;" /><p>因此, MASS最大的特点也是在BERT和GPT之间做了一个<strong>折中</strong>, 以Mask的形式进一步将BERT和GPT的特点结合到同一个框架下.</p><h3 id="Discussions"><a href="#Discussions" class="headerlink" title="Discussions"></a>Discussions</h3><p>这部分作者简述了MASS的几个特点:</p><ul><li><p>Encoder和Decoder联合训练. 在其他语言模型或BERT中都只是单独训练Encoder或Decoder. </p><blockquote><p>这点是针对预训练来说的, MASS将问题转化为Seq2Seq后, 当然可以做到对于任何问题, 都用联合训练好的Encoder和Decoder解决.</p></blockquote></li><li><p>连续的Mask能带给Encoder更好的NLU能力和Decoder更好的解码能力.</p></li><li><p>使Decoder能从Encoder获取更多的信息, 而不是获取Decoder自身生成的信息.</p></li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文.</p><h3 id="Neural-Meachine-Translation"><a href="#Neural-Meachine-Translation" class="headerlink" title="Neural Meachine Translation"></a>Neural Meachine Translation</h3><p>无监督机器翻译上的BLEU得分如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mass4.jpg" style="zoom: 50%;" /><p>MASS在无监督机器翻译上表现还可以, 除了英语翻译到法语, 剩下的数据集基本上涨了一个点.</p><p>与其他预训练方法相比, 结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mass5.jpg" style="zoom: 50%;" /><p>MASS比其他的预训练方法平均涨了3个点左右.</p><p>作者还展示了MASS在少资源状态下的机器翻译下的效果:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mass6.jpg" style="zoom: 50%;" /><blockquote><p>这个实验的Baseline选取还是拿没有预训练的模型和做过预训练的MASS去Fine Tuning后的结果进行比较… 自然是预训练后又微调的效果好. 所以少资源状态下的实验结果和结论都没有说服力, 在下一小节中的低资源实验也是类似的情况, 不再特意说明.</p></blockquote><h3 id="Text-Summarization"><a href="#Text-Summarization" class="headerlink" title="Text Summarization"></a>Text Summarization</h3><p>文本摘要任务上的结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mass7.jpg" style="zoom: 50%;" /><p>在高资源状态下, 相较于Baseline, 效果有一些提升, 但似乎没有很大.</p><p>与其他的预训练方法比较结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mass8.jpg" style="zoom: 50%;" /><p>MASS结果有些许提升.</p><h3 id="Conversational-Response-Generation"><a href="#Conversational-Response-Generation" class="headerlink" title="Conversational Response Generation"></a>Conversational Response Generation</h3><p>对话生成任务的PPL如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mass9.jpg" style="zoom: 50%;" /><p>与BERT相比, MASS的PPL低了很多.</p><h3 id="Analysis-of-MASS"><a href="#Analysis-of-MASS" class="headerlink" title="Analysis of MASS"></a>Analysis of MASS</h3><h4 id="Study-of-Different-k"><a href="#Study-of-Different-k" class="headerlink" title="Study of Different k"></a>Study of Different k</h4><p>作者探究了$k$ 值的设定对各类任务性能的影响:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mass10.jpg" style="zoom: 50%;" /><p>非常神奇的是所有任务几乎都在为$k=50\% \ast m$ 效果最好.</p><h4 id="Ablation-Study-of-MASS"><a href="#Ablation-Study-of-MASS" class="headerlink" title="Ablation Study of MASS"></a>Ablation Study of MASS</h4><p>作者对MASS的改动做了消融实验:</p><ol><li>将MASS的连续Mask方式改为离散, 记为Discrete.</li><li>将Decoder输入的Token不再打Mask, 记为Feed.</li></ol><p>在en - fr上的无监督机器翻译结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mass11.jpg" style="zoom: 50%;" /><p>MASS的这两个改动确实有性能提升, 但似乎没有特别大的变化.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>MASS与BART属于相同类型的模型, 二者处理思路都是重新把<strong>BERT拉回到Seq2Seq框架</strong>下做<strong>生成</strong>任务. 这二者的侧重点不同, MASS更侧重对<strong>Decoder</strong>的改进, 而BART更侧重对<strong>加噪方法</strong>做出调整. </p><p>从实验结果来看, MASS的性能提升并没有很大, 似乎其有效性也受到争议, 主要原因是实验设计<strong>说服力不够</strong>.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> BERT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SpanBERT: Improving Pre-training by Representing and Predicting Spans</title>
      <link href="/posts/34019.html"/>
      <url>/posts/34019.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>BERT: 详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>.</li></ul></blockquote><h1 id="SpanBERT-Improving-Pre-training-by-Representing-and-Predicting-Spans"><a href="#SpanBERT-Improving-Pre-training-by-Representing-and-Predicting-Spans" class="headerlink" title="SpanBERT: Improving Pre-training by Representing and Predicting Spans"></a>SpanBERT: Improving Pre-training by Representing and Predicting Spans</h1><p>本文是论文<a href="https://www.aclweb.org/anthology/2020.tacl-1.5/" target="_blank" rel="noopener">SpanBERT: Improving Pre-training by Representing and Predicting Spans</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者认为, 许多NLP任务中所涉及到的推理任务是关于<strong>多个</strong>Text Token之间的, 而非基于单个Token之间的. </p><p>例如, 在回答问题<code>Which NFL team won Super Bowl 50?</code>时, 直接给出答案<code>Denver Broncos</code>比在<code>Denver</code>后给出<code>Broncos</code>要困难的多, 但前者却更贴近与现实场景, 难度也更大.</p><p>作者尝试提出SpanBERT来解决这种Span Level Prediction的问题.</p><h2 id="SpanBERT"><a href="#SpanBERT" class="headerlink" title="SpanBERT"></a>SpanBERT</h2><p>SpanBERT通过三种方式来帮助模型理解语言:</p><ol><li>将Token Level Mask替换为<strong>Span Level</strong> Mask.</li><li>引入一种新的辅助目标来<strong>SBO</strong>帮助模型训练.</li><li>只使用<strong>单个句子</strong>训练, 而非BERT所使用的句子对.</li></ol><h3 id="Span-Masking"><a href="#Span-Masking" class="headerlink" title="Span Masking"></a>Span Masking</h3><p>在BERT中, 对于给定的输入序列$X=\left(x_{1}, x_{2}, \ldots, x_{n}\right)$, 其编码表示为:<br>$$<br>\operatorname{enc}\left(x_{1}, x_{2}, \ldots, x_{n}\right)=\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{n}<br>$$<br>但在BERT中, 打Mask是对单个Token做的. 在SpanBERT中, 作者使用Span Level的Mask, 对一连串的Token做指定长度的Mask. 通过<strong>均匀分布</strong>随机采样得到Span Mask的初始位置.</p><p>而Span Mask的长度是通过<strong>迭代随机采样</strong>的来的, 继续扩展Span的几率服从<strong>几何分布</strong>$\ell \sim \operatorname{Geo}(p=0.2)$, 最大的Span长度$\ell_{\max }=10$. 即连续采样, 每次都有$p=0.2$ 的几率继续扩展Span, 每次都有$q = 1 - p = 0.8$ 的几率停止扩展Span. 其概率分布如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/spanbert2.jpg" style="zoom:33%;" /><p>那么Span长度的期望值为3.8, 其推导过程如下:<br>$$<br>\begin{aligned}<br>&amp;p = 0.2 \\<br>&amp;q = 1 - p = 0.8 \\<br>&amp;p^\prime = \frac{p}{1-q^{10}} = 0.224 \\<br>\end{aligned}<br>$$<br>$p^\prime$ 代表Span最大长度为10以下的概率的归一化. $1-q^{10}$ 为当前Span不超过10的概率和, 需要用$p$ 去除以这个值, 让$p^\prime$ 排除掉Span长度大于10的情况.</p><blockquote><p>参考<a href="https://zhuanlan.zhihu.com/p/75893972" target="_blank" rel="noopener">SpanBert：对 Bert 预训练的一次深度探索</a>的评论区.</p></blockquote><p>故限定Span最大长为10的期望为:<br>$$<br>\begin{aligned}<br>E(x) &amp;= p^\prime \sum_{n=1}^{10}n q^{n-1} \\<br> &amp;= p^\prime (1 + 2q + 3q^2 + \dots + 10q^9) \\<br>  &amp;= 0.224 \cdot 16.9469=3.797 \approx3.8<br>\end{aligned}<br>$$<br>即Span长度期望为3.8, 如果向上取整就是4.</p><p>Span Masking将Token Level的Mask变更为了Span Level的Mask, 任务的训练难度更为复杂.</p><h3 id="Span-Boundary-Objective"><a href="#Span-Boundary-Objective" class="headerlink" title="Span Boundary Objective"></a>Span Boundary Objective</h3><p>作者希望Span的<strong>结尾</strong>能尽可能多的表示出Span<strong>内部</strong>的内容, 因此作者直接引入一个新的辅助目标来实现.</p><blockquote><p>SBO的提出应该也受到一些其他模型的启发, 例如<strong>ERNIE(Baidu)</strong>, <strong>BERT WWM</strong>等模型, 都是基于Span Level Mask做的额外处理.</p></blockquote><p>对于Span外部的起始边界和结束边界$(s, e)$, 作者希望通过某种方式$f(\cdot)$, 来根据其边界两侧的表示$\mathbf{x}_{s-1}, \mathbf{x}_{e+1}$, 以及<strong>每个Span内部的Token</strong> $x_i$ 所对应的位置编码$\mathbf{p}_{i-s+1}$ 来得到预测结果$\mathbf{y}_i$, 从而预测出Span内的每个Token:</p><p>$$<br>\mathbf{y}_{i}=f\left(\mathbf{x}_{s-1}, \mathbf{x}_{e+1}, \mathbf{p}_{i-s+1}\right)<br>$$</p><p>这里作者简单的使用两层FFN和GeLU来将$\mathbf{y}_i$ 转换为$x_i$:</p><p>$$<br>\begin{array}{l}<br>\mathbf{h}_{0}=\left[\mathbf{x}_{s-1} ; \mathbf{x}_{e+1} ; \mathbf{p}_{i-s+1}\right] \\<br>\mathbf{h}_{1}=\text { LayerNorm }\left(\operatorname{GeLU}\left(\mathbf{W}_{1} \mathbf{h}_{0}\right)\right) \\<br>\mathbf{y}_{i}=\text { LayerNorm }\left(\operatorname{GeLU}\left(\mathbf{W}_{2} \mathbf{h}_{1}\right)\right)<br>\end{array}<br>$$</p><p>$[\cdot]$ 代表拼接操作.</p><p>SBO将和MLM的损失函数共同优化, 二者之间是简单的加和关系即可:<br>$$<br>\begin{aligned}<br>\mathcal{L}\left(x_{i}\right) &amp;=\mathcal{L}_{\mathrm{MLM}}\left(x_{i}\right)+\mathcal{L}_{\mathrm{SBO}}\left(x_{i}\right) \\<br>&amp;=-\log P\left(x_{i} \mid \mathbf{x}_{i}\right)-\log P\left(x_{i} \mid \mathbf{y}_{i}\right)<br>\end{aligned}<br>$$<br>那么根据SBO, 模型可能会学到一些关于Span的内容, 因为SBO要求模型必须用边界信息来猜Span内部指定位置的内容.</p><p>下面给出一个说明SBO的例子:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/spanbert1.jpg" style="zoom: 50%;" /><p>在该例中, $\text{football}$ 被选中, 将在其附近给长度为4的区域打上Mask, 这个词应该根据被Mask的区域的<strong>边界表示</strong>$\mathbf{x}_3, \mathbf{x}_9$, 以及$\text{football}$ 在Span Mask中的<strong>相对位置</strong>的编码$\mathbf{p}_3$ 共同预测出来.</p><p>所以在该例中, 损失函数为MLM预测$\text{football}$ 和SBO预测$\text{football}$ 的损失之和:<br>$$<br>\begin{aligned}<br>\mathcal{L}(\text { football }) &amp;=\mathcal{L}_{\mathrm{MLM}}(\text { football })+\mathcal{L}_{\mathrm{SBO}}(\text { football }) \\<br>&amp;=-\log P\left(\text { football } \mid \mathbf{x}_{7}\right)-\log P\left(\text { football } \mid \mathbf{x}_{4}, \mathbf{x}_{9}, \mathbf{p}_{3}\right)<br>\end{aligned}<br>$$</p><h3 id="Single-Sequence-Training"><a href="#Single-Sequence-Training" class="headerlink" title="Single - Sequence Training"></a>Single - Sequence Training</h3><p>在SpanBERT中, 作者舍弃了BERT的句子对训练法, 仅使用<strong>单句训练</strong>, 并抛弃NSP任务, 理由如下: </p><ol><li>句子对的引入限制了<strong>单句最长文本长度</strong>. 使用单句训练, 最长文本长度可以直接<strong>翻倍</strong>.</li><li>句子对上下不相关时, 会引入非常大的<strong>噪声</strong>.</li></ol><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文.</p><h3 id="Extractive-Question-Answering"><a href="#Extractive-Question-Answering" class="headerlink" title="Extractive Question Answering"></a>Extractive Question Answering</h3><p>抽取式QA的结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/spanbert3.jpg" style="zoom:33%;" /><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/spanbert4.jpg" style="zoom:33%;" /><p>在这些数据集上的提升趋势是一致的, 每个数据集都有提升.</p><h3 id="Coreference-Resolution"><a href="#Coreference-Resolution" class="headerlink" title="Coreference Resolution"></a>Coreference Resolution</h3><p><strong>指代消解</strong>的任务目标是将文本中提到的同一实体的不同表述找出来, 即<strong>将同一事物的不同自然语言描述链接到文本中的同一事物</strong>.</p><p>对指代消解任务, 每个Mention Span $x$, 它与之前文本中对应的多个定长Span $y^\prime \in Y$ 都有一个得分$s(x, y)$, 根据得分使用Softmax能够判断出$y$ 是哪个Span的指代:</p><p>$$<br>P(y)=\frac{e^{s(x, y)}}{\sum_{y^{\prime} \in Y} e^{s\left(x, y^{\prime}\right)}}<br>$$</p><p>其中Span Pair的打分函数$s(x, y)$ 是由FFN得来的:</p><p>$$<br>\begin{aligned}<br>s(x, y) &amp;=s_{m}(x)+s_{m}(y)+s_{c}(x, y) \\<br>s_{m}(x) &amp;=\mathrm{FFNN}_{m}\left(\mathbf{g}_{\mathrm{x}}\right) \\<br>s_{c}(x, y) &amp;=\mathrm{FFNN}_{c}\left(\mathbf{g}_{\mathrm{x}}, \mathbf{g}_{\mathbf{y}}, \phi(x, y)\right)<br>\end{aligned}<br>$$</p><p>在这里, $\mathbf{g}_x, \mathbf{g}_y$ 代表两个Transformer提取出Span<strong>两个端点</strong>的隐态输出和Span内部的<strong>Attention的加权求和</strong>后的拼接向量, $\text{FFNN}_m, \text{FFNN}_c$ 代表两个有一层隐层的前馈神经网络, $\phi(x, y)$ 代表人工构建的特征.</p><blockquote><p>和论文<a href="https://arxiv.org/abs/1908.09091" target="_blank" rel="noopener">BERT for Coreference Resolution: Baselines and Analysis</a>的使用方法一致, BERT系列指代消解模型是BiLSTM + Attention系列模型在C2F - Coref上的升级.</p><p>更多关于神经网络指代消解的论文, 还可以参考:</p><ul><li><a href="https://arxiv.org/abs/1707.07045" target="_blank" rel="noopener">End-to-end Neural Coreference Resolution</a></li><li><a href="https://arxiv.org/abs/1804.05392" target="_blank" rel="noopener">Higher-order Coreference Resolution with Coarse-to-fine Inference</a></li></ul></blockquote><p>指代消解的数据集OntoNotes上表现如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/spanbert5.jpg" style="zoom:33%;" /><p>相较于其他的BERT Baseline, SpanBERT有些许提升, 并且提升在每个数据集上都是一致的.</p><h3 id="Relation-Extraction"><a href="#Relation-Extraction" class="headerlink" title="Relation Extraction"></a>Relation Extraction</h3><p>在关系抽取的数据集TACRED上结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/spanbert6.jpg" style="zoom:33%;" /><p>SpanBERT在关系抽取上的标现达到平均水平, 在Recall上进步比较大.</p><h3 id="GLUE"><a href="#GLUE" class="headerlink" title="GLUE"></a>GLUE</h3><p>GLUE上的结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/spanbert7.jpg" style="zoom:33%;" /><p>相较于其他BERT Baseline, SpanBERT提升也是全面的. 尤其是在二分类数据集QNLI上提升比较大, 我认为这可能是SBO带来的提升.</p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><h4 id="Masking-Schemes"><a href="#Masking-Schemes" class="headerlink" title="Masking Schemes"></a>Masking Schemes</h4><p>作者将Subword, Whole Words, Named Entities, Noun Phrases, Geometric Spans几种Mask策略放在一起做了对比:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/spanbert8.jpg" style="zoom:33%;" /><p>能够看出, SpanBERT中的Geometric Spans几乎是最有效的, 但在Coreference上似乎没有与其他提升一致. 但其他的策略也都不如BERT最开始使用的Subword Tokens策略要好.</p><h4 id="Auxiliary-Objectives"><a href="#Auxiliary-Objectives" class="headerlink" title="Auxiliary Objectives"></a>Auxiliary Objectives</h4><p>不同的辅助目标对Span BERT的影响如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/spanbert9.jpg" style="zoom:33%;" /><p>如果使用NSP任务, 性能会比不使用NSP并单句训练的SpanBERT有损, 加上SBO任务后会使得Coreference上的表现大幅提升(相较于上表).</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>SpanBERT作为一种BERT的改进方案, 提出了更完备的Span Masking方案. SpanBERT加入了SBO任务, 使用边界信息和被Mask的Token在Span内的相对位置信息能预测出Span内部的内容. 也没有使用句子对作为训练方式, 抛弃了对性能有损害的NSP任务.</p><p>SpanBERT在各类任务上性能皆超过BERT, 应该算作是一种比较有效的改进方式, 方法也比较巧妙.</p><blockquote><p>我感觉效果全面提升的原因主要是增大了模型的训练难度, 并且包含有一定的随机性, 在预测时, 需要对每个Span内的Token都做预测, 大大的强化了Span左右两端判断Span内部内容的能力, 这样比漫无目的的Mask要有效得多, 在不同的位置编码下, 需要判断Span内部不同的内容, 进一步的提高了模型对两端信息的利用率, 使得两端的隐态更有意义, 更有内容.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> BERT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding</title>
      <link href="/posts/51848.html"/>
      <url>/posts/51848.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>BERT: 详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>.</li></ul></blockquote><h1 id="StructBERT-Incorporating-Language-Structures-into-Pre-training-for-Deep-Language-Understanding"><a href="#StructBERT-Incorporating-Language-Structures-into-Pre-training-for-Deep-Language-Understanding" class="headerlink" title="StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding"></a>StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding</h1><p>本文是论文<a href="http://arxiv.org/abs/1908.04577" target="_blank" rel="noopener">StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>虽然BERT和RoBERTa将注意力放在了NLU问题上, 并大幅度的化简NLU相关的任务难度, 但仍然没有把<strong>语言结构信息</strong>集成进去.</p><blockquote><p>如”研表究明, 汉字序顺并不定一影阅响读. 比如当你看完这句话后, 才发这现里的字全是都乱的”. 尤其在使用了Self  Attention后, Token之间的最短距离恒为1, 自然语言中的单词顺序可能对BERT没那么重要了, 语言的结构可能也就得不到关注.</p></blockquote><p>因此, 作者希望改进BERT的训练方式, 使其能够注意到句子中的结构信息的变化.</p><h2 id="StructBERT"><a href="#StructBERT" class="headerlink" title="StructBERT"></a>StructBERT</h2><p>作者在保留BERT现有的两个训练任务的基础上, 额外引入两个<strong>辅助任务</strong>, 分别是Word Structural Objective和Sentence Structural Objective, 即从<strong>单词</strong>和<strong>句子</strong>的两个角度来提升BERT对语言结构的理解.</p><h3 id="Input-Representation-and-Transformer-Encoder"><a href="#Input-Representation-and-Transformer-Encoder" class="headerlink" title="Input Representation and Transformer Encoder"></a>Input Representation and Transformer Encoder</h3><p>和BERT一样, 对于输入序列(可能是单句子也可能是句子对)的每个Token $t_i$, 将它的Word Embedding, Segment Embedding, Position Embedding相加作为其初始表示$\mathbf{x}_i$, 然后能通过对$L$ 层Transformer Encoder的堆叠获得其在当前语境下的表示$\mathbf{h}_i$.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bert2.jpg" style="zoom: 50%;" /><p>同理, 输入句子时, 句向量$\mathbf{X}=\left\{\mathbf{x}_{i}\right\}_{i=1}^{N}$ 会被$L$ 层Transformer Encoder编码为$\mathbf{H}^l$:<br>$$<br>\mathbf{H}^{l}=\text { Transformer }_{l}\left(\mathbf{H}^{l-1}\right)<br>$$<br>其中$l \in [1, L]$, $\mathbf{H}^0 = X$, $\mathbf{H}^{L}=\left[\mathbf{h}_{1}^{L}, \cdots, \mathbf{h}_{N}^{L}\right]$.</p><h3 id="Word-Structural-Objective"><a href="#Word-Structural-Objective" class="headerlink" title="Word Structural Objective"></a>Word Structural Objective</h3><p>作者认为, 一个好的语言模型必须能够通过句子中打乱顺序的单词组<strong>恢复</strong>出单词的原来顺序, 而BERT仍不能正确的做到这一点. 针对这点, 作者直接将这种想法作为训练任务补充到BERT的训练当中.</p><p>该任务的目标为:<br>$$<br>\arg \max _{\theta} \sum \log P\left(\operatorname{pos}_{1}=t_{1}, \operatorname{pos}_{2}=t_{2}, \ldots, \operatorname{pos}_{K}=t_{K} \mid t_{1}, t_{2}, \ldots, t_{K}, \theta\right)<br>$$<br>$\theta$ 为参数, $K$ 为打乱顺序的子序列长度. </p><p>如果$K$ 比较大, 意味着模型需要对比较长的序列重建, 噪声比较多, $K$ 比较小, 模型只需要对比较短的子序列重建, 噪声相应的也比较少. 为达到模型鲁棒性和模型重建能力的平衡, 作者设定$K=3$.</p><p>该任务与原始MLM训练目标是不冲突的, 能够<strong>联合训练</strong>:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/structbert1.jpg" style="zoom:33%;" /><p>在输入为<code>[MASK]</code>的地方应该能被预测出正确被Mask掉的Token, 对于输入打乱的地方应该能根据$\mathbf{h}_i^L$ 恢复出正确顺序的Token.</p><blockquote><p>Word Structural Objective主要针对单句子任务.</p></blockquote><h3 id="Sentence-Structural-Objective"><a href="#Sentence-Structural-Objective" class="headerlink" title="Sentence Structural Objective"></a>Sentence Structural Objective</h3><p>除去单词级别的结构, 还需要关注句子和句子之间的结构.</p><p>在BERT训练中, 使用的是<strong>NSP任务</strong>, 而在RoBERTa中提出NSP任务由于过于<strong>简单</strong>, 有害于模型性能. </p><p>其实NSP任务也不是不能用, 必须增加它的任务难度. 在StructBERT中, 沿着组装句子的思路, NSP被<strong>改进</strong>成一个<strong>三分类问题</strong>, 分别令当前句子$S_1$ 与另一个句子$S_2$ 组合, $S_2$ 可能是以下的其中一种:</p><ol><li>$S_2$ 为$S_1$ 的<strong>下</strong>一句, 此时任务为原始的NSP任务.</li><li>$S_2$ 为$S_1$ 的<strong>上</strong>一句.</li><li>$S_2$ 为<strong>其他文档</strong>的<strong>随机</strong>一句.</li></ol><p>这三种情况均为<strong>等概率</strong>发生, 即发生概率均为$\frac{1}{3}$, <code>[SEP]</code> 的添加与BERT相同, 采用<code>[CLS]</code>处的输出做三分类结果. 示意图如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/structbert2.jpg" style="zoom: 50%;" /><blockquote><p>Sentence Structural Objective主要针对句子对任务.</p></blockquote><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的实验设置请参照原论文.</p><h3 id="General-Language-Understanding"><a href="#General-Language-Understanding" class="headerlink" title="General Language Understanding"></a>General Language Understanding</h3><h4 id="GLUE-benchmark"><a href="#GLUE-benchmark" class="headerlink" title="GLUE benchmark"></a>GLUE benchmark</h4><p>在GLUE上的结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/structbert3.jpg" style="zoom: 50%;" /><p>StructBERT在GLUE上的平均表现超过了其他的PLM.</p><h4 id="SNLI"><a href="#SNLI" class="headerlink" title="SNLI"></a>SNLI</h4><p>在自然语言推理的数据集SNLI上结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/structbert4.jpg" style="zoom:33%;" /><h3 id="Extractive-Question-Answering"><a href="#Extractive-Question-Answering" class="headerlink" title="Extractive Question Answering"></a>Extractive Question Answering</h3><p>在抽SQuAD1.1上的结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/structbert5.jpg" style="zoom:33%;" /><p>即使没有使用任何的数据增强和额外数据, StructBERT还是仅次于使用了数据增强和额外数据的XLNet.</p><blockquote><p>其实打乱顺序这种处理也可以看做是一种数据增强.</p></blockquote><h3 id="Effect-of-Different-Structural-Objectives"><a href="#Effect-of-Different-Structural-Objectives" class="headerlink" title="Effect of Different Structural Objectives"></a>Effect of Different Structural Objectives</h3><p>消融实验便是针对StructBERT的两个额外任务做的:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/structbert6.jpg" style="zoom:33%;" /><p>前三个数据集是单句任务, 在去掉Word Structural Objective后, 前三个任务的性能有退化. 后三个是句子对任务, 去掉Sentence Structural Objective后对后三个任务影响也比较大. 证明了这两种新增的任务的有效性.</p><p>下图分别是Word Prediction Loss, Word Prediction Acc, Sentence Prediction Loss, Sentence Prediction Acc随着训练步长的增长的变化曲线:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/structbert7.jpg" style="zoom: 50%;" /><p>红色代表原始BERT, 蓝色为StructBERT, 绿色为StructBERT在Masked Token任务上的表现.</p><p>作者认为, 加入Word Structural Objective使得MLM任务做的更好, 加入Sentence Structural Objective明显使得任务变得更难.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>StructBERT通过在原有任务的基础上从Word Level和Sentence Level添加了两个新的辅助训练任务, 使得BERT能够关注一些语言结构, 想法非常非常简单, 但效果却出乎意料的好. </p><p>StructBERT属于对BERT的改进, 没有太多值得进一步的建议.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BART和mBART</title>
      <link href="/posts/1394.html"/>
      <url>/posts/1394.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li><p>Transformer: 详见<a href="https://adaning.github.io/posts/6744.html">Transformer精讲</a>.</p></li><li><p>BERT, GPT: 详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>.</p></li></ul></blockquote><h1 id="BART和mBART"><a href="#BART和mBART" class="headerlink" title="BART和mBART"></a>BART和mBART</h1><p>本文是如下论文的阅读笔记和个人理解:</p><ul><li><a href="https://arxiv.org/abs/1910.13461" target="_blank" rel="noopener">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a></li><li><a href="http://arxiv.org/abs/2001.08210" target="_blank" rel="noopener">Multilingual Denoising Pre-training for Neural Machine Translation</a></li></ul><h2 id="BART"><a href="#BART" class="headerlink" title="BART"></a>BART</h2><h3 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h3><p>Transformer浑身都是宝, 相较于直接延续Transformer的Seq2Seq架构, 更广为使用的是把<strong>Encoder</strong>和<strong>Decoder</strong>的单独堆叠, 分别对应着只使用Encoder的BERT和只使用Decoder的GPT. 如果只是单纯的使用其中的某一部分, 就会造成两个鸿沟:</p><ul><li><p><strong>BERT</strong>: 具备<strong>双向语言理解</strong>能力的却不具备做<strong>生成任务</strong>的能力.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bart1.jpg" style="zoom: 50%;" /></li><li><p><strong>GPT</strong>: 拥有<strong>自回归</strong>特性的却不能更好的从<strong>双向理解</strong>语言.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bart2.jpg" style="zoom: 50%;" /><p>因此, 作者希望将二者融合, 让模型保留双向语言理解能力的同时, 也能解决生成任务.</p></li></ul><h3 id="BART-Model"><a href="#BART-Model" class="headerlink" title="BART Model"></a>BART Model</h3><p><strong>BART</strong>(<strong>B</strong>idirectional and <strong>A</strong>uto - <strong>R</strong>egressive <strong>T</strong>ransformers)的结构非常的简单, 既然只使用Encoder或者只使用Decoder不能让鱼和熊掌兼得, 那就重新把它们组装回来:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bart3.jpg" style="zoom: 50%;" /><p>左侧为BERT(Transformer Encoder), 右侧为GPT(Transformer Decoder).</p><p>所以BART采用的其实还是<strong>标准Transformer</strong>结构, 如果把Encoder和Decoder组装回来, 就又成了标准Transformer:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer.jpg" style="zoom: 50%;" /><p>但是, BART所采用的<strong>输入数据</strong>和<strong>训练目标</strong>和Transformer<strong>完全不一样</strong>, 换句话说, 作者希望BART所做的事情和Transformer是完全不一样的, 这也是BART与Transformer的最大区别.</p><p>作者指出, BART和BERT有两点最大的  不同:</p><ol><li>除了对Encoder的堆叠外, 还使用了Decoder, 并添加了Encoder和Decoder之间的<strong>Cross Attention</strong>(其实就是结构和标准Transformer一样).</li><li>BERT在预测时加了额外的<strong>FFN</strong>, 而BART没使用FFN. 整体来说, 包含了Decoder的BART只是比同级别的BERT多了<strong>10%</strong>的参数.</li></ol><h3 id="Pre-Training-BART"><a href="#Pre-Training-BART" class="headerlink" title="Pre - Training BART"></a>Pre - Training BART</h3><p>BART使用的是类似BERT的<strong>Denoising AutoEncoder</strong>的形式来训练的, 即模型需要对被添加噪声的数据<strong>去噪</strong>, <strong>恢复</strong>出原始数据.</p><blockquote><p>我猜测, 之所以BART名字是仿照BERT, 而不是仿照Transformer最大原因, 是因为BERT和BART都是去噪自编码器, 而Transformer不是.</p></blockquote><p>BART允许对原始数据做<strong>任意形式</strong>的噪声干扰, 作者提出了五种可行的添加噪声的方式:</p><ul><li><strong>Token Masking</strong>: 与BERT打<code>[Mask]</code>的策略完全相同.</li><li><strong>Token Deletion</strong>: 直接随机删除某些Token.</li><li><strong>Text Infilling</strong>: 同时选中多个连续的Token, 仅替换成一个<code>[Mask]</code>, 或者在原始数据中随机插入Mask Token(即使没有数据缺失). 模型不知道<code>[Mask]</code>对应的是多少个Token, 也不知道<code>[Mask]</code>是否有效. 这就要求模型有强大的学习能力.</li><li><strong>Sentence Permutation</strong>: 将一个文档中的句子之间的顺序打乱.</li><li><strong>Document Rotation</strong>: 从文档中随机选定一个Token作为整个文档的起始Token, 对文档Rotation. 该方法令模型能识别文档的起始Token.</li></ul><p>上述五种方式的示例如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bart4.jpg" style="zoom: 33%;" /><blockquote><p>即使有Decoder, 也不需要让Encoder和Decoder对应的部分对齐. 在极端情况下, 所有的原始信息都丢失了, 此时BART相当于Language Model. BART添加噪声的方式不单包含了Token Masking, 还包含了更为复杂的噪声, 它们也可以互相组合. 想从这些噪声中按照原语序恢复出句子还是非常困难的, 因为它们包含了缺失, 乱序的情况. </p></blockquote><p>在输入加噪的数据后, BART先通过Encoder双向编码, 再通过Decoder用自回归极大似然解码, 恢复出原始序列.</p><h3 id="Fine-Tuning-BART"><a href="#Fine-Tuning-BART" class="headerlink" title="Fine - Tuning BART"></a>Fine - Tuning BART</h3><p>BART在预训练后, 需要对下游任务微调. 对于不同的任务, BART有不同的使用方法:</p><ul><li><p><strong>Sequence Classification</strong>: 对序列分类任务, Encoder和Decoder以相同的数据输入, 利用Decoder的最后一次输出代表整个句子的表示, 类似于BERT中的<code>[CLS]</code>. 只不过BART是有Decoder的, 所以需要让Decoder的输出作为整个句子的表示.</p></li><li><p><strong>Token Classification</strong>: 对于Token分类任务, Encoder和Decoder也以相同的数据输入, BART也将Decoder中所对应Token的输出作为分类的隐藏状态.</p></li><li><p><strong>Sequence Generation</strong>: 对于序列生成任务, BART直接适应生成任务, 对Encoder和Decoder微调即可. 该类任务包括文本摘要, 问答等.</p></li><li><p><strong>Machine Translation</strong>: 机器翻译任务比较特殊, 因为它的任务输入和输出是两种不同的语言. </p><p>结合先前在机器翻译上的研究, 额外添加一个专门用于外语映射的Encoder(例如其他语言映射到英语)将有助于模型性能的提升. 所以BART需要训练一个新的Encoder来将源语言与目标语言语义空间对齐, 来替代掉BART原来的Word Embedding, 在完成对齐后, BART将把源语言转换为目标语言, 与Transformer保持一致.</p><p>训练分为了两个阶段:</p><ol><li>一阶段中, 对BART的大多数参数冻结, 只更新随机初始化的源语言Encoder, BART的位置编码, BART Encoder的第一层Self - Attention投影矩阵.</li><li>二阶段中, 对整个BART(包含后来添加的Encoder)中的所有参数做少次迭代.</li></ol></li></ul><p>对于上述四种任务的处理概括为下图:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bart5.jpg" style="zoom: 45%;" /><p>左侧为处理分类问题的示意图, 右侧为处理机器翻译的示意图.</p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>详细的实验设置请参照原论文.</p><h4 id="Comparsion-Pre-Training-Objectives"><a href="#Comparsion-Pre-Training-Objectives" class="headerlink" title="Comparsion Pre - Training Objectives"></a>Comparsion Pre - Training Objectives</h4><p>作者做了各不同预训练目标的模型的效果对比, 这些模型并不是原始论文中的模型, 而是作者或多或少调整过的. 其中所使用的模型分别类似于:</p><ul><li>Language Model: GPT.</li><li>Permuted Language Model: XLNet.</li><li>Masked Language Mode: BERT.</li><li>Multitask Masked Language Model: UniLM.</li><li>Masked Seq - to - Seq: MASS.</li></ul><p>实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bart6.jpg" style="zoom: 50%;" /><p>从实验中看到, 使用Text Infilling的效果非常好, 只使用Document Rotation和Sentence Shuffling的效果比较差. 并且, 预训练的有效性高度取决于任务, 自回归式的模型有利于解决生成类任务.</p><h4 id="Discriminative-Tasks"><a href="#Discriminative-Tasks" class="headerlink" title="Discriminative Tasks"></a>Discriminative Tasks</h4><p>各类Large模型在SQuAD和GLUE上的结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bart7.jpg" style="zoom: 50%;" /><p>RoBERTa和BART的表现相似, 但是BART能够在不牺牲性能的情况下将任务扩展到生成任务上, 这对BART来说是一个独天得厚的优势, 因为扩展只带来了将近10%的参数量增长.</p><h4 id="Generation-Tasks"><a href="#Generation-Tasks" class="headerlink" title="Generation Tasks"></a>Generation Tasks</h4><h5 id="Summarization"><a href="#Summarization" class="headerlink" title="Summarization"></a>Summarization</h5><p>在摘要任务上的实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bart8.jpg" style="zoom: 50%;" /><h5 id="Dialogue"><a href="#Dialogue" class="headerlink" title="Dialogue"></a>Dialogue</h5><p>在对话任务上的结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bart9.jpg" style="zoom: 50%;" /><h5 id="Abstractive-QA"><a href="#Abstractive-QA" class="headerlink" title="Abstractive QA"></a>Abstractive QA</h5><p>抽象问答数据集上结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bart10.jpg" style="zoom: 50%;" /><h4 id="Machine-Translation"><a href="#Machine-Translation" class="headerlink" title="Machine Translation"></a>Machine Translation</h4><p>在机器翻译任务上结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bart11.jpg" style="zoom: 50%;" /><p>Baseline是标准Transformer. Fixed BART和Tuned BART分别代表单向翻译和使用<strong>反向翻译</strong>的BART. 没有反向翻译的BART效果不太好, 可能涉及到过拟合, 使用反向翻译后应该增强了泛化能力, 效果得到了改善.</p><blockquote><p>反向翻译指的是, 将目标语言输入模型得到源语言的预测结果, 其中预测错误的内容将视为噪声, 将该预测结果再重新输入新的模型, 让它生成目标语言, 这是一种常见的<strong>文本数据增强</strong>方式.</p></blockquote><h2 id="mBART"><a href="#mBART" class="headerlink" title="mBART"></a>mBART</h2><blockquote><p>但我不是MT方向的, 所以实验部分我只挑着看了比较关键的部分. 之所以把mBART也写在这篇文章中, 是因为mBART只是作为BART的多语言版本, 没有本质上的结构变化, 作者展示了一种基于BART处理机器翻译问题的方法, 比较有趣, 在本文中作为扩展内容. </p></blockquote><p><strong>mBART</strong>(<strong>M</strong>ultilingual <strong>B</strong>idirectional and <strong>A</strong>uto - <strong>R</strong>egressive <strong>T</strong>ransformers)是BART的<strong>多语言</strong>版本, 用于处理不同语言之间的<strong>机器翻译</strong>问题. </p><h3 id="mBART-Model"><a href="#mBART-Model" class="headerlink" title="mBART Model"></a>mBART Model</h3><p>mBART仍然是分为<strong>Pre - Training</strong>和<strong>Fine - Tuning</strong>两个阶段. Pre - Training将使用多个语种的语料作为预训练数据.</p><h4 id="Pre-Training-mBART"><a href="#Pre-Training-mBART" class="headerlink" title="Pre - Training mBART"></a>Pre - Training mBART</h4><p>预训练阶段延续了BART的做法, 仍然采用<strong>Denoising AutoEncoder</strong>的方法令mBART的每条数据在<strong>单语言</strong>内训练. 目标是将单语言文本加噪干扰后再恢复回来. 作者采取了两种BART中的加噪方式: </p><ul><li><strong>Sentence Permutation</strong>: 打乱句子和句子之间的顺序.</li><li><strong>Word - Span masking</strong>: 连续的Mask掉一些内容, 并且只用一个<code>[Mask]</code>替换.</li></ul><p>除此外, mBART的初衷是多语言模型, 必须将语种的信息加入. 在文本输入结束后, 在句子末尾处需要加上句子结尾标识<code>&lt;\s&gt;</code>和对应语言的标识<code>[LID]</code>. </p><h4 id="Fine-Tuning-mBART"><a href="#Fine-Tuning-mBART" class="headerlink" title="Fine - Tuning mBART"></a>Fine - Tuning mBART</h4><p>微调阶段才针对机器翻译任务训练. 用<code>[LID]</code>替换Decoder原来的第一个输入<code>[Start]</code>, 表明要翻译成哪个语种.</p><p>Sentence Level MT和Document Level MT的主要区别就在于文本的长度不同, Document Level MT更困难一些.</p><p>上述方法概括为下图:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mbart1.jpg" style="zoom: 67%;" /><p>左侧为预训练阶段, 每条数据使用单语言文本并加噪, mBART将其恢复为原来的单语言文本. 右侧为微调阶段, 针对某两种语言之间做微调, 输入为源语言, 期望输出为目标语言, Decoder的首次输入为目标语言的<code>&lt;LID&gt;</code>.</p><h3 id="Language-Transfer"><a href="#Language-Transfer" class="headerlink" title="Language Transfer"></a>Language Transfer</h3><p>作者额外提出了一种新的语言迁移的无监督机器翻译方式.</p><p>常见的的反向翻译示意图如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mbart2.jpg" style="zoom: 50%;" /><p>先用预训练权重初始化翻译模型, 然后对目标语言到源语言的翻译模型做训练, 生成源语言的文本作为扩充数据, 再将之前的平行语料和新生成源语言的文本共同作为训练数据, 训练源语言到目标语言的模型.</p><p>而作者提出的语言迁移方法如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mbart3.jpg" style="zoom: 50%;" /><p>直接在预训练阶段就注入多语言的平行语料, 使得模型能学习到不同语种之间潜在的共性, 在Fine Tuning后, 直接就能应对相似语种的翻译任务.</p><h3 id="Experiments-1"><a href="#Experiments-1" class="headerlink" title="Experiments"></a>Experiments</h3><p>详细的实验设置请参照原论文.</p><p>文中mBART25代表用25种语言预训练出的mBART. 文章中实验所涉及到的语言代码分别如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mbart4.jpg" style="zoom: 50%;" /><p>在低资源数据集上结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mbart5.jpg" style="zoom: 50%;" /><p>Random代表不对每种翻译任务使用预训练, 直接在该任务上训练. mBART25效果要好.</p><p>在高资源数据集上结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mbart6.jpg" style="zoom: 50%;" /><p>结果显示, 当数据集大小增加后, 不做预训练的效果反而是要好一点, mBART和Random表现相近.</p><p>作者尝试了不同语种到英文的Fine Tuning, 并使用不同的Testting Language观察不同Fine Tuning Language对最终结果的影响:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/mbart7.jpg" style="zoom: 33%;" /><p>灰色的部分为同一个语系中的语言. </p><p>这个表格意味着:</p><ul><li>主对角线上Fine Tuning时所采用的语料为<code>X-EN</code>, Testing时测试的任务为<code>X-X</code>.</li><li>同一X轴上代表使用的Fine Tuning Language相同(<code>X-EN</code>), 但采用了不同的Testing Language.</li><li>同一Y轴上代表采用了不同的Fine Tuning Language, 但使用的Testing Language相同(<code>X-X</code>).</li></ul><p>除去翻译最好的是自己的语言外, 次优的一般都是同一语系下的其他语言. 确实说明了语言之间存在一定的共性.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>BART算是BERT和GPT的集大成者, 它的结构和标准Transformer一致, 但与Transformer不同点在于<strong>数据输入</strong>和<strong>训练目标</strong>, 以<strong>自回归式去噪自动编码器</strong>的形式存在. </p><p>因为使用了Transformer Decoder, 使得BART具有了BERT不具备的处理生成任务的能力, 实验结果表明, 没有损失性能, 也没有添加过多的参数.</p><p>作者尝试了包括Masking在内的多种<strong>噪声</strong>的添加方式, 这些噪声的干扰非常强大, 强制要求模型也要有与之匹配的预测能力.</p><p>mBART作为BART多语言版本, 给出了一种基于BART的多语言机器翻译上的处理思路, 也揭示了机器翻译中同一语系下不同语种之间的一些潜在共性.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GAKE: Graph Aware Knowledge Embedding</title>
      <link href="/posts/60222.html"/>
      <url>/posts/60222.html</url>
      
        <content type="html"><![CDATA[<h1 id="GAKE-Graph-Aware-Knowledge-Embedding"><a href="#GAKE-Graph-Aware-Knowledge-Embedding" class="headerlink" title="GAKE: Graph Aware Knowledge Embedding"></a>GAKE: Graph Aware Knowledge Embedding</h1><p>本文是论文<a href="https://www.aclweb.org/anthology/C16-1062/" target="_blank" rel="noopener">GAKE: Graph Aware Knowledge Embedding</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>在现有的KGE方法中, 都是基于<strong>三元组</strong>的, 但三元组之间是相互<strong>孤立</strong>的, 并没有利用上图信息.</p><p>作者尝试提出一种<strong>融入图结构信息</strong>的KGE方法.</p><h2 id="GAKE"><a href="#GAKE" class="headerlink" title="GAKE"></a>GAKE</h2><p>KG中的知识以图的结构按如下方式存在着:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/gake1.jpg" style="zoom: 50%;" /><p>作者将图中的结构信息划分为三类:</p><ul><li><strong>邻居上下文</strong>(Neighbor Context)</li><li><strong>边上下文</strong>(Edge Context)</li><li><strong>路径上下文</strong>(Path Context)</li></ul><p>如果能够处理图信息, 那么与其他KGE方法相比, 能够对<strong>Triple, Path, Edge</strong>同时建模:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/gake2.jpg" style="zoom: 50%;" /><p>能够看到, 之前的方法一般都以三元组的方式对待KG, 即使融入路径信息, 也从未考虑图中的<strong>边信息</strong>.</p><h3 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h3><p>有向图可以被表示为$G=(V, E)$, $V, E$ 均为$G$ 中的Subject, 在GAKE中, 知识图谱$G=(V, E)$ 中的每一种<strong>顶点</strong>(实体)和每一种<strong>边</strong>(关系), 都是GAKE要学习的Embedding.</p><p>作者更一般的将$V, E$ 统称Subject $S$, 其中$s=(t, k)$. 当$t=0$ 时代表$s$ 为顶点, $t=1$ 时代表$s$ 为边. $k$ 为$s$ 在Subject Set $S$ 中的索引.</p><p>在给出Subject的定义后, 也能给出$s$ <strong>上下文</strong>的定义. 对于给定的$s_i$, 其图上下文$c(s_i)$ 为与$s_i$ 相关的Subject Set $\left\{s_w \mid s_w \in S, s_w \text{ relevant to }s_i \right\}$. 因为Subject是很抽象的, 所以对于<strong>不同类型的Subject会有不同的Context定义</strong>.</p><p>作者还定义了如下符号: $C(s)$ 代表$s$ 的图上下文, $\phi(s)$ 代表取$s$ 的Embedding, $\pi(C(s))$ 代表获取$s$ 在上下文情况下的表示, $\theta$ 为模型参数.</p><h3 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h3><p>GAKE是一种利用图结构信息的KGE范式, 其目标为预测图中缺失的Subject $s_i$, 那么缺失Subject $s_i$ 的概率应该在其上下文已知的情况下得到最大化:<br>$$<br>P\left(s_{i} \mid c\left(s_{i}\right)\right)=\frac{\exp \left(\phi\left(s_{i}\right)^{\top} \pi\left(c\left(s_{i}\right)\right)\right)}{\sum_{j=1}^{|S|} \exp \left(\phi\left(s_{j}\right)^{\top} \pi\left(c\left(s_{i}\right)\right)\right)}<br>$$<br>其中, $\pi(\cdot)$ 为聚合函数, 在这里先用简单的<strong>平均求和</strong>来计算各种上下文因素对$s_i$ 的影响:<br>$$<br>\pi\left(c\left(s_{i}\right)\right)=\frac{1}{\left|c\left(s_{i}\right)\right|} \sum_{s_{j} \in c\left(s_{i}\right)} \phi\left(s_{j}\right)<br>$$</p><h4 id="Neighbor-Context"><a href="#Neighbor-Context" class="headerlink" title="Neighbor Context"></a>Neighbor Context</h4><p>给定一个Subject $s_i$, 以$s_i$ 是实体为例, 它的上下文是它的每个一阶邻居和它们之间的关系. 所以$s_i$ 为实体时, 邻居上下文和$s_i$ 的所有相关三元组无异.</p><p>然后采用<strong>极大似然</strong>优化$s_i$ 的概率:<br>$$<br>O_{N}=\sum_{s_{i} \in S} \sum_{c_{N}\left(s_{i}\right) \in C_{N}\left(s_{i}\right)} \log p\left(s_{i} \mid c_{N}\left(s_{i}\right)\right)<br>$$<br>$C_N(s_i)$ 是$s_i$ 的Neighbor Context.</p><blockquote><p>作者没有说明Relation的邻居上下文是什么样的, 代码里也只是构造的实体邻居, 应该是本来就没有定义.</p></blockquote><h4 id="Path-Context"><a href="#Path-Context" class="headerlink" title="Path Context"></a>Path Context</h4><p>KG中的路径直接或间接的反映了<strong>实体之间</strong>的关系. 因此, 作者定义Path Context $c_P(s_i)$ 为<strong>Random Walk</strong>生成的一系列<strong>顶点</strong>和<strong>边</strong>的集合.</p><p>与Neighbor Context类似, 也是用<strong>极大似然</strong>去优化:<br>$$<br>O_{P}=\sum_{s_{i} \in S} \sum_{c_{P}\left(s_{i}\right) \in C_{P}\left(s_{i}\right)} \log p\left(s_{i} \mid c_{P}\left(s_{i}\right)\right)<br>$$</p><h4 id="Edge-Context"><a href="#Edge-Context" class="headerlink" title="Edge Context"></a>Edge Context</h4><p>对于一个实体, 所有与之<strong>直接相连</strong>的<strong>边</strong>都可能反映实体本身的情况. 因此, 当Subject为顶点时, Edge Context $c_E(s_i)$ 为$s_i$ 的边集.</p><p>同样, 还是用<strong>极大似然</strong>去优化:<br>$$<br>O_{E}=\sum_{s_{i} \in S} \log p\left(s_{i} \mid c_{E}\left(s_{i}\right)\right)<br>$$</p><h4 id="Context-extension"><a href="#Context-extension" class="headerlink" title="Context extension"></a>Context extension</h4><p>在作者提出的范式下, 如果有其他形式的Graph Context也可以继续<strong>扩展</strong>, 因为$c(s_i)$ 的含义可以随着对Context的定义变化而改变, 保证了一定的<strong>灵活性</strong>.</p><h3 id="Attention-Mechanism"><a href="#Attention-Mechanism" class="headerlink" title="Attention Mechanism"></a>Attention Mechanism</h3><p>在现实生活中, 每个Subject Context对Subject的本身的影响可能是不同的, 如果使用之前说的平均求和就略显不妥. 作者使用Attention机制对$s_i$ 的上下文元素加权求和, 使得模型更加灵活, 也更契合场景.</p><p> $a(s)$ 代表$s$ 所占的注意力权重, Attention计算方式如下:<br>$$<br>a\left(s_{i}\right)=\frac{\exp \left(\theta_{i}\right)}{\sum_{s_{j} \in C\left(s_{i}\right)} \exp \left(\theta_{j}\right)}<br>$$<br>$\theta$ 为要优化的参数.</p><p>在Attention机制下, $s_i$ 会受到其上下文元素的<strong>加权影响</strong>:<br>$$<br>\pi\left(c\left(s_{i}\right)\right)=\sum_{s_{j} \in c\left(s_{i}\right)} a\left(s_{j}\right) \phi\left(s_{j}\right)<br>$$<br>当模型预测<code>English</code>时, 对上下文所分配的各类权重是不同的:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/gake3.jpg" style="zoom: 50%;" /><p>深暖色代表分配的注意力权重多, 冷色代表分配的注意力权重少. <code>United_States, Nationality-1, SpeakLanguage</code> 分配的权重就比较大, 因为它们能直接或间接的推导出<code>English</code>.</p><h3 id="Model-Learning"><a href="#Model-Learning" class="headerlink" title="Model Learning"></a>Model Learning</h3><p>在训练模型时, 作者将上述三种目标函数<strong>整合</strong>到一起:<br>$$<br>O=\lambda_{N} O_{N}+\lambda_{P} O_{P}+\lambda_{E} O_{E}<br>$$<br>$\lambda_N, \lambda_P, \lambda_E$ 分别是Neighbor Context, Path Context, Edge Context所对应的占比, 总和为1. 在后续实验中, 作者定义$\lambda_N=0.8, \lambda_P=0.1, \lambda_E=0.1$, 强调了Neighbor Context的作用.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文.</p><h3 id="Triple-Classification"><a href="#Triple-Classification" class="headerlink" title="Triple Classification"></a>Triple Classification</h3><p>作者在FB15K上做了三元组分类实验, 并与其他方法对比:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/gake4.jpg" style="zoom: 50%;" /><p>GAKE要优于其他方法, 达到近乎90%左右的准确率.</p><p>为了更好展示Attention在GAKE中起到的作用, 作者对预测<code>Terminate2:JudgementDay</code>时的其他<strong>路径上下文</strong>所占的<strong>注意力权重</strong>展示出来:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/gake5.jpg" style="zoom: 50%;" /><p><code>Action</code>和<code>Sequel</code>所占的比重非常大, 因为终结者2是一部动作电影, 还是续集. <code>Genre</code>占的比重比较少, 因为每部电影都有所对应的体裁, 最起码在这条路径中, 体裁没那么重要.</p><h3 id="Link-Prediction"><a href="#Link-Prediction" class="headerlink" title="Link Prediction"></a>Link Prediction</h3><p>作者还做了链接预测任务, 在FB15K上的实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/gake6.jpg" style="zoom: 50%;" /><p>相较于其他早期的KGE方法, GAKE的性能有明显提升.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>本文发布于COLING2016, 算是一篇相对比较早的文章了. 作者提出了在KGE中同时使用Neighbor Context, Path Context, Edge Context三种上下文来<strong>融入图结构信息</strong>, 并用<strong>Attention</strong>去增强图结构对Subject的表示, 取得了比较好的效果.</p><blockquote><p>但仔细想想, GAKE其实并没有强调Relation在KG中的不对等地位, 对Entity和Relation统一的使用Subject概括. 其实在后续的KGE的研究方向表明, Relation的作用是至关重要的, Entity的作用可能比Relation小. </p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HAKE: Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction</title>
      <link href="/posts/19612.html"/>
      <url>/posts/19612.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>RotatE: 详见<a href="https://adaning.github.io/posts/60792.html">RotatE: Relational Rotation in Complex Vector Space</a></li></ul></blockquote><h1 id="Learning-Hierarchy-Aware-Knowledge-Graph-Embeddings-for-Link-Prediction"><a href="#Learning-Hierarchy-Aware-Knowledge-Graph-Embeddings-for-Link-Prediction" class="headerlink" title="Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction"></a>Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction</h1><p>本文是论文<a href="https://arxiv.org/abs/1911.09419" target="_blank" rel="noopener">Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction</a>的阅读笔记和个人理解.</p><p>这篇讲解之所以把<strong>RotatE</strong>作为前置知识, 是因为HAKE与RotatE有很大关系, 并且通篇论文的论述也是与RotatE的对比. 本文有一部分参考于<a href="https://zhuanlan.zhihu.com/p/107646185" target="_blank" rel="noopener">AAAI 2020 | 中科大：可建模语义分层的知识图谱补全方法</a>.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>现有的KGE方法将注意力聚焦在对关系模式的建模上, 但没有对<strong>语义层级</strong>结构的建模, 而这种层级结构在现实世界的应用中是普遍存在的.</p><p>例如, <code>mammal</code>, <code>dog</code>, <code>run</code>, <code>move</code> 处于不同语义层级中, <code>rose</code>, <code>peony</code>, <code>truck</code>, <code>lorry</code>处于相同的语义层级中. 语义分层的现象在知识图谱所使用的<strong>三元组</strong>结构中是随处可见的. 并且可以进一步的将这种分层现象抽象成<strong>树形结构</strong>:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/hake10.jpg" style="zoom: 25%;" /><p>在这棵树中, 语义层级由上至下逐渐变得具体, 离根部越近的节点语义层级越高, 越抽象.</p><blockquote><p>在本文中, 作者定义如下符号:</p><p>$$<br>[\mathbf{a} \circ \mathbf{b}]_{i}=[\mathbf{a}]_{i} \cdot[\mathbf{b}]_{i}<br>$$</p><p>$[\mathbf{h}]_i$ 代表第$i$ 个实体的Embedding, $\lVert\cdot\rVert_1$, $\lVert \cdot \rVert_2$ 分别代表L1范数和L2范数.</p></blockquote><h2 id="HAKE"><a href="#HAKE" class="headerlink" title="HAKE"></a>HAKE</h2><p>有了RotatE在复数空间建模的前车之鉴, HAKE希望在<strong>极坐标</strong>中直接对语义层次建模. 在极坐标中, 用<strong>极径(模长)</strong>$r$ 和<strong>角度</strong>$\rho$ 来表示在二维坐标系中的一个点, 模长和角度能完美的对<strong>语义层级</strong>建模:</p><ul><li><strong>模长</strong>部分对<strong>不同层级</strong>的实体建模.</li><li><strong>角度</strong>部分对<strong>同一层级</strong>的实体建模.</li></ul><p>想通过头实体$\mathbf{h}$ 在关系$\mathbf{r}$ 的作用下找到尾实体$\mathbf{t}$, 需要在极坐标中按照极坐标的运算规则来找到:</p><ul><li><strong>模长</strong>部分对应着固定角度下的<strong>放缩</strong>.</li><li><strong>角度</strong>部分对应着固定模长时的<strong>旋转</strong>.</li></ul><p>所以, 能把之前抽象出的树形结构直接用极坐标来表达:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/hake2.jpg" style="zoom: 50%;" /><h3 id="Hierarchy-Aware-Knowledge-Graph-Embedding"><a href="#Hierarchy-Aware-Knowledge-Graph-Embedding" class="headerlink" title="Hierarchy - Aware Knowledge Graph Embedding"></a>Hierarchy - Aware Knowledge Graph Embedding</h3><h4 id="Modulus-Part"><a href="#Modulus-Part" class="headerlink" title="Modulus Part"></a>Modulus Part</h4><p>模长部分的关系被定义为对头实体模长的<strong>放缩</strong>:<br>$$<br>\mathbf{h}_{m} \circ \mathbf{r}_{m}=\mathbf{t}_{m}, \text { where } \mathbf{h}_{m}, \mathbf{t}_{m} \in \mathbb{R}^{k}, \text { and } \mathbf{r}_{m} \in \mathbb{R}_{+}^{k}<br>$$</p><p>模长所对应的距离函数为:</p><p>$$<br>d_{r, m}\left(\mathbf{h}_{m}, \mathbf{t}_{m}\right)=\lVert\mathbf{h}_{m} \circ \mathbf{r}_{m}-\mathbf{t}_{m}\rVert_{2}<br>$$</p><p>作者规定实体嵌入可以为负, 但约束Relation Embedding必须为<strong>正</strong>, 因为符号有助于尾实体的预测. 例如, 三元组$(h, r, t_1)$ 为正例, $(h, r, t_2)$ 为负例, 目标为最小化$d_r(\mathbf{h}_m, \mathbf{t}_{1, m})$, 最大化$d_r(\mathbf{h}_m, \mathbf{t}_{2, m})$. </p><ul><li>对于正样本, $[\mathbf{h}]_i, [\mathbf{t}_1]_i$ 倾向于使用同一个符号, 因为$[\mathbf{r}_m]_i&gt;0$. </li><li>对于负例, $[\mathbf{h}_m]_i, [\mathbf{t}_{2,m}]_i$ 可能是随机的符号. </li></ul><p>这样, $d_r(\mathbf{h}_m, \mathbf{t}_{2, m})$ 会倾向于比$d_r(\mathbf{h}_m, \mathbf{t}_{1, m})$ 更大, 更有利于优化.</p><h4 id="Phrase-Part"><a href="#Phrase-Part" class="headerlink" title="Phrase Part"></a>Phrase Part</h4><p>相位部分的关系被定义为角度的<strong>旋转</strong>:</p><p>$$<br>\mathbf{h}_p+\mathbf{r}_p \approx\mathbf{t}_p<br>$$<br>通过<strong>求余</strong>的方式来解决相位的周期问题:</p><p>$$<br>\left(\mathbf{h}_{p}+\mathbf{r}_{p}\right) \bmod 2 \pi=\mathbf{t}_{p}, \text { where } \mathbf{h}_{p}, \mathbf{r}_{p}, \mathbf{t}_{p} \in[0,2 \pi)^{k}<br>$$</p><p>由于相位具有<strong>周期性</strong>, 所以相位所使用的距离度量函数应该与模长部分不同. 不能以简单的相位之差$\mathbf{h}_p + \mathbf{r}_p - \mathbf{t}_p$ 来度量两实体之间在相位部分的距离.</p><p>作者在这里使用了$\sin$ 来描述三元组中两实体的距离:</p><p>$$<br>d_{r, p}\left(\mathbf{h}_{p}, \mathbf{t}_{p}\right)=\lVert\sin \left(\left(\mathbf{h}_{p}+\mathbf{r}_{p}-\mathbf{t}_{p}\right) / 2\right)\rVert_{1}<br>$$</p><p>因为$\sin$ 是有负值的, 所以这里用L1范数(即绝对值)来约束$\sin$ 的结果.</p><blockquote><p>该距离度量能很好的描述三者之间的相位距离关系.</p><p>因为对相位除了2, 所以周期变为$\pi$, 所以每当头实体的相位, 和关系相位, 与尾实体的相位的差$\mathbf{h}_p + \mathbf{r}_p - \mathbf{t}_p = k\pi, k\in(2k+1)$时, $(\mathbf{h}_p + \mathbf{r}_p - \mathbf{t}_p)/2 = \frac{\pi}{2}k, k\in (2k+1)$, 可以取到$\sin$ 函数的最大值1, 即此时距离最大.</p></blockquote><h4 id="Map-into-Polar-Coordinate-System"><a href="#Map-into-Polar-Coordinate-System" class="headerlink" title="Map into Polar Coordinate System"></a>Map into Polar Coordinate System</h4><p>综合前两个小节, HAKE被整合进了极坐标系统中, 实体能在极坐标下唯一的被二维坐标标识.</p><p>$$<br>\left\{\begin{array}{l}<br>\mathbf{h}_{m} \circ \mathbf{r}_{m}=\mathbf{t}_{m}, \text { where } \mathbf{h}_{m}, \mathbf{t}_{m} \in \mathbb{R}^{k}, \mathbf{r}_{m} \in \mathbb{R}_{+}^{k} \\<br>\left(\mathbf{h}_{p}+\mathbf{r}_{p}\right) \bmod 2 \pi=\mathbf{t}_{p}, \text { where } \mathbf{h}_{p}, \mathbf{t}_{p}, \mathbf{r}_{p} \in[0,2 \pi)^{k}<br>\end{array}\right.<br>$$</p><p>距离函数是<strong>模长部分</strong>的距离函数和<strong>相位部分</strong>的组合:</p><p>$$<br>d_{r}(\mathbf{h}, \mathbf{t})=d_{r, m}\left(\mathbf{h}_{m}, \mathbf{t}_{m}\right)+\lambda d_{r, p}\left(\mathbf{h}_{p}, \mathbf{t}_{p}\right)<br>$$</p><p>其中$\lambda$ 为可学习的参数.</p><p>HAKE的打分函数如下:</p><p>$$<br>f_{r}(\mathbf{h}, \mathbf{t})=-d_{r}(\mathbf{h}, \mathbf{t})=-d_{r, m}(\mathbf{h}, \mathbf{t})-\lambda d_{r, p}(\mathbf{h}, \mathbf{t})<br>$$</p><p>因为<strong>距离越大</strong>, <strong>打分应该越小</strong>, 所以这里在距离前面加上一个负号.</p><p>下图是HAKE与常见KGE模型的打分函数对比:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/hake1.jpg" style="zoom: 50%;" /><p>在跑实验时, 作者还发现了一种<strong>混合距离度量</strong>, 比原度量更加有效:<br>$$<br>d_{r, m}^{\prime}(\mathbf{h}, \mathbf{t})=\lVert\mathbf{h}_{m} \circ \mathbf{r}_{m}+\left(\mathbf{h}_{m}+\mathbf{t}_{m}\right) \circ \mathbf{r}_{m}^{\prime}-\mathbf{t}_{m}\rVert_{2}<br>$$</p><p>下面这个式子与上式是<strong>等价</strong>的:</p><p>$$<br>d_{r, m}^{\prime}(\mathbf{h}, \mathbf{t})=\lVert\mathbf{h}_{m} \circ\left(\left(\mathbf{r}_{m}+\mathbf{r}_{m}^{\prime}\right) /\left(1-\mathbf{r}_{m}^{\prime}\right)\right)-\mathbf{t}_{m}\rVert_{2}<br>$$</p><p>作者在后续实验发现这种方式有很大提升.</p><blockquote><p>这个混合度量还有一些疑问, 以后研究.</p></blockquote><h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><p>HAKE与RotatE一样, 使用<strong>最大化间隔</strong>的损失函数来优化:</p><p>$$<br>L=-\log \sigma\left(\gamma-d_{r}(\mathbf{h}, \mathbf{t})\right)<br>-\sum_{i=1}^{n} p\left(h_{i}^{\prime}, r, t_{i}^{\prime}\right) \log \sigma\left(d_{r}\left(\mathbf{h}_{i}^{\prime}, \mathbf{t}_{i}^{\prime}\right)-\gamma\right)<br>$$</p><p>其中$\gamma$ 为间隔, $\sigma$ 为Sigmoid函数, $(h_i^\prime, r, t_i^\prime)$ 为负三元组. </p><p>HAKE也使用了RotatE中提到的<strong>自对抗负采样</strong>, 我在<a href="https://adaning.github.io/posts/60792.html#Self-Adversarial-Negative-Sampling">RotatE</a>中已经有过讲解:<br>$$<br>p\left(h_{j}^{\prime}, r, t_{j}^{\prime} \mid\left\{\left(h_{i}, r_{i}, t_{i}\right)\right\}\right)=\frac{\exp \alpha f_{r}\left(\mathbf{h}_{j}^{\prime}, \mathbf{t}_{j}^{\prime}\right)}{\sum_{i} \exp \alpha f_{r}\left(\mathbf{h}_{i}^{\prime}, \mathbf{t}_{i}^{\prime}\right)}<br>$$</p><p>其中$\alpha$ 为Temperature. </p><blockquote><p>除了带来性能上的提升外, HAKE在后续与RotatE对比时也显得更加<strong>公平</strong>.</p></blockquote><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文.</p><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><p>作者分别在WN18RR, FB15k - 237, YAGO3 - 10上做了实验, 它们的统计信息如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/hake3.jpg" style="zoom: 50%;" /><p>实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/hake4.jpg" style="zoom: 50%;" /><p>HAKE将相较于RotatE有一些提升, ModE和RotatE差不多.</p><h3 id="Relation-Embeddings-Analysis"><a href="#Relation-Embeddings-Analysis" class="headerlink" title="Relation Embeddings Analysis"></a>Relation Embeddings Analysis</h3><h4 id="Modulus-Part-1"><a href="#Modulus-Part-1" class="headerlink" title="Modulus Part"></a>Modulus Part</h4><p>作者把关系中所对应的实体<strong>模长</strong>分布直方图画了出来:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/hake5.jpg" style="zoom: 50%;" /><p>对于以关系$\mathbf{r}_m$作者有如下期望:</p><ul><li>头实体相比于尾实体的语义层次更<strong>高</strong>时, 关系的模长$\mathbf{r}_{m}=\mathbf{t}_{m} / \mathbf{h}_{m}&gt;1$.</li><li>头实体相比于尾实体的语义层次更<strong>低</strong>时, 关系的模长$\mathbf{r}_{m}=\mathbf{t}_{m} / \mathbf{h}_{m}&lt;1$.</li><li>头实体与尾实体位于<strong>相同</strong>的语义层次时, 关系的模长$\mathbf{r}_{m}=\mathbf{t}_{m} / \mathbf{h}_{m} \approx 1$.</li></ul><p>在作者展示出的六种关系中, 作者猜想得到了佐证.</p><h4 id="Phrase-Part-1"><a href="#Phrase-Part-1" class="headerlink" title="Phrase Part"></a>Phrase Part</h4><p>因为在上幅图中, <code>_similar_to</code>和<code>friend</code>没能利用模长做出区分, 作者将它们的相位分布做了出来:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/hake6.jpg" style="zoom: 50%;" /><p>大多数的相位区分都集中在$\pi$ 的整数倍, 当相位差为$\pi$ 时, 实体可以被区分开.</p><h3 id="Entity-Embedding-Analysis"><a href="#Entity-Embedding-Analysis" class="headerlink" title="Entity Embedding Analysis"></a>Entity Embedding Analysis</h3><p>将RotatE的实部, 虚部, HAKE的模长部分, 相位部分视为是二维坐标系中的坐标, 将它们的每个维度放到2D平面上可视化:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/hake9.jpg" style="zoom: 67%;" /><p>在RotatE中, 不同语义层级关系的实体是不能被区分开的, HAKE能保证不同语义层级的实体之间有一定的间隔, 相同语义层级之间的实体大致相同.</p><blockquote><p>相同语义层级之间的实体可能被多元关系所包含, 它们从图(b)上来看是不好区分的, 暴露了HAKE不擅长处理多元关系的特性.</p></blockquote><h3 id="Ablation-Studies"><a href="#Ablation-Studies" class="headerlink" title="Ablation Studies"></a>Ablation Studies</h3><p>作者在三个数据集上做了相应的消融实验, 结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/hake7.jpg" style="zoom: 50%;" /><p>$\mathbf{b}$ 代表作者所称的混合距离度量.</p><p>如果只使用模长的部分, 性能是非常差的. 但如果只使用相位部分, HAKE将退化为<strong>pRotatE</strong>. 把二者混合性能可以得到进一步提升. 如果再采用混合距离度量性能还会有点提升.</p><h3 id="Comparsion-with-Other-Related-Work"><a href="#Comparsion-with-Other-Related-Work" class="headerlink" title="Comparsion with Other Related Work"></a>Comparsion with Other Related Work</h3><p>在FB15k上与四种TKRL的模型对比:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/hake8.jpg" style="zoom: 50%;" /><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>HAKE与RotatE相似, RotatE利用了<strong>复数</strong>空间的旋转建模关系, HAKE利用<strong>极坐标</strong>中的<strong>角度</strong>和<strong>模长</strong>建模语义层级结构.</p><p>作者着重与RotatE做了对比, 它确实比RotatE能针对语义层级关系做处理.</p><p>HAKE也十分的简洁, 但和RotatE一样, 理论上它还是只能做<strong>一对一</strong>的建模, 不善于处理多对一, 一对多, 多对多的<strong>多元关系</strong>.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for KGE</title>
      <link href="/posts/21954.html"/>
      <url>/posts/21954.html</url>
      
        <content type="html"><![CDATA[<h1 id="ReInceptionE-Relation-Aware-Inception-Network-with-Joint-Local-Global-Structural-Information-for-KGE"><a href="#ReInceptionE-Relation-Aware-Inception-Network-with-Joint-Local-Global-Structural-Information-for-KGE" class="headerlink" title="ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for KGE"></a>ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for KGE</h1><p>本文是论文<a href="https://www.aclweb.org/anthology/2020.acl-main.526.pdf" target="_blank" rel="noopener">ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for Knowledge Graph Embedding</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者观察到<strong>基于卷积</strong>的方法存在如下限制:</p><ol><li>ConvE中没有融入<strong>结构化信息</strong>.</li><li>ConvE的性能仍然被<strong>交互次数</strong>所限制.</li></ol><p>而KBAT直接从<strong>多跳</strong>的<strong>图</strong>角度出发, 考虑N阶<strong>邻居</strong>的关系和它们本身对中心节点的影响, 但是仍需多跳推理, 所以它只考虑了邻居的局部信息. 因此, 利用<strong>局部</strong>和<strong>全局</strong>信息结合可能会从中受益.</p><p>例如, 在下图中, 对于<code>(Jack London, nationality, ?)</code>这个问题, 我们能够从局部邻居观察到<code>(Jack London, place_lived, Oakland)</code>存在, 因为<code>Oakland</code>在Embedding中与<code>America</code>非常近, 所以也会增加<code>America</code>的预测分数. 同时, 对于关系<code>nationality</code>的全局信息, 即它存在的三元组所对应的头实体集合和尾实体集合也能帮助我们提升结果得出<code>America</code>的概率:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/reinceptione1.jpg" style="zoom: 50%;" /><blockquote><p>这也可能会<strong>潜在</strong>的导致局部和全局信息<strong>冲突</strong>所带来的<strong>混淆</strong>问题, 即多个实体的分数<strong>高度近似</strong>, 模型将会很难区分它们. 可能说的比较极端.</p><p>此外, 在这个例子中, <code>nationality</code>应该不是在<a href="https://adaning.github.io/posts/53954.html">这篇论文</a>中提出的<strong>笛卡尔积关系</strong>, 因为头实体和尾实体的数量实在是太大了.</p></blockquote><p>根据上述Conv类模型存在的缺陷, 作者希望用Inception来增加头实体和关系之间的交互次数, 用Attention丰富局部和全局信息.</p><h2 id="ReInceptionE"><a href="#ReInceptionE" class="headerlink" title="ReInceptionE"></a>ReInceptionE</h2><p><strong>ReInceptionE</strong>(<strong>Re</strong>lation - aware <strong>Inception</strong> network with joint local-global structural information for knowledge graph <strong>E</strong>mbedding) 将Inception和KBAT的优势结合到了一起, 作者先用Inception在实体和关系之间<strong>交互</strong>, 生成头实体和关系之间的<strong>查询向量</strong>, 然后从<strong>局部</strong>和<strong>全局</strong>两个角度集成信息, 用<strong>Attention</strong>把信息融合到一起.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/reinceptione7.jpg" style="zoom: 50%;" /><h3 id="Inception-based-Query-Encoder"><a href="#Inception-based-Query-Encoder" class="headerlink" title="Inception - based Query Encoder"></a>Inception - based Query Encoder</h3><blockquote><p>除去和ConvE实体关系嵌入的<strong>拼接方式</strong>不同, 这小节基本就是在讲Inception, 有基础可以跳过.</p></blockquote><p>作者指出, 在ConvE中其实并没有最大化实体和关系的交互次数. 因为ConvE是通过<strong>Concat</strong>的方式把二者拼接到一起, 只有最中间的<strong>交界处</strong>才有卷积核对二者的交互. 为了能够尽可能多的使头实体嵌入$\mathbf{v}_h$ 和关系嵌入$\mathbf{v}_r$ 整合, 作者使用<strong>Stack</strong>的方式在Channel维上把二者<strong>堆叠</strong>到一起, 这样在抽取特征时直接就能对二者共同交互:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/reinceptione2.jpg" style="zoom: 50%;" /><blockquote><p><a href="https://adaning.github.io/posts/14355.html">CoPER</a>也是针对这个点改进的, 只不过使用的是<strong>参数生成</strong>的方法, 避免了Concatenate这种加性操作.</p></blockquote><p>在Inception中, 在这里, 先使用$1\times1$ 卷积能提供<strong>最直接</strong>的实体嵌入$\mathbf{v}_h$ 和关系嵌入$\mathbf{v}_r$ 的信息整合作用:</p><p>$$<br>\mathbf{v}_{1 \times 1}=\operatorname{Relu}\left(\left[\mathbf{v}_{h} \lVert\mathbf{v}_{r}\right] \ast \omega_{1 \times 1}\right)<br>$$</p><p>其中$\ast$ 为卷积操作, $\omega_{1\times1}$ 为$1\times1$ 的卷积核权重. $\lVert$ 为Stack操作. </p><p>在整合过后, 由于Inception通过多尺度的方式捕获<strong>不同角度</strong>的高阶信息, 所以会使用不同大小的卷积核, $\mathbf{v}_{2\times2}, \mathbf{v}_{3\times3}$ 分别代表在$\mathbf{v}_{1\times1}$ 后接一个$2\times2$ 和$3\times3$ 卷积的结果.</p><blockquote><p>Inception中, 不同大小卷积核抽取后的特征图可能宽高不同, 使用<strong>Padding</strong>可以保证它们的特征图不变, 以正确的Stack到一起.</p></blockquote><p>接着, 在Inception中, 还使用了两个$3\times3$ 的卷积代替一个$5\times5$ 的卷积, 以此捕获<strong>大空间</strong>上的交互:</p><p>$$<br>\mathbf{v}_{2(3 \times 3)}=\operatorname{Relu}\left(\operatorname{Relu}\left(\mathbf{v}_{1 \times 1}^{2(3 \times 3)} \ast \omega_{3 \times 3}^{1}\right) \ast \omega_{3 \times 3}^{2}\right)<br>$$</p><p>$\mathbf{v}_{1 \times 1}^{2(3 \times 3)}$ 为输入的交互特征, 即$1\times1$ 卷积后的结果,  $\omega_{3\times3}^1, \omega_{3\times3}^2$ 分别为两个$3\times3$ 的卷积核参数.</p><p>最后, 把前面的多尺度信息Stack起来, 然后打平, 扔进一层FC层:<br>$$<br>\begin{aligned}<br>\mathbf{v}_{q} &amp;=\text { Inception }\left(\mathbf{v}_{h}, \mathbf{v}_{r}\right) \\<br>&amp;=\operatorname{Relu}\left(\operatorname{vec}\left(\left[\mathbf{v}_{1 \times 1}\lVert \mathbf{v}_{2 \times 2} \lVert \mathbf{v}_{3 \times 3} \lVert \mathbf{v}_{2(3 \times 3)}\right]\right) \mathbf{W}\right)<br>\end{aligned}<br>$$</p><p>$\mathbf{W}$ 为一层FC的参数.</p><p>整体交互的输出结果$\mathbf{v}_q$, 视作是头实体嵌入$\mathbf{v}_h$ 和关系嵌入$\mathbf{v}_r$ 之间的查询结果.</p><h3 id="Relation-Aware-Local-Attention"><a href="#Relation-Aware-Local-Attention" class="headerlink" title="Relation - Aware Local Attention"></a>Relation - Aware Local Attention</h3><p>下面, 作者从<strong>图视角</strong>对<strong>局部信息</strong>用<strong>KBAT</strong>的方式做集成.</p><p>在上小节中, 已经用Inception获得了头实体嵌入$\mathbf{v}_h$ 和关系嵌入$\mathbf{v}_r$ 之间的查询结果$\mathbf{v}_q$. </p><p>在图视角中, 头实体$h$ 的邻居集为$\mathcal{N}_{q}=\left\{n_{i}=\left(e_{i}, r_{i}\right) \mid\left(e_{i}, r_{i}, h\right) \in \mathcal{G}\right\}$, 对于每个邻居$n_i = \left(e_i, r_i\right)$, 都可以计算它们的查询编码:<br>$$<br>\mathbf{v}_{n_{i}}=\operatorname{Inception}\left(\mathbf{v}_{e_{i}}, \mathbf{v}_{r_{i}}\right)<br>$$</p><p>接着用$\mathbf{v}_q$ 和$\mathbf{v}_{ni}$ 计算Attention Score $s_i$:</p><p>$$<br>s_{i}=\operatorname{LeakyRelu}\left(\mathbf{W}_{1}\left[\mathbf{W}_{2} \mathbf{v}_{q} \lVert \mathbf{W}_{3} \mathbf{v}_{n_{i}}\right]\right)<br>$$</p><p>然后计算Attention Weight $\alpha_i$:</p><p>$$<br>\alpha_{i}=\frac{\exp \left(s_{i}\right)}{\sum_{n_{j} \in \mathcal{N}_{a}} \exp \left(s_{j}\right)}<br>$$</p><p>为了保留原始查询嵌入的信息, 这里也加上了类似<strong>残差</strong>的操作:</p><p>$$<br>\mathbf{v}_{n}=\operatorname{Relu}\left(\sum_{n_{i} \in \mathcal{N}_{q}} \alpha_{i} \mathbf{W}_{3} \mathbf{v}_{n_{i}}\right)+\mathbf{W}_{2} \mathbf{v}_{q}<br>$$</p><p>把上述<strong>Relation - Aware Attention</strong>操作记为下式:</p><p>$$<br>\mathbf{v}_{n}=Re A t t\left(\mathbf{V}_{n}, \mathbf{v}_{q}\right)<br>$$</p><p>$\mathbf{V}_n = \left\{ \mathbf{v}_n \mid n_i \in \mathcal{N}_q \right\}$ 为局部邻居向量的集合.</p><h3 id="Relation-Aware-Global-Attention"><a href="#Relation-Aware-Global-Attention" class="headerlink" title="Relation - Aware Global Attention"></a>Relation - Aware Global Attention</h3><p>每个实体的局部邻居数量都不同, 可能会导致<strong>稀疏性</strong>. 稀疏性会影响KGE的准确率. 作者根据关系$r$, 提取出同一种关系的头实体$\mathcal{H}_{r}=\left\{e_{i} \mid\left(e_{i}, r, e_{j}\right) \in \mathcal{G}\right\}$ 所共享的信息, 以及尾实体$\mathcal{T}_r=\left\{e_{j} \mid\left(e_{i}, r, e_{j}\right) \in \mathcal{G}\right\}$ 共享的一些隐含信息.</p><p>即构建下面的这两个子图<code>Relation - aware global heads</code>和<code>Relation - aware global tails</code>, 对每个关系所对应的头尾实体集合用Attention做了聚合.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/reinceptione1.jpg" style="zoom: 50%;" /><p>上述操作只需要将同关系所对应的头实体集$\mathcal{H}_r$ 视为查询向量$\mathbf{v}_q$ 的邻居, 尾实体集$\mathcal{T}_r$ 也视为$\mathbf{v}_q$ 的邻居, 再做Relation - Aware Local Attention就可以了:</p><p>$$<br>\mathbf{v}_{r h}=Re A t t\left(\mathbf{V}_{r h}, \mathbf{v}_{q}\right)<br>$$</p><p>$\mathbf{V}_{rh}=\left\{\mathbf{v}_{h_{ri}}\mid h_{ri} \in \mathcal{H}_r\right\}$ 是对应关系中的所有<strong>头实体</strong>集合.<br>$$<br>\mathbf{v}_{r t}=ReAtt\left(\mathbf{V}_{r t}, \mathbf{v}_{q}\right)<br>$$</p><p>$\mathbf{V}_{rt}=\left\{\mathbf{v}_{t_{ri}}\mid t_{ri} \in \mathcal{T}_r\right\}$ 是对应关系中的所有<strong>尾实体</strong>集合.</p><h3 id="Joint-Relation-Aware-Attention"><a href="#Joint-Relation-Aware-Attention" class="headerlink" title="Joint Relation - Aware Attention"></a>Joint Relation - Aware Attention</h3><p>最后, 将前面包含局部信息的查询向量$\mathbf{v}_n$, 包含全局信息的查询向量$\mathbf{v}_{rh}, \mathbf{v}_{rt}$ 一并拼接起来, 再做最后一次线性变换, 就得到了最终的查询向量$\mathbf{v}_q^\prime$:</p><p>$$<br>\mathbf{v}_{q}^{\prime}=\mathbf{W}_{4}\left[\mathbf{v}_{n}\lVert\mathbf{v}_{r h} \lVert \mathbf{v}_{r t}\right]+\mathbf{b}<br>$$</p><p>$\mathbf{W}_4, \mathbf{b}$ 为权重矩阵和偏置项.</p><p>打分函数为计算查询嵌入$\mathbf{v}_q^\prime$ 与尾实体嵌入$\mathbf{v}_t$ 的<strong>点积相似度</strong>:<br>$$<br>f(h, r, t)=\mathbf{v}_{q}^{\prime T} \mathbf{v}_{t}<br>$$</p><p>然后通过最大化正例三元组的条件概率的方式来调整模型参数:</p><p>$$<br>p(t\mid h, r)=\frac{\exp (\lambda f(h, r, t))}{\sum_{\left(h, r, t^{\prime}\right) \in \mathcal{G}^{\prime} \cup\{(h, r, t)\}}\exp \left(\lambda f\left(h, r, t^{\prime}\right)\right)}<br>$$</p><p>$\lambda$ 为平滑参数, $\mathcal{G}^\prime$ 为负采样获得的三元组集合.</p><p>总体上用交叉熵来优化模型:</p><p>$$<br>\mathcal{L}=-\frac{1}{|\mathcal{E}|} \sum_{i=0}^{|\mathcal{E}|} \log p\left(t_{i} \mid h_{i}, r_{i}\right)<br>$$</p><p>$\mathcal{E}$ 为$\mathcal{G}$ 中的有效实体总数.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的超参数设置请参照原论文.</p><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><p>作者在WN18RR和FB15k - 237这两个数据集上与诸多Baseline进行了比较:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/reinceptione3.jpg" style="zoom: 50%;" /><p>Baseline有很多是基于图或基于CNN的方法. 右上角角标b代表从<a href="">另一篇论文</a>重新运行代码得出的结果, a代表作者从原论文中得到的结果.</p><p>ConvKB, CapsE, KBGAT的评估协议是不公平的, 我建议不要关注这三个模型的结果.</p><p>ReInceptionE其实也并没有超出其他Baseline很多的性能, 感觉效果其实<strong>差不多</strong>.</p><blockquote><p>明显在关系比较少的WN18RR上效果比关系很多的FB15k - 237上效果要好, 我估计是因为Attention的问题. Attention在数据量小的情况下非常脆弱. 关系数量多了但数据总量没怎么增长时, 依赖关系作出判断的模型会因关系的长尾问题暴露出很大缺陷.</p></blockquote><h3 id="Impact-of-Different-Modules"><a href="#Impact-of-Different-Modules" class="headerlink" title="Impact of Different Modules"></a>Impact of Different Modules</h3><p>作者探究了在两个数据集上ReInceptionE各组件的影响:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/reinceptione4.jpg" style="zoom: 50%;" /><p><code>ReInception w/o N</code>是模型不使用局部信息, <code>ReInception w/o E</code>是不使用全局信息. 单独使用InceptionE的效果就比ConvE好很多, 在加入局部信息后, 效果比KBAT也有改善. 与只使用局部信息相比, 只使用全局信息更不利于模型区分三元组. 二者结合的情况下, 效果最佳.</p><h3 id="Evaluation-on-Different-Relation-Types"><a href="#Evaluation-on-Different-Relation-Types" class="headerlink" title="Evaluation on Different Relation Types"></a>Evaluation on Different Relation Types</h3><p>在WN18RR和FB15k - 237上对不同种类的关系预测结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/reinceptione5.jpg" style="zoom: 50%;" /><p>相比于ConvE和InceptionE, ReInceptionE有改善, 看起来加入Inception的作用要更大.</p><h3 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h3><p>作者还分析了两个小案例:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/reinceptione6.jpg" style="zoom: 50%;" /><p>通过Attention学习得到的重要邻居往往权重都比较高, 它们都是与Target相关或能够利用它推断出Target的三元组.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>ReInceptionE使用<strong>Inception</strong>作为增加实体与关系交互的基本组件, 并在<strong>KBAT</strong>的基础上集成<strong>局部</strong>和<strong>全局</strong>信息, 用<strong>Relation Aware Attention</strong>的方式将二者融合到一起, </p><p>我之前一直都想做一种基于图的多尺度关系感知的Attention, 和这篇论文思路几乎一致.<br>有句话说得好:</p><center>你觉得你idea足够多, 一定是你读的Paper还不够.</center><p><strong>增加交互</strong>来提升性能的方法应该是有上限的, 基于CNN的方法核心就是最大化实体Embedding和关系Embedding的交互, 如果再以交互为核心扩展基于CNN的方法, 可能从中的<strong>收益不大</strong>.</p><blockquote><p>从本文中看到, 即使利用CV早期做出的某个具体BottleNeck套过来, 尤其是引入早期CV中效果最好的多尺度的BottleNeck来提升交互次数, 也仍然没有带来质的飞跃. 所以不得不让人怀疑继续增加交互的有效性.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> CNN </tag>
            
            <tag> GNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KBAT: Learning Attention-based Embeddings for Relation Prediction in KGs</title>
      <link href="/posts/37736.html"/>
      <url>/posts/37736.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>GAT: 详见<a href="https://adaning.github.io/posts/31958.html">图神经网络入门</a>.</li></ul></blockquote><h1 id="Learning-Attention-based-Embeddings-for-Relation-Prediction-in-Knowledge-Graphs"><a href="#Learning-Attention-based-Embeddings-for-Relation-Prediction-in-Knowledge-Graphs" class="headerlink" title="Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs"></a>Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs</h1><p>本文是论文<a href="https://arxiv.org/abs/1906.01195" target="_blank" rel="noopener">Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者观察到, 虽然基于CNN的模型能生成关于三元组更丰富的信息, 但处理三元组时是<strong>独立</strong>的, 并没有覆盖<strong>周围邻居</strong>的复杂信息. 现有的方法要么只关注实体特征, 要么不会对实体和关系的特征共同嵌入.</p><p>作者希望利用<strong>Attention机制</strong>来共同捕捉节点与邻居的实体和关系特征.</p><h2 id="KBAT"><a href="#KBAT" class="headerlink" title="KBAT"></a>KBAT</h2><h3 id="Graph-Attention-Network"><a href="#Graph-Attention-Network" class="headerlink" title="Graph Attention Network"></a>Graph Attention Network</h3><blockquote><p>这部分对GAT有基础的可以直接跳过, 作者在原论文中描述GAT的式子与GAT本身论文中没区别.</p></blockquote><p>对于有$N$ 个节点的图, 输入节点特征为$\mathbf{x}=\left\{\vec{x}_{1}, \vec{x}_{2}, \ldots, \vec{x}_{N}\right\}$, 在经过一层GNN变换后, 节点特征被更新为$\mathbf{x}^{\prime}=\left\{\vec{x}_{1}^{\prime}, \vec{x}_{2}^{\prime}, \ldots, \vec{x}_{N}^{\prime}\right\}$. </p><p>在注意力机制中, 边$(e_i, e_j)$ 的注意力值$e_{ij}$ 由下式决定:<br>$$<br>e_{i j}=a\left(\mathbf{W} \overrightarrow{x_{i}}, \mathbf{W} \overrightarrow{x_{j}}\right)<br>$$<br>$\mathbf{W}$ 为线性变换矩阵, 能将节点特征投影到更高的维度, $a$ 是可选择的Attention Function.</p><p>在更新节点隐态信息时, 将在GCN的基础上对节点的邻居<strong>加权聚合</strong>:<br>$$<br>\overrightarrow{x_{i}^{\prime}}=\sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j} \mathbf{W} \overrightarrow{x_{j}}\right)<br>$$<br>$\alpha_{ij}$ 是用<strong>Softmax</strong>对节点$i$ 所有邻居$j$ 的注意力值由$e_{ij}$ 计算得到的<strong>注意力权重</strong>. </p><p>与Transformer类似, GAT中引入了<strong>多头注意力</strong>, 更新方程如下:<br>$$<br>\overrightarrow{x_{i}^{\prime}}=\lVert_{k=1}^{K} \sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k} \overrightarrow{x_{j}}\right)<br>$$<br>$\lVert$ 代表Concat操作, $\sigma$ 为非线性函数, $K$ 代表多头的头数. $\alpha_{ij}^k$ 代表第$k$ 个头内归一化的注意力权重, $\mathbf{W}^k$ 是第$k$ 个头的线性变换矩阵.</p><p>最后一层将GAT输出作为Embedding, 将多头的信息通过<strong>平均</strong>的方式整合:<br>$$<br>\overrightarrow{x_{i}^{\prime}}=\sigma\left(\frac{1}{K} \sum_{k=1}^{K} \sum_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k} \overrightarrow{x_{j}}\right)<br>$$</p><h3 id="Relations-are-Important"><a href="#Relations-are-Important" class="headerlink" title="Relations are Important"></a>Relations are Important</h3><p>在KG中, 相同的实体在不同关系的作用下意义会变得完全不一样, 所以<strong>关系嵌入</strong>是非常重要的. </p><p>例如, 在下述场景中, <code>Christopher Nolan</code>出现在了两个不同的三元组中, 所对应的关系分别是<code>directed</code>和<code>brother_of</code>, 他的身份分别是导演和哥哥:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kbat1.jpg" style="zoom: 50%;" /><p>因此, 在将GAT引入KGE时, 必须将<strong>关系嵌入</strong>也一并纳入注意力的计算范畴.</p><p>在每层GAT中, $\mathbf{H}, \mathbf{G}$ 分别代表输入时实体嵌入和关系嵌入矩阵. $\mathbf{H}^\prime, \mathbf{G}^\prime$ 为输出时实体嵌入, 关系嵌入矩阵. 特别定义$t_{ij}^k=(e_i, r_k, e_j)$ 为三元组.</p><p>我们从输入实体嵌入矩阵$\mathbf{H}$ 中取出实体表示$\vec{h_i}, \vec{h_j}$, 从输入关系嵌入矩阵$\mathbf{G}$ 中取出关系表示$\vec{g_k}$, 获取三元组$t_{ij}^k$ 整体在嵌入空间中的表示$\vec{c_{i j k}}$:<br>$$<br>\vec{c_{i j k}}=\mathbf{W}_{1}\left[\vec{h}_{i}\lVert\vec{h}_{j}\lVert \vec{g}_{k}\right]<br>$$<br>$\mathbf{W}_1$ 为线性变换矩阵.</p><p>然后利用整体表示再次经过变换, 对$c_{ijk}$ 变换后用<strong>LeakyReLU</strong>过滤得到Attention Score $b_{ijk}$:<br>$$<br>b_{i j k}=\operatorname{LeakyReLU}\left(\mathbf{W}_{2} c_{i j k}\right)<br>$$<br>$\mathbf{W}_2$ 是线性变换矩阵.</p><p>根据Attention Score, 用Softmax可以得出三元组的Attention Weight $\alpha_{ijk}$:<br>$$<br>\begin{aligned}<br>\alpha_{i j k} &amp;=\operatorname{softmax}_{j k}\left(b_{i j k}\right) \\<br>&amp;=\frac{\exp \left(b_{i j k}\right)}{\sum_{n \in \mathcal{N}_{i}} \sum_{r \in \mathcal{R}_{i n}} \exp \left(b_{i n r}\right)}<br>\end{aligned}<br>$$<br>$\mathcal{N}_i$ 代表节点$i$ 的邻居节点集, $\mathcal{R}_{in}$ 代表节点$i$ 与邻居$n$ 之间的关系集.</p><p>因此, KBAT的前半部分与GAT相比, 只是把关系嵌入$g_k$ 塞进了GAT中:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kbat3.jpg" style="zoom: 33%;" /><p>在更新节点表示时, 对实体$i$ 为头实体的三元组$\vec{c_{ijk}}$ 加权求和得到新的节点表示:<br>$$<br>\overrightarrow{h_{i}^{\prime}}=\sigma\left(\sum_{j \in \mathcal{N}_{i}} \sum_{k \in \mathcal{R}_{i j}} \alpha_{i j k} \vec{c_{i j k}}\right)<br>$$<br>和GAT相同, 也可以采用<strong>多头注意力</strong>的方式来增强GAT的稳定性:<br>$$<br>\overrightarrow{h_{i}^{\prime}}=\lVert_{m=1}^{M} \sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j k}^{m} c_{i j k}^{m}\right)<br>$$<br>其中$\lVert$ 代表Concat操作, $M$ 为注意力头数. </p><p>我们已经获取了实体在单层GAT的更新后的表示, 也需要对关系做一些改变, 保证节点嵌入更新的同时, 关系嵌入也是在更新的:<br>$$<br>G^{\prime}=G \cdot \mathbf{W}^{R}<br>$$<br>$\mathbf{W}^R$ 是权重矩阵, 可以改变关系嵌入的维度.</p><h3 id="Final-Output-Layer"><a href="#Final-Output-Layer" class="headerlink" title="Final Output Layer"></a>Final Output Layer</h3><p>在最终KBAT输出时, 需要聚合多头注意力的信息, 并对节点表示做进一步处理.</p><p>输出节点表示时, 与GAT类似, 将多个头的信息直接<strong>求平均</strong>聚合作为节点表示:<br>$$<br>\overrightarrow{h_{i}^{\prime}}=\sigma\left(\frac{1}{M} \sum_{m=1}^{M} \sum_{j \in \mathcal{N}_{i}} \sum_{k \in \mathcal{R}_{i j}} \alpha_{i j k}^{m} c_{i j k}^{m}\right)<br>$$<br>但是初始信息可能会随着提取特征而丢失, 作者在最终的输出前把初始节点嵌入$\mathbf{H}^t$ 再加回来:<br>$$<br>\mathbf{H}^{\prime \prime}=\mathbf{W}^{E} \mathbf{H}^{t}+\mathbf{H}^{f}<br>$$<br>$\mathbf{W}^E$ 是为了让维度能够匹配的变换矩阵, $\mathbf{H}^f$ 为最终层前的KBAT节点表示输出.</p><blockquote><p>其实就是类似<strong>残差</strong>的方法, 但是在原论文中却对残差只字未提.</p></blockquote><p>至此, 整个KBAT的运算过程如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kbat4.jpg" style="zoom: 50%;" /><p>图中绿色圆圈为关系嵌入, 黄色圆圈为实体嵌入. 对图中的流程总结描述如下:</p><ol><li>先将头实体, 尾实体, 关系的嵌入<strong>拼接</strong>起来.</li><li>经过GAT层的<strong>多头注意力</strong>后得到两个<strong>实体更新</strong>后的表示, 同时将<strong>关系嵌入</strong>做一次<strong>更新</strong>. </li><li>在下个GAT层前, 将更新后的头实体, 尾实体, 关系表示<strong>拼接</strong>起来.</li><li>再次经过<strong>GAT</strong>层. 得到两个实体的输出.</li><li>将最初的实体嵌入通过一次<strong>投影</strong>到与最终输出维度相匹配的维度, 和GAT输出的实体表示<strong>相加</strong>.</li><li>利用关系嵌入和实体嵌入共同计算<strong>Loss</strong>.</li></ol><p>最后, 作者谈了一下多层GAT能够聚合N阶邻居信息的优势:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kbat2.jpg" style="zoom: 50%;" /><p>在图中, 多阶邻居的信息能够以多次迭代的方式被有可能相关联的实体所捕捉.</p><blockquote><p>这种优势也只来自于GAT本身, 并不是由于KBAT的特殊性.</p></blockquote><h3 id="Training-Objective"><a href="#Training-Objective" class="headerlink" title="Training Objective"></a>Training Objective</h3><p>训练目标和TransE完全一致, 损失函数采用<strong>Hinge Loss</strong>:<br>$$<br>L(\Omega)=\sum_{t_{i j} \in S} \sum_{t_{i j}^{\prime} \in S^{\prime}} \max \left\{d_{t_{i j}^{\prime}}-d_{t_{i j}}+\gamma, 0\right\}<br>$$<br>$\gamma$ 为间隔, 距离的度量函数采用TransE的一范数, 即$d_{t_{ij}} = \lVert \vec{h_i} + \vec{g_k} - \vec{h_j} \rVert_1$.</p><p>也同样使用了负采样, $S$ 为有效三元组, $S^\prime$ 为负样本三元组:<br>$$<br>S^{\prime}=\underbrace{\left\{t_{i^{\prime} j}^{k} \mid e_{i}^{\prime} \in \mathcal{E} \backslash e_{i}\right\}}_{\text {replace head entity }} \cup \underbrace{\left\{t_{i j^{\prime}}^{k} \mid e_{j}^{\prime} \in \mathcal{E} \backslash e_{j}\right\}}_{\text {replace tail entity }}<br>$$</p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>KBAT在做链接预测时仍然像其他图方法一样, 沿用了<strong>Encoder - Decoder</strong>架构.</p><p>作者的模型中使用<a href="https://adaning.github.io/posts/52280.html">ConvKB</a>作为解码器, 作者的目标是使用卷积层捕获三元组$t_{ij}^k$ 的全局信息. ConvKB的打分函数如下:<br>$$<br>f\left(t_{i j}^{k}\right)=\left(\prod_{m=1}^{\Omega} \operatorname{ReLU}\left(\left[\vec{h}_{i}, \vec{g}_{k}, \vec{h}_{j}\right] \ast \omega^{m}\right)\right) \cdot \mathbf{W}<br>$$<br>其中$\omega^m$ 为第$m$ 个卷积核, $\Omega$ 为卷积核总数, $\ast$ 为卷积操作. $\mathbf{W}$ 代表获取最终三元组得分的线性变换矩阵.</p><p>然后用软间隔损失来优化ConvKB:<br>$$<br>\mathcal{L}=\sum_{t_{i j}^{k} \in\left\{S \cup S^{\prime}\right\}} \log \left(1+\exp \left(l_{t_{i j}^{k}} \cdot f\left(t_{i j}^{k}\right)\right)\right)+\frac{\lambda}{2}|\mathbf{W}|_{2}^{2} \\<br>\text { where } l_{t_{i j}^{k}}=\left\{\begin{array}{ll}<br>1 &amp; \text { for } t_{i j}^{k} \in S \\<br>-1 &amp; \text { for } t_{i j}^{k} \in S^{\prime}<br>\end{array}\right.<br>$$</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文. </p><blockquote><p><strong>这篇论文的实验结果有很大问题, 在文末的Summary里会提到.</strong></p></blockquote><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><p>作者将训练过程分为两步:</p><ol><li>先用Hinge Loss单独训练KBAT作为<strong>Encoder</strong>, 使得GAT有编码能力.</li><li>再用Soft Margin Loss训练ConvKB作为<strong>Decoder</strong>, 使得ConvKB有解码能力.</li></ol><blockquote><p>原文并没有说是KBAT和ConvKB联合训练还是单独训练ConvKB, 但在代码中是ConvKB直接加载了已经训练好的KBAT的Embedding, 然后继续训练. 也就是直接使用KBAT的Embedding当做ConvKB的Embedding<strong>初始化</strong>, 同时训练Embedding和ConvKB, <strong>并没有对Embedding做冻结</strong>.</p><p>作者针对此问题有<a href="https://github.com/deepakn97/relationPrediction/issues/14" target="_blank" rel="noopener">回复</a>, 但我仍感觉这样做可能会把KBAT提取好的Embedding信息<strong>破坏</strong>掉, 而非在KBAT的基础上去找ConvKB的更优解.</p></blockquote><p>作者在五个常用数据集做了链接预测实验, 统计信息如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kbat5.jpg" style="zoom: 50%;" /><p>在WN18RR和FB15k - 237上结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kbat6.jpg" style="zoom: 33%;" /><p>R - GCN在FB15k - 237和WN18RR上的结果没有超过ConvE, 但KBAT超过了. WN18RR上的Hits@1表现不太好. 作者在后面分析了原因.</p><p>在NELL - 995和Kinship上结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kbat7.jpg" style="zoom: 33%;" /><p>在这两个数据集上来看KBAT效果不错, R - GCN就不太行.</p><h3 id="PageRank-Analysis"><a href="#PageRank-Analysis" class="headerlink" title="PageRank Analysis"></a>PageRank Analysis</h3><p>作者假设复杂的多跳信息在稠密图中比在稀疏图中要更难捕获, 作者设计了类似ConvE中的实验, 探究了PageRank对模型的影响, 即测试将DistMult替换成KBAT后MRR的相对变化:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kbat10.jpg" style="zoom: 33%;" /><p>基本上PageRank越高, KBAT进步就越明显. 作者将在WN18RR上表现不好归因于WN18RR的高度稀疏和分级结构.</p><h3 id="Attention-Values-vs-Epochs"><a href="#Attention-Values-vs-Epochs" class="headerlink" title="Attention Values vs Epochs"></a>Attention Values vs Epochs</h3><p>下图是FB15k - 237中注意力值随epoch增大的变化情况:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kbat8.jpg" style="zoom: 50%;" /><p>下图是WN18RR中注意力值随epoch增大的变化情况:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kbat9.jpg" style="zoom: 50%;" /><p>总体来说, 随epoch的增大, 多跳邻居会被分配给更多的注意力.</p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>作者逐渐移除模型中的信息, 做出NELL - 995上MR随epoch的变化曲线:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kbat11.jpg" style="zoom: 50%;" /><p>红色是作者的模型曲线, 绿色是移除多跳路径信息的曲线, 蓝色是移除关系信息的曲线. </p><blockquote><p>作者也没说如何移除, 估计是用随机初始化来代替.</p></blockquote><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>KBAT将<strong>GAT</strong>引入了KGE领域, 结合KG中的十分重要的<strong>关系</strong>, 利用<strong>图注意力网络</strong>的聚合特性, 获取实体之间的<strong>多跳</strong>信息, 并取得了比R - GCN更好的效果.</p><p>虽然有实验, 但是本论文除了将GAT引入KG领域中, 感觉没有什么创新点… 文中有很大篇幅都在介绍GAT. 有一些小改动也没有给出说明.</p><p>后续有一些<a href="https://arxiv.org/abs/1911.03903" target="_blank" rel="noopener">论文</a>说明KBAT给出的代码导致了Test Data存在Leakage, 详见<a href="https://github.com/deepakn97/relationPrediction/issues/28" target="_blank" rel="noopener">此处</a>, 所以实验结果大多<strong>站不住脚</strong>, 很多疑问都没有必要再追究了. 但使用GAT在思路上应该仍然有效.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> GNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CompGCN: Composition-based Multi-Relational Graph Convolutional Networks</title>
      <link href="/posts/45769.html"/>
      <url>/posts/45769.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>GCN: 详见<a href="https://adaning.github.io/posts/31958.html">图神经网络入门</a></li><li>R - GCN: 详见<a href="https://adaning.github.io/posts/3226.html">R - GCN: Modeling Relational Data with Graph Convolutional Networks</a></li></ul></blockquote><h1 id="Composition-based-Multi-Relational-Graph-Convolutional-Networks"><a href="#Composition-based-Multi-Relational-Graph-Convolutional-Networks" class="headerlink" title="Composition-based Multi-Relational Graph Convolutional Networks"></a>Composition-based Multi-Relational Graph Convolutional Networks</h1><p>本文是论文<a href="https://arxiv.org/abs/1911.03082" target="_blank" rel="noopener">Composition-based Multi-Relational Graph Convolutional Networks</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者观察到, 现在GCN在图建模中存在两个问题:</p><ol><li><p>现实世界中的图经常是<strong>有向图</strong>, 但现在图领域的研究常常专注于简单的无向图问题.</p></li><li><p>GCN的<strong>过参数化</strong>问题很严重, 并且<strong>只学习节点表示</strong>限制了模型表达.</p></li></ol><p>作者希望提出一种对节点和关系<strong>联合嵌入</strong>的GCN模型来应对上述问题.</p><h2 id="CompGCN"><a href="#CompGCN" class="headerlink" title="CompGCN"></a>CompGCN</h2><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/compgcn1.jpg" style="zoom: 33%;" /><h3 id="GCN-and-R-GCN"><a href="#GCN-and-R-GCN" class="headerlink" title="GCN and R - GCN"></a>GCN and R - GCN</h3><p>对于图$\mathcal{G}=(\mathcal{V}, \mathcal{E}, \mathcal{X})$, $\mathcal{V}, \mathcal{E}, \mathcal{X}$ 分别代表图中存在的顶点, 边, 和节点的初始特征. </p><p>让我们先来回顾一下经典的<a href="https://adaning.github.io/posts/31958.html">GCN</a>的节点特征更新方程:</p><p>$$<br>\boldsymbol{H}^{k+1}=f\left(\hat{\boldsymbol{A}} \boldsymbol{H}^{k} \boldsymbol{W}^{k}\right)<br>$$<br>其中$\hat{\boldsymbol{A}}$ 是由度矩阵和自邻接矩阵归一化得到的.</p><p>在<a href="https://adaning.github.io/posts/3226.html">R - GCN</a>中, 引入了对不同关系$\mathcal{R}$ 的特化处理, 图结构变为了$\mathcal{G}=(\mathcal{V}, \mathcal{R}, \mathcal{E}, \mathcal{X})$:<br>$$<br>\boldsymbol{H}^{k+1}=f\left(\hat{\boldsymbol{A}} \boldsymbol{H}^{k} \boldsymbol{W}_{r}^{k}\right)<br>$$</p><p>其中, $\boldsymbol{W}_r$ 为关系特化的变换矩阵.</p><p>但和大多数只嵌入节点的常规GCN方法不同, CompGCN同时嵌入<strong>节点</strong>和<strong>关系</strong>, 图结构信息变为$\mathcal{G}=(\mathcal{V}, \mathcal{R}, \mathcal{E}, \mathcal{X}, \mathcal{Z})$, $\mathcal{Z}$ 代表<strong>初始化</strong>的关系特征.</p><p>边的种类也被作者额外区分, 能对<strong>逆边</strong>和<strong>自环边</strong>加以区分, 即:<br>$$<br>\left.\mathcal{E}^{\prime}=\mathcal{E} \cup\left\{\left(v, u, r^{-1}\right) \mid(u, v, r) \in \mathcal{E}\right\} \cup\{(u, u, \top) \mid u \in \mathcal{V})\right\}<br>$$</p><p>相应的, 关系也被加以区分, 即$\mathcal{R}^{\prime}=\mathcal{R} \cup \mathcal{R}_{i n v} \cup\{\top\}$, 其中$\mathcal{R}_{i n v}=\left\{r^{-1} \mid r \in \mathcal{R}\right\}$. </p><h3 id="Relation-Based-Composition"><a href="#Relation-Based-Composition" class="headerlink" title="Relation - Based Composition"></a>Relation - Based Composition</h3><p>引入关系嵌入后, 与KGE方法一样, 作者利用关系嵌入$\mathbf{e}_s$ 和实体嵌入$\mathbf{e}_r$ 的<strong>组合操作</strong>将节点和关系的信息作为整体共同$\mathbf{e}_o$, 纳入GCN的更新方程:<br>$$<br>\boldsymbol{e}_{o}=\phi\left(\boldsymbol{e}_{s}, \boldsymbol{e}_{r}\right)<br>$$</p><p>其中, $\phi: \mathbb{R}^{d} \times \mathbb{R}^{d} \rightarrow \mathbb{R}^{d} $ 是关系和实体嵌入的组合方式.</p><p>将关系表示为Embedding减轻了图神经网络中针对关系的<strong>过参数化</strong>问题.</p><blockquote><p>我个人的理解是, 在R - GCN中, 关系特化以变换矩阵$\boldsymbol{W}_r$ 的形式存在, 而CompGCN中以Embedding的形式存在, 减少了每种关系所对应的参数个数. Embedding是信息的低维表征, <strong>万物皆可Embedding</strong>.</p></blockquote><p>作者尝试了三种实体关系组合方式:</p><ul><li><strong>Subtraction</strong>(<strong>Sub</strong>): $\phi\left(\mathbf{e}_{s}, \mathbf{e}_{r}\right)=\mathbf{e}_{s}-\mathbf{e}_{r}$.</li><li><strong>Multiplication</strong>(<strong>Mult</strong>): $\phi\left(\boldsymbol{e}_{s}, \boldsymbol{e}_{r}\right)=\boldsymbol{e}_{s} \ast \boldsymbol{e}_{r}$.</li><li><strong>Circular - correlation</strong>(<strong>Corr</strong>): $\phi\left(\boldsymbol{e}_{s}, \boldsymbol{e}_{r}\right)=\boldsymbol{e}_{s} \star \boldsymbol{e}_{r}$.</li></ul><p>分别是相减, 相乘, 循环相关运算. 循环相关运算出自论文<a href="https://dl.acm.org/doi/10.5555/3016100.3016172" target="_blank" rel="noopener">Holographic embeddings of knowledge graphs</a>, 即HoLE的论文. 三阶时的计算方法与行列式的主对角线计算中组合方式是相似的:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/compgcn11.png" style="zoom: 25%;" /><p>循环相关运算相比于前两种更为复杂, 考虑了更多向量间的<strong>不同组合</strong>方式.</p><blockquote><p>通过关系实体组合操作, 关系和实体的嵌入组合特征将在同一特征空间下对节点信息更新.</p></blockquote><h3 id="CompGCN-Update-Equation"><a href="#CompGCN-Update-Equation" class="headerlink" title="CompGCN Update Equation"></a>CompGCN Update Equation</h3><p>普通GCN的节点表示更新方程如下:</p><p>$$<br>\boldsymbol{h}_{v}=f\left(\sum_{(u, r) \in \mathcal{N}(v)} \boldsymbol{W}_{r} \boldsymbol{h}_{u}\right)<br>$$</p><p>其中, $\mathcal{N}(v)$ 是节点$v$ 的<strong>出边邻居</strong>, $(u,r)$ 是邻居节点和相对应的和关系. $\boldsymbol{h}_u$ 为节点$u$ 的特征.</p><p>而CompGCN引入关系实体嵌入的组合后, 的节点更新方式如下:</p><p>$$<br>\boldsymbol{h}_{v}=f\left(\sum_{(u, r) \in \mathcal{N}(v)} \boldsymbol{W}_{\lambda(r)} \phi\left(\boldsymbol{x}_{u}, \boldsymbol{z}_{r}\right)\right)<br>$$</p><p>其中$\lambda(r)$ 针对<strong>边的种类</strong>做了特化, 同一种关系在不同的边类型上也许会得到区分. 作者将边分为<strong>出边</strong>, <strong>入边</strong>, 和<strong>自环边</strong>三种. 在这里$\lambda(r)=\text{dir}(r)$:</p><p>$$<br>\boldsymbol{W}_{\mathrm{dir}(r)}=\left\{\begin{array}{ll}<br>\boldsymbol{W}_{O}, &amp; r \in \mathcal{R} \\<br>\boldsymbol{W}_{I}, &amp; r \in \mathcal{R}_{i n v} \\<br>\boldsymbol{W}_{S}, &amp; r=\top(\text { self-loop })<br>\end{array}\right.<br>$$</p><p>在该模式下, 一些以前的方法能够被纳入CompGCN的框架之中:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/compgcn3.jpg" style="zoom: 33%;" /><p>从CompGCN的身上能偶找到这些过去方法的影子.</p><p>关系嵌入同节点嵌入的聚合一样, 也需要做类似的更新, 但关系没有邻居之类的概念, 直接用简单的投影即可:<br>$$<br>\boldsymbol{h}_{r}=\boldsymbol{W}_{\mathrm{rel}} \boldsymbol{z}_{r}<br>$$</p><p>$\boldsymbol{W_{\text{rel}}}$ 是<strong>与关系无关</strong>的变换矩阵, 它只是将初始关系嵌入$\boldsymbol{z}_{r}$ 投影到关系嵌入空间而已.</p><p>与R - GCN一样, CompGCN也使用<strong>基函数分解</strong>来减轻GCN的过参数化. 但用途仅为获得<strong>初始化</strong>的关系嵌入$\boldsymbol{z}_r$:<br>$$<br>\boldsymbol{z}_{r}=\sum_{b=1}^{\mathcal{B}} \alpha_{b r} \boldsymbol{v}_{b}<br>$$<br>基函数分解并没有在后续更新关系(边)的Embedding中使用到.</p><p>上述为CompGCN在<strong>只有一层</strong>情况下的说明.</p><h3 id="CompGCN-in-Multilayer"><a href="#CompGCN-in-Multilayer" class="headerlink" title="CompGCN in Multilayer"></a>CompGCN in Multilayer</h3><p>对于多层的情况, 只需要将上面的方程改写为含有层数$k$ 的形式:<br>$$<br>\boldsymbol{h}_{v}^{k+1}=f\left(\sum_{(u, r) \in \mathcal{N}(v)} \boldsymbol{W}_{\lambda(r)}^{k} \phi\left(\boldsymbol{h}_{u}^{k}, \boldsymbol{h}_{r}^{k}\right)\right)<br>$$</p><p>关系表示$\boldsymbol{h}_r^k$ 的更新也按照多层形式写出:</p><p>$$<br>\boldsymbol{h}_{r}^{k+1}=\boldsymbol{W}_{\mathrm{rel}}^{k} \boldsymbol{h}_{r}^{k}<br>$$</p><p>注意, $\boldsymbol{W}_{\text{rel}}$ 每层参数都是不同的, 多层GCN也对关系嵌入多次迭代更新.</p><p>作者对比了CompGCN与普通GCN方法的优势:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/compgcn2.jpg" style="zoom: 50%;" /><p>$K$ 为GCN的层数, $d$ 为Embedding维度, $\mathcal{B}$ 为基的数量, $\lvert \mathcal{R} \rvert$ 为关系个数.</p><p>CompGCN复杂度中的第一项来自于与层数相关的$\boldsymbol{W}_{\text{rel}}, \boldsymbol{W}_{\lambda(r)}$, 第二, 三项分别来自于基函数分解中的基$\boldsymbol{v}_{b}$ 和标量$\alpha_{b r}$.</p><p>相较于其他的GCN算法, 尤其是R - GCN, CompGCN保持了相对少的参数量.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的实验参数设置请参照原论文.</p><h3 id="Link-Prediction"><a href="#Link-Prediction" class="headerlink" title="Link Prediction"></a>Link Prediction</h3><p>CompGCN在链接预测上的玩法仍然沿用了R - GCN中<strong>Encoder - Decoder</strong>的架构</p><p>R - GCN使用的Decoder是DistMult, 解码时关系嵌入需要额外训练, 并且存在实体嵌入和关系嵌入的方法<strong>不匹配</strong>问题, 这就割裂了实体嵌入和关系嵌入的关系. 但CompGCN在编码阶段就引入了<strong>关系嵌入</strong>, 不会存在关系和实体嵌入训练不匹配的问题:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/compgcn6.jpg" style="zoom: 50%;" /><h4 id="Main-Result"><a href="#Main-Result" class="headerlink" title="Main Result"></a>Main Result</h4><p>作者在FB15k - 237和WN18RR上做了Link Prediction, 结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/compgcn4.jpg" style="zoom: 50%;" /><p>无论是比传统的KGE方法, 还是其他基于图的KGE方法, CompGCN的综合表现比较不错, 在WN18RR上的Hits@10比RotatE少了三个点.</p><h4 id="Comparison-in-CompGCN"><a href="#Comparison-in-CompGCN" class="headerlink" title="Comparison in CompGCN"></a>Comparison in CompGCN</h4><p>作者主要比较了不同GCN方法在不同Decoder下的链接预测性能表现, 其形式为$\mathbf{X}+\mathbf{M}(\mathbf{Y})$, $\mathbf{X}$ 代表某种KGE方法的打分函数, $\mathbf{M}$ 代表用来获取实体和关系Embedding的GCN方法名称, $\mathbf{Y}$ 代表组合操作符.</p><p>在FB15k - 237上的结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/compgcn5.jpg" style="zoom: 50%;" /><p>几乎所有的GCN方法会导致TransE的性能退化, 但CompGCN没有. 可能因为关系和实体的联合学习. 不同的组合操作所擅长的优化指标是不相同的.</p><h4 id="Scalability-of-CompGCN"><a href="#Scalability-of-CompGCN" class="headerlink" title="Scalability of CompGCN"></a>Scalability of CompGCN</h4><h5 id="Effect-of-Varying-Relation-Basis-Vectors"><a href="#Effect-of-Varying-Relation-Basis-Vectors" class="headerlink" title="Effect of Varying Relation Basis Vectors"></a>Effect of Varying Relation Basis Vectors</h5><p>作者比较了FB15k - 237上不同基向量个数对相对MRR的影响:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/compgcn7.jpg" style="zoom: 50%;" /><p>当基向量个数为237, 即关系不使用基向量时性能达到最好. 但采用更少的基向量并不会带来非常明显的性能衰减.</p><h5 id="Effect-of-Number-of-Relations"><a href="#Effect-of-Number-of-Relations" class="headerlink" title="Effect of Number of Relations"></a>Effect of Number of Relations</h5><p>作者比较了FB15k - 237中, 对于相同的基函数为5的情况下, 不同关系数量上的相对MRR:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/compgcn9.jpg" style="zoom: 50%;" /><p>即使固定基函数为5, 仍然能比较好的应对繁多的关系.</p><blockquote><p>上述两个实验说明关系之间可能存在着大量线性相关, 少量的基向量完全可以重新组合表示它们.</p></blockquote><h5 id="Comparison-with-R-GCN"><a href="#Comparison-with-R-GCN" class="headerlink" title="Comparison with R - GCN"></a>Comparison with R - GCN</h5><p>作者对比了R - GCN和CompGCN在FB15k - 237的不同关系数量子集中的表现:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/compgcn8.jpg" style="zoom: 50%;" /><p>仅仅在5个基函数的情况下, CompGCN全面优于R - GCN.</p><h3 id="Node-and-Graph-Classification"><a href="#Node-and-Graph-Classification" class="headerlink" title="Node and Graph Classification"></a>Node and Graph Classification</h3><p>在节点和图分类任务中, CompGCN表现如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/compgcn10.jpg" style="zoom: 33%;" /><p>这两个数据集上, 对于简单的节点分类和图分类问题, CompGCN表现良好.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>CompGCN进一步地将Knowledge Embedding中对待关系的方法引入了GCN领域, 在之前的GCN类方法中, 常不将关系嵌入引入, 而在KGE领域中, 将关系和实体嵌入共同使用是非常普遍的.</p><p>CompGCN算是<strong>集大成者</strong>, 作者考虑了很多关于GCN<strong>过参数化</strong>的内容. 同时兼顾<strong>有向</strong>, <strong>多关系</strong>, <strong>支持关系嵌入</strong>三种特性. </p><p>作者着重论述了CompGCN的各方面优势, 例如复杂度, 可扩展性, 以及具体任务的性能等.</p><blockquote><p>作者在文中使用的是非参数化的组合方式, 其实也可以尝试参数化的组合, 这样也会引入更高的复杂度. </p><p>作者也没有考虑引入Attention来增强边对节点的影响, 而是使用了对边类型的特化来处理, 可能Attention的泛用性更强一些.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> GNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KEQA: Knowledge Graph Embedding Based Question Answering</title>
      <link href="/posts/15146.html"/>
      <url>/posts/15146.html</url>
      
        <content type="html"><![CDATA[<h1 id="Knowledge-Graph-Embedding-Based-Question-Answering"><a href="#Knowledge-Graph-Embedding-Based-Question-Answering" class="headerlink" title="Knowledge Graph Embedding Based Question Answering"></a>Knowledge Graph Embedding Based Question Answering</h1><p>本文是论文<a href="https://dl.acm.org/doi/abs/10.1145/3289600.3290956" target="_blank" rel="noopener">Knowledge Graph Embedding Based Question Answering</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者发现KBQA中的三个挑战问题:</p><ol><li>谓词(关系)有多种<strong>自然语言</strong>的表示法.</li><li>实体也会有严重的模糊性而产生大量候选答案, 即实体的<strong>歧义问题</strong>.</li><li>用户的问题领域可能是<strong>开放</strong>的, KG也有可能是不完整的, 这对鲁棒性有一定的要求.</li></ol><p>Knowledge Embedding可以将实体映射为低维向量, KG中的<strong>关系信息</strong>会被保存, 各种<strong>KRL</strong>会非常有益于下游任务. 因此, 作者希望提出一种基于<strong>KGE</strong>方法, 并能够回答所有<strong>自然语言问题</strong>的框架.</p><p>KGE能够保证得到的单词表示的<strong>质量</strong>, 因为每个嵌入表示都是与<strong>整个KG交互</strong>的结果, 并且它也能保持相似的关系或实体之间的<strong>表示相似性</strong>.</p><blockquote><p>在本文中, 作者只关注了最简单的QA, 直接使用一个<strong>三元组</strong>就能描述整个问题, 而非多跳推理的问题.</p></blockquote><h2 id="KEQA"><a href="#KEQA" class="headerlink" title="KEQA"></a>KEQA</h2><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/keqa1.jpg" style="zoom: 50%;" /><p>KEQA的大致思路是通过某种结构, 对自然语言中的<strong>整个句子</strong>抽取出与Knowledge Embedding<strong>相似</strong>的表示, 即希望用句子抽取后的表示空间<strong>等价于</strong>Knowledge Embedding的空间.</p><h3 id="Predicate-and-Head-Entity-Learning-Models"><a href="#Predicate-and-Head-Entity-Learning-Models" class="headerlink" title="Predicate and Head Entity Learning Models"></a>Predicate and Head Entity Learning Models</h3><h4 id="Knowledge-Graph-Embedding"><a href="#Knowledge-Graph-Embedding" class="headerlink" title="Knowledge Graph Embedding"></a>Knowledge Graph Embedding</h4><p>对于头实体$\mathbf{e}_h$, 谓词(关系)$\mathbf{p}_\ell$, KGE方法能通过打分函数$f(\mathbf{e}_t, \mathbf{p}_\ell)$得出对应的尾实体$\mathbf{e}_t$, 即$\mathbf{e}_t \approx f(\mathbf{e}_h, \mathbf{p}_\ell)$. 通过KGE, 我们能获得一个KG中的实体嵌入表示集$\mathbf{E}$ 和谓词嵌入表示集$\mathbf{P}$.</p><blockquote><p>当KGE训练完成时, 实体和关系的表示将会<strong>固定</strong>下来, 这样才能保存住KG的信息. 若继续在后续训练时更新Embedding, 将会对原有信息扰动. 所以KGE只是做了空间指示作用.</p></blockquote><h4 id="Neural-Network-Based-Predicate-Representation-Learning"><a href="#Neural-Network-Based-Predicate-Representation-Learning" class="headerlink" title="Neural Network Based Predicate Representation Learning"></a>Neural Network Based Predicate Representation Learning</h4><p>有了KGE中获取的$\mathbf{P}, \mathbf{E}$, 接着需要将自然语言中的<strong>谓词</strong>表示(在三元组中也是关系)与KGE空间相<strong>对齐</strong>, 作者在这里采用了比较简单的<strong>双向LSTM + Attention</strong>:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/keqa2.jpg" style="zoom: 50%;" /><blockquote><p>其实在作者发布的代码中, 用的是<strong>GRU</strong>, 不过都大差不差了.</p></blockquote><p>图中在绿色向量$\mathbf{r}_j$ 和拼接后的向量$\mathbf{h}_j$ 之间还有一个FC层没画出来.</p><p>先将自然语言的Token转换为Embedding(图中灰色), 记为$\mathbf{x}_j$, 然后再用双向LSTM获取双向表示$\mathbf{h}_j$ (图中红色蓝色).</p><p>下述数学表达都是LSTM, 左右双向同理, 下面只是单向:<br>$$<br>\begin{array}{l}<br>\mathrm{f}_{j}=\sigma\left(\mathbf{W}_{x f} \mathbf{x}_{j}+\mathbf{W}_{h f} \overleftarrow{\mathbf{h}}_{j+1}+\mathbf{b}_{f}\right) \\<br>\mathbf{i}_{j}=\sigma\left(\mathbf{W}_{x i} \mathbf{x}_{j}+\mathbf{W}_{h i} \overleftarrow{\mathbf{h}}_{j+1}+\mathbf{b}_{i}\right) \\<br>\mathbf{o}_{j}=\sigma\left(\mathbf{W}_{x o} \mathbf{x}_{j}+\mathbf{W}_{h o} \overleftarrow{\mathbf{h}}_{j+1}+\mathbf{b}_{o}\right) \\<br>\mathbf{c}_{j}=\mathbf{f}_{j} \circ \mathbf{c}_{j+1}+\mathbf{i}_{j} \tanh \left(\mathbf{W}_{x c} \mathbf{x}_{j}+\mathbf{W}_{h c} \overleftarrow{\mathbf{h}}_{j+1}+\mathbf{b}_{c}\right) \\<br>\overleftarrow{\mathbf{h}}_{j}=\mathbf{o}_{j} \circ \tanh \left(\mathbf{c}_{j}\right)<br>\end{array}<br>$$</p><p>将最初的$\mathbf{x}_j$ 和双向LSTM抽取得到的$\mathbf{h}_j$ 拼到一起, 计算Attention Score $q_j$ 和注意力权重$\alpha_j$ (图中紫色):<br>$$<br>\begin{aligned}<br>\alpha_{j} &amp;=\frac{\exp \left(q_{j}\right)}{\sum_{i=1}^{L} \exp \left(q_{i}\right)} \\<br>q_{j} &amp;=\tanh \left(\mathbf{w}^{\top}\left[\mathbf{x}_{j} ; \mathbf{h}_{j}\right]+b_{q}\right)<br>\end{aligned}<br>$$<br>将注意力权重与$\mathbf{h}_j$ 加权, 与$\mathbf{x}_j$ 拼接得到隐态$\mathbf{s}_j=[\mathbf{x}_j;\alpha_j\mathbf{h}_j]$. 之后再经过一个FC层得到最终的结果$\mathbf{r}_j$.</p><p>然后对求得的每个Token的最终表示$\mathbf{r}_j$ (图中绿色) 求个平均, 得到与KGE中谓词表示$\mathbf{p}_{\ell}$ 相似的表示$\hat{\mathbf{p}}_{\ell}$:<br>$$<br>\hat{\mathbf{p}}_{\ell}=\frac{1}{L} \sum_{j=1}^{L} \mathbf{r}_{j}^{\top}<br>$$</p><p>其中$L$ 是问题的句子长度.</p><h4 id="Neural-Network-based-Head-Entity-Learning-Model"><a href="#Neural-Network-based-Head-Entity-Learning-Model" class="headerlink" title="Neural Network based Head Entity Learning Model"></a>Neural Network based Head Entity Learning Model</h4><p>对于一个自然语言问题, 作者用类似获取谓词表示的方法获取头实体表示$\hat{\mathbf{e}}_{h}$, 用同样的架构获取头实体的表示, 也与KGE空间相对齐.</p><blockquote><p>这样直接把NER这步给省略掉了, 规避了自然语言中实体表示的模糊性问题.</p></blockquote><h3 id="Head-Entity-Detection-Model"><a href="#Head-Entity-Detection-Model" class="headerlink" title="Head Entity Detection Model"></a>Head Entity Detection Model</h3><p>因为自然语言问题中的候选实体可能过多, 所以需要一个<strong>头实体检测模型</strong>来减少问题中的候选实体, 从一个句子中选择一个或几个连续的Token作为实体名称.</p><p>同样是使用双向LSTM去处理整个句子:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/keqa3.jpg" style="zoom: 50%;" /><p>先获取词嵌入$\mathbf{x}_j$, 再使用双向LSTM获得$\mathbf{h}_j$, 后接一个FC层和Softmax, 能获得两个概率, 分别是该Token属于实体名的概率$\text{HED}_{\text{entity}}$, 和Token不属于实体名的概率$\text{HED}_\text{non}$.</p><p>这样, 相连的HED可能是同一个实体, 而不相连的HED可能是多个实体名. 在获取完实体名后, 再从KG中找到对应的<strong>候选实体集</strong>, 很大程度上缩小了与KG中实体匹配的范围.</p><h3 id="Joint-Search-on-Embedding-Spaces"><a href="#Joint-Search-on-Embedding-Spaces" class="headerlink" title="Joint Search on Embedding Spaces"></a>Joint Search on Embedding Spaces</h3><p>如果只是想简单的缩小模型与KGE空间中的距离, 只需计算模型抽取出的表示与KGE空间中表示的范数. 但这显然没有考虑过KGE中保留的<strong>关系信息</strong>.</p><p>基于前面的谓词学习模型, 实体学习模型, 头实体检测模型, 作者提出了对三种模型的<strong>联合距离度量</strong>:</p><p>$$<br>\begin{aligned}<br>\underset{(h, \ell, t) \in C} {\operatorname{minimize}} \quad\lVert\mathbf{p}_{\ell}-\hat{\mathbf{p}}_{\ell}\rVert_{2}+\beta_{1}\lVert\mathbf{e}_{h}-\hat{\mathbf{e}}_{h}\rVert_{2}+\beta_{2}\lVert f\left(\mathbf{e}_{h}, \mathbf{p}_{\ell}\right)-\hat{\mathbf{e}}_{t}\rVert_{2}<br> \\\ -\beta_{3} \operatorname{sim}\left[n(h), \mathrm{HED}_{\text {entity}}\right]-\beta_{4} \operatorname{sim}\left[n(\ell), \mathrm{HED}_{\text {non}}\right]<br>\end{aligned}<br>$$</p><p>其中, $\beta_1,\beta_2,\beta_3,\beta_4$ 是预定义的超参, 即损失中四项的权重, 用来衡量所对应的影响力. $n(\cdot)$ 代表取实体或谓词的具体名的操作.</p><p>第一, 二项均用于缩小模型抽取出的头实体和谓词表示与KGE空间中相应表示的距离, 第三项用于缩小尾实体表示与KGE中相应实体表示的距离. 第四项和第五项针对头实体检测模型, $\operatorname{sim}[\cdot;\cdot]$ 是度量两个<strong>字符串</strong>之间相似性的函数, 在这里最大化标出的头实体和目标实体之间的相似度, 以及句子的非实体部分和真实谓词之间的相似度.</p><p>KEQA的框架执行流程如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/keqa4.jpg" style="zoom: 67%;" /><p>大致流程是:</p><ol><li>分别<strong>训练</strong>谓词学习模型, 实体学习模型, 头实体检测模型. 训练谓词和实体表示模型时最小化模型结果与KGE相应表示的距离. 训练头实体检测模型时采用极大似然.</li><li>训练结束后, 面对每个问题, 分别用谓词学习模型和实体学习模型抽取出问题的谓词表示$\hat{\mathbf{p}}_\ell$ 和头实体表示$\hat{\mathbf{e}}_h$.</li><li>使用头实体检测模型检测出问题中出现的头实体$\text{HED}_{\text{entity}}$.</li><li>在KG中找到与检测出的头实体所匹配的候选集$\mathcal{C}$, 然后用KGE的打分函数$f(\cdot)$, 将$\hat{\mathbf{p}}_\ell$ 和$\hat{\mathbf{e}}_h$ 预测出相应的尾实体.</li><li>根据预测出的尾实体, 计算出与当前三元组<strong>联合距离最小</strong>的三元组, 作为答案返回.</li></ol><blockquote><p>头实体检测模型训练时采用的损失与联合距离度量略有不同, 训练时采用的是极大似然, 但在计算联合距离时采用的是字符串相似度.</p><p>KEQA对不同的损失项的优化实际上是<strong>分步</strong>运行的, 并且不同损失项针对的是不同模型.</p></blockquote><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>实验部分详细的参数请参照原论文.</p><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><p>因为本文探讨的是<strong>简单问题</strong>的回答, 作者除了采用FB2M, FB5M外, 还使用了从FB2M中的子集数据集SimpleQuestions, 统计信息如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/keqa5.jpg" style="zoom: 67%;" /><h3 id="Effectiveness-of-KEQA"><a href="#Effectiveness-of-KEQA" class="headerlink" title="Effectiveness of KEQA"></a>Effectiveness of KEQA</h3><p>KEQA在FB2M, FB5M上的实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/keqa8.jpg" style="zoom: 67%;" /><p>其中, KEQA_noEmbed是<strong>随机</strong>得到的Embedding, 没有经过KGE训练.</p><p>相较于提出的Baseline, 提升比较大. 在引入Knowledge Embedding后, 模型性能有了进一步的提升, 但其实在没引入KGE模型的情况下, 本身的效果也还可以, 因为它仍然超过了其他的Baseline.</p><h3 id="Generalizability-and-Robustness-Evaluation"><a href="#Generalizability-and-Robustness-Evaluation" class="headerlink" title="Generalizability and Robustness Evaluation"></a>Generalizability and Robustness Evaluation</h3><p>为检测KEQA的<strong>鲁棒性</strong>, 作者从两个角度做了实验:</p><ol><li>对KEQA使用不同的KGE方法.</li><li>对所有的谓词划分为三个组, 然后将训练集, 验证集, 测试集的问题按照谓词划分为三组. 这样测试集中出现的谓词肯定没有在训练集和验证集中出现过, 三类数据的谓词都是相互独立的. 按照该方法将SimpleQuestions改进为SimpleQ_Missing.</li></ol><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/keqa6.jpg" style="zoom: 67%;" /><p>不同的KGE方法对结果其实没有太大的影响. 最简单的TransE就能有不错的结果. 对于新的数据集, 在TransE的帮助下仍然有40%左右的精度, 比不依赖KGE方法要高一些.</p><h3 id="Parameter-Analysis"><a href="#Parameter-Analysis" class="headerlink" title="Parameter Analysis"></a>Parameter Analysis</h3><p>为了探究联合距离度量中每项的贡献程度, 作者分别采用三种设置:</p><ul><li>Only_Keep: <strong>仅保留</strong>五项中的该度量.</li><li>Remove: 从五项中<strong>删除</strong>该项度量.</li><li>Accumulate: <strong>逐项添加</strong>度量.</li></ul><p>实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/keqa7.jpg" style="zoom: 67%;" /><p>仅保留实体相关项的精度都非常低, 而仅保留谓词项的精度都比较高, 这也侧面说明了关系在KGE中发挥的重要作用, 实际上就是在针对关系学习. </p><p>在第一二项均保留的情况下, 结合谓词信息, 实体信息能够得到充分的利用.</p><p>第五项仍然使得模型性能涨了一个点, 说明问题中的某些Token可能和谓词<strong>共享信息</strong>.</p><blockquote><p>我认为第三项和第一二项可能是<strong>近似等价</strong>的, 而不是<strong>互为补充</strong>的.</p><p>因为第三项的引入并没有使得模型性能提升产生多大的变化, 但仅保留时仍然发挥了相当重要的作用, 而且在删除第三项时也没有对性能产生太大的影响. </p><p>即仅保留第三项时没有第一二项, 性能好. 删除第三项, 剩下其余四项, 性能没有明显变化. 在已经有第一二项的情况下, 加上第三项, 几乎无提升.</p></blockquote><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>KEQA是一种基于<strong>Knowledge Embedding</strong>的<strong>问答</strong>框架, 能从繁杂的<strong>自然语言</strong>中直接抽取出<strong>谓词表示</strong>和<strong>实体表示</strong>, 缓解了自然语言的<strong>模糊性</strong>和<strong>歧义性</strong>问题. 并通过<strong>头实体检测</strong>模型过<strong>滤掉</strong>非常多的候选实体三元组, <strong>缩小搜索范围</strong>. 同时, 作者充分结合了KGE能够保存<strong>关系信息</strong>的特性, 提出了<strong>联合距离度量</strong>.</p><p>除此外, 我个人认为联合度量中第三项的必要性是有待商榷的.</p><p>BERT具有类似KGE获取表示的能力, 在依托KGE方法的KEQA的框架下, 结合一些<strong>上下文相关</strong>的KGE方法可能会有奇效, 例如<a href="https://adaning.github.io/posts/42304.html">CoKE</a>, <a href="https://adaning.github.io/posts/28100.html">CoLAKE</a>, 因为它们与KEQA类似能够利用<strong>自然语言信息</strong>的能力, 而不单单是利用知识本身.</p><p>本文只关注于最基本的单跳简单问题, 该如何扩展到多跳复杂问题?</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KBQA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CoPER: Contextual Parameter Generation for Knowledge Graph Link Prediction</title>
      <link href="/posts/14355.html"/>
      <url>/posts/14355.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>ConvE: 详见<a href="https://adaning.github.io/posts/42031.html">ConvE: Convolutional 2D Knowledge Graph Embeddings</a></li></ul></blockquote><h1 id="Contextual-Parameter-Generation-for-Knowledge-Graph-Link-Prediction"><a href="#Contextual-Parameter-Generation-for-Knowledge-Graph-Link-Prediction" class="headerlink" title="Contextual Parameter Generation for Knowledge Graph Link Prediction"></a>Contextual Parameter Generation for Knowledge Graph Link Prediction</h1><p>本文是论文<a href="https://ojs.aaai.org/index.php/AAAI/article/view/5693" target="_blank" rel="noopener">Contextual Parameter Generation for Knowledge Graph Link Prediction</a>的阅读笔记和个人理解. 本文是CMU发布的<strong>参数生成</strong>系列论文中的一篇, 系列文章请参见<a href="https://zhuanlan.zhihu.com/p/210942711" target="_blank" rel="noopener">参数生成（Parameter Generation/Adaption）相关论文整理</a>.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者发现, 现在的KGE模型中Relation Embedding经常被限制为<strong>加性</strong>的, 这使得模型在处理不同关系变换时的表达能力被<strong>限制</strong>.</p><p>为解决此问题, 作者使用<strong>上下文参数生成</strong>方法, 去除掉加性限制. 更具体的, 作者将关系作为<strong>上下文</strong>来做Knowledge Embedding, 下图是作者改进<strong>ConvE</strong>的示意图:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/coper1.jpg" style="zoom: 33%;" /><p>如图构想所示, Relation Embedding作为用于<strong>生成</strong>操作Entity Embedding的<strong>参数</strong>信息而存在. 这种方式被作者称为<strong>CoPER</strong>(<strong>Co</strong>ntextual <strong>P</strong>arameters from <strong>E</strong>mbedded <strong>R</strong>elations).</p><h2 id="Limited-Expressive-Power"><a href="#Limited-Expressive-Power" class="headerlink" title="Limited Expressive Power"></a>Limited Expressive Power</h2><p>现在Knowledge Embedding框架普遍可以由如下四部组成:</p><p>$$<br>\begin{array}{lr}<br>\mathbf{e}_{s}=\mathbf{E} e_{s}, &amp;\quad \text { (embedding) }\\<br>\mathbf{r}=\mathbf{R} r, &amp;\quad \text { (embedding) }\\<br>\mathbf{z}=h_{\phi}\left(\mathbf{e}_{s}, \mathbf{r}, \ldots\right), &amp;\quad\text { (merge) }\\<br>\text {ans}=f_{\theta}(\mathbf{z}, \ldots), &amp;\quad \text { (prediction) }<br>\end{array}<br>$$</p><p>其中$e_s , r$ 分别是头实体和关系的<strong>独热编码</strong>, $\mathbf{E}, \mathbf{R}$ 是实体和关系的Embedding矩阵. $h$ 为<strong>Merge Function</strong>, 参数为$\phi$, 用于获取关系和头实体的聚合表示. $f$ 为<strong>Prediction Function</strong>, 参数为$\theta$, 用于预测出尾实体.</p><p>四步可以概括为获取头实体表示, 获取关系表示, KGE聚合, 投影预测.</p><p>Merge和Prediction的具体操作在不同的KGE方法中通常是不同的, 作者以<strong>ConvE</strong>为例, 具体的说明了表达是如何被限制的. 在ConvE中, 后两步的数学表达如下:<br>$$<br>\begin{array}{lr}<br>\mathbf{z}=\operatorname{Conv2D}\left(\operatorname{Reshape}\left(\left[\mathbf{e}_{s} ; \mathbf{r}\right]\right)\right), &amp; \text { (merge) } \\<br>\hat{e}_{t} =f_{\theta}(\mathbf{z}), &amp; \text { (prediction) }<br>\end{array}<br>$$</p><p>ConvE中, Merge Function为将头实体和关系的嵌入表示Concat, 后Reshape成二维数据, 然后用<strong>二维卷积</strong>获取聚合后的表示$\mathbf{z}$, $f_\theta$ 为一次线性变换, 即$\theta$ 代表投影矩阵.</p><p>考虑一种最简单的情况, 当Merge Function为Concat + Linear时的场景:</p><p>$$<br>h_{\phi}\left(\mathbf{e}_{s}, \mathbf{r}\right)=\phi \cdot\left[\mathbf{e}_{s} ; \mathbf{r}\right]<br>$$</p><p>$[\cdot; \cdot]$ 为Concat, $\phi$ 为单层线性投影. 正是因为Concat在操作上使得$\mathbf{e}_s, \mathbf{r}$ 之间是<strong>相互独立</strong>的, $h_\phi (\mathbf{e}_s; \mathbf{r})$ 可以被写为$h_\phi (\mathbf{e}_s; \mathbf{r})=\phi_s \mathbf{e}_s + \phi_r\mathbf{r}$, 所以$\mathbf{e}_s, \mathbf{r}$ 只能做<strong>加性交互</strong>. ConvE在大多数情况下都符合上述情况, 唯独除了卷积核在对$\mathbf{e}_s$ 和$\mathbf{r}$ 的<strong>交界处</strong>做交互的情况.</p><blockquote><p>作者所提出的问题仅适用于ConvE这种没有进行<strong>模块堆叠</strong>的方法, 因为ConvE仅仅只使用了一层卷积. </p><p>同样也不适用于能够同时覆盖$\mathbf{e}_o$ 和$\mathbf{e}_r$ 的操作. 如果操作能够覆盖二者, 那么二者之间将不单进行简单的加和运算, 还会有其他部分的交互运算.</p><p>如果是多层卷积堆叠, 随着堆叠加深, $\mathbf{e}_s$ 和$\mathbf{r}$ 交界处所映射的范围也越来越大, 直到感受野可以覆盖住整个特征图, 每次卷积都可以对整块数据交互, 即使是使用了Concat, 也不符合作者所述的情况.</p></blockquote><p>作者举出了一个非常简单的例子(Toy Example)来详细说明一些KGE模型在应对这些场景时所暴露的问题:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/coper2.jpg" style="zoom: 50%;" /><p>如果以<strong>三元组</strong>作为结构, 那么$\mathbf{e_2}, \mathbf{e_3}$ 可以按照如下方式表示:<br>$$<br>\begin{array}{l}<br>\mathbf{e}_{2}=\phi_{e} \mathbf{e}_{0}+\phi_{r} \mathbf{r}_{0} \\<br>\mathbf{e}_{3}=\phi_{e} \mathbf{e}_{0}+\phi_{r} \mathbf{r}_{1} \\<br>\mathbf{e}_{3}=\phi_{e} \mathbf{e}_{1}+\phi_{r} \mathbf{r}_{0} \\<br>\mathbf{e}_{2}=\phi_{e} \mathbf{e}_{1}+\phi_{r} \mathbf{r}_{1}<br>\end{array}<br>$$</p><p>将上述四个式子可以分别做减法得到如下两个$\mathbf{e}_2, \mathbf{e}_3$ 的表示式:</p><p>$$<br>\begin{array}{l}<br>\left(\mathbf{e}_{2}-\mathbf{e}_{3}\right)=\phi_{r}\left(\mathbf{r}_{0}-\mathbf{r}_{1}\right) \\<br>\left(\mathbf{e}_{3}-\mathbf{e}_{2}\right)=\phi_{r}\left(\mathbf{r}_{0}-\mathbf{r}_{1}\right)<br>\end{array}<br>$$</p><p>这两个表示式非常的矛盾, 除非满足如下情况:</p><ul><li>$\mathbf{e}_2 = \mathbf{e}_3$: 将导致$\mathbf{e}_2, \mathbf{e}_3$无法区分.</li><li>$\phi_r=0$ 或$\mathbf{r}_0 = \mathbf{r}_1$: 将导致$\mathbf{r}_1, \mathbf{r}_2$ 无法区分或失效.</li><li>$\phi_e=0$ 或$\mathbf{e}_0 = \mathbf{e}_1$: 将导致$\mathbf{e}_0, \mathbf{e}_1$ 无法区分.</li></ul><p>即模型<strong>无法处理</strong>Toy Example的情况. 主要原因就在于, 虽然Merge Function操作对象表面是$\mathbf{e}_s$ 和$\mathbf{r}$, 但做后续操作的时候还是近似对它们<strong>单独处理</strong>的.</p><h2 id="CoPER"><a href="#CoPER" class="headerlink" title="CoPER"></a>CoPER</h2><p>在CoPER中, Relation被定义为, 为了产生尾实体, 应该对头实体进行<strong>何种操作</strong>. 因此, $h$ 只作用于$\mathbf{e}_s$, 而$\mathbf{r}$ 将用于生成$f$ 的参数$\theta$. $f$ 的参数将不再由学习得来, 而由<strong>CPG</strong>(<strong>C</strong>ontextual <strong>P</strong>arameter <strong>G</strong>neration)模块的<strong>输出</strong>得来, CPG是一种能够对于给定的Relation独热编码$r$ 给出操作头实体的参数$\theta$ 的模块.</p><h3 id="Parameter-Generator-Network"><a href="#Parameter-Generator-Network" class="headerlink" title="Parameter Generator Network"></a>Parameter Generator Network</h3><p>有多种方式可以通过$r$ 来获取$\theta$. 最简单的一种是通过<strong>查表</strong>:<br>$$<br>g_{\text {lookup }}(r)=\mathbf{W}_{\text {lookup }} r<br>$$</p><p>但是这种方式不能在跨关系时实现信息共享, 而且非常容易<strong>过拟合</strong>, 尤其是面对训练集中所出现的关系.</p><p>换一种方法, 也可以是简单的<strong>线性变换</strong>:<br>$$<br>g_{\text {linear }}(r)=\mathbf{W}_{\text {linear }} \mathbf{R} r+\mathbf{b}<br>$$</p><p>这样, 通过一个权重矩阵$\mathbf{W}$, 不同的关系嵌入$\mathbf{r}$ 可能<strong>共享</strong>一些潜在的<strong>共性</strong>, 因为$\mathbf{r}$ 会被重新线性组合. </p><p>一般非常大的线性变换矩阵很容易造成<strong>过拟合</strong>, 而MLP可以作为一种线性变换的<strong>低秩近似</strong>:<br>$$<br>g_{\mathrm{MLP}}(r)=\operatorname{MLP}(\mathbf{R} r)<br>$$</p><p>当然也可以是其他更复杂的结构, 这里只列举了最简单的三种.</p><h3 id="Enhanced-Expressive-Power"><a href="#Enhanced-Expressive-Power" class="headerlink" title="Enhanced Expressive Power"></a>Enhanced Expressive Power</h3><p>其实作者的解决思路非常的简朴, 把<strong>加性变成乘性</strong>的不就完了么. 还是针对前面的Toy Example, 构建最简单的情况. 假设Merge Function有$h_\phi(\mathbf{e}_s)=\mathbf{e}_s$, $f$ 是最简单的线性投影, 即$f_\theta(x)=\theta x$, $\theta=g_{\text{linear}}(r)=\mathbf{WR}r+\mathbf{b}=\mathbf{Wr}+b$, 因此: </p><p>$$<br>\hat{e}_{t}=f_{\theta}\left(h_{\phi}\left(\mathbf{e}_{s}\right)\right)=f_{\theta}\left(\mathbf{e}_{s}\right)=\theta \mathbf{e}_{s}=(\mathbf{W r}+\mathbf{b}) \mathbf{e}_{\mathbf{s}}<br>$$</p><p>这使得Relation Embedding将以<strong>乘性</strong>地作用于$\mathbf{e}_s$, Toy Example重新表示为如下四个式子:</p><p>$$<br>\begin{array}{l}<br>\mathbf{e}_{2}=\left(\mathbf{W r}_{0}+\mathbf{b}\right) \mathbf{e}_{0} \\<br>\mathbf{e}_{3}=\left(\mathbf{W r}_{0}+\mathbf{b}\right) \mathbf{e}_{1} \\<br>\mathbf{e}_{2}=\left(\mathbf{W} \mathbf{r}_{1}+\mathbf{b}\right) \mathbf{e}_{1} \\<br>\mathbf{e}_{3}=\left(\mathbf{W} \mathbf{r}_{1}+\mathbf{b}\right) \mathbf{e}_{0}<br>\end{array}<br>$$</p><p>还是将上式做减法, 能得出在两个三元组下的表示结果是不同的:</p><p>$$<br>\begin{array}{l}<br>\mathbf{e}_{3}-\mathbf{e}_{2}=\left(\mathbf{W r}_{0}+\mathbf{b}\right)\left(\mathbf{e}_{1}-\mathbf{e}_{0}\right) \\<br>\mathbf{e}_{3}-\mathbf{e}_{2}=\left(\mathbf{W} \mathbf{r}_{1}+\mathbf{b}\right)\left(\mathbf{e}_{0}-\mathbf{e}_{1}\right)<br>\end{array}<br>$$</p><p>除去一组解$\mathbf{e}_0=\mathbf{e}_1$, 其他解都可以使得该式有意义:</p><p>$$<br>\mathbf{W}\left(\mathbf{e}_{1}-\mathbf{e}_{0}\right)\left(\mathbf{r}_{0}+\mathbf{r}_{1}\right)+2 \mathbf{b}\left(\mathbf{e}_{1}-\mathbf{e}_{0}\right)=0<br>$$</p><p>因此, $\mathbf{e}_2, \mathbf{e}_3$ 找到了更合适的表示. </p><h3 id="CoPER-ConvE"><a href="#CoPER-ConvE" class="headerlink" title="CoPER - ConvE"></a>CoPER - ConvE</h3><p>基于上述方法, 作者将ConvE改写为如下过程:<br>$$<br>\begin{aligned}<br>\mathbf{z} &amp;=\operatorname{Conv2D}\left(\operatorname{Reshape}\left(\mathbf{e}_{s}\right)\right)\\<br>\theta &amp;=g(r)\\<br>\hat{e}_{t} &amp;=f_{\theta}(\mathbf{z})=\theta_{1}+\theta_{2: D_{\theta}} \mathbf{z}<br>\end{aligned}<br>$$</p><p>即投影层的参数是由$\mathbf{r}$ 生成的, $\theta_1, \theta_2$ 是由$\theta$ 分割而来, 即$\theta = [\theta_1; \theta_2]$.</p><h3 id="CoPER-MINERVA"><a href="#CoPER-MINERVA" class="headerlink" title="CoPER - MINERVA"></a>CoPER - MINERVA</h3><p>同样, 对于基于LSTM的方法MINERVA也可以改写为下述式子:</p><p>$$<br>\mathbf{h}_{i}=\mathbf{L S T M}\left(\mathbf{h}_{i-1},\left[\mathbf{e}_{i} ; \mathbf{r}_{i-1}\right]\right)<br>$$</p><p>$$<br>\begin{array}{lr}<br>\mathbf{o}_{i}=\operatorname{MLP}\left(\left[\mathbf{h}_{i} ; \mathbf{e}_{i} ; \mathbf{r}_{q}\right]\right), &amp;\quad \text { (merge) }\\<br>a_{j}=\operatorname{Categorical}\left(\mathbf{A}_{i} \mathbf{o}_{i}\right)&amp;\quad \text { (prediction) }<br>\end{array}<br>$$<br>这个模型不是很了解, 在此不做说明了.</p><p>上述两种方法更改前与更改后的对比图如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/coper3.jpg" style="zoom: 50%;" /><p>主要区别在于不将实体嵌入和关系嵌入一起输入模型, 仅将关系嵌入作为对头实体嵌入操作参数生成的依据.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>作者将当前SOTA的模型在五个常用数据集上做了实验, 统计信息如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/coper5.jpg" style="zoom: 50%;" /><p>$N_e$ 为实体数量, $N_r$ 为关系数量, $\tilde{N}_a$ 为每个问题平均的答案数量, $\tilde{d}$ 为平均节点度.</p><p>下面是实验结果:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/coper4.jpg" style="zoom: 50%;" /><p>效果显著的变好了, 涨点特别多. </p><p>除涨点外, 最为显著的是其<strong>收敛提速</strong>效果:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/coper6.jpg" style="zoom: 50%;" /><p>在不将实体嵌入和关系嵌入一并Merge过后, 收敛提速效果极其明显. 但是加速效果具体应该有多少似乎与数据集没有固定的规律.</p><p>消融实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/coper7.jpg" style="zoom: 50%;" /><p>“CoPER - PL - ConvE”代表作者前面所述的查表的方法. 而CoPER指的是使用$g_{\text{linear}}$ 或者$g_{\text{MLP}}$ 的方法.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>CoPER针对作者提出的场景解决方法简单而有效, 将<strong>参数生成</strong>应用到链接预测任务上, 将<strong>关系视为上下文</strong>, 生成操作头实体的函数参数. </p><p>但作者的方法仅能用于改进<strong>无交互性操作</strong>的模型, 例如ConvE等.</p><p>这篇论文其实可以与类似的方法<a href="https://adaning.github.io/posts/20145.html">ConvR</a>进行对比, 这二者同样都可以针对<strong>ConvE</strong>做出改进, 也同样都是对<strong>关系的特化</strong>处理. </p><p>ConvR侧重于从结构的角度将关系嵌入直接作为卷积核, 而CoPER - ConvE将关系嵌入作为参数生成的上下文依据, 生成投影所需的参数.</p><p>所以甚至可以集二者之优, 沿着CoPER中参数生成的思路去改进ConvR? 或许一定程度上可以缓解ConvR中关系嵌入没有<strong>深层次化</strong>的问题.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>R-GCN: Modeling Relational Data with Graph Convolutional Networks</title>
      <link href="/posts/3226.html"/>
      <url>/posts/3226.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>GNN: 详见<a href="https://adaning.github.io/posts/31958.html">图神经网络入门</a></li></ul></blockquote><h1 id="Modeling-Relational-Data-with-Graph-Convolutional-Networks"><a href="#Modeling-Relational-Data-with-Graph-Convolutional-Networks" class="headerlink" title="Modeling Relational Data with Graph Convolutional Networks"></a>Modeling Relational Data with Graph Convolutional Networks</h1><p>本文是论文<a href="https://arxiv.org/abs/1703.06103" target="_blank" rel="noopener">Modeling Relational Data with Graph Convolutional Networks</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>大规模知识图谱仍然不完整, 并且知识图谱中还没有针对<strong>图结构</strong>建模的方法, 而节点缺失的信息经常可以由邻居<strong>编码</strong>而来.</p><p>在KG中, 经常以Entity Classification和Link Prediction作为任务来衡量模型对KG补全的能力:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rgcn1.jpg" style="zoom: 50%;" /><p>图中红色字体为缺失信息, 对应着实体类型和关系.</p><p>在大规模知识图谱中, <strong>多关系数据</strong>特性十分显著, 作者希望针对多关系数据, 提出一种基于图的方法处理知识图谱补全问题.</p><h2 id="R-GCN"><a href="#R-GCN" class="headerlink" title="R - GCN"></a>R - GCN</h2><p>有向且有标签的多图被表示为$G=(\mathcal{V, E, R})$, 其中节点$v_i \in \mathcal{V}$, 边$(v_i, r, v_j) \in \mathcal{E}$, 关系类型$r \in \mathcal{R}$.</p><h3 id="Relational-Graph-Conovlutional-Network"><a href="#Relational-Graph-Conovlutional-Network" class="headerlink" title="Relational Graph Conovlutional Network"></a>Relational Graph Conovlutional Network</h3><p>R - GCN是一种GCN的扩展形式, GCN能够聚合周围邻居的信息. </p><p>GCN遵循<strong>消息传递</strong>机制:</p><p>$$<br>h_{i}^{(l+1)}=\sigma\left(\sum_{m \in \mathcal{M}_{i}} g_{m}\left(h_{i}^{(l)}, h_{j}^{(l)}\right)\right)<br>$$</p><p>其中$h_{i}^{(l)}$ 是节点$v_i$ 第$l$ 层的隐藏状态, $\mathcal{M_i}$ 为节点$v_i$ 的<strong>入边</strong>. $\sigma$ 为非线性激活函数, 例如$\operatorname{ReLU}$.</p><p> $g_m(\cdot, \cdot)$ 为邻居节点的<strong>聚合方式</strong>, 一般只用简单的线性变换作为替代, 即$g_m(h_i, h_j) = Wh_j$.</p><blockquote><p>这部分有问题请参见<a href="https://adaning.github.io/posts/31958.html">图神经网络入门</a>中的GCN和消息传递部分.</p></blockquote><p>R - GCN针对不同种类的关系进行了特化处理, 即针对不同的关系采用<strong>不同的聚合方式</strong>, 在这里是使用了不同的线性变换矩阵$W_r$. 其更新方程为:<br>$$<br>h_{i}^{(l+1)}=\sigma\left(\sum_{r \in \mathcal{R}} \sum_{j \in \mathcal{N}_{i}^{r}} \frac{1}{c_{i, r}} W_{r}^{(l)} h_{j}^{(l)}+W_{0}^{(l)} h_{i}^{(l)}\right)<br>$$</p><p>其中$c_{i, r}$ 代表归一化因子, 可以由学习得来, 或规定$c_{i, r} = |\mathcal{N}_i^r|$. $W_0$ 代表节点自身闭环所对应的变换矩阵.</p><p>针对节点$v_i$ 及其所有邻居$\mathcal{N_i}$, 分别考虑$v_i$ 与邻居$v_j$ 二者之间的关系$r$, 施加以不同的关系变换$W_r$, 再将它们求和并归一化, 加上自身的闭环影响, 在激活函数的影响下获得下一层的节点表示.</p><p>对于有向异质图(有向多关系图), R - GCN每个节点表示的更新方式大致可以由下图描述:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rgcn2.jpg" style="zoom: 50%;" /><p>蓝色块对应着不同关系下蓝色的不同邻居对红色中心节点的影响, 每种关系分别经过变换求和得到绿色块, 对所有关系的绿色块求和并激活, 得到红色中心节点的新表示.</p><p>同一类型关系的不同指向被视为不同关系(同关系下的出入被视为是不同的).</p><blockquote><p>对于关系非常多的图, 每一种关系都对应着一个独特的变换矩阵, <strong>参数量</strong>将是巨大的, 增加了潜在的<strong>过拟合</strong>风险.</p></blockquote><h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><p>针对上小节图神经网络过参数化的问题, 作者提出两种减少参数从而减轻过拟合的方法. 分别是<strong>基函数分解</strong>和<strong>对角块分解</strong>.</p><h4 id="Basic-Decomposition"><a href="#Basic-Decomposition" class="headerlink" title="Basic Decomposition"></a>Basic Decomposition</h4><p>基函数分解将所有关系的权重矩阵视为不同系数和基的<strong>线性组合</strong>:</p><p>$$<br>W_{r}^{(l)}=\sum_{b=1}^{B} a_{r b}^{(l)} V_{b}^{(l)}<br>$$</p><p>其中$B$ 为基的数量, $a_{rb}$ 是不同关系$r$ 下的系数, $V_b$ 是线性基.</p><p>这种表示方法可以被视为是不同关系下的<strong>权重共享</strong>, 对于少关系的情况能够缓解过拟合现象, 因为关系之间的基是相互共享的, 基总会被其他关系所频繁更新.</p><h4 id="Block-Diagonal-Decomposition"><a href="#Block-Diagonal-Decomposition" class="headerlink" title="Block - Diagonal Decomposition"></a>Block - Diagonal Decomposition</h4><p>对角块分解将变换矩阵$W_r$ 拆分为$B$ 个大小<strong>相同</strong>的块:<br>$$<br>W_{r}^{(l)}=\bigoplus_{b=1}^{B} Q_{b r}^{(l)}<br>$$</p><p>其中$Q_{b r}^{(l)} \in \mathbb{R}^{\left(d^{(l+1)} / B\right) \times\left(d^{(l)} / B\right)}$, $\bigoplus$ 为生成对角阵的操作$\operatorname{diag}$.</p><p>对角块分解又可以视为是一种$W_r$ 的一种<strong>稀疏性约束</strong>, 除去块的位置其余位置都是0, 与未分解时相比更为稀疏. 而且由于分块的影响, 在每个块内部, 隐特征的联系将比块与块之间更加<strong>紧密</strong>.</p><p>至此, R - GCN已经能够作为一个单独的层, 按照层级结构堆叠获得$L$ 层的多层输出. 如果没有节点的初始特征, 可以采用<strong>Embedding</strong>的方式获取.</p><blockquote><p>原文中说的独热 + 单层线性变换获取的稠密表示实际上就是Embedding.</p></blockquote><h3 id="Entity-Classification"><a href="#Entity-Classification" class="headerlink" title="Entity Classification"></a>Entity Classification</h3><p>对于实体分类问题, R - GCN能直接将堆叠过后最后一层的节点输出作为Logits, 预测实体类别:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rgcn3.jpg" style="zoom: 67%;" /><p>将最后一层的激活函数更换为$\operatorname{softmax}$, 接着最小化<strong>多分类交叉熵</strong>即可:</p><p>$$<br>\mathcal{L}=-\sum_{i \in \mathcal{Y}} \sum_{k=1}^{K} t_{i k} \ln h_{i k}^{(L)}<br>$$</p><p>其中$\mathcal{Y}$ 为有标签的节点集合, $K$ 为类别总数.</p><h3 id="Link-Prediction"><a href="#Link-Prediction" class="headerlink" title="Link Prediction"></a>Link Prediction</h3><p>对于链接预测问题, 通常在三元组$(s, r, o)$ 上(也就是边$\mathcal{E}$) 预测缺失的尾实体$o$. 但R - GCN只能根据某节点不同关系的邻居获得自身的表示, 并不能结合关系信息做尾实体的预测.</p><p>因此, 作者将R - GCN集成进<strong>Auto Encoder</strong>的框架, 将R - GCN视为一个获取所有节点编码的<strong>Encoder</strong>, 再用其他的KGE模型对节点(实体)表示<strong>打分</strong>, 以完成链接预测任务:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rgcn4.jpg" style="zoom: 67%;" /><p>在这里, 作者选用DistMult作为Decoder. 其打分函数如下:</p><p>$$<br>f(s, r, o)=e_{s}^{T} R_{r} e_{o}<br>$$</p><p>DistMult是基于<strong>语义匹配</strong>的模型, 对每个不同的关系, DistMult都有不同的对角矩阵$R_r$ 来变换头实体$e_s$, 然后与尾实体$e_o$ 做<strong>相似度匹配</strong>. 所有的Embedding, $e_s, e_o$ 都来源于R - GCN.</p><blockquote><p>DistMult虽然也对关系做了特化处理, 但只靠<strong>对角矩阵</strong>做变换显然是无法应对多元化关系的.</p><p>所以R - GCN的在链接预测上的性能可能受到了<strong>约束</strong>.</p></blockquote><p>然后用<strong>BCE</strong>作为损失函数优化:<br>$$<br>\mathcal{L}=-\frac{1}{(1+\omega)|\hat{\mathcal{E}}|} \sum_{(s, r, o, y) \in \mathcal{T}} y \log l(f(s, r, o))+<br>(1-y) \log (1-l(f(s, r, o)))<br>$$</p><p>其中$\omega$ 代表每个正样本采样多少个负样本. $l$ 为$\operatorname{sigmoid}$ 函数. </p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的超参数设置和Trick请参照原论文.</p><h3 id="Entity-Classification-Experiments"><a href="#Entity-Classification-Experiments" class="headerlink" title="Entity Classification Experiments"></a>Entity Classification Experiments</h3><p>作者分别在四个实体分类数据集AIFB, MUTAG, BGS, AM上进行了实体分类任务, 数据集的统计信息如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rgcn5.jpg" style="zoom: 50%;" /><p>实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rgcn6.jpg" style="zoom: 50%;" /><p>在AIFB和AM上的效果都要高于其他模型, 而MUTAG和BGS是<strong>特定领域</strong>的数据集, 与其他两个数据集都不太一样, 导致了R - GCN的性能差距. </p><blockquote><p>作者提到, 对所有邻居都采用<strong>相同权重</strong>的归一化方式可能会有损性能, 一种潜在的解决方案是采用<strong>注意力机制</strong>, 通过学习分配给周围邻居不同的权重.</p><p>其实就是同年10月份提出的GAT, 而R - GCN发布于同年3月份.</p></blockquote><h3 id="Link-Prediction-Experiments"><a href="#Link-Prediction-Experiments" class="headerlink" title="Link Prediction Experiments"></a>Link Prediction Experiments</h3><p>作者主要在FB15k, WN18, FB15k - 237上做了链接预测的实验.</p><p>同样是针对关系特化的模型, 作者做出了在FB15k的验证集中不同度对MRR的影响曲线:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rgcn7.jpg" style="zoom: 50%;" /><p>作者认为R - GCN在度比较高, 即上下文信息比较多时处理很占优势, 而DistMult在度比较低时占优势.</p><blockquote><p>可能也只是作者的一个推测, DistMult在度比较低的时候并没有比R - GCN好特别多.</p></blockquote><p>观察到R - GCN和DistMult上的<strong>互补</strong>优势, 作者尝试将两种模型融合, 仍然采用DistMult的打分函数, 但Embedding分别来自于已经训练好的不同模型:<br>$$<br>f(s, r, t)_{\mathrm{R}-\mathrm{GCN}+}=\alpha f(s, r, t)_{\mathrm{R}-\mathrm{GCN}}+(1-\alpha) f(s, r, t)_{\text {DistMult }}<br>$$<br>$\alpha$ 为选择模型得分的权重. 在FB15k中设置为$\alpha=0.4$, 即来自DistMult的打分要多一些, R - GCN少一些, 因为作者不希望改进后的模型性能显著高于纯R - GCN.</p><p>在FB15k, WN18上结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rgcn8.jpg" style="zoom: 50%;" /><p>在关系比较少的WN18上, R - GCN并不占优势, 但在FB15k上的表现十分不错. DistMult作为关系特化的模型也在FB15k上显示出一些优势, 但仍不及能够对对称, 反对称, 逆关系同时建模的ComplEx.</p><p>在FB15k - 237上结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rgcn9.jpg" style="zoom: 50%;" /><p>R - GCN比其他方法效果要好特别多, 因为把DistMult也塞进去了, 所以比DistMult也显著的好.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>R - GCN是一篇比较早的GNN论文, 已经作为一种KGE中常用的图方法Baseline出现, 也正是R - GCN使得大家对图方法在KG上的应用提起了注意.</p><p>R - GCN在GCN的基础上对<strong>多关系</strong>进行特化, 并针对图神经网络的<strong>过拟合</strong>问题提出了两种可替代的解决方法, <strong>基函数分解</strong>和<strong>对角块分解</strong>. 也提出了图神经网络在处理Entity Classification和Link Prediction问题上的框架, 其实主要还是把R - GCN的输出作为一种<strong>Node Embedding</strong>的方法来使用.</p><p>作者在文中还提出了两种可以优化的地方:</p><ol><li>Decoder还可以替换为任意的KGE模型, 例如能对更多关系建模的ComplEx.</li><li>对邻居节点的权重分配可以通过<strong>Attention</strong>学习得来.</li></ol>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> GNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RotatE: Relational Rotation in Complex Vector Space</title>
      <link href="/posts/60792.html"/>
      <url>/posts/60792.html</url>
      
        <content type="html"><![CDATA[<h1 id="RotatE-Knowledge-Grpah-Embedding-by-Relational-Rotation-Complex-Space"><a href="#RotatE-Knowledge-Grpah-Embedding-by-Relational-Rotation-Complex-Space" class="headerlink" title="RotatE: Knowledge Grpah Embedding by Relational Rotation Complex Space"></a>RotatE: Knowledge Grpah Embedding by Relational Rotation Complex Space</h1><p>本文是论文<a href="https://openreview.net/forum?id=HkgEQnRqYQ" target="_blank" rel="noopener">ROTATE: KNOWLEDGE GRAPH EMBEDDING BY RELATIONAL ROTATION IN COMPLEX SPACE</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者的出发点如下:</p><ol><li>在许多任务中的成功推断都严重依赖于<strong>关系</strong>模式.</li><li>现有的Knowledge Embedding Model都不能对<strong>所有关系</strong>同时建模.</li></ol><p>基于这两个出发点, 作者希望能构建出一种对<strong>所有关系</strong>建模的模型.</p><h2 id="RotatE"><a href="#RotatE" class="headerlink" title="RotatE"></a>RotatE</h2><h3 id="Modeling-and-Inferring-Relation-Patterns"><a href="#Modeling-and-Inferring-Relation-Patterns" class="headerlink" title="Modeling and Inferring Relation Patterns"></a>Modeling and Inferring Relation Patterns</h3><p>假设有三实体$x, y, z$, 存在关系$r$, 作者定义了如下三种关系的形式:</p><ol><li><p><strong>对称</strong>关系和<strong>反对称</strong>关系:</p><p>对称关系:  如果$x$ 能通过变换$r$ 得出$y$, 那么$y$ 必然能通过同样的$r$ 得出$x$.</p><p>反对称关系: 若$x$ 能通过$r$ 找到$y$, 那么$y$ 必然不能通过同样的$r$ 找到$x$, 或者说$y$ 对$x$ 必然不存在关系$r$. </p><p>即:</p></li></ol><p>$$<br>\begin{aligned}<br>r(x, y) &amp;\Rightarrow r(y, x) \\<br>r(x, y) &amp;\Rightarrow \neg r(y, x)<br>\end{aligned}<br>$$</p><ol start="2"><li><p><strong>互逆</strong>关系: </p><p>如果$r_1$ 和$r_2$ 互为逆关系, $x$ 必然能通过$r_1$ 得到$y$, 同时$y$ 也能通过$r_1$ 的逆变换$r_2$ 得到$x$, 即:</p></li></ol><p>$$<br>r_{2}(x, y) \Rightarrow r_{1}(y, x)<br>$$</p><ol start="3"><li><p><strong>组合</strong>关系:</p><p>如果$r_3$ 是由变换$r_1$ 和变换$r_2$ 组成的, 那么它们之间存在<strong>递进关系</strong>, 即:</p></li></ol><p>$$<br>r_{2}(\mathrm{x}, \mathrm{y}) \wedge \mathrm{r}_{3}(\mathrm{y}, \mathrm{z}) \Rightarrow \mathrm{r}_{1}(\mathrm{x}, \mathrm{z})<br>$$</p><blockquote><p>我这里将所有的”关系”都解释为”<strong>变换</strong>“, 是因为KGE模型的本质是在构造一种”<strong>变换方式</strong>“, 使得头实体尽可能的能够通过特定的<strong>关系变换</strong>后找到<strong>唯一</strong>对应尾实体.</p></blockquote><p>而现有的方法往往不能对上述常见的三种关系模式进行建模:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rotate1.jpg" style="zoom: 50%;" /><p>除了ConvE是神经网络型的KGE模型无法分析, 其他模型都没有同时对所有关系建模:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rotate2.jpg" style="zoom: 50%;" /><p>而作者接下来提出的RotatE却能处理列出的所有情况.</p><h3 id="Modeling-Relations-as-Rotations-in-Complex-Vector-Space"><a href="#Modeling-Relations-as-Rotations-in-Complex-Vector-Space" class="headerlink" title="Modeling Relations as Rotations in Complex Vector Space"></a>Modeling Relations as Rotations in Complex Vector Space</h3><h4 id="Euler’s-Inspired"><a href="#Euler’s-Inspired" class="headerlink" title="Euler’s Inspired"></a>Euler’s Inspired</h4><p>RotatE的灵感来源于<strong>欧拉公式</strong>:<br>$$<br>e^{i \theta}=\cos \theta+i \sin \theta<br>$$</p><p>在复数空间内, 虚数乘法的运算意义为<strong>旋转</strong>(Rotation), 这一特性能使RotatE很好地对作者所定义的关系进行建模, 记复数空间内的三点为$x, y, z$, 旋转角度为$\theta$:</p><ol><li>对称关系: 将$x$ 旋转$\theta=\pi$ 得到$y$, 在$y$ 处再次旋转$\theta=\pi$ 回到$x$.</li><li>互逆关系: 将$x$ 旋转$\theta_1$ 得到$y$, 在$y$ 处旋转$\theta_2=-\theta_1$, 即反方向旋转$\theta_1$, 可以回到$x$.</li><li>组合关系: 将$x$ 先旋转$\theta_1$, 记为$y$, 再旋转$\theta_2$ 记为$z$. 该过程等价于从$x$ 处旋转$\theta_3=\theta_1+\theta_2$, 直接得到$z$.</li></ol><p>因此, 我们只需要在复平面上做头实体$h$ 和关系$r$ 的<strong>元素按位乘</strong>就可以了:</p><p>$$<br>\mathbf{t}=\mathbf{h} \circ \mathbf{r}<br>$$</p><p>其中$\circ$ 代表Hadmard Product(元素按位乘). 因为关系变换被定义为旋转, 所以需要限制$\mathbf{r}$ 中的每个维度的$|r_i|=1$, 这样就可以让角度$\theta \in [-\pi, \pi]$. </p><p>距离函数被作者定义为:<br>$$<br>d_{r}(\mathbf{h}, \mathbf{t})=\lVert\mathbf{h} \circ \mathbf{r}-\mathbf{t}\rVert<br>$$</p><p>即头实体通过旋转后与三元组真实尾实体的距离越近越好.</p><h4 id="Connection-to-TransE"><a href="#Connection-to-TransE" class="headerlink" title="Connection to TransE"></a>Connection to TransE</h4><p>TransE仅在<strong>一维</strong>数轴上做关系变换, 而将关系变换定义为旋转的RotatE可以在<strong>二维</strong>复平面上做关系变换:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rotate3.jpg" style="zoom: 50%;" /><p>一维数轴的限制导致TransE无法处理对称关系, 除非将关系变为0向量, 但这会引入更多的问题.</p><h3 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h3><h4 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h4><p>RotatE仍然采用类似TransE的<strong>Hinge Loss</strong>, 并加上<strong>负采样</strong>:<br>$$<br>L=-\log \sigma\left(\gamma-d_{r}(\mathbf{h}, \mathbf{t})\right)-\sum_{i=1}^{n} \frac{1}{k} \log \sigma\left(d_{r}\left(\mathbf{h}_{i}^{\prime}, \mathbf{t}_{i}^{\prime}\right)-\gamma\right)<br>$$</p><h4 id="Self-Adversarial-Negative-Sampling"><a href="#Self-Adversarial-Negative-Sampling" class="headerlink" title="Self - Adversarial Negative Sampling"></a>Self - Adversarial Negative Sampling</h4><p>通常情况下, 模型认为打分函数越高的三元组越真实, 而模型已经知道打分比较低的三元组不真实了, 就没有必要<strong>过多</strong>的注意这类三元组. 而对于那些模型认为是正确, 实际是错误的三元组, 需要被模型提起注意. </p><p>所以作者按照如下分布采样:<br>$$<br>p\left(h_{j}^{\prime}, r, t_{j}^{\prime} \mid\left\{\left(h_{i}, r_{i}, t_{i}\right)\right\}\right)=\frac{\exp \alpha f_{r}\left(\mathbf{h}_{j}^{\prime}, \mathbf{t}_{j}^{\prime}\right)}{\sum_{i} \exp \alpha f_{r}\left(\mathbf{h}_{i}^{\prime}, \mathbf{t}_{i}^{\prime}\right)}<br>$$<br>其中$f_r$ 为KGE模型的打分函数, $\alpha$ 为采样率. 得分比较高的三元组更容易被采样到, 因为它们更容易把模型迷惑, 得分比较低的三元组将被少量采样到, 因为模型已经有一定能力判断它们是否正确.</p><p>因为每次负样本采样都是昂贵的, 不单是采样可以使用计算得出的概率, 它们还可以在计算Loss时复用, 将作为负样本的<strong>权重</strong>:<br>$$<br>L=-\log \sigma\left(\gamma-d_{r}(\mathbf{h}, \mathbf{t})\right)-\sum_{i=1}^{n} p\left(h_{i}^{\prime}, r, t_{i}^{\prime}\right) \log \sigma\left(d_{r}\left(\mathbf{h}_{i}^{\prime}, \mathbf{t}_{i}^{\prime}\right)-\gamma\right)<br>$$</p><p>打分本身较低的三元组所对应的负采样权重较低, 打分较高的三元组权重较高. 即<strong>越容易迷惑模型的三元组权重越高</strong>.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的超参设置请参照原论文.</p><p>除了RotatE, 作者还提出了其变体pRotatE, 它的实体嵌入的<strong>模长</strong>是被<strong>限制</strong>的, 即$|h_i| = |t_i| = C$, 它的距离函数被相应的改写为$2C \lVert\sin \frac{\theta_h + \theta_r - \theta_r}{2}\rVert$.</p><p>理论上它与RotatE一样, 也能同时处理多种关系. 但因缺少了模长信息, 所以它处理的效果应该会稍差一些. 反过来看, pRotatE也能说明旋转建模所带来的强大能力.</p><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><p>作者在FB15k, WN18上的实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rotate4.jpg" style="zoom: 33%;" /><p>RotatE表现出很强大的性能, 和变体差不多.</p><p>作者在FB15k - 237, WN18RR上的实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rotate5.jpg" style="zoom: 33%;" /><p>RotatE比ConvE的结果要好一些. 但对于MR来说, RotatE的结果要好得多, 说明ConvE对于某些关系学习到的特征是<strong>模糊</strong>的.</p><h3 id="Relation-Pattern-Experiments"><a href="#Relation-Pattern-Experiments" class="headerlink" title="Relation Pattern Experiments"></a>Relation Pattern Experiments</h3><p>在Countries上的结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rotate6.jpg" style="zoom: 33%;" /><p>在最困难的S3上, RotatE要显著好于其他方法.</p><p>WN18上的各类关系所对应的$r$ 值如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rotate7.jpg" style="zoom: 33%;" /><p><code>similar_to</code>是明显的对称关系,  <code>hypernym, hyponym</code>是一对明显的逆关系, 二者的组合刚好是什么也不做, <code>for1, for2, winner</code>三者的组合也被学习到了. RotatE很好的掌握了这些关系的变换方式.</p><h3 id="Negative-Sampling-Techniques"><a href="#Negative-Sampling-Techniques" class="headerlink" title="Negative Sampling Techniques"></a>Negative Sampling Techniques</h3><p>分别在3个基准数据集上, 三种负采样的效果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rotate8.jpg" style="zoom: 33%;" /><p>明显作者提出的自对抗负采样效果要好.</p><p>为了能更公平的对比TransE, ComplEx, RotatE, 作者将自对抗负采样同时用于其他两种方法, 结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rotate9.jpg" style="zoom: 33%;" /><p>RotatE仍然是最优秀的方法. 但TransE比RotatE在S3上稍好一些, 作者解释为Countries不包含TransE最不擅长的对称关系, 在这个数据集上显示出TransE的强大.</p><h3 id="Results-by-Relation-Category"><a href="#Results-by-Relation-Category" class="headerlink" title="Results by Relation Category"></a>Results by Relation Category</h3><p>作者在FB15k上按照多类别统计的实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rotate10.jpg" style="zoom: 33%;" /><p>其实从原理上来讲, RotatE应该不擅长处理一对多, 多对一, 多对多关系. 不过看起来结果还不错.</p><blockquote><p>个人观点: 可能是RotatE采用了比较大的Embedding Size, 在高维中或许点和点之间的距离比想象的要远, 即这些点在高维空间中可能是可分的.</p></blockquote><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>在联结主义被大肆吹鼓的今天看到一篇这样的文章实属不易. </p><p>RotatE将关系变换定义为<strong>旋转</strong>, 在<strong>复数空间</strong>建模, 建模的方式<strong>简洁而优雅</strong>. 同时提出了一种优化的自对抗<strong>负采样</strong>算法, 能让模型聚焦于更<strong>容易混淆</strong>的三元组. RotatE还是一种线性复杂度的算法, 所以易于扩展到大规模的KG中.</p><p>其实在后面Appendix还有在YAGO3 - 10上做的实验, 还有自对抗负采样的Ablation Study, 算是很扎实了.</p><p>虽然RotatE十分简洁, 但它仍然具有以下缺点:</p><ol><li>与TransE相同, 只能对<strong>一对一</strong>关系建模. 这个缺陷在实验中并没有被很好的体现, 我认为是较大的Embedding Size遮盖了这个缺陷.</li><li>旋转操作并不能区分关系变换的<strong>前后顺序</strong>. 例如”父亲的儿子”和”儿子的父亲”在旋转中是一样的, 因为$\theta_3 = \theta_1+\theta_2=\theta_2+\theta_1$.</li></ol>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SACN: End-to-end Structure-Aware Convolutional Networks for KBC</title>
      <link href="/posts/63236.html"/>
      <url>/posts/63236.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>GNN: 详见<a href="https://adaning.github.io/posts/31958.html">图神经网络入门</a></li><li>ConvE: 详见<a href="https://adaning.github.io/posts/42031.html">ConvE: Convolutional 2D Knowledge Graph Embeddings</a></li><li>ConvKB: 详见<a href="https://adaning.github.io/posts/52280.html">ConvKB: A Novel Embedding Model for KB Completion Based on CNN</a></li></ul></blockquote><h1 id="End-to-end-Structure-Aware-Convolutional-Networks-for-Knowledge-Base-Completion"><a href="#End-to-end-Structure-Aware-Convolutional-Networks-for-Knowledge-Base-Completion" class="headerlink" title="End-to-end Structure-Aware Convolutional Networks for Knowledge Base Completion"></a>End-to-end Structure-Aware Convolutional Networks for Knowledge Base Completion</h1><p>本文是论文<a href="http://arxiv.org/abs/1811.04441" target="_blank" rel="noopener">End-to-end Structure-Aware Convolutional Networks for Knowledge Base Completion</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者注意到ConvE中存在如下不足:</p><ol><li>ConvE中没有融入太多的<strong>结构信息</strong>(特指<strong>图结构信息和节点属性信息</strong>).</li><li>ConvE没有像TransE一样保留<strong>平移</strong>的特性, 即$e_s + e_r \approx e_o$.</li></ol><p>基于上述两点不足, 作者希望能够在ConvE架构下融入图中的结构信息, 并保留类似平移的特性. </p><p>作者观察到, <a href="https://adaning.github.io/posts/52280.html">ConvKB</a>中存在类似保留平移特性的方法, 它与ConvE有几点不同:</p><ol><li>ConvKB只使用了<strong>1D</strong>卷积, 而ConvE使用了<strong>2D</strong>卷积. </li><li>ConvKB使用了<strong>Stack</strong>, ConvE使用的是<strong>Reshape</strong>.</li><li><strong>损失函数</strong>不同.</li></ol><p>同时, ConvKB的作者也指出, 在特殊情况下ConvKB可以退化成TransE, 即能够保留平移特性. 受到ConvKB的启发, 作者提出了<strong>结构感知</strong>的Conv系列方法.</p><h2 id="SACN"><a href="#SACN" class="headerlink" title="SACN"></a>SACN</h2><p><strong>SACN</strong>(end-to-end <strong>S</strong>tructure - <strong>A</strong>ware <strong>C</strong>onvolutional <strong>N</strong>etwork)将融入结构信息的过程设计为<strong>Encoder - Decoder</strong>架构. 通过Encoder捕获图结构信息, 然后用Decoder从编码中解码出尾实体Embedding.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/sacn1.jpg" style="zoom:50%;" /><h3 id="Weighted-Graph-Convolutional-Layer"><a href="#Weighted-Graph-Convolutional-Layer" class="headerlink" title="Weighted Graph Convolutional Layer"></a>Weighted Graph Convolutional Layer</h3><p><strong>W</strong>eighted <strong>G</strong>raph <strong>C</strong>onvolutional <strong>N</strong>etwork(<strong>WGCN</strong>)是GCN的一种扩展类型, 它将多关系图看做是<strong>多个单关系的子图</strong>. 因此对于每种不同的关系, WGCN都能决定子图中的节点以多少权重被集成, 即给予图结构多种不同的关系以不同的权重. 在SACN中, 它充当Encoder, 用于<strong>提取图结构信息</strong>.</p><p>其隐态更新方程如下:<br>$$<br>h_{i}^{l+1}=\sigma\left(\sum_{j \in \mathbf{N}_{\mathbf{i}}} \alpha_{t}^{l} g\left(h_{i}^{l}, h_{j}^{l}\right)\right)<br>$$</p><p>其中, $\alpha_t$ 为<strong>关系特化</strong>的权重. $T$ 为关系的总数, $t \in [1, T]$. $g$ 为聚合方式.</p><p>如下图所示, 红色的中心节点周围有4个相邻节点, 但只有3种不同的关系, 它们以3种不同的权重被聚合.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/sacn8.jpg" style="zoom: 33%;" /><p>在WGCN中, 聚合方式采用了最简单的线性变换:<br>$$<br>g\left(h_{i}^{l}, h_{j}^{l}\right)=h_{j}^{l} W^{l}<br>$$</p><p>$W^l$ 为第$l$ 层的线性变换矩阵.</p><p>因为在考虑中心节点$i$ 的邻居节点$\mathbf{N_i}$ 时没有考虑节点自身到自身的<strong>闭环</strong>, 所以还是将闭环添加进来:<br>$$<br>h_{i}^{l+1}=\sigma\left(\sum_{j \in \mathbf{N}_{\mathbf{i}}} \alpha_{t}^{l} h_{j}^{l} W^{l}+h_{i}^{l} W^{l}\right)<br>$$</p><p>相当于给闭环分配了一种特殊的关系$\text{self-loop}$, 且权重$\alpha _{\text{self-loop}}=1$.</p><p>公式中的闭环是可以进行合并的:</p><p>$$<br>\begin{aligned}<br>h_{i}^{l+1}&amp;=\sigma\left(\sum_{j \in \mathbf{N}_{\mathbf{i}}} \alpha_{t}^{l} h_{j}^{l} W^{l}+h_{i}^{l} W^{l}\right) \\<br>&amp;=\sigma\left[\left(\sum_{i \in \mathbf{N}_i} \alpha_t^l h_j^l + h_i^l\right) W^l \right]<br>\end{aligned}<br>$$</p><p>在计算时, 将每种关系的邻接矩阵$A_t$ 分别乘以它们对应的权重$\alpha_t$, 视为第$l$ 层整张图的邻接矩阵$A^l$, 能一次性更新<strong>所有</strong>不同关系的节点隐态, 整个过程写成矩阵形式如下:<br>$$<br>\begin{aligned}<br>H^{l+1} &amp;= \sigma \left[ \left( \sum_{t=1}^T \left(\alpha_t^l A_t \right) + I \right) H^l W^l  \right] \\\<br>&amp;=\sigma\left(A^{l} H^{l} W^{l}\right)<br>\end{aligned}<br>$$<br>计算流程如下图所示:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/sacn2.jpg" style="zoom:50%;" /><p>最左侧的一堆矩阵表示<strong>闭环</strong>的单位矩阵$I$ 和代表<strong>不同关系</strong>的邻接矩阵$A_t$, 第二个矩阵代表它们在不同关系$t$ 下与对应权重$\alpha_t$ 的加权和$A^l$. 第三个矩阵为$H^l$, 第四个为$W^l$.</p><blockquote><p>但WGCN没有像经典GCN一样将<strong>度</strong>的信息集成进来.</p></blockquote><h3 id="Node-Attributes"><a href="#Node-Attributes" class="headerlink" title="Node Attributes"></a>Node Attributes</h3><p>作者提到, 在当前KG中有一部分<strong>属性</strong>三元组, 即<code>(entity, relation, attribute)</code>, 例如<code>(Tom, people.person.gender, male)</code>. </p><p>这种<strong>属性三元组</strong>建模会带来两种潜在问题:</p><ol><li>属性与一般节点不同, 它<strong>不能再延伸</strong>出其他的节点, 会导致属性特征非常<strong>稀疏</strong>.</li><li>由于其稀疏性, 属性特征中的0值可能会产生歧义, 可能是节点没有特殊的属性, 也可能是节点丢失了属性. 会影响KGE准确率.</li></ol><p>如果需要减少过多的属性节点, 作者提出一种方法, 每一种属性将作为一个单独的<strong>属性节点</strong>. </p><blockquote><p>这里有很大疑问, 作者的<a href="https://github.com/JD-AI-Research-Silicon-Valley/SACN/issues/7" target="_blank" rel="noopener">回复</a>也比较模糊. 建议自行阅读原论文. 关于这块的吐槽在后面构建FB15k - 237 - Attr时我会详细说.</p></blockquote><p>WGCN同时使用了图结构信息和属性信息, 这也就是”<strong>Structure Awared</strong>“的来源.</p><h3 id="Conv-TransE"><a href="#Conv-TransE" class="headerlink" title="Conv - TransE"></a>Conv - TransE</h3><p>Conv - TransE在SACN中扮演Decoder的角色. 它能像TransE一样保留<strong>平移</strong>特性. 它仍然沿用ConvE的架构, 但它的核心点在于: <strong>不进行Reshape</strong>. </p><p>作者认为正是ConvE中的Reshape操作将头实体嵌入$e_o$ 和关系嵌入$e_r$ 转化为2D向量才使得平移特性不能保存. ConvKB中的<strong>Stack</strong>操作却没有破坏原本的$e_o, e_r$形状.</p><p>因此, 从Encoder(WGCN) 中得到实体嵌入$e_o$后, 与关系嵌入$e_r$ 一起<strong>Stack</strong>起来, 不经过Reshape, 直接用<strong>宽度为2的2D卷积</strong>抽取得到Feature map. </p><blockquote><p>WGCN只训练了Entity Embedding, Relation Embedding此时还是刚初始化的状态.</p></blockquote><p>后面的流程和<strong>ConvE</strong>一样, 将Feature map<strong>打平</strong>, 再用<strong>投影层</strong>投回Embedding的维度, 和整个Embedding矩阵相乘得到Logits. 最后用Sigmoid得到概率, <strong>BCE</strong>计算损失:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/sacn9.jpg" style="zoom: 25%;" /><blockquote><p>ConvKB采用的是宽度为1的2D卷积, 所以作者将ConvKB视为Conv - TransE的一种特殊情况.</p></blockquote><p>卷积运算的数学描述如下:</p><p>$$<br>\begin{aligned}<br>m_{c}\left(e_{s}, e_{r}, n\right)&amp;= \sum_{\tau=0}^{K-1} \omega_{c}(\tau, 0) \hat{e}_{s}(n+\tau)+\omega_{c}(\tau, 1) \hat{e}_{r}(n+\tau) \\<br>M_c(e_s, e_r) &amp;= \left[ m_c(e_s, e_r, 0), \dots, m_c(e_s, e_r, F^L - 1)\right]<br>\end{aligned}<br>$$</p><p>其中$K$ 为卷积核宽度, $\omega_{c}$ 为卷积核权重.</p><blockquote><p>卷积核能够分别对$e_s, e_r$ 加权, 并将二者相加, 作者认为这样保留了基于平移的性质.</p><p>我个人认为后面的投影层可能会破坏这种性质.</p></blockquote><p>打分函数如下:</p><p>$$<br>\psi\left(e_{s}, e_{o}\right)=f\left(\operatorname{vec}\left(\mathbf{M}\left(e_{s}, e_{r}\right)\right) W\right) e_{o}<br>$$</p><p>其中$\mathbf{M}$ 为在WGCN上编码后的卷积操作, $\operatorname{vec}(\cdot)$ 为Flatten操作. $W$ 为投影层的投影矩阵, $f$ 为非线性激活函数. $e_o$ 为尾实体的Embedding.</p><p>与ConvE相同, 得到得分后再用对数几率函数得到概率:<br>$$<br>p\left(e_{s}, e_{r}, e_{o}\right)=\sigma\left(\psi\left(e_{s}, e_{o}\right)\right)<br>$$</p><p>得到概率后用<strong>BCE</strong>做损失函数优化.</p><p>SACN的打分函数整体形式与ConvE相同:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/sacn3.jpg" style="zoom:50%;" /><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的实验参数请参照原论文.</p><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>除了两个Benchmark数据集FB15k - 237和WN18RR, 作者还利用实体的<strong>属性特征</strong>构建了一个新的数据集FB15k - 237 - Attr. 它是作者从FB24k抽取了FB15k - 237中的所有实体属性所构建的数据集. 它具有14541个节点, 203种属性, 484种关系. 共计78334个属性三元组, 这将近8w个三元组被作者全部并入训练集中.</p><blockquote><p>花了很久看这里, 也不知道作者到底是如何具体构建的.</p><p>在实际的FB15k - 237 - Attr中, 作者直接将所有FB24k中同实体的属性三元组拿了过来. 但有些相同含义的三元组在FB15k - 237中是已经存在的, 这些已经存在的三元组没有被删除. 这样一来, 这些属性不单以”关系”的身份存在, 也以”属性”的身份存在, 而且在属性三元组中, 有些属性节点得到了合并, 但有些又没有, 感觉有点奇怪.</p><p>单纯从后续的实验结果来看, 使用该数据集是会涨点的.</p></blockquote><p>作者将SACN在FB15k - 237和WN18RR上做了Link Prediction:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/sacn4.jpg" style="zoom: 33%;" /><p>Conv - TransE相比较于ConvE有一定提升, 在引入WGCN后的SACN又有一定提升, 如果让SACN使用属性信息还会有一点点提升.</p><h3 id="Convergence-Analysis"><a href="#Convergence-Analysis" class="headerlink" title="Convergence Analysis"></a>Convergence Analysis</h3><p>作者分析了SACN + Attr(绿), SACN(红), Conv - TransE(黄)在Hits@1和MRR上的收敛性:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/sacn5.jpg" style="zoom:50%;" /><p>Conv - TransE在Epoch较少时性能是比SACN要好的, 随着轮数的增加, SACN性能反超了Conv - TransE. </p><p>而加入属性信息后, SACN性能得到了完全的提升.</p><h3 id="Kernel-Size-Analysis"><a href="#Kernel-Size-Analysis" class="headerlink" title="Kernel Size Analysis"></a>Kernel Size Analysis</h3><p>作者分析了不同卷积核大小对性能的影响:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/sacn6.jpg" style="zoom: 33%;" /><p>增大卷积核的大小会获得一点性能上的提升, 对于不同的数据集有不同的最优超参数设置.</p><h3 id="Node-Indegree-Analysis"><a href="#Node-Indegree-Analysis" class="headerlink" title="Node Indegree Analysis"></a>Node Indegree Analysis</h3><p>作者分析了节点入度对结果的影响:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/sacn7.jpg" style="zoom: 33%;" /><p>在度不够高时, SACN要略好于Conv - TransE, 在度比较大时, Conv - TransE会好于SACN. 作者解释为高入度导致SACN学到的邻居特征被平滑了.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>SACN是一种基于<strong>Encoder - Decoder</strong>架构的卷积KGE方法, 保留了TransE基于<strong>平移</strong>的特性, 并集成了<strong>图结构</strong>和<strong>属性</strong>信息. WGCN也是一种GCN的扩展形式, 可以单独针对多关系作为基本的GNN组件存在.</p><p>WGCN抽取Entity Embedding的同时没有对Relation Embedding做更新, 或许可以利用图中的边对Relation Embedding增益, 例如尝试用Entity Embedding和Relation Embedding的联合训练.</p><p>关于属性节点的部分没太搞懂, 感觉比较模糊.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> GNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch实现: BERT</title>
      <link href="/posts/52648.html"/>
      <url>/posts/52648.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>Transformer: 详见<a href="https://adaning.github.io/posts/6744.html">Transformer精讲</a></li><li>BERT: 详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a></li><li><a href="https://adaning.github.io/posts/63679.html">Pytorch实现: Transformer</a></li></ul></blockquote><p>本文是BERT的Pytorch版本实现. 实现并没有完全参照BERT原论文中的设置, 有些细枝末节的地方可能没有考虑进去, 每个人实现的方法可能也不同, 可以不必过于纠结这些. BERT的实现比Transformer更简单, 因为不用考虑Decoder.</p><p>本文参考如下文章:</p><ul><li><a href="https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html#BertModel" target="_blank" rel="noopener">Hugging Face的BERT实现</a></li><li><a href="https://wmathor.com/index.php/archives/1457/" target="_blank" rel="noopener">BERT 的 PyTorch 实现</a></li></ul><p>本文的代码已经放到了Colab上, 打开设置GPU就可以复现(需要科学上网).</p><p><a href="https://colab.research.google.com/drive/13PZiE-UQlGy-gSF_KZ-L9wq9RYsBbXDj?usp=sharing" target="_blank" rel="noopener"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="右键我在COLAB中打开!"></a></p><p>如果你不能科学上网, 应该看不到<code>Open in Colab</code>的图标.</p><h2 id="Preparing"><a href="#Preparing" class="headerlink" title="Preparing"></a>Preparing</h2><p>先导包:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> random<span class="token keyword">import</span> re<span class="token keyword">from</span> math <span class="token keyword">import</span> sqrt <span class="token keyword">as</span> msqrt<span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">import</span> Adadelta<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader<span class="token punctuation">,</span> Dataset<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>定义与BERT相关的参数:</p><pre class="line-numbers language-python"><code class="language-python">max_len <span class="token operator">=</span> <span class="token number">30</span>max_vocab <span class="token operator">=</span> <span class="token number">50</span>max_pred <span class="token operator">=</span> <span class="token number">5</span>d_k <span class="token operator">=</span> d_v <span class="token operator">=</span> <span class="token number">64</span>d_model <span class="token operator">=</span> <span class="token number">768</span>  <span class="token comment" spellcheck="true"># n_heads * d_k</span>d_ff <span class="token operator">=</span> d_model <span class="token operator">*</span> <span class="token number">4</span>n_heads <span class="token operator">=</span> <span class="token number">12</span>n_layers <span class="token operator">=</span> <span class="token number">6</span>n_segs <span class="token operator">=</span> <span class="token number">2</span>p_dropout <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token number">1</span><span class="token comment" spellcheck="true"># BERT propability defined</span>p_mask <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token number">8</span>p_replace <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token number">1</span>p_do_nothing <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">-</span> p_mask <span class="token operator">-</span> p_replacedevice <span class="token operator">=</span> <span class="token string">"cuda"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><p>max_len: 输入序列的最大长度.</p></li><li><p>max_vocab: 字典的最大大小.</p></li><li><p>max_pred: Mask时最大的Mask数量.</p></li><li><p>d_k, d_v: 自注意力中K和V的维度, Q的维度直接用K的维度代替, 因为这二者必须始终相等.</p></li><li><p>d_model: Embedding的大小.</p></li><li><p>d_ff: 前馈神经网络的隐藏层大小, 一般是d_model的四倍.</p></li><li><p>n_heads: 多头注意力的头数.</p></li><li><p>n_layers: Encoder的堆叠层数.</p></li><li><p>n_segs: 输入BERT的句子段数. 用于制作Segment Embedding.</p></li><li><p>p_dropout: BERT中所有dropout的概率.</p></li><li><p>p_mask, p_replace, p_do_nothing:</p><ul><li><p><strong>80%</strong>的概率将被选中的单词替换为<code>[MASK]</code>.</p></li><li><p><strong>10%</strong>的概率将被选中的单词替换为<strong>随机词</strong>.</p></li><li><p><strong>10%</strong>的概率对被选中的单词<strong>不进行替换</strong>.</p></li></ul></li></ul><h2 id="Mask-and-GELU"><a href="#Mask-and-GELU" class="headerlink" title="Mask and GELU"></a>Mask and GELU</h2><p>在BERT中没有Decoder, 所以我们的Mask实际上只为<strong>Padding</strong>服务. </p><h3 id="Mask"><a href="#Mask" class="headerlink" title="Mask"></a>Mask</h3><p>因为预期的Token输入序列大小为<code>[batch, seq_len]</code>, 如果token中的索引与pad索引相同, 那么则</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_pad_mask</span><span class="token punctuation">(</span>tokens<span class="token punctuation">,</span> pad_idx<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''    suppose index of [PAD] is zero in word2idx    tokens: [batch, seq_len]    '''</span>    batch<span class="token punctuation">,</span> seq_len <span class="token operator">=</span> tokens<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>    pad_mask <span class="token operator">=</span> tokens<span class="token punctuation">.</span>data<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>pad_idx<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    pad_mask <span class="token operator">=</span> pad_mask<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> seq_len<span class="token punctuation">)</span>    <span class="token keyword">return</span> pad_mask<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="GELU"><a href="#GELU" class="headerlink" title="GELU"></a>GELU</h3><p>在BERT中采用GELU作为激活函数, 它与ReLU相比具有一些概率上的性质:<br>$$<br>\displaylines{<br>\operatorname{GELU}(x)=x P(X \leq x)= x \Phi(x)=x \cdot \frac{1}{2}[1+\operatorname{erf}(x / \sqrt{2})] \\<br> or \\<br>0.5 x\left(1+\tanh \left[\sqrt{2 / \pi}\left( x+ 0.044715 x^{3}\right)\right]\right)<br>}<br>$$<br>第二行的是GELU的近似表达式. 它实现起来非常简单:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">gelu</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''    Two way to implements GELU:    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))    or    0.5 * x * (1. + torch.erf(torch.sqrt(x, 2)))     '''</span>    <span class="token keyword">return</span> <span class="token punctuation">.</span><span class="token number">5</span> <span class="token operator">*</span> x <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">.</span> <span class="token operator">+</span> torch<span class="token punctuation">.</span>erf<span class="token punctuation">(</span>x <span class="token operator">/</span> msqrt<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>两种方式均可, 我这里采用第一种.</p><h2 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h2><p>BERT中含有三种编码, Word Embedding, Position Embedding, Segment Embedding:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bert2.jpg" style="zoom: 50%;" /><p>其中Position Embedding不像Transformer中是用正余弦编码计算得到, 而是通过学习获得.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Embeddings</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Embeddings<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>seg_emb <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>n_segs<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>word_emb <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>max_vocab<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>pos_emb <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>max_len<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p_dropout<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> seg<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">'''        x: [batch, seq_len]        '''</span>        word_enc <span class="token operator">=</span> self<span class="token punctuation">.</span>word_emb<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># positional embedding</span>        pos <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>long<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>        pos <span class="token operator">=</span> pos<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        pos_enc <span class="token operator">=</span> self<span class="token punctuation">.</span>pos_emb<span class="token punctuation">(</span>pos<span class="token punctuation">)</span>        seg_enc <span class="token operator">=</span> self<span class="token punctuation">.</span>seg_emb<span class="token punctuation">(</span>seg<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>word_enc <span class="token operator">+</span> pos_enc <span class="token operator">+</span> seg_enc<span class="token punctuation">)</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># return: [batch, seq_len, d_model]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里的LayerNorm有些版本的实现加了, 有些没有加, 看个人爱好吧.</p><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>这里的点积缩放注意力和多头注意力完全和Transformer一致, 不再细说, 直接照搬过来就行.</p><h3 id="Scaled-DotProduct-Attention"><a href="#Scaled-DotProduct-Attention" class="headerlink" title="Scaled DotProduct Attention"></a>Scaled DotProduct Attention</h3><p>$$<br>\operatorname{Attention}(Q, K, V) = \operatorname{softmax}(\frac{QK^T}{\sqrt{d_k}})V<br>$$</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">ScaledDotProductAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>ScaledDotProductAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> Q<span class="token punctuation">,</span> K<span class="token punctuation">,</span> V<span class="token punctuation">,</span> attn_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>        scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>Q<span class="token punctuation">,</span> K<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">/</span> msqrt<span class="token punctuation">(</span>d_k<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># scores: [batch, n_heads, seq_len, seq_len]</span>        scores<span class="token punctuation">.</span>masked_fill_<span class="token punctuation">(</span>attn_mask<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1e9</span><span class="token punctuation">)</span>        attn <span class="token operator">=</span> nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span>scores<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># context: [batch, n_heads, seq_len, d_v]</span>        context <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>attn<span class="token punctuation">,</span> V<span class="token punctuation">)</span>        <span class="token keyword">return</span> context<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi - Head Attention"></a>Multi - Head Attention</h3><p>$$<br>\begin{aligned}<br>\operatorname{MultiHead}(Q, K, V) &amp;= \operatorname{Concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_h)W^O \\<br>\text{where } \text{head}_i &amp;= \operatorname{Attention}(QW^Q_i, KW^K_i, VW^V_i)<br>\end{aligned}<br>$$</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>MultiHeadAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>W_Q <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_k <span class="token operator">*</span> n_heads<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>W_K <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_k <span class="token operator">*</span> n_heads<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>W_V <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_v <span class="token operator">*</span> n_heads<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>n_heads <span class="token operator">*</span> d_v<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> Q<span class="token punctuation">,</span> K<span class="token punctuation">,</span> V<span class="token punctuation">,</span> attn_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">'''        Q, K, V: [batch, seq_len, d_model]        attn_mask: [batch, seq_len, seq_len]        '''</span>        batch <span class="token operator">=</span> Q<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        <span class="token triple-quoted-string string">'''        split Q, K, V to per head formula: [batch, seq_len, n_heads, d_k]        Convenient for matrix multiply opearation later        q, k, v: [batch, n_heads, seq_len, d_k / d_v]        '''</span>        per_Q <span class="token operator">=</span> self<span class="token punctuation">.</span>W_Q<span class="token punctuation">(</span>Q<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> d_k<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        per_K <span class="token operator">=</span> self<span class="token punctuation">.</span>W_K<span class="token punctuation">(</span>K<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> d_k<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        per_V <span class="token operator">=</span> self<span class="token punctuation">.</span>W_V<span class="token punctuation">(</span>V<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> d_v<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        attn_mask <span class="token operator">=</span> attn_mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># context: [batch, n_heads, seq_len, d_v]</span>        context <span class="token operator">=</span> ScaledDotProductAttention<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>per_Q<span class="token punctuation">,</span> per_K<span class="token punctuation">,</span> per_V<span class="token punctuation">,</span> attn_mask<span class="token punctuation">)</span>        context <span class="token operator">=</span> context<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>            batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads <span class="token operator">*</span> d_v<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># output: [batch, seq_len, d_model]</span>        output <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>context<span class="token punctuation">)</span>        <span class="token keyword">return</span> output<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Feed-Forward-Neural-Network"><a href="#Feed-Forward-Neural-Network" class="headerlink" title="Feed Forward Neural Network"></a>Feed Forward Neural Network</h2><p>BERT中的FFN实现将激活函数换为了GELU:<br>$$<br>\operatorname{FFN}(x)=\operatorname{GELU}(xW_1+b_1)W_2 + b_2<br>$$</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">FeedForwardNetwork</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>FeedForwardNetwork<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_ff<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_ff<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p_dropout<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>gelu <span class="token operator">=</span> gelu    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>gelu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我把残差部分移到了EncoderLayer的设计中, 见下节.</p><h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p>我对Encoder的实现进行了调整. 在Encoder中控制两个Sub Layer的Layer Norm和Residual connection. 在论文<a href="https://openreview.net/pdf?id=B1x8anVFPr" target="_blank" rel="noopener">ON LAYER NORMALIZATION IN THE TRANSFORMER ARCHITECTURE</a>中曾经提到, Transformer中Layer Norm的位置加的有问题, 在Sub Layer后加Layer Norm被称为Post Norm, 它是不规范的. Layer Norm如果调整到Sub Layer前会对训练有很大帮助, 称为<strong>Pre Norm</strong>:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/prenorm.jpg" style="zoom: 50%;" /><p>我这里采用了Pre Norm版本的实现:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">EncoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>EncoderLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>norm1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>norm2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>enc_attn <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>ffn <span class="token operator">=</span> FeedForwardNetwork<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> pad_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">'''        pre-norm        see more detail in https://openreview.net/pdf?id=B1x8anVFPr        x: [batch, seq_len, d_model]        '''</span>        residual <span class="token operator">=</span> x        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>enc_attn<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> pad_mask<span class="token punctuation">)</span> <span class="token operator">+</span> residual        residual <span class="token operator">=</span> x        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>ffn<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x <span class="token operator">+</span> residual<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Pooler"><a href="#Pooler" class="headerlink" title="Pooler"></a>Pooler</h2><p>Pooler是<a href="https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html#BertModel" target="_blank" rel="noopener">Hugging Face实现BERT</a>时加上的额外组件. NSP任务需要提取<code>[CLS]</code>处的特征, Hugging Face的做法是将<code>[CLS]</code>处的输出接上一个FC, 并用tanh激活, 最后再接上二分类输出层. 他们将这一过程称为”<strong>Pool</strong>“.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Pooler</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Pooler<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>tanh <span class="token operator">=</span> nn<span class="token punctuation">.</span>Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">'''        x: [batch, d_model] (first place output)        '''</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>因为额外添加了一个FC层, 所以能增强表达能力, 同样提升了训练难度.</p><h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p>现在大框架中的Embedding, EncoderLayer, Pooler已经定义好了, 只需要额外定义输出时需要的其他组件. 在NSP任务输出时需要额外定义一个二分类输出层<code>next_cls</code>, 还有MLM任务输出所需的<code>word_classifier</code>, 以及前向传递<code>forward</code>.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">BERT</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>BERT<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> Embeddings<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>encoders <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>            EncoderLayer<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_layers<span class="token punctuation">)</span>        <span class="token punctuation">]</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>pooler <span class="token operator">=</span> Pooler<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>next_cls <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>gelu <span class="token operator">=</span> gelu        <span class="token comment" spellcheck="true"># Sharing weight between some fully connect layer</span>        shared_weight <span class="token operator">=</span> self<span class="token punctuation">.</span>pooler<span class="token punctuation">.</span>fc<span class="token punctuation">.</span>weight        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc<span class="token punctuation">.</span>weight <span class="token operator">=</span> shared_weight        <span class="token comment" spellcheck="true"># Sharing weight between word embedding and classifier</span>        shared_weight <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>word_emb<span class="token punctuation">.</span>weight        self<span class="token punctuation">.</span>word_classifier <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> max_vocab<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>word_classifier<span class="token punctuation">.</span>weight <span class="token operator">=</span> shared_weight    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tokens<span class="token punctuation">,</span> segments<span class="token punctuation">,</span> masked_pos<span class="token punctuation">)</span><span class="token punctuation">:</span>        output <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>tokens<span class="token punctuation">,</span> segments<span class="token punctuation">)</span>        enc_self_pad_mask <span class="token operator">=</span> get_pad_mask<span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>encoders<span class="token punctuation">:</span>            output <span class="token operator">=</span> layer<span class="token punctuation">(</span>output<span class="token punctuation">,</span> enc_self_pad_mask<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># output: [batch, max_len, d_model]</span>        <span class="token comment" spellcheck="true"># NSP Task</span>        hidden_pool <span class="token operator">=</span> self<span class="token punctuation">.</span>pooler<span class="token punctuation">(</span>output<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        logits_cls <span class="token operator">=</span> self<span class="token punctuation">.</span>next_cls<span class="token punctuation">(</span>hidden_pool<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Masked Language Model Task</span>        <span class="token comment" spellcheck="true"># masked_pos: [batch, max_pred] -> [batch, max_pred, d_model]</span>        masked_pos <span class="token operator">=</span> masked_pos<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># h_masked: [batch, max_pred, d_model]</span>        h_masked <span class="token operator">=</span> torch<span class="token punctuation">.</span>gather<span class="token punctuation">(</span>output<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> index<span class="token operator">=</span>masked_pos<span class="token punctuation">)</span>        h_masked <span class="token operator">=</span> self<span class="token punctuation">.</span>gelu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>h_masked<span class="token punctuation">)</span><span class="token punctuation">)</span>        logits_lm <span class="token operator">=</span> self<span class="token punctuation">.</span>word_classifier<span class="token punctuation">(</span>h_masked<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># logits_lm: [batch, max_pred, max_vocab]</span>        <span class="token comment" spellcheck="true"># logits_cls: [batch, 2]</span>        <span class="token keyword">return</span> logits_cls<span class="token punctuation">,</span> logits_lm<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>做个说明:</p><ol><li>为了减少模型训练上的负担, 这里对<code>pooler</code>的<code>fc</code>和MLM输出时使用的<code>fc</code>做了权重共享, 也对Word Embedding和<code>word_classifier</code>的权重做了共享.</li><li><code>torch.gather</code>能收集特定维度的指定位置的数值. <code>h_masked</code>使用的<code>gather</code>是为了检索<code>output</code>中 <code>max_len</code>维度上被Mask的位置上的表示, 总大小<code>[batch, max_pred, d_model]</code>. 因为<code>masked_pos</code>大小为<code>[batch, max_pred, d_model]</code>. 可能我表述不太清楚, 请参照<a href="https://adaning.github.io/posts/1216.html">Pytorch之张量进阶操作</a>中的例子理解.</li><li>没在模型中使用<code>Softmax</code>和<code>Simgoid</code>的原因是<code>nn.CrossEntropyLoss</code>自带将Logits转为概率的效果.</li></ol><h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>这部分主要是准备数据.</p><h3 id="Text-data"><a href="#Text-data" class="headerlink" title="Text data"></a>Text data</h3><p>采用一段简单的对话来作为原始数据. 为了方便起见, 这里没有采用<strong>Subword</strong>.</p><pre class="line-numbers language-python"><code class="language-python">test_text <span class="token operator">=</span> <span class="token punctuation">(</span>    <span class="token string">'Hello, how are you? I am Romeo.\n'</span>  <span class="token comment" spellcheck="true"># R</span>    <span class="token string">'Hello, Romeo My name is Juliet. Nice to meet you.\n'</span>  <span class="token comment" spellcheck="true"># J</span>    <span class="token string">'Nice meet you too. How are you today?\n'</span>  <span class="token comment" spellcheck="true"># R</span>    <span class="token string">'Great. My baseball team won the competition.\n'</span>  <span class="token comment" spellcheck="true"># J</span>    <span class="token string">'Oh Congratulations, Juliet\n'</span>  <span class="token comment" spellcheck="true"># R</span>    <span class="token string">'Thank you Romeo\n'</span>  <span class="token comment" spellcheck="true"># J</span>    <span class="token string">'Where are you going today?\n'</span>  <span class="token comment" spellcheck="true"># R</span>    <span class="token string">'I am going shopping. What about you?\n'</span>  <span class="token comment" spellcheck="true"># J</span>    <span class="token string">'I am going to visit my grandmother. she is not very well'</span>  <span class="token comment" spellcheck="true"># R</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># we need [MASK] [SEP] [PAD] [CLS]</span>word2idx <span class="token operator">=</span> <span class="token punctuation">{</span>f<span class="token string">'[{name}]'</span><span class="token punctuation">:</span> idx <span class="token keyword">for</span> idx<span class="token punctuation">,</span>            name <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'PAD'</span><span class="token punctuation">,</span> <span class="token string">'CLS'</span><span class="token punctuation">,</span> <span class="token string">'SEP'</span><span class="token punctuation">,</span> <span class="token string">'MASK'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true"># {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}</span>sentences <span class="token operator">=</span> re<span class="token punctuation">.</span>sub<span class="token punctuation">(</span><span class="token string">"[.,!?\\-]"</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">,</span> test_text<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">)</span>word_list <span class="token operator">=</span> list<span class="token punctuation">(</span>set<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>holdplace <span class="token operator">=</span> len<span class="token punctuation">(</span>word2idx<span class="token punctuation">)</span><span class="token keyword">for</span> idx<span class="token punctuation">,</span> word <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>word_list<span class="token punctuation">)</span><span class="token punctuation">:</span>    word2idx<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token operator">=</span> idx <span class="token operator">+</span> holdplaceidx2word <span class="token operator">=</span> <span class="token punctuation">{</span>idx<span class="token punctuation">:</span> word <span class="token keyword">for</span> word<span class="token punctuation">,</span> idx <span class="token keyword">in</span> word2idx<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>vocab_size <span class="token operator">=</span> len<span class="token punctuation">(</span>word2idx<span class="token punctuation">)</span><span class="token keyword">assert</span> len<span class="token punctuation">(</span>word2idx<span class="token punctuation">)</span> <span class="token operator">==</span> len<span class="token punctuation">(</span>idx2word<span class="token punctuation">)</span>token_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> sentence <span class="token keyword">in</span> sentences<span class="token punctuation">:</span>    token_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">[</span>        word2idx<span class="token punctuation">[</span>s<span class="token punctuation">]</span> <span class="token keyword">for</span> s <span class="token keyword">in</span> sentence<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里需要编写两个函数:</p><ul><li><code>padding</code>: 句子长度不够时, 用<code>[PAD]</code>补全.</li><li><code>masking_produce</code>: 按照BERT论文中提到的Mask方式.</li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">padding</span><span class="token punctuation">(</span>ids<span class="token punctuation">,</span> n_pads<span class="token punctuation">,</span> pad_symb<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> ids<span class="token punctuation">.</span>extend<span class="token punctuation">(</span><span class="token punctuation">[</span>pad_symb <span class="token keyword">for</span> _ <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_pads<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">masking_procedure</span><span class="token punctuation">(</span>cand_pos<span class="token punctuation">,</span> input_ids<span class="token punctuation">,</span> masked_symb<span class="token operator">=</span>word2idx<span class="token punctuation">[</span><span class="token string">'[MASK]'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    masked_pos <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    masked_tokens <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> pos <span class="token keyword">in</span> cand_pos<span class="token punctuation">:</span>        masked_pos<span class="token punctuation">.</span>append<span class="token punctuation">(</span>pos<span class="token punctuation">)</span>        masked_tokens<span class="token punctuation">.</span>append<span class="token punctuation">(</span>input_ids<span class="token punctuation">[</span>pos<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> p_mask<span class="token punctuation">:</span>            input_ids<span class="token punctuation">[</span>pos<span class="token punctuation">]</span> <span class="token operator">=</span> masked_symb        <span class="token keyword">elif</span> random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token punctuation">(</span>p_mask <span class="token operator">+</span> p_replace<span class="token punctuation">)</span><span class="token punctuation">:</span>            rand_word_idx <span class="token operator">=</span> random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> vocab_size <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span>            input_ids<span class="token punctuation">[</span>pos<span class="token punctuation">]</span> <span class="token operator">=</span> rand_word_idx    <span class="token keyword">return</span> masked_pos<span class="token punctuation">,</span> masked_tokens<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我们需要给句子加上<code>[CLS], [&#39;SEP&#39;], [MASK]</code>来符合BERT的输入格式. 并且保持样本中句子相邻和不相邻的比例是半对半, 即有一半样本中输入句子对是相邻的, 有一半不相邻.</p><p>这里简单的用两个句子的index是否相邻来判断上下文是否相邻, 不是很严谨, 只用于自己实现测试.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">make_data</span><span class="token punctuation">(</span>sentences<span class="token punctuation">,</span> n_data<span class="token punctuation">)</span><span class="token punctuation">:</span>    batch_data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    positive <span class="token operator">=</span> negative <span class="token operator">=</span> <span class="token number">0</span>    len_sentences <span class="token operator">=</span> len<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 50% sampling adjacent sentences, 50% sampling not adjacent sentences</span>    <span class="token keyword">while</span> positive <span class="token operator">!=</span> n_data <span class="token operator">/</span> <span class="token number">2</span> <span class="token operator">or</span> negative <span class="token operator">!=</span> n_data <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">:</span>        tokens_a_idx <span class="token operator">=</span> random<span class="token punctuation">.</span>randrange<span class="token punctuation">(</span>len_sentences<span class="token punctuation">)</span>        tokens_b_idx <span class="token operator">=</span> random<span class="token punctuation">.</span>randrange<span class="token punctuation">(</span>len_sentences<span class="token punctuation">)</span>        tokens_a <span class="token operator">=</span> sentences<span class="token punctuation">[</span>tokens_a_idx<span class="token punctuation">]</span>        tokens_b <span class="token operator">=</span> sentences<span class="token punctuation">[</span>tokens_b_idx<span class="token punctuation">]</span>        input_ids <span class="token operator">=</span> <span class="token punctuation">[</span>word2idx<span class="token punctuation">[</span><span class="token string">'[CLS]'</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">+</span> tokens_a <span class="token operator">+</span> <span class="token punctuation">[</span>word2idx<span class="token punctuation">[</span><span class="token string">'[SEP]'</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">+</span> tokens_b <span class="token operator">+</span> <span class="token punctuation">[</span>word2idx<span class="token punctuation">[</span><span class="token string">'[SEP]'</span><span class="token punctuation">]</span><span class="token punctuation">]</span>        segment_ids <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>            <span class="token number">1</span> <span class="token operator">+</span> len<span class="token punctuation">(</span>tokens_a<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token number">1</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> len<span class="token punctuation">(</span>tokens_b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>        n_pred <span class="token operator">=</span> min<span class="token punctuation">(</span>max_pred<span class="token punctuation">,</span> max<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> int<span class="token punctuation">(</span>len<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        cand_pos <span class="token operator">=</span> <span class="token punctuation">[</span>i <span class="token keyword">for</span> i<span class="token punctuation">,</span> token <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>                    <span class="token keyword">if</span> token <span class="token operator">!=</span> word2idx<span class="token punctuation">[</span><span class="token string">'[CLS]'</span><span class="token punctuation">]</span> <span class="token operator">and</span> token <span class="token operator">!=</span> word2idx<span class="token punctuation">[</span><span class="token string">'[SEP]'</span><span class="token punctuation">]</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># shuffle all candidate position index, to sampling maksed position from first n_pred</span>        masked_pos<span class="token punctuation">,</span> masked_tokens <span class="token operator">=</span> masking_procedure<span class="token punctuation">(</span>            cand_pos<span class="token punctuation">[</span><span class="token punctuation">:</span>n_pred<span class="token punctuation">]</span><span class="token punctuation">,</span> input_ids<span class="token punctuation">,</span> word2idx<span class="token punctuation">[</span><span class="token string">'[MASK]'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># zero padding for tokens</span>        padding<span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> max_len <span class="token operator">-</span> len<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span><span class="token punctuation">)</span>        padding<span class="token punctuation">(</span>segment_ids<span class="token punctuation">,</span> max_len <span class="token operator">-</span> len<span class="token punctuation">(</span>segment_ids<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># zero padding for mask</span>        <span class="token keyword">if</span> max_pred <span class="token operator">></span> n_pred<span class="token punctuation">:</span>            n_pads <span class="token operator">=</span> max_pred <span class="token operator">-</span> n_pred            padding<span class="token punctuation">(</span>masked_pos<span class="token punctuation">,</span> n_pads<span class="token punctuation">)</span>            padding<span class="token punctuation">(</span>masked_tokens<span class="token punctuation">,</span> n_pads<span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>tokens_a_idx <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">==</span> tokens_b_idx <span class="token operator">and</span> positive <span class="token operator">&lt;</span> <span class="token punctuation">(</span>n_data <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            batch_data<span class="token punctuation">.</span>append<span class="token punctuation">(</span>                <span class="token punctuation">[</span>input_ids<span class="token punctuation">,</span> segment_ids<span class="token punctuation">,</span> masked_tokens<span class="token punctuation">,</span> masked_pos<span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">]</span><span class="token punctuation">)</span>            positive <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token keyword">elif</span> <span class="token punctuation">(</span>tokens_a_idx <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">!=</span> tokens_b_idx <span class="token operator">and</span> negative <span class="token operator">&lt;</span> <span class="token punctuation">(</span>n_data <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            batch_data<span class="token punctuation">.</span>append<span class="token punctuation">(</span>                <span class="token punctuation">[</span>input_ids<span class="token punctuation">,</span> segment_ids<span class="token punctuation">,</span> masked_tokens<span class="token punctuation">,</span> masked_pos<span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">]</span><span class="token punctuation">)</span>            negative <span class="token operator">+=</span> <span class="token number">1</span>    <span class="token keyword">return</span> batch_data<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>除了Tokens需要添加Zero Padding, Mask也要添加Zero Padding, 因为每个样本中添加Mask的数量是不定的.</p><p>此外, 这里实现逻辑不是很好, 可以针对循环优化.</p></blockquote><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>其实上面已经把数据准备的工作做完了, 下面就直接继承<code>Dataset</code>实现自己的数据集:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">BERTDataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_ids<span class="token punctuation">,</span> segment_ids<span class="token punctuation">,</span> masked_tokens<span class="token punctuation">,</span> masked_pos<span class="token punctuation">,</span> is_next<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>BERTDataset<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>input_ids <span class="token operator">=</span> input_ids        self<span class="token punctuation">.</span>segment_ids <span class="token operator">=</span> segment_ids        self<span class="token punctuation">.</span>masked_tokens <span class="token operator">=</span> masked_tokens        self<span class="token punctuation">.</span>masked_pos <span class="token operator">=</span> masked_pos        self<span class="token punctuation">.</span>is_next <span class="token operator">=</span> is_next    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>input_ids<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>input_ids<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>segment_ids<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>masked_tokens<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>masked_pos<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>is_next<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Training-and-Evaluation"><a href="#Training-and-Evaluation" class="headerlink" title="Training and Evaluation"></a>Training and Evaluation</h2><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>下面是训练代码, 没有什么值得注意的地方.</p><pre class="line-numbers language-python"><code class="language-python">batch_size <span class="token operator">=</span> <span class="token number">6</span>batch_data <span class="token operator">=</span> make_data<span class="token punctuation">(</span>token_list<span class="token punctuation">,</span> n_data<span class="token operator">=</span>batch_size<span class="token punctuation">)</span>batch_tensor <span class="token operator">=</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>ele<span class="token punctuation">)</span> <span class="token keyword">for</span> ele <span class="token keyword">in</span> zip<span class="token punctuation">(</span><span class="token operator">*</span>batch_data<span class="token punctuation">)</span><span class="token punctuation">]</span>dataset <span class="token operator">=</span> BERTDataset<span class="token punctuation">(</span><span class="token operator">*</span>batch_tensor<span class="token punctuation">)</span>dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>model <span class="token operator">=</span> BERT<span class="token punctuation">(</span>n_layers<span class="token punctuation">)</span>lr <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span>epochs <span class="token operator">=</span> <span class="token number">500</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>optimizer <span class="token operator">=</span> Adadelta<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># training</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> one_batch <span class="token keyword">in</span> dataloader<span class="token punctuation">:</span>        input_ids<span class="token punctuation">,</span> segment_ids<span class="token punctuation">,</span> masked_tokens<span class="token punctuation">,</span> masked_pos<span class="token punctuation">,</span> is_next <span class="token operator">=</span> <span class="token punctuation">[</span>ele<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token keyword">for</span> ele <span class="token keyword">in</span> one_batch<span class="token punctuation">]</span>        logits_cls<span class="token punctuation">,</span> logits_lm <span class="token operator">=</span> model<span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> segment_ids<span class="token punctuation">,</span> masked_pos<span class="token punctuation">)</span>        loss_cls <span class="token operator">=</span> criterion<span class="token punctuation">(</span>logits_cls<span class="token punctuation">,</span> is_next<span class="token punctuation">)</span>        loss_lm <span class="token operator">=</span> criterion<span class="token punctuation">(</span>logits_lm<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> max_vocab<span class="token punctuation">)</span><span class="token punctuation">,</span> masked_tokens<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        loss_lm <span class="token operator">=</span> <span class="token punctuation">(</span>loss_lm<span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss <span class="token operator">=</span> loss_cls <span class="token operator">+</span> loss_lm        <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">10</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'Epoch:{epoch + 1} \t loss: {loss:.6f}'</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h3><p>这里只采用了单个样本模拟Evaluation的过程.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Using one sentence to test</span>test_data_idx <span class="token operator">=</span> <span class="token number">3</span>model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    input_ids<span class="token punctuation">,</span> segment_ids<span class="token punctuation">,</span> masked_tokens<span class="token punctuation">,</span> masked_pos<span class="token punctuation">,</span> is_next <span class="token operator">=</span> batch_data<span class="token punctuation">[</span>test_data_idx<span class="token punctuation">]</span>    input_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    segment_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>segment_ids<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    masked_pos <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>masked_pos<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    masked_tokens <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>masked_tokens<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    logits_cls<span class="token punctuation">,</span> logits_lm <span class="token operator">=</span> model<span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> segment_ids<span class="token punctuation">,</span> masked_pos<span class="token punctuation">)</span>    input_ids<span class="token punctuation">,</span> segment_ids<span class="token punctuation">,</span> masked_tokens<span class="token punctuation">,</span> masked_pos<span class="token punctuation">,</span> is_next <span class="token operator">=</span> batch_data<span class="token punctuation">[</span>test_data_idx<span class="token punctuation">]</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"========================================================"</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Masked data:"</span><span class="token punctuation">)</span>    masked_sentence <span class="token operator">=</span> <span class="token punctuation">[</span>idx2word<span class="token punctuation">[</span>w<span class="token punctuation">]</span> <span class="token keyword">for</span> w <span class="token keyword">in</span> input_ids <span class="token keyword">if</span> idx2word<span class="token punctuation">[</span>w<span class="token punctuation">]</span> <span class="token operator">!=</span> <span class="token string">'[PAD]'</span><span class="token punctuation">]</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>masked_sentence<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># logits_lm: [batch, max_pred, max_vocab]</span>    <span class="token comment" spellcheck="true"># logits_cls: [batch, 2]</span>    cpu <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cpu'</span><span class="token punctuation">)</span>    pred_mask <span class="token operator">=</span> logits_lm<span class="token punctuation">.</span>data<span class="token punctuation">.</span>max<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>cpu<span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>    pred_next <span class="token operator">=</span> logits_cls<span class="token punctuation">.</span>data<span class="token punctuation">.</span>max<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>data<span class="token punctuation">.</span>to<span class="token punctuation">(</span>cpu<span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    bert_sentence <span class="token operator">=</span> masked_sentence<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>    original_sentence <span class="token operator">=</span> masked_sentence<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>masked_pos<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        pos <span class="token operator">=</span> masked_pos<span class="token punctuation">[</span>i<span class="token punctuation">]</span>        <span class="token keyword">if</span> pos <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            <span class="token keyword">break</span>        bert_sentence<span class="token punctuation">[</span>pos<span class="token punctuation">]</span> <span class="token operator">=</span> idx2word<span class="token punctuation">[</span>pred_mask<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span>        original_sentence<span class="token punctuation">[</span>pos<span class="token punctuation">]</span> <span class="token operator">=</span> idx2word<span class="token punctuation">[</span>masked_tokens<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"BERT reconstructed:"</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>bert_sentence<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Original sentence:"</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>original_sentence<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"===============Next Sentence Prediction==============="</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'Two sentences are continuous? {True if is_next else False}'</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'BERT predict: {True if pred_next else False}'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出:</p><pre><code>========================================================Masked data:[&#39;[CLS]&#39;, &#39;team&#39;, &#39;[MASK]&#39;, &#39;[MASK]&#39;, &#39;you&#39;, &#39;i&#39;, &#39;am&#39;, &#39;romeo&#39;, &#39;[SEP]&#39;, &#39;hello&#39;, &#39;romeo&#39;, &#39;my&#39;, &#39;name&#39;, &#39;is&#39;, &#39;juliet&#39;, &#39;nice&#39;, &#39;to&#39;, &#39;meet&#39;, &#39;you&#39;, &#39;[SEP]&#39;]BERT reconstructed:[&#39;[CLS]&#39;, &#39;hello&#39;, &#39;how&#39;, &#39;are&#39;, &#39;you&#39;, &#39;i&#39;, &#39;am&#39;, &#39;romeo&#39;, &#39;[SEP]&#39;, &#39;hello&#39;, &#39;romeo&#39;, &#39;my&#39;, &#39;name&#39;, &#39;is&#39;, &#39;juliet&#39;, &#39;nice&#39;, &#39;to&#39;, &#39;meet&#39;, &#39;you&#39;, &#39;[SEP]&#39;]Original sentence:[&#39;[CLS]&#39;, &#39;hello&#39;, &#39;how&#39;, &#39;are&#39;, &#39;you&#39;, &#39;i&#39;, &#39;am&#39;, &#39;romeo&#39;, &#39;[SEP]&#39;, &#39;hello&#39;, &#39;romeo&#39;, &#39;my&#39;, &#39;name&#39;, &#39;is&#39;, &#39;juliet&#39;, &#39;nice&#39;, &#39;to&#39;, &#39;meet&#39;, &#39;you&#39;, &#39;[SEP]&#39;]===============Next Sentence Prediction===============Two sentences are continuous? TrueBERT predict: True</code></pre>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Transformer </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ConvR: Adaptive Convolution for Multi-Relational Learning</title>
      <link href="/posts/20145.html"/>
      <url>/posts/20145.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>ConvE: 详见<a href="https://adaning.github.io/posts/42031.html">ConvE: Convolutional 2D Knowledge Graph Embeddings</a></li></ul></blockquote><h1 id="ConvR-Adaptive-Convolution-for-Multi-Relational-Learning"><a href="#ConvR-Adaptive-Convolution-for-Multi-Relational-Learning" class="headerlink" title="ConvR: Adaptive Convolution for Multi-Relational Learning"></a>ConvR: Adaptive Convolution for Multi-Relational Learning</h1><p>本文是论文<a href="https://www.researchgate.net/publication/334601450_Adaptive_Convolution_for_Multi-Relational_Learning" target="_blank" rel="noopener">Adaptive Convolution for Multi-Relational Learning</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>即使ConvE在KGE上利用CNN取得了突破性的成就, 但它的设计仍然<strong>忽略了实体和关系之间的交互性</strong>, 限制了Link Prediction的性能.</p><p>在ConvE中, 是将头实体和关系Reshape, 然后用<strong>标准卷积</strong>的卷积核进行运算, 而实际包含有实体和关系Embedding交互的只有卷积核经过的中间那一条(图中用红色标出):</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/convr1.jpg" style="zoom:50%;" /><p>少量的交互次数使得关系对实体Embedding的影响降低了许多, 如果能进一步提升交互应该能改善性能.</p><p>因此, 作者同样是希望利用<strong>卷积</strong>, <strong>最大化实体和关系间的交互次数</strong>, 从而提升卷积类模型在链接预测的性能.</p><h2 id="ConvR"><a href="#ConvR" class="headerlink" title="ConvR"></a>ConvR</h2><p>ConvR其实非常简单, 它仍然没有脱离ConvE的框架. 它仅仅只是将卷积核直接拿掉了, “<strong>关系就是卷积核</strong>“.</p><p>ConvR的精髓在于, 适应性的将Relation Embedding构建成卷积核, 与Entity Embedding交互:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/convr2.jpg" style="zoom:50%;" /><p>这样每次卷积都是100%的交互.</p><h4 id="The-ConvR-Model"><a href="#The-ConvR-Model" class="headerlink" title="The ConvR Model"></a>The ConvR Model</h4><p>与ConvE一样, 先将Entity Embedding Reshape成2D矩阵$\mathbf{S} \in \mathbb{R}_{e}^{d_{e}^{h} \times d_{e}^{w}}$, 且$d_e = d_e^h d_e^w$. 在ConvE中已经论证过在2D上做卷积运算比1D卷积的优越性了.</p><p>然后需要用Relation Embedding构建成卷积核. 首先将Relation Embedding划分为大小相同的若干小块$\mathbf{r}^{(1)}, \cdots, \mathbf{r}^{(c)}$, 该过程称为<strong>Split</strong>.</p><p>因为是2D卷积, 对于每个小块$\mathbf{r}^{(\ell)} \in \mathbb{R}^{d_{r} / c}$, 都重新Reshape成高$h$, 宽$w$ 的卷积核$\mathbf{R}^{(\ell)} \in \mathbb{R}^{h \times w}$, $d_{r}=chw$.</p><blockquote><p>若拿$chw$ 作为$d_r$, 可能导致维数过大.</p></blockquote><p>在卷积时就采用<strong>关系特化</strong>的卷积核$\mathbf{R}^{(\ell)}$进行运算, 卷积过程的数学表示如下:</p><p>$$<br>c_{m, n}^{(\ell)}=f\left(\sum_{i, j} s_{m+i-1, n+j-1} \times r_{i, j}^{(\ell)}\right)<br>$$</p><p>其中$f$ 为非线性函数. 每个卷积核都能产生大小为$\mathbb{R}^{\left(d_{e}^{h}-h+1\right) \times\left(d_{e}^{w}-w+1\right)}$ 的特征图$\mathbf{C}^{(\ell)}$:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/convr3.jpg" style="zoom:50%;" /><p>最后, 将前面卷积产生的所有特征图$\mathbf{C}^{(1)}, \cdots, \mathbf{C}^{(c)}$全部<strong>打平</strong>, 然后<strong>堆叠</strong>成一个向量$\mathbf{c}$, 然后通过一个投影层将其转化为大小为$\mathbb{R}^{d_{e}}$ 的向量, 与尾实体向量$\mathbf{o}$ 做点积, 打分函数如下:<br>$$<br>\psi(s, r, o)=f(\mathbf{W} \mathbf{c}+\mathbf{b})^{\top} \mathbf{o}<br>$$</p><p>其中$\mathbf{W} \in \mathbb{R}^{d_{e} \times c\left(d_{e}^{h}-h+1\right)\left(d_{e}^{w}-w+1\right)}$, $\mathbf{b} \in \mathbb{R}^{d_{e}}$. 形式与ConvE<strong>完全一致</strong>.</p><h3 id="Parameter-Learning"><a href="#Parameter-Learning" class="headerlink" title="Parameter Learning"></a>Parameter Learning</h3><p>首先计算尾实体的概率:<br>$$<br>p_{o}^{s, r}=\sigma(\psi(s, r, o))<br>$$<br>其中$\sigma$ 是对数几率函数, 即$\sigma(x)=\frac{1}{1+e^{-x}}$.</p><p>与ConvE提出的1 - n Scoring相同, 然后用<strong>二分类交叉熵</strong>(BCE)损失函数进行优化:<br>$$<br>\mathcal{L}(s, r)=- \frac{1}{|\mathcal{E}|} \sum_{o \in \mathcal{E}} y_{o}^{s, r} \log \left(p_{o}^{s, r}\right)+<br>\left(1-y_{o}^{s, r}\right) \log \left(1-p_{o}^{s, r}\right)<br>$$</p><p>其他小Trick例如Dropout, BN, Label Smoothing仍然沿用ConvE.</p><h3 id="Advantages-over-ConvE"><a href="#Advantages-over-ConvE" class="headerlink" title="Advantages over ConvE"></a>Advantages over ConvE</h3><p>既然是ConvE的优化方法, 作者将着重论证ConvR比ConvE的优越性. 主要有两点:</p><ol><li><p><strong>交互次数</strong>比ConvE变多了, 每次卷积都是Entity Embedding和Relation Embedding的交互.</p></li><li><p>参数比ConvE少了, 因为”<strong>关系就是卷积核</strong>“.</p><p><strong>ConvE</strong>:$\mathcal{O}\left(d|\mathcal{E}|+d|\mathcal{R}|+c h w+c d\left(2 d^{h}-h+1\right)\left(d^{w}- w + 1\right)\right)$</p><p><strong>ConvR</strong>: $\mathcal{O}\left(d_{e}|\mathcal{E}|+d_{r}|\mathcal{R}|+c d_{e}\left(d_{e}^{h}-h+1\right)\left(d_{e}^{w}-w+1\right)\right)$</p><p>ConvR少掉的部分$chw$ 被集成进了Relation Embedding中.</p></li></ol><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的实验参数设置请参照原论文.</p><h4 id="Link-Prediction-Results"><a href="#Link-Prediction-Results" class="headerlink" title="Link Prediction Results"></a>Link Prediction Results</h4><p>作者主要探究了ConvR在WN18, FB15K, WN18RR, FB15K - 237上的性能:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/convr4.jpg" style="zoom: 50%;" /><p>WN18和FB15K这两个数据集是存在<strong>大量逆关系</strong>的, 结果可能是过拟合了的.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/convr5.jpg" style="zoom:50%;" /><p>主要的对比还是集中在卷积类的模型, ConvR比ConvE有了小幅提升, 比ConvKB的效果也要好.</p><p>其中所采用的最佳配置如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/convr6.jpg" style="zoom:50%;" /><blockquote><p>能看到, 如果使用ConvR, $d_r=chw$, 在列出的最佳结果中$d_r$ 都是远比$d_e$ 大的.</p></blockquote><h3 id="Parameter-Efficiency"><a href="#Parameter-Efficiency" class="headerlink" title="Parameter Efficiency"></a>Parameter Efficiency</h3><p>作者探究了不同卷积核大小和不同数量对模型性能的影响:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/convr7.jpg" style="zoom:50%;" /><p>总体来说各类参数对性能影响不大, 可以认为算法对超参是比较<strong>不敏感</strong>的.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>从卷积核<strong>自身结构</strong>下手, 与关系信息融合是一个非常好的思路. ConvR的设计即减少了参数量, 也增加了实体关系嵌入的交互次数.</p><p>但是以结构为基础的融合也是一把<strong>双刃剑</strong>, 我认为ConvR还存在两个重要的问题:</p><ul><li>虽然提升了关系和实体之间的交互次数, 但受制于直接拿关系Embedding作为卷积核的方式, ConvR无法像其他卷积类方法把模型做得更<strong>深</strong>. 一般来讲, 卷积层数越深, 所抽取到的特征也就越高阶, 也就意味着<strong>更复杂的实体与关系交互</strong>, 但在ConvR中只能做一层卷积.</li><li>因为卷积的特殊机制, $d_e$ 和$d_r$ 大概率不相同, 可能会对下游任务使用产生一定影响(许多场景要求$d_e$ 和$d_r$ 是<strong>相同维度</strong>的, 所以还需要<strong>额外维度压缩</strong>). 作者所给出的示例$d_r$ 都比较大, 如果强行调整至一致可能会导致性能衰减. </li></ul>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LightCAKE: A Lightweight Framework for Context-Aware Knowledge Graph Embedding</title>
      <link href="/posts/21282.html"/>
      <url>/posts/21282.html</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>2021.3.9</strong>: 修正关于引入逆三元组的影响.</p><p><strong>2021.4.18</strong>: 更新一篇更早的类似论文<a href="https://adaning.github.io/posts/60222.html">GAKE</a>.</p></blockquote><h1 id="LightCAKE-A-Lightweight-Framework-for-Context-Aware-Knowledge-Graph-Embedding"><a href="#LightCAKE-A-Lightweight-Framework-for-Context-Aware-Knowledge-Graph-Embedding" class="headerlink" title="LightCAKE: A Lightweight Framework for Context-Aware Knowledge Graph Embedding"></a>LightCAKE: A Lightweight Framework for Context-Aware Knowledge Graph Embedding</h1><p>本文是<a href="https://arxiv.org/abs/2102.10826" target="_blank" rel="noopener">LightCAKE: A Lightweight Framework for Context-Aware Knowledge Graph Embedding</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者提出, 现有的KGE模型无法很好地平衡<strong>图上下文</strong>信息与模型计算的<strong>复杂度</strong>.</p><p>作者指出, 许多的KGE模型都忽略了图中所蕴含的上下文信息(其实用图的<strong>多跳信息</strong>概括更为生动). 而作者将图中蕴含的信息分为两种, <strong>实体</strong>上下文信息和<strong>关系</strong>上下文信息:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lightcake1.jpg" style="zoom:50%;" /><p>图中普通虚线代表需要被预测的关系, 而红色虚线所圈的内容是对预测有重大帮助的部分. 许多模型直接将二者全部忽略, 或是只关注其中的一种. </p><blockquote><p>左侧图中从<strong>实体邻居</strong>的角度出发, 特朗普是拜登的一阶邻居, 考虑二者之间的关系将对预测有很大帮助.</p><p>右侧图中从<strong>关系对应的实体</strong>对角度出发, “president_of” 关系下所有的头尾实体都对预测特朗普是否是总统有帮助.</p></blockquote><p>因此, 作者希望提出一种轻量级的<strong>框架</strong>, <strong>LightCAKE</strong>(<strong>Light</strong>weight Framework for <strong>C</strong>ontext-<strong>A</strong>ware <strong>K</strong>nowledge Graph <strong>E</strong>mbedding) 来解决上述问题. 既然涉及到图信息的集成, 那么采用<strong>图算法</strong>更为合适. 例如GNN, 能够很好地聚合邻居节点的信息, 也能够捕获一部分来自高阶邻居的<strong>多跳信息</strong>.</p><h2 id="LightCAKE"><a href="#LightCAKE" class="headerlink" title="LightCAKE"></a>LightCAKE</h2><h3 id="Context-Star-Graph"><a href="#Context-Star-Graph" class="headerlink" title="Context Star Graph"></a>Context Star Graph</h3><p>在说明框架的运作方式之前, 先对实体上下文和关系上下文下个定义:</p><ul><li><strong>Entity Context</strong>: 对于头实体$h$, 实体上下文被定义为$h$ 的邻居, $\mathcal{C}_{\text {ent }}(h)=\{(r, t) \mid(h, r, t) \in \mathcal{G}\}$.</li><li><strong>Relation Context</strong>: 对于关系$r$, 关系上下文被定义为$r$ 下的全部头尾实体对, $\mathcal{C}_{r e l}(r)=\{(h, t) \mid(h, r, t) \in \mathcal{G}\}$.</li></ul><p>在图结构中, 实体和关系的上下文能<strong>一次性</strong>的构建:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lightcake2.jpg" style="zoom:50%;" /><p>作者在论文中计算实体上下文$\mathcal{C}_{\text {ent }}(h)$ 时只考虑了$h$ 的出边邻居, 忽略了入边邻居. 对于三元组$(h, r, t) \in \mathcal{G}$, 作者都创建一个逆三元组$(t, r^{-1}, h)$, 使得头实体和尾实体之间的边变为双向边(图中虚线部分). </p><blockquote><p>Inverse Triplet是否会导致Inverse Leakage? </p><p>实际上这里不会导致标签泄露. 在FB15K和WN18中引入的大量逆关系导致Leakage的原因并不是因为主动引入了逆三元组. 而是因为原本的三元组构建中就包含逆三元组, 在划分训练集和测试集的时候, 将部分互逆三元组对划分到了训练集和测试集两部分, 所以在测试集中的三元组可以由训练集过拟合得到, 从而导致的标签泄露.</p><p>而这里主动引入逆三元组, 若原数据集中不存在互逆三元组对, 那么训练时是不会将信息泄露给测试集的. 比如原来的三元组存在于训练集, 引入的新逆三元组也应该属于训练集, 这样就不会造成标签泄露.</p><p>事实上, 现在很多KGE方法都是这样做的.</p></blockquote><h3 id="LightCAKE-Details"><a href="#LightCAKE-Details" class="headerlink" title="LightCAKE Details"></a>LightCAKE Details</h3><p>LightCAKE被作者分为两部分:</p><ol><li>将实体或关系上下文<strong>编码</strong>进Embedding(<strong>Encode</strong>).</li><li><strong>迭代聚合</strong>上下文节点的信息(<strong>Attention</strong>).</li></ol><p>每次迭代的<strong>更新方程</strong>如下:</p><p>$$<br>\begin{array}{l}<br>e_{h}^{(l+1)}=e_{h}^{(l)}+\sum\limits_{\left(r^{\prime}, t^{\prime}\right) \in \mathcal{C}_{\text {ent }}(h)} \alpha_{h,\left(r^{\prime}, t^{\prime}\right)}^{(l)} \phi_{\text {ent }}\left(e_{r^{\prime}}, e_{t^{\prime}}\right) \\<br>e_{r}^{(l+1)}=e_{r}^{(l)}+\sum\limits_{\left(h^{\prime}, t^{\prime}\right) \in \mathcal{C}_{r e l}(r)} \beta_{r,\left(h^{\prime}, t^{\prime}\right)}^{(l)} \phi_{\text {rel }}\left(e_{h^{\prime}}, e_{t^{\prime}}\right)<br>\end{array}<br>$$</p><p>每次在原来Embedding的基础上<strong>有权重地聚合</strong>了图结构中的多跳信息. 其中$\phi$ 代表编码函数, $\phi(\cdot): \mathbb{R}^{d} \times \mathbb{R}^{d} \rightarrow \mathbb{R}$. $l$ 是迭代的次数, $0\leq l\leq L$. </p><p>$\alpha_{h,\left(r^{\prime}, t^{\prime}\right)}^{(l)}, \beta_{r,\left(h^{\prime}, t^{\prime}\right)}^{(l)}$ 分别是在实体上下文星图和关系上下文星图中对其他节点的<strong>注意力</strong>:<br>$$<br>\begin{aligned}<br>\alpha_{h,\left(r^{\prime}, t^{\prime}\right)}^{(l)}=\frac{\exp \left(\psi\left(e_{h}^{(l)}, e_{r^{\prime}}^{(l)}, e_{t^{\prime}}^{(l)}\right)\right)}{\sum_{\left(r^{\prime \prime}, t^{\prime \prime}\right) \in \mathcal{C}_{e n t}(h)} \exp \left(\psi\left(e_{h}^{(l)}, e_{r^{\prime \prime}}^{(l)}, e_{t^{\prime \prime}}^{(l)}\right)\right)} \\<br>\beta_{r,\left(h^{\prime}, t^{\prime}\right)}^{(l)}=\frac{\exp \left(\psi\left(e_{h^{\prime}}^{(l)}, e_{r}^{(l)}, e_{t^{\prime}}^{(l)}\right)\right)}{\sum_{\left(h^{\prime \prime}, t^{\prime \prime}\right) \in \mathcal{C}_{r e l}(r)} \exp \left(\psi\left(e_{h^{\prime \prime}}^{(l)}, e_{r}^{(l)}, e_{t^{\prime \prime}}^{(l)}\right)\right)}<br>\end{aligned}<br>$$</p><p>其中, $\psi$ 是具体的打分函数, 也就是具体的KGE方法, 例如TransE, DistMult等.</p><p>通过$L$ 次迭代, 就能得到一组<strong>上下文增强</strong>的Embedding $e_{h}^{(L)}, e_{r}^{(L)}, e_{t}^{(L)}$. 然后分别计算出在已知头实体和尾实体条件下未知关系$r$ 条件概率:<br>$$<br>p(r \mid h, t)=\frac{\exp \left(\psi\left(e_{h}^{(L)}, e_{r}^{(L)}, e_{t}^{(L)}\right)\right)}{\sum_{r^{\prime} \in \mathcal{R}} \exp \left(\psi\left(e_{h}^{(L)}, e_{r^{\prime}}^{(L)}, e_{t}^{(L)}\right)\right)}<br>$$<br>根据计算得出的条件概率$p(r \mid h, t)$, 用<strong>极大似然</strong>来优化Embedding:<br>$$<br>\mathcal{L}=-\frac{1}{|\mathcal{D}|} \sum_{i=0}^{|\mathcal{D}|} \log p\left(r_{i} \mid h_{i}, t_{i}\right)<br>$$<br>对于作者文中所采用的两种方法TransE和DistMult, 作者将它们的打分函数代入后写出了改进方法的详细形式, 在此没有必要列出了.</p><p>作者注意到, 与图相关的GNN算法存在<strong>过参数化</strong>的问题, 所以整套LightCAKE框架<strong>没有添加任何额外的参数</strong>.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>作者认为, 使用Relation Prediction与使用Link Prediction一定程度上是等价的, 所以这里采用了Relation Prediction作为评估任务.</p><blockquote><p>我个人认为还是有些不同, Relation Prediction的难度比Link Prediction难度要小得多.</p></blockquote><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><p>作者选用了现在效果比较好的Baseline, ComplEx, SimplE, RotatE, DRUM, R - GCN和改进后的TransE, DistMult性能做了对比:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lightcake3.jpg" style="zoom:50%;" /><p>改进后的方法不但性能上比现在已有的方法性能要好, 而且性能与没改进前要高许多(针对MRR来说).</p><h3 id="Ablation-Study-and-Analysis-on-Number-of-Iterations"><a href="#Ablation-Study-and-Analysis-on-Number-of-Iterations" class="headerlink" title="Ablation Study and Analysis on Number of Iterations"></a>Ablation Study and Analysis on Number of Iterations</h3><p>作者在消融实验中<strong>逐步引入图多跳信息</strong>, 查看引入的信息对性能产生的影响, 也探究了<strong>迭代次数</strong>对Embedding产生的影响. 作者将引入信息的程度分为四个等级:</p><ol><li><strong>不引入</strong>任何图结构的信息.</li><li>只引入<strong>关系</strong>上下文信息, 记为$\mathcal{L}_{rel}$.</li><li>只引入<strong>实体</strong>上下文信息, 记为$\mathcal{L}_{ent}$.</li><li>引入<strong>实体和关系</strong>上下文信息, 记为$\mathcal{L}$.</li></ol><p>实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lightcake4.jpg" style="zoom: 50%;" /><p>随着引入信息的逐渐增加, 模型性能逐渐提高. 引入实体信息后的提升比引入关系信息要大. </p><p>关于迭代轮数, 迭代过多次数反而会导致性能下降, 控制在3 ~ 4 次为宜.</p><h3 id="Efficiency-Analysis"><a href="#Efficiency-Analysis" class="headerlink" title="Efficiency Analysis"></a>Efficiency Analysis</h3><p>作者将改进前后和基于图的算法(R - GCN)做了效率上的比较:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lightcake5.jpg" style="zoom: 50%;" /><p>DistMult在改进后增加了L倍的复杂度, 但却获得了实体和关系上的图结构信息<strong>双加成</strong>, 效果提升显著. 而R - GCN的时间复杂度比较高, 没有利用图上的关系信息.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>LightCAKE是一个<strong>轻量级</strong>的<strong>集成图多跳信息</strong>的KGE框架, 无需添加任何额外的参数, 仅用部分计算量就将关系和实体上下文信息全部集成了起来, 整体非常简洁. 作者也注意到了GNN中的过参数化问题, 这点很宝贵. 但实验部分采用了Relation Prediction作为评估, 我仍然期待它在Link Prediction上的效果.</p><p>对我的启发:</p><ol><li>要利用好<strong>图结构优势</strong>, 图中蕴含着多种<strong>多跳信息</strong>, 不能白白浪费它们. 从可解释的角度, 图结构也占有优势.</li><li>注意各类方法在KGE中的<strong>过参数化</strong>(尤其是GNN), 有时候参数过多也不是啥好事, 可能不利于优化, 还增加了时间复杂度.</li><li>引入实体, 关系双角度信息, 能更好的刻画节点. 如果有更多角度的信息, 能更精确的找到节点定位.</li></ol><blockquote><p>经评论区老哥指路, LightCAKE的Entity Context的定义和<a href="https://adaning.github.io/posts/60222.html">GAKE(COLING2016)</a>的<strong>Neighbor Context</strong>定义是一样的, 并且同样也是用Attention分配上下文的对Embedding的影响, 用极大似然优化. 只是GAKE还额外定义了Path Context和Edge Context, 而LightCAKE可以使用别的打分函数. 整体上来讲, 这两篇论文非常像.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ConvBERT: Improving BERT with Span-based Dynamic Convolution</title>
      <link href="/posts/22934.html"/>
      <url>/posts/22934.html</url>
      
        <content type="html"><![CDATA[<h1 id="ConvBERT-Improving-BERT-with-Span-based-Dynamic-Convolution"><a href="#ConvBERT-Improving-BERT-with-Span-based-Dynamic-Convolution" class="headerlink" title="ConvBERT: Improving BERT with Span-based Dynamic Convolution"></a>ConvBERT: Improving BERT with Span-based Dynamic Convolution</h1><blockquote><p>本文前置知识:</p><ul><li>Light Weight Convolution: 详见<a href="https://adaning.github.io/posts/40162.html">基于轻量级卷积和动态卷积替代的注意力机制</a>.</li><li>Depthwise Separable Convolution, Group Convolution: 详见<a href="https://adaning.github.io/posts/13629.html">深度可分离卷积与分组卷积</a>.</li><li>BERT: 详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>.</li></ul></blockquote><p>本文是论文<a href="https://arxiv.org/abs/2008.02496" target="_blank" rel="noopener">ConvBERT: Improving BERT with Span-based Dynamic Convolution</a>的阅读笔记和个人理解. 属于随缘填坑系列.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者发现, BERT的中所使用的Self Attention每次请求都需要全局的信息, 但实际上并不是每次Attention都需要全局信息, 有时只需要<strong>局部信息</strong>即可, 所以Attention总是伴随着<strong>计算冗余</strong>, 总体来看BERT也没有高度局部化的操作, 但很多头部都只学习了自然语言局部信息.</p><p>因此, 作者希望能够使用一种<strong>直接捕捉局部信息</strong>的方法, 从而降低Attention的复杂度. </p><h2 id="ConvBERT"><a href="#ConvBERT" class="headerlink" title="ConvBERT"></a>ConvBERT</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>既然作者是希望能够获得捕捉局部信息的能力, 还对参数压缩有一定的需求, 那么很容易就想到了用<strong>卷积</strong>.</p><p>在经典Transformer中的Attention机制的公式为:</p><p>$$<br>\operatorname{Self}-\operatorname{Attn}(Q, K, V)=\operatorname{softmax}\left(\frac{Q^{\top} K}{\sqrt{d_{k}}}\right) V<br>$$</p><p>即最简单的缩放点积, 在这里就不多做说明了.</p><h4 id="Parameters-Redundancy"><a href="#Parameters-Redundancy" class="headerlink" title="Parameters Redundancy"></a>Parameters Redundancy</h4><p>Self - Attention在处理问题时可能会存在参数冗余的问题:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/convbert2.jpg" style="zoom: 50%;" /><p>从BERT的注意力中能够看到, 相当多的权重被分配在在<strong>对角线邻近</strong>的位置上, 说明了自然语言结构中的邻近信息在很多时候是起作用的, 这也引起了相当多的权重<strong>冗余</strong>.</p><h4 id="Convolutional-based-Attention"><a href="#Convolutional-based-Attention" class="headerlink" title="Convolutional based Attention"></a>Convolutional based Attention</h4><p>在前面的研究工作中介绍过, 使用卷积能够一定程度上来缓解参数冗余的问题, 例如<a href="https://adaning.github.io/posts/40162.html">轻量级卷积</a>:<br>$$<br>\operatorname{LConv}(X, W, i)=\sum_{j=1}^{k} W_{j} \cdot X_{\left(i+j-\left\lceil\frac{k+1}{2}\right\rceil\right)}<br>$$</p><p>以及以轻量级卷积为基础, 做出一定改进的动态卷积:</p><p>$$<br>\operatorname{DConv}\left(X, W_{f}, i\right)=\operatorname{LConv}\left(X, \operatorname{softmax}\left(W_{f} X_{i}\right), i\right)<br>$$</p><h3 id="Span-based-Dynamic-Convolution"><a href="#Span-based-Dynamic-Convolution" class="headerlink" title="Span - based Dynamic Convolution"></a>Span - based Dynamic Convolution</h3><p>在<a href="https://adaning.github.io/posts/40162.html">基于轻量级卷积和动态卷积替代的注意力机制</a>中提到过:</p><blockquote><p>注意力权重的生成只取决于<strong>当前时刻的输入</strong>, 而与前时刻和后时刻输入无关, 这是一个严重缺陷.</p></blockquote><p>基于这个改进点, 作者将当前时刻输入的邻近信息也加入了动态调整当前时刻输出的机制, 并称之为<strong>区间动态卷积</strong>.</p><p>自注意力, 动态卷积, 区间动态卷积这三者之间的差别能够很容易的用如下图示对比, 也可以顺带引出区间动态卷积的本质:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/convbert1.jpg" style="zoom: 50%;" /><p>自注意力与所有基于卷积方法, 特征之间有更加<strong>稠密</strong>的影响, 但在作者的论文中认为许多交互是不必要的, 即存在特征冗余, 导致了模型参数的冗余. 动态卷积的当前时刻输出仅会取决于<strong>当前时刻输入</strong>, 结合<strong>周围语义</strong>动态调整权重的能力比较差, 因为它分配权重时完全没有结合上下文信息. 区间动态卷积算是在二者之间取了个<strong>折中</strong>, 能结合<strong>小区间范围</strong>内的信息对卷积核的参数做更好的权重分配, 即能够契合语言局部性的特点, 也能结合<strong>语境</strong>做出判断.</p><p>因此, 考虑将区间动态卷积和自注意力兼容, 需要考虑一种结合区间信息<strong>动态生成卷积核</strong>的方法. 作者使用输入$X$ 用Self - Attention的方式生成对应的$Q$ 和 $V$, 接着使用<strong>深度可分离卷积</strong>抽取与区间内容相关的$K_s$, 然后动态地生成卷积核权重:</p><p>$$<br>f\left(Q, K_{s}\right)=\operatorname{softmax}\left(W_{f}\left(Q \odot K_{s}\right)\right)<br>$$</p><p>$\odot$ 为逐元素点乘, $W_f$ 为可训练的权重矩阵. </p><p>自注意力, 动态卷积, 区间动态卷积三者的运算流程结构图如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/convbert3.jpg" style="zoom:33%;" /><p>明显的能看出来, 在附加了局部信息后, 区间动态卷积与动态卷积相比更像自注意力, 仅替换了某些线性算子, 相较动态卷积而言去掉了GLU, 不会引入过多的参数. </p><p>将这种生成卷积核的方式融入轻量级卷积, 称为区间动态卷积:<br>$$<br>\operatorname{SDConv}\left(Q, K_{s}, V ; W_{f}, i\right)=\operatorname{LConv}\left(V, \operatorname{softmax}\left(W_{f}\left(Q \odot K_{s}\right)\right), i\right)<br>$$</p><p>与动态卷积的式子相比, 将$\operatorname{LConv}$ 中的第一个参数由输入$X$ 替换为自注意力模式中的$V$, 并将生成卷积核参数的方式替换为结合区间信息的$\operatorname{softmax}\left(W_{f}\left(Q \odot K_{s}\right)\right)$.</p><h3 id="ConvBERT-Architecture"><a href="#ConvBERT-Architecture" class="headerlink" title="ConvBERT Architecture"></a>ConvBERT Architecture</h3><h4 id="Mixed-Attention"><a href="#Mixed-Attention" class="headerlink" title="Mixed Attention"></a>Mixed Attention</h4><p>因为自注意力机制和区间动态卷积是可以兼容的, 所以可以将这二者以某种形式混合起来.</p><p>混合Attention就是将区间动态卷积与Self - Attention做了混合, 这二者之间是没有交互的, 直接通过一个Concat操作将二者拼接起来:</p><p>$$<br>\text {Mixed-Attn}\left(K, Q, K_{s}, V ; W_{f}\right)=\operatorname{Cat}\left(\text {Self-Attn}(Q, K, V), \operatorname{SDConv}\left(Q, K_{s}, V ; W_{f}\right)\right)<br>$$</p><p>这样的设计可以让模型不仅限于局部特征的捕捉, 而是以<strong>多角度</strong>来捕捉整个文本的信息. </p><p>自注意力和区间动态卷积共享相同的$Q, V$, 使用不同的方式来生成$K$.</p><h4 id="Bottleneck-Design-for-Self-Attention"><a href="#Bottleneck-Design-for-Self-Attention" class="headerlink" title="Bottleneck Design for Self - Attention"></a>Bottleneck Design for Self - Attention</h4><p>针对冗余参数, Bottleneck的设计能够有助于模型学习到更<strong>紧凑</strong>的信息:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/convbert4.jpg" style="zoom:33%;" /><p>作者提出的Bottleneck加入<strong>缩放因子</strong>$\gamma$ , 当$\gamma&gt;1$ 时, 将特征维数缩小为$d/\gamma$, 并降低注意力头的数量为原来的$1/\gamma$. </p><p>同时, 这样的设计也可以保证模块的有序堆叠, 输入大小和输出大小一致.</p><h4 id="Grouped-Feed-Forward-Module"><a href="#Grouped-Feed-Forward-Module" class="headerlink" title="Grouped Feed - Forward Module"></a>Grouped Feed - Forward Module</h4><p>因为FFN中占了很多参数, 所以作者希望通过<strong>分组</strong>的方式来减小开销. 与多头注意力类似, 分组卷积分别将特征分组提取后再Concat起来:</p><p>$$<br>\displaylines{<br>M=\Pi_{i=0}^{g}\left[f_{\frac{d}{g} \rightarrow \frac{m}{g}}^{i}\left(H_{\left[:, i-1: i \times \frac{d}{g}\right]}\right)\right], M^{\prime}=\operatorname{GeLU}(M)<br>\\<br>H^{\prime}=\Pi_{i=0}^{g}\left[f_{\frac{m}{g} \rightarrow \frac{d}{g}}^{i}\left(M_{\left[:, i-1: i \times \frac{m}{g}\right]}^{\prime}\right)\right]<br>}<br>$$</p><p>其中$H, H^\prime \in R^{n\times n}$,   $M, M^\prime \in R^{n\times n}$, $f_{d_1 \rightarrow d_2}(\cdot)$ 表示将$d_1$ 维映射到$d_2$ 维的FC层. $g$ 为分组的组数, $\Pi$ 为Concat操作.</p><p>后续的实验表明, 精度下降可以忽略不计.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>在实验中, 作者使用了开源网页数据集OpenWebText(32G)来对标BERT所用的训练数据. 作者采用$\gamma=2$ 来缩小特征维度, 注意力头的数量也为原来的一半. 其余详细的实验设置请参考原论文.</p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>消融实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/convbert5.jpg" style="zoom:33%;" /><p>小Trick对提升模型性能有一些帮助, 但总体来说区间动态卷积的提升比较大.</p><h4 id="Kernel-Size"><a href="#Kernel-Size" class="headerlink" title="Kernel Size"></a>Kernel Size</h4><p>对于Kernel Size实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/convbert6.jpg" style="zoom: 50%;" /><p>模型性能先是随着卷积核的大小提升而提升, 在达到一定阈值后性能反而下降. 很有可能是感受野逐渐扩大, 覆盖到整个输入序列, 此前性能均在提升. 而Kernel Size覆盖整个输入序列时, 模型效果稍有下降.</p><p>这个实验再一次佐证了作者的猜想.</p><h4 id="Ways-to-integrate-convolution"><a href="#Ways-to-integrate-convolution" class="headerlink" title="Ways to integrate convolution"></a>Ways to integrate convolution</h4><p>如下作者探究不同形式的卷积对模型性能的影响:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/convbert7.jpg" style="zoom:33%;" /><p>肯定是区间动态卷积比较好, 因为区间动态卷积本身就是由其他卷积形式变换而来的.</p><h3 id="Comparison-results"><a href="#Comparison-results" class="headerlink" title="Comparison results"></a>Comparison results</h3><h4 id="GLUE"><a href="#GLUE" class="headerlink" title="GLUE"></a>GLUE</h4><p>其中标有十字架标志的模型是基于知识蒸馏的方法, 在GLUE上的结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/convbert8.jpg" style="zoom:33%;" /><p>ConvBERT在低资源的情况下结果良好.</p><h4 id="SQuAD"><a href="#SQuAD" class="headerlink" title="SQuAD"></a>SQuAD</h4><p>在问答数据集SQuAD上的结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/convbert9.jpg" style="zoom:33%;" /><p>延长训练后, 虽然训练所需的计算量下降了, 但是仍然能够达到近似ELECTRA的效果. 极大地减少了训练成本.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>本文中仍然是一篇NLP领域的论文, 我读它的原因主要是为了获取在Attention上改动的灵感.</p><p>作者在轻量级卷积的基础上进一步的将卷积核的权重生成动态化. 设计了一种新型Bottleneck, 进一步的优化了BERT的结构, 并在其中加入了卷积操作, 大幅的降低了训练成本, 但似乎这种方法<strong>不够简洁</strong>. </p><p>作者没有将自注意力完完全全的去掉, 也说明了NLP中Self - Attention必然是不能被其他方法直接取代的.</p><p>后面实验主要也对比的Baseline主要也是小模型系列, 不知与大模型相比效果如何(虽然有点不公平).</p><p>训练过程中也使用了一些小Trick, 没有这些小Trick可能不会有特别亮眼的效果. </p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> CNN </tag>
            
            <tag> Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PPKE: Knowledge Representation Learning by Path-based Pre-training</title>
      <link href="/posts/11653.html"/>
      <url>/posts/11653.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>BERT(Transformer Encoder).</li></ul></blockquote><h1 id="PPKE-Knowledge-Representation-Learning-by-Path-based-Pre-training"><a href="#PPKE-Knowledge-Representation-Learning-by-Path-based-Pre-training" class="headerlink" title="PPKE: Knowledge Representation Learning by Path-based Pre-training"></a>PPKE: Knowledge Representation Learning by Path-based Pre-training</h1><p>本文是论文<a href="https://arxiv.org/abs/2012.03573" target="_blank" rel="noopener">PPKE: Knowledge Representation Learning by Path-based Pre-training</a>的个人理解和阅读笔记. 论文本身非常短, 公式插图风格完全沿用了<a href="https://adaning.github.io/posts/42304.html">CoKE</a>, 也出自同一研究小组.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>传统的KGE方法将三元组作为一个训练单元, 但忽略了图中存在的<strong>上下文拓扑结构信息</strong>. 与语言模型中的上下文一样, 关系路径能够被认为是KG中的一种上下文关系, 作者称之为”<strong>图上下文信息</strong>“.</p><p>不可靠关系在KG中常见, 但使用可靠的关系做KRL是非常必要的, 尤其是在涉及到<strong>推理</strong>的问题上.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/ppke1.jpg" style="zoom:33%;" /><blockquote><p>作者还指出, 前人所发现的基于路径或上下文信息的方法都无法通过基准数据集来证明图上下文信息能改善模型性能.</p></blockquote><p>因此, 作者尝试提出基于<strong>路径</strong>的<strong>预训练</strong>KGE模型PPKE(<strong>P</strong>ath - based <strong>P</strong>re - training model to learn <strong>K</strong>nowledge <strong>E</strong>mbeddings), 目标是将实体间的图上下文信息集成进KGE模型的参数中, 并且它是一种<strong>预训练模型</strong>.</p><h2 id="PPKE"><a href="#PPKE" class="headerlink" title="PPKE"></a>PPKE</h2><p>如果你已经了解了CoKE, 那么</p><p>基于<strong>预训练模型</strong>的思路, 模型的训练分为<strong>Pre - train</strong>和<strong>Tuning</strong>两个部分.</p><h3 id="Path-Based-Pre-training"><a href="#Path-Based-Pre-training" class="headerlink" title="Path - Based Pre - training"></a>Path - Based Pre - training</h3><h4 id="Input-Representation"><a href="#Input-Representation" class="headerlink" title="Input Representation"></a>Input Representation</h4><p>头实体$h$ 到尾实体$t$ 的路径输入被表示为$\left\{h, r_{1}, \ldots, r_{n}, t\right\}$, 其中$r_i$ 代表长度为$n$ 的路径中第$i$ 跳的关系, </p><p>那么三元组$\left\{ h, r, t \right\}$ 可以被认为是路径长度$n=1$ 时的一个特殊情况.</p><p>与CoKE不太一样的是, 为了<strong>避免位置偏差</strong>, 作者将尾实体的位置放到了<strong>头实体后</strong>, <strong>关系路径之前</strong>. 这点也是与<strong>CoKE</strong>的<strong>最大不同点</strong>.</p><p>故, 三元组的输入形式应该为:<br>$$<br>\boldsymbol{x}=\left\{h, t, r_{1}, \ldots, r_{n}\right\}<br>$$<br>为了区分实体和关系的不同角色, 必须对它们加以不同的<strong>位置编码</strong>. </p><blockquote><p>这点与CoKE相同, CoKE也没有对不同的元素加以不同的类型编码, 实体和关系直接不加以区分, 只加位置编码.</p></blockquote><p>如果将输入变为Embedding的形式, 如下所示:<br>$$<br>\mathbf{E} = \left\{\mathbf{E}^{h}, \mathbf{E}^{t}, \mathbf{E}^{r_{1}}, \ldots, \mathbf{E}^{r_{n}}\right\}<br>$$</p><h4 id="Masked-Entity-Predicition"><a href="#Masked-Entity-Predicition" class="headerlink" title="Masked Entity Predicition"></a>Masked Entity Predicition</h4><p>与BERT的食用方法相同, 也是用Masked Language Model的训练方式来训练. 针对Link Prediction任务, 只需要将要预测的实体所在位置打上<code>[Mask]</code>. 例如:<br>$$<br>\begin{aligned}<br>\mathrm{Input} &amp;=[\text {Barack Obama}] [\text{MASK}] [place\  of\  birth] [country]  \\<br>\mathrm{Label} &amp;=[\text{USA}]<br>\end{aligned}<br>$$<br>其中$[\cdot]$ 代表输入的元素.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/ppke2.jpg" style="zoom: 33%;" /><p>与上图一致, PPKE使用Transformer Encoder对图上下文信息编码, 假设$e$ 为被Mask的实体, $\mathbf{T}^{[\mathrm{MASK}]}$ 代表$[\text{MASK}]$ 位置上的隐态输出, 那么目标就是最大化$[\text{MASK}]$ 位置的输出分类概率:<br>$$<br>\begin{aligned}<br>\boldsymbol{p}^{[\mathrm{MASK}]} &amp;=\operatorname{softmax}\left(\mathbf{T}^{[\mathrm{MASK}]} \cdot \mathbf{V}^{\mathcal{E}}\right) \\<br>\mathcal{L} &amp;=-\log \boldsymbol{p}_{e}^{[\mathrm{MASK}]}<br>\end{aligned}<br>$$<br>其中$\boldsymbol{p}^{[\mathrm{MASK}]} $ 为实体词表中的候选实体的概率向量, $\mathbf{V}^{\mathcal{E}}$ 为实体的Embedding矩阵.</p><p>然后用极大似然来优化模型参数:<br>$$<br>\hat{\boldsymbol{\theta}}=\underset{\boldsymbol{\theta}}{\operatorname{argmax}} \sum_{\left(\boldsymbol{x}, \boldsymbol{m}_{e}\right) \in \mathcal{X}} \log p\left(\boldsymbol{x}_{e} \mid \boldsymbol{x} \circ \boldsymbol{m}_{e} ; \boldsymbol{\theta}\right)<br>$$</p><h3 id="Fine-tuning"><a href="#Fine-tuning" class="headerlink" title="Fine - tuning"></a>Fine - tuning</h3><h4 id="Link-Prediction"><a href="#Link-Prediction" class="headerlink" title="Link Prediction"></a>Link Prediction</h4><p>链接预测的目标是预测三元组中缺失的头实体$h$ 或者尾实体$t$. 即预测$((h, r, ?) \rightarrow t)$, 或者$((?, r, t) \rightarrow h)$.</p><p>详细的例子如下:<br>$$<br>\begin{aligned}<br>\text { Input }_{\text {head }}&amp;=[\text{MASK}] \text {[USA]}[country\ of\ citizenship] \\<br>\text { Label }_{\text {head }}&amp;=[\text {Barack Obama}] \\<br>\text { Input }_{\text {tail }}&amp;=[\text {Barack Obama}] \text {[MASK]} [country\ of\ citizenship]\\<br>\text { Label }_{\text {tail }}&amp;=[\text{USA}]<br>\end{aligned}<br>$$<br>对于这类任务, 给定训练集$\mathcal{D}_{x e}$, 训练目标是最大化<strong>给定数据集</strong>中由输入预测出实体$e$ 的概率:</p><p>$$<br>\hat{\boldsymbol{\theta}}_{\boldsymbol{x} \rightarrow \boldsymbol{e}}=\underset{\boldsymbol{\theta}_{\boldsymbol{x} \rightarrow e}}{\operatorname{argmax}} \sum_{\boldsymbol{x} \in \mathcal{D}_{\boldsymbol{x} e}} \log p\left(e \mid \boldsymbol{x} ; \boldsymbol{\theta}_{\boldsymbol{x} \rightarrow \boldsymbol{e}}\right)<br>$$</p><h4 id="Relation-Prediction"><a href="#Relation-Prediction" class="headerlink" title="Relation Prediction"></a>Relation Prediction</h4><p>与Link Prediction类似, 任务由预测三元组中的头尾实体变为了预测它们之间的关系$r$,  即$((h, ?, t) \rightarrow r)$. 例如:<br>$$<br>\begin{aligned}<br>\text { Input }_{\text {rel }}&amp;=[\text {Barack Obama}][\text {USA}] \text {[MASK]} \\<br>\text { Label }_{\text {rel }}&amp;=[country\ of\ citizenship]<br>\end{aligned}<br>$$<br>同样的, 给定训练集$\mathcal{D}_{\boldsymbol{x} \boldsymbol{r}}$, 目标是最大化给定数据集中由输入预测出关系$r$ 的概率:<br>$$<br>\hat{\boldsymbol{\theta}}_{\boldsymbol{x} \rightarrow \boldsymbol{r}}=\underset{\boldsymbol{\theta}_{\boldsymbol{x} \rightarrow r}}{\operatorname{argmax}} \sum_{\boldsymbol{x} \in \mathcal{D}_{\boldsymbol{x} r}} \log p\left(r \mid \boldsymbol{x} ; \boldsymbol{\theta}_{\boldsymbol{x} \rightarrow \boldsymbol{r}}\right)<br>$$</p><blockquote><p>所以针对任务和数据集进行Fine - tuning时, 作者使用的全部是<strong>三元组</strong>.</p></blockquote><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>作者主要针对FB15k, FB15k - 237, WN18RR这三个数据集做了实验. </p><h3 id="Settings"><a href="#Settings" class="headerlink" title="Settings"></a>Settings</h3><p>在预训练的阶段, 作者只使用了两跳关系组成的组合$(h, r_1, r_2, t)$, 称为<strong>四元组</strong>进行预训练. 受制于计算资源的限制, 作者在FB15k - 237和FB15k上只随机选取了一部分作为数据集. 其余的参数设置请参见原论文.</p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><h4 id="Link-Prediction-1"><a href="#Link-Prediction-1" class="headerlink" title="Link Prediction"></a>Link Prediction</h4><p>在FB15k - 237和WN18RR上的Link Prediction结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/ppke3.jpg" style="zoom:33%;" /><p>使用的Baseline比较有代表性, 也包括了之前的工作CoKE. PPKE取得了不错的效果, 尤其是在WN18RR上表现比较好. </p><blockquote><p>当然我觉得可能还有来自于训练方式上的收益, 预训练可能会带来更高的精度.</p></blockquote><h4 id="Relation-Prediction-1"><a href="#Relation-Prediction-1" class="headerlink" title="Relation Prediction"></a>Relation Prediction</h4><p>作者在FB15k和FB15k - 237上的Relation Prediction结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/ppke4.jpg" style="zoom:33%;" /><p>能够看到, 作者给出的Baseline只在FB15k上有结果, 而FB15k - 237上只与CoKE做了对比. 预测数据集中的关系比预测实体要简单的多. 所以CoKE和PPKE的差距并不是很大.</p><h3 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h3><h4 id="Ablation-study"><a href="#Ablation-study" class="headerlink" title="Ablation study"></a>Ablation study</h4><p>为探究PPKE取得好效果的原因, 作者将CoKE, 预训练和未预训练的PPKE做了对比:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/ppke5.jpg" style="zoom: 50%;" /><p>如果只看MRR和Hits@10的话, 不使用预训练的PPKE比CoKE的结果还要差一些, 结果大致相同, 作者认为差异来自于参数差异. 但是引入预训练后, PPKE的性能有很大提升.</p><h4 id="Visual-Illustration"><a href="#Visual-Illustration" class="headerlink" title="Visual Illustration"></a>Visual Illustration</h4><p>同样使用了类似CoKE的实验手法, PPKE也是用T - SNE做了可视化:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/ppke6.jpg" style="zoom:33%;" /><p>作者将预训练过程中<strong>头尾实体相同</strong>的<strong>三元组</strong>和<strong>四元组</strong>做采样, 将尾实体Mask掉后让<strong>已经预训练好的模型</strong>去预测尾实体, 然后对得到的隐态输出做降维可视化.</p><p>不同的颜色代表不同的采样组, 相同的颜色中的点拥有一致的头尾实体, 圆点代表三元组预测得出的隐态输出, 倒三角代表四元组预测得出的隐态输出.</p><p>在大多数的采样组中, <strong>聚合度</strong>都很高, 意味着PPKE无论对于三元组和四元组的分类结果都比较好, 包含路径的知识已经被注入到参数中.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>PPKE总体上来说是CoKE的<strong>延续</strong>, 与CoKE差距很小. 其主要贡献是将<strong>预训练思想</strong>引入了KGE领域中(其实这点很重要), 分为预训练和Fine - tuning两个阶段.</p><p>总感觉在预训练和精调时候使用的数据输入模式不太一样有点怪怪的, 但BERT也是一个怎么使用都可以的结构, 再加上作者已经对三元组和四元组输入都做了实验, PPKE能起作用也不足为奇了.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Argparse和Logging</title>
      <link href="/posts/27666.html"/>
      <url>/posts/27666.html</url>
      
        <content type="html"><![CDATA[<h1 id="Argparse和Logging"><a href="#Argparse和Logging" class="headerlink" title="Argparse和Logging"></a>Argparse和Logging</h1><p><strong>Argparse</strong>和<strong>Logging</strong>是Python实验中常用的两个模块. 之前没有整理过, 特此整理.</p><h2 id="Argparse"><a href="#Argparse" class="headerlink" title="Argparse"></a>Argparse</h2><p>Argparse是用来<strong>解析Python命令行</strong>的<strong>标准库</strong>. </p><h3 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h3><p>大致使用框架如下:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> argparseparser <span class="token operator">=</span> argparse<span class="token punctuation">.</span>ArgumentParser<span class="token punctuation">(</span>description<span class="token operator">=</span><span class="token string">"This is the description for this python script"</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">'''Add some arugments...'''</span>args <span class="token operator">=</span> parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在命令行中调用脚本时可以<strong>附加参数</strong>:</p><pre class="line-numbers language-sh"><code class="language-sh">python argparse_test.py -n anning<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>总体来说, 流程如下:</p><ol><li>需要导入<code>Argparse</code>.</li><li>创建一个<code>parser</code>, 并对其添加各种参数.</li><li>使用<code>parse_args()</code>解析参数.</li><li>使用Python命令行时可以附加相应的参数.</li></ol><h4 id="添加参数"><a href="#添加参数" class="headerlink" title="添加参数"></a>添加参数</h4><p>添加参数统一使用函数<code>parser.add_argument()</code>, 在该函数中可以设置参数的各种<strong>属性</strong>和<strong>约束</strong>. 在下文会逐一说明. </p><h4 id="获取参数"><a href="#获取参数" class="headerlink" title="获取参数"></a>获取参数</h4><p>设定的argument可以作为属性, 在<strong>解析参数后</strong>, 使用<code>args.argument_name</code>来获取.</p><h3 id="位置参数"><a href="#位置参数" class="headerlink" title="位置参数"></a>位置参数</h3><p>位置参数当然是<strong>必选</strong>的参数, 在启动Python脚本时必须按照<strong>位置</strong>依次输入. 例如:</p><pre class="line-numbers language-python"><code class="language-python">parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'name'</span><span class="token punctuation">,</span> type<span class="token operator">=</span>str<span class="token punctuation">,</span> help<span class="token operator">=</span><span class="token string">'Your name'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>添加了参数<code>name</code>, 并将其设置为<code>str</code>类型的变量.</p><h3 id="可选参数"><a href="#可选参数" class="headerlink" title="可选参数"></a>可选参数</h3><p>顾名思义, 在执行脚本时可选参数可能会被用户附加, 也有可能不附加. 参数的添加方式与位置参数类似, 必须在参数名前加<code>-</code>. 约定上<code>-</code>后应只跟单个字母, 为某个参数的<strong>缩写</strong>, <code>--</code>后跟参数的<strong>全拼</strong>. 例如:</p><pre class="line-numbers language-python"><code class="language-python">parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'-n'</span><span class="token punctuation">,</span> type<span class="token operator">=</span>str<span class="token punctuation">,</span> help<span class="token operator">=</span><span class="token string">'Your name'</span><span class="token punctuation">)</span>parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--age'</span><span class="token punctuation">,</span> type<span class="token operator">=</span>int<span class="token punctuation">,</span> help<span class="token operator">=</span><span class="token string">'Your age'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>添加了两个可选参数<code>n</code>和<code>age</code>, 分别为<code>str</code>和<code>int</code>类型.</p><p>当然, 想要同时将缩写和全拼指向同一个变量也是可以的:</p><pre class="line-numbers language-python"><code class="language-python">parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'-a'</span><span class="token punctuation">,</span> <span class="token string">'--age'</span><span class="token punctuation">,</span> type<span class="token operator">=</span>int<span class="token punctuation">,</span> help<span class="token operator">=</span><span class="token string">'Your age'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>此时, 你既可以通过<code>-a</code>, 也可以通过<code>--age</code>来定义年龄.</p><blockquote><p>如果你同时指定了参数的缩写和全拼, 那么在访问该参数值时, 必须通过<strong>全拼</strong>来访问.</p></blockquote><h3 id="默认值和类型"><a href="#默认值和类型" class="headerlink" title="默认值和类型"></a>默认值和类型</h3><p><code>default</code>设定字段的<strong>默认值</strong>, <code>type</code>可以指定附加参数后将其转化为什么<strong>数据类型</strong>. 例如:</p><pre class="line-numbers language-python"><code class="language-python">parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--name'</span><span class="token punctuation">,</span> type<span class="token operator">=</span>str<span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token string">'anning'</span><span class="token punctuation">,</span> help<span class="token operator">=</span><span class="token string">'Your name'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>添加了一个参数<code>name</code>, 当命令行中不显式声明时缺省值为<code>&#39;anning&#39;</code>, 类型为字符串.</p><h3 id="别名"><a href="#别名" class="headerlink" title="别名"></a>别名</h3><p><code>dest</code>可以指定该参数所对应的<code>args</code>的<strong>别名</strong>, 在Python中获取该参数可以通过指定的别名来获取. 例如:</p><pre class="line-numbers language-python"><code class="language-python">parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--name'</span><span class="token punctuation">,</span> dest<span class="token operator">=</span><span class="token string">'user_name'</span><span class="token punctuation">,</span> type<span class="token operator">=</span>str<span class="token punctuation">,</span> help<span class="token operator">=</span><span class="token string">'Your name'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在没指定别名时, 访问参数<code>name</code>可以通过<code>args.name</code>获取, 而指定别名后, 只能通过<code>args.user_name</code>来获取.</p><h3 id="必须参数"><a href="#必须参数" class="headerlink" title="必须参数"></a>必须参数</h3><p><code>required</code>设定参数是否<strong>必须</strong>填入. 否则会提示该参数没有指定, 因此可以将可选参数转化为必选参数.</p><h3 id="动作"><a href="#动作" class="headerlink" title="动作"></a>动作</h3><p><code>action</code>一般常用于可选参数, 例如有参数<code>--verbose</code>时, 该属性设置为True:</p><pre class="line-numbers language-python"><code class="language-python">parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--verbose'</span><span class="token punctuation">,</span> action<span class="token operator">=</span><span class="token string">'store_true'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>此时, 如果在调用脚本时加上了<code>--verbose</code>, 那么在解析后, 对应的<code>args.verbose</code>就为<code>True</code>.</p><p>当然这只是<code>action</code>其中一种用法, 更多请参见节尾处的链接.</p><h3 id="值约束"><a href="#值约束" class="headerlink" title="值约束"></a>值约束</h3><p><code>choice</code>可以设定用户添加参数时, 参数的<strong>取值范围</strong>. 例如:</p><pre class="line-numbers language-python"><code class="language-python">parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--name'</span><span class="token punctuation">,</span>choice<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'anning'</span><span class="token punctuation">,</span> <span class="token string">'daning'</span><span class="token punctuation">]</span> type<span class="token operator">=</span>str<span class="token punctuation">,</span> help<span class="token operator">=</span><span class="token string">'Your name'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>那么用户只能在<code>&#39;anning&#39;</code>和<code>&#39;daning&#39;</code>之间选择<code>name</code>.</p><h3 id="多参数设定"><a href="#多参数设定" class="headerlink" title="多参数设定"></a>多参数设定</h3><p><code>nargs=&#39;N&#39;</code>可以将命令行的N个参数汇总到一个列表中, <code>nargs=&#39;+&#39;</code>或<code>nargs=&#39;*&#39;</code>能将<strong>所有当前参数汇聚到一个列表中</strong>. 例如:</p><pre class="line-numbers language-python"><code class="language-python">parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--salary'</span><span class="token punctuation">,</span> dest<span class="token operator">=</span><span class="token string">'salary'</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> nargs<span class="token operator">=</span><span class="token string">"+"</span><span class="token punctuation">,</span> type<span class="token operator">=</span>int<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在调用脚本时, 能够传入多个符合条件的值, 它们将以<strong>列表</strong>的形式共同存在.</p><h3 id="互斥参数组"><a href="#互斥参数组" class="headerlink" title="互斥参数组"></a>互斥参数组</h3><p>有的时候不希望用户按照自己想象之外的使用方法传入参数, 就需要用到<strong>互斥参数</strong>. 例如:</p><pre class="line-numbers language-python"><code class="language-python">ab_group <span class="token operator">=</span> parser<span class="token punctuation">.</span>add_mutually_exclusive_group<span class="token punctuation">(</span><span class="token punctuation">)</span>ab_group<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'-a'</span><span class="token punctuation">)</span>ab_group<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'-b'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>那么, 在调用Python脚本时, 两个参数是不能同时启用的, 否则会报错.</p><h3 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h3><p><code>var(args)</code>能将解析好后的内容直接转为<strong>字典</strong>.</p><blockquote><p>在使用Notebook时, 与<code>argparse</code>相关的代码以<code>.py</code>形式存在, 不方便直接从里面把参数扒下来, 而模型必须使用<code>args</code>初始化. </p><p>这时可以使用<code>pickle</code>库将<code>args</code>直接以对象的形式<strong>保存</strong>下来, 然后再使用<code>pickle.load</code>在Notebook中重新读取出来, 再对模型初始化. 示例:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> pickle<span class="token comment" spellcheck="true"># 保存</span><span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">'args.pkl'</span><span class="token punctuation">,</span> <span class="token string">'wb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>   pickle<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>args<span class="token punctuation">,</span> f<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 读取</span><span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">'args.pkl'</span><span class="token punctuation">,</span> <span class="token string">'rb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>   args <span class="token operator">=</span> pickle<span class="token punctuation">.</span>load<span class="token punctuation">(</span>f<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><p>上述列举的只是我遇到的关于Argparse的用法, 可能说的比较碎而且不全面. 更多内容请参见<a href="https://docs.python.org/3/library/argparse.html" target="_blank" rel="noopener">Python官方文档</a>和<a href="http://blog.xiayf.cn/2013/03/30/argparse/" target="_blank" rel="noopener">argparse - 命令行选项与参数解析（译）</a>.</p><h2 id="Logging"><a href="#Logging" class="headerlink" title="Logging"></a>Logging</h2><p>Logging可以记载Python脚本运行的过程, 即记录<strong>日志信息</strong>. 其实实验中用到的Logging非常简单.</p><h3 id="Logger"><a href="#Logger" class="headerlink" title="Logger"></a>Logger</h3><h4 id="简单配置"><a href="#简单配置" class="headerlink" title="简单配置"></a>简单配置</h4><p>如果我们只是想简单的记录日志, 不考虑程序的后续维护问题, 那么我们只需要简单的进行<code>logger</code>的初始化. 例如:</p><pre class="line-numbers language-python"><code class="language-python">logging<span class="token punctuation">.</span>basicConfig<span class="token punctuation">(</span>level<span class="token operator">=</span>logging<span class="token punctuation">.</span>DEBUG<span class="token punctuation">,</span>                    filename<span class="token operator">=</span><span class="token string">'output.log'</span><span class="token punctuation">,</span>                    datefmt<span class="token operator">=</span><span class="token string">'%Y/%m/%d %H:%M:%S'</span><span class="token punctuation">,</span>                    format<span class="token operator">=</span><span class="token string">'%(asctime)s - %(name)s - %(levelname)s - %(lineno)d - %(module)s - %(message)s'</span><span class="token punctuation">)</span>logger <span class="token operator">=</span> logging<span class="token punctuation">.</span>getLogger<span class="token punctuation">(</span><span class="token string">"process_name"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="basicConfig参数"><a href="#basicConfig参数" class="headerlink" title="basicConfig参数"></a>basicConfig参数</h4><ul><li><strong>filename</strong>: 即日志输出的文件名, 如果指定了这个信息之后, 实际上会启用 FileHandler, 而不再是 StreamHandler, 这样日志信息便会输出到文件中了. </li><li><strong>filemode</strong>: 这个是指定日志文件的写入方式, 有两种形式, 一种是<code>w</code>, 一种是<code>a</code>, 分别代表清除后写入和追加写入. </li><li><strong>format</strong>: 指定日志信息的输出格式, 详细格式会在后面补出.</li><li><strong>datefmt</strong>: 指定时间的输出格式. </li><li><strong>style</strong>: 如果 format 参数指定了, 这个参数就可以指定格式化时的占位符风格, 如 %、{、$ 等. </li><li><strong>level</strong>: 指定日志输出的类别, 程序会输出大于等于此级别的信息. </li><li><strong>stream</strong>: 在没有指定 filename 的时候会默认使用 StreamHandler, 这时 stream 可以指定初始化的文件流. </li><li><strong>handlers</strong>: 可以指定日志处理时所使用的 Handlers, 必须是可迭代的. </li></ul><h4 id="配置文件配置"><a href="#配置文件配置" class="headerlink" title="配置文件配置"></a>配置文件配置</h4><p>一般情况下, 为了把配置写活, 人们都会将配置写入<strong>配置文件</strong>, 在记录日志的时候, 读取配置文件中的配置, 方便管理.</p><p>无论以何种方式读取文件, 例如<code>yaml</code>, <code>json</code>, 只要读取成Python的字典, 就可以完成初始化, 例如:</p><pre><code>config_dict = json.load(config_path)logging.config.dictConfig(config_dict)logger = logging.getLogger(&quot;process_name&quot;)</code></pre><p>在这里, 以<code>json</code>作为示范, 在配置文件中写入的内容有:</p><pre class="line-numbers language-json"><code class="language-json"><span class="token punctuation">{</span>    <span class="token property">"version"</span><span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>    <span class="token property">"disable_existing_loggers"</span><span class="token operator">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span>    <span class="token property">"formatters"</span><span class="token operator">:</span> <span class="token punctuation">{</span>        <span class="token property">"simple"</span><span class="token operator">:</span> <span class="token punctuation">{</span>            <span class="token property">"format"</span><span class="token operator">:</span> <span class="token string">"%(asctime)s - %(name)s - [%(levelname)s] - %(message)s"</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">,</span>    <span class="token property">"handlers"</span><span class="token operator">:</span> <span class="token punctuation">{</span>        <span class="token property">"file_handler"</span><span class="token operator">:</span> <span class="token punctuation">{</span>            <span class="token property">"class"</span><span class="token operator">:</span> <span class="token string">"logging.FileHandler"</span><span class="token punctuation">,</span>            <span class="token property">"level"</span><span class="token operator">:</span> <span class="token string">"DEBUG"</span><span class="token punctuation">,</span>            <span class="token property">"formatter"</span><span class="token operator">:</span> <span class="token string">"simple"</span><span class="token punctuation">,</span>            <span class="token property">"filename"</span><span class="token operator">:</span> <span class="token string">"python_logging.log"</span><span class="token punctuation">,</span>            <span class="token property">"encoding"</span><span class="token operator">:</span> <span class="token string">"utf8"</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">,</span>    <span class="token property">"root"</span><span class="token operator">:</span> <span class="token punctuation">{</span>        <span class="token property">"level"</span><span class="token operator">:</span> <span class="token string">"DEBUG"</span><span class="token punctuation">,</span>        <span class="token property">"handlers"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"file_handler"</span><span class="token punctuation">]</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Level"><a href="#Level" class="headerlink" title="Level"></a>Level</h3><p>日志记录分为五个级别, 分别为<strong>CRITICAL &gt; ERROR &gt; WARNING &gt; INFO &gt; DEBUG</strong>.</p><p>它们分别可以通过下述命令来记录在日志中:</p><pre class="line-numbers language-python"><code class="language-python">logger<span class="token punctuation">.</span>debug<span class="token punctuation">(</span><span class="token string">"mess"</span><span class="token punctuation">)</span>logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">"mess"</span><span class="token punctuation">)</span>logger<span class="token punctuation">.</span>warning<span class="token punctuation">(</span><span class="token string">"mess"</span><span class="token punctuation">)</span>logger<span class="token punctuation">.</span>error<span class="token punctuation">(</span><span class="token string">"mess"</span><span class="token string">"mess"</span><span class="token punctuation">)</span>logger<span class="token punctuation">.</span>critical<span class="token punctuation">(</span><span class="token string">"mess"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>logger</code>中的<code>level</code>参数可以指定程序输出的信息级别.</p><h3 id="Handler"><a href="#Handler" class="headerlink" title="Handler"></a>Handler</h3><p><code>Handler</code>是<strong>处理日志</strong>的方法. 我们可以不使用<code>basicConfig</code>来配置<code>logger</code>, 而是单独对<code>logger</code>指定处理方法:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> logginglogger <span class="token operator">=</span> logging<span class="token punctuation">.</span>getLogger<span class="token punctuation">(</span><span class="token string">"process_name"</span><span class="token punctuation">)</span>logger<span class="token punctuation">.</span>setLevel<span class="token punctuation">(</span>level<span class="token operator">=</span>logging<span class="token punctuation">.</span>INFO<span class="token punctuation">)</span>handler <span class="token operator">=</span> logging<span class="token punctuation">.</span>FileHandler<span class="token punctuation">(</span><span class="token string">'output.log'</span><span class="token punctuation">)</span>formatter <span class="token operator">=</span> logging<span class="token punctuation">.</span>Formatter<span class="token punctuation">(</span><span class="token string">'%(asctime)s - %(name)s - %(levelname)s - %(message)s'</span><span class="token punctuation">)</span>handler<span class="token punctuation">.</span>setFormatter<span class="token punctuation">(</span>formatter<span class="token punctuation">)</span>logger<span class="token punctuation">.</span>addHandler<span class="token punctuation">(</span>handler<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我们常用的是<code>FileHandler</code>, 这样能将日志以<strong>文件</strong>形式导出. 除此以外, 还有如下<code>Handler</code>:</p><ul><li>StreamHandler: logging.StreamHandler, 日志输出到流, 可以是 sys.stderr, sys.stdout 或者文件. </li><li>FileHandler: logging.FileHandler, 日志输出到文件. </li><li>BaseRotatingHandler: logging.handlers.BaseRotatingHandler, 基本的日志回滚方式. </li><li>RotatingHandler: logging.handlers.RotatingHandler, 日志回滚方式, 支持日志文件最大数量和日志文件回滚. </li><li>TimeRotatingHandler: logging.handlers.TimeRotatingHandler, 日志回滚方式, 在一定时间区域内回滚日志文件. </li><li>SocketHandler: logging.handlers.SocketHandler, 远程输出日志到 TCP/IP sockets. </li><li>DatagramHandler: logging.handlers.DatagramHandler, 远程输出日志到 UDP sockets. </li><li>SMTPHandler: logging.handlers.SMTPHandler, 远程输出日志到邮件地址. </li><li>SysLogHandler: logging.handlers.SysLogHandler, 日志输出到 syslog. </li><li>NTEventLogHandler: logging.handlers.NTEventLogHandler, 远程输出日志到 Windows NT/2000/XP 的事件日志. </li><li>MemoryHandler: logging.handlers.MemoryHandler, 日志输出到内存中的指定 buffer. </li><li>HTTPHandler: logging.handlers.HTTPHandler, 通过”GET” 或者”POST” 远程输出到 HTTP 服务器. </li></ul><p>实际上, 我们可以给<code>logger</code>添加多个<code>Handler</code>:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> logging<span class="token keyword">from</span> logging<span class="token punctuation">.</span>handlers <span class="token keyword">import</span> HTTPHandler<span class="token keyword">import</span> syslogger <span class="token operator">=</span> logging<span class="token punctuation">.</span>getLogger<span class="token punctuation">(</span><span class="token string">"process_name"</span><span class="token punctuation">)</span>logger<span class="token punctuation">.</span>setLevel<span class="token punctuation">(</span>level<span class="token operator">=</span>logging<span class="token punctuation">.</span>DEBUG<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># StreamHandler</span>stream_handler <span class="token operator">=</span> logging<span class="token punctuation">.</span>StreamHandler<span class="token punctuation">(</span>sys<span class="token punctuation">.</span>stdout<span class="token punctuation">)</span>stream_handler<span class="token punctuation">.</span>setLevel<span class="token punctuation">(</span>level<span class="token operator">=</span>logging<span class="token punctuation">.</span>DEBUG<span class="token punctuation">)</span>logger<span class="token punctuation">.</span>addHandler<span class="token punctuation">(</span>stream_handler<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># FileHandler</span>file_handler <span class="token operator">=</span> logging<span class="token punctuation">.</span>FileHandler<span class="token punctuation">(</span><span class="token string">'output.log'</span><span class="token punctuation">)</span>file_handler<span class="token punctuation">.</span>setLevel<span class="token punctuation">(</span>level<span class="token operator">=</span>logging<span class="token punctuation">.</span>INFO<span class="token punctuation">)</span>formatter <span class="token operator">=</span> logging<span class="token punctuation">.</span>Formatter<span class="token punctuation">(</span><span class="token string">'%(asctime)s - %(name)s - %(levelname)s - %(message)s'</span><span class="token punctuation">)</span>file_handler<span class="token punctuation">.</span>setFormatter<span class="token punctuation">(</span>formatter<span class="token punctuation">)</span>logger<span class="token punctuation">.</span>addHandler<span class="token punctuation">(</span>file_handler<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># HTTPHandler</span>http_handler <span class="token operator">=</span> HTTPHandler<span class="token punctuation">(</span>host<span class="token operator">=</span><span class="token string">'localhost:8001'</span><span class="token punctuation">,</span> url<span class="token operator">=</span><span class="token string">'log'</span><span class="token punctuation">,</span> method<span class="token operator">=</span><span class="token string">'POST'</span><span class="token punctuation">)</span>logger<span class="token punctuation">.</span>addHandler<span class="token punctuation">(</span>http_handler<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Log</span>logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">'This is a log info'</span><span class="token punctuation">)</span>logger<span class="token punctuation">.</span>debug<span class="token punctuation">(</span><span class="token string">'Debugging'</span><span class="token punctuation">)</span>logger<span class="token punctuation">.</span>warning<span class="token punctuation">(</span><span class="token string">'Warning exists'</span><span class="token punctuation">)</span>logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">'Finish'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Formatter"><a href="#Formatter" class="headerlink" title="Formatter"></a>Formatter</h3><p>在对日志格式化输出时, 可以不借助<code>basicConfig</code>来全局化<strong>输出格式</strong>, 使用<code>Formatter</code>灵活单独配置:</p><pre class="line-numbers language-python"><code class="language-python">logger <span class="token operator">=</span> logging<span class="token punctuation">.</span>getLogger<span class="token punctuation">(</span><span class="token string">"process_name"</span><span class="token punctuation">)</span>logger<span class="token punctuation">.</span>setLevel<span class="token punctuation">(</span>level<span class="token operator">=</span>logging<span class="token punctuation">.</span>WARN<span class="token punctuation">)</span>formatter <span class="token operator">=</span> logging<span class="token punctuation">.</span>Formatter<span class="token punctuation">(</span>fmt<span class="token operator">=</span><span class="token string">'%(asctime)s - %(name)s - %(levelname)s - %(message)s'</span><span class="token punctuation">,</span> datefmt<span class="token operator">=</span><span class="token string">'%Y/%m/%d %H:%M:%S'</span><span class="token punctuation">)</span>handler <span class="token operator">=</span> logging<span class="token punctuation">.</span>StreamHandler<span class="token punctuation">(</span><span class="token punctuation">)</span>handler<span class="token punctuation">.</span>setFormatter<span class="token punctuation">(</span>formatter<span class="token punctuation">)</span>logger<span class="token punctuation">.</span>addHandler<span class="token punctuation">(</span>handler<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>更多的信息输出格式如下:</p><ul><li>%(levelno) s : 打印日志级别的数值.</li><li>%(levelname) s : 打印日志级别的名称. </li><li>%(pathname) s : 打印当前执行程序的路径, 其实就是 sys.argv [0]. </li><li>%(filename) s : 打印当前执行程序名. </li><li>%(funcName) s : 打印日志的当前函数. </li><li>%(lineno) d : 打印日志的当前行号. </li><li>%(asctime) s : 打印日志的时间. </li><li>%(thread) d : 打印线程 ID. </li><li>%(threadName) s : 打印线程名称. </li><li>%(process) d : 打印进程 ID. </li><li>%(processName) s : 打印线程名称. </li><li>%(module) s : 打印模块名称. </li><li>%(message) s : 打印日志信息. </li></ul><p>上述内容也并非全部的使用方法, 参考了<a href="https://docs.python.org/3/library/logging.html" target="_blank" rel="noopener">Python官方文档</a>和<a href="https://cuiqingcai.com/6080.html" target="_blank" rel="noopener">Python 中 logging 模块的基本用法</a>.</p>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RREA: Relational Reflection Entity Alignment</title>
      <link href="/posts/51197.html"/>
      <url>/posts/51197.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>GNN</li></ul></blockquote><h1 id="Relational-Reflection-Entity-Alignment"><a href="#Relational-Reflection-Entity-Alignment" class="headerlink" title="Relational Reflection Entity Alignment"></a>Relational Reflection Entity Alignment</h1><p>本文是论文<a href="http://arxiv.org/abs/2008.07962" target="_blank" rel="noopener">Relational Reflection Entity Alignment</a>的阅读笔记和个人理解. 这是我第一次接触关于<strong>实体对齐</strong>领域的文章.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>整篇文章的出发点都源于作者观察到的两个<strong>反直觉</strong>的现象:</p><ol><li>在实体对齐问题中, GNN中使用<strong>标准线性变换</strong>表现得不好.</li><li>那些在<strong>链接预测</strong>上表现很好的Knowledge Embedding Model在<strong>实体对齐</strong>上表现不好.</li></ol><p>针对上述两个现象, 作者说明GNN中约束变换矩阵的重要性, 并提出了<strong>统一</strong>的实体对齐框架, 将现有的两种主流方法相统一.</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>因为是第一次接触实体对齐领域的文章, 所以我在这里将实体对齐相关的背景补充上.</p><p>实体对齐的目标是检测<strong>多源</strong>Knowledge Graph中的<strong>实体对</strong>是否<strong>等价</strong>, 这对多源知识图谱的<strong>合并</strong>是至关重要的. 通俗一点说, 在两个KG中, 对于同一种实体可能有两种不同的表述, 通过实体对齐能够将两种表述相统一. 在下一节, 我会补充上实体对齐问题的数学描述.</p><h3 id="Translation-Based-Methods"><a href="#Translation-Based-Methods" class="headerlink" title="Translation - Based Methods"></a>Translation - Based Methods</h3><p>基于平移的方法灵感来自于Word Embedding中的跨语言表示方法, 例如TransE等, 在实体对齐领域使用时, 有一个非常重要的假设: 在不同的Knowledge Graph中各实体的相对位置是<strong>相似</strong>的.</p><p>一般分为两个模块, <strong>Translation Module</strong>和<strong>Alignment Module</strong>.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/relationalreflectionentityalignment1.jpg" style="zoom:50%;" /><h4 id="Translation-Module"><a href="#Translation-Module" class="headerlink" title="Translation Module"></a>Translation Module</h4><p>平移模块负责学习到两个KG中相似的实体位置分布, 例如最简单的TransE使用$h+r\approx t$ 来学习位置分布.</p><h4 id="Alignment-Module"><a href="#Alignment-Module" class="headerlink" title="Alignment Module"></a>Alignment Module</h4><p>对齐模块负责将两个KG中的实体做对齐. 在Translation Based Model中, 一般有两种方式:</p><ul><li><p><strong>Mapping</strong>: 使用变换矩阵将两个KG中的实体统一到一个空间中最小化它们的距离. 例如$W e_{1} \approx e_{2}$ 或 $W_{1} e_{1} \approx W_{2} e_{2}$.</p></li><li><p><strong>Sharing</strong>: 更为激进, 直接<strong>共享</strong>两个KG中实体的Embedding. 例如在MTransE中通过最小化$||\boldsymbol{e}_{1}-\boldsymbol{e}_{2}||$ 达到共享的效果.</p></li></ul><h3 id="GNNs-Based-Methods"><a href="#GNNs-Based-Methods" class="headerlink" title="GNNs - Based Methods"></a>GNNs - Based Methods</h3><p>受<strong>孪生网络</strong>启发, 大多方法使用两个多层的GNN编码, 再加上额外的损失函数. 与基于平移的方法不同, 基于平移的方法使用独立的三元组, 而缺少了实体和关系的全局信息, GNN摆脱了三元组的束缚, 依靠<strong>聚合</strong>从邻居节点获取实体的Embedding.</p><p>在获取完不同KG中的实体Embedding后, 采用Contrastive Loss或Triplet Loss来对齐实体:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/relationalreflectionentityalignment2.jpg" style="zoom:50%;" /><p>许多基于GNN的方法中经常约束变换矩阵为<strong>对角阵</strong>或<strong>单位阵</strong>, 但从未交代原因.</p><h2 id="Preliminary"><a href="#Preliminary" class="headerlink" title="Preliminary"></a>Preliminary</h2><p>本节描述实体对齐任务的数学描述, 并介绍后续实验中使用的数据集, 为接下来的内容做铺垫.</p><h3 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h3><p>一般的KG都定义为$G=(E,R,T)$, 其中$E,R,T$ 分别代表实体, 关系, 和三元组的集合.</p><p>实体对齐的目标是找到两个不同源的KG中的等价的实体对, 例如$G_1, G_2$ 分别为两个不同源的KG, 对齐的实体对表示为:<br>$$<br>P=\left\{\left(e_{i_{1}}, e_{i_{2}}\right) \mid e_{i_{1}} \in E_{1}, e_{i_{2}} \in E_{2}\right\}_{i=1}^{p}<br>$$<br>模型应该能够利用已知的对齐好的实体来预测新的实体对.</p><h3 id="DataSets"><a href="#DataSets" class="headerlink" title="DataSets"></a>DataSets</h3><p>在后续的实验中, 作者主要使用了两种数据集:</p><ul><li>DBP15K: <strong>跨语言</strong>数据集, 其中含有中文(ZH), 英文(EN), 日语(JA), 法语(FR)的数据. 包括三种跨语言的对齐实体对, 分别是ZH - EN, JA - EN, FR - EN.</li><li>DWY100K: <strong>跨KG</strong>数据集, 它是从DBpedia, Wikidata, YAGO3中抽取出来的. 包含两个不同源KG的对齐实体对集, 分别是DBpedia - Wikidata, DBpedia - YAGO3.</li></ul><p>具体信息如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/relationalreflectionentityalignment3.jpg" style="zoom: 33%;" /><h2 id="Unified-Entity-Alignment-Framework"><a href="#Unified-Entity-Alignment-Framework" class="headerlink" title="Unified Entity Alignment Framework"></a>Unified Entity Alignment Framework</h2><p>在本节中, 作者抽取了基于平移的模型和基于GNN的共性, 将二者统一到一个框架下.</p><h3 id="Shape-Builder-amp-Alignment"><a href="#Shape-Builder-amp-Alignment" class="headerlink" title="Shape - Builder &amp; Alignment"></a>Shape - Builder &amp; Alignment</h3><h4 id="Shape-Builder"><a href="#Shape-Builder" class="headerlink" title="Shape - Builder"></a>Shape - Builder</h4><p>Shape - Builder的主要任务是将随机初始化的实体分布调整到合适形状的分布. 例如上图中, 其实任何Knowledge Embedding Model都可以做为Shape - Builder. 但是仍然要注意<strong>前提条件</strong>, <strong>等价实体在不同空间中具有相似的分布</strong>.</p><h4 id="Alignment"><a href="#Alignment" class="headerlink" title="Alignment"></a>Alignment</h4><p>当<strong>空间相似性</strong>保持时, 就能够使用Mapping之类的手段对齐实体:</p><p>$$<br>\min _{W} \sum_{\left(e_{i}, e_{j}\right) \in P}||\boldsymbol{W} \boldsymbol{h}_{e_{i}}-\boldsymbol{h}_{e_{j}}||<br>$$</p><p>其中$(e_i, e_j)$ 是已知对齐的实体对. $\boldsymbol{h}_{e_i}$ 代表实体$e_i$ 的Embedding.</p><p>但是, 当变换矩阵$\boldsymbol{W}$ 不加以任何约束时, 空间相似性是极其容易被<strong>破坏</strong>的. 除非$\boldsymbol{W}$ 能够满足<strong>正交性</strong>来保证空间中的实体只是经过旋转操作, 而不改变它们之间的相对位置.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/relationalreflectionentityalignment4.jpg" style="zoom:50%;" /><p>在上图中有:</p><ul><li>(a): MTransE没有约束变换矩阵$\boldsymbol{W}$, 所以在对齐后产生了一些混乱.</li><li>(b): OTEA对矩阵加以正交约束, 比较好的对齐了实体.</li><li>(c): JAPE直接对两个Knowledge Graph使用相同的实体嵌入, 最后再做Shape - Building操作, 也取得了比较好的效果.</li></ul><h3 id="GNNs-Based-Methods-Are-Also-Subject-to-Unified-Framework"><a href="#GNNs-Based-Methods-Are-Also-Subject-to-Unified-Framework" class="headerlink" title="GNNs - Based Methods Are Also Subject to Unified Framework"></a>GNNs - Based Methods Are Also Subject to Unified Framework</h3><h4 id="GNN-Based-Methods"><a href="#GNN-Based-Methods" class="headerlink" title="GNN Based Methods"></a>GNN Based Methods</h4><p>GNN的循环迭代由<strong>聚合</strong>和<strong>更新</strong>两部分组成:</p><p><strong>聚合</strong>:<br>$$<br>h^l_{N_{e_{i}}^{e}} \leftarrow \text { Aggregate }\left(\left\{\boldsymbol{h}_{e_{k}}^{l}, \forall e_{k} \in\left\{e_{i}\right\} \cup \mathcal{N}_{e_{i}}^{e}\right\}\right)<br>$$</p><p>聚合时包括了节点自身到自身的特征, 即视为节点有一条自己到自己的<strong>闭环</strong>.</p><p>聚合方式$\text{Aggregate}$有多种, 在GCN中是求平均, 在GAT中是加权求和, 不详细展开说了.</p><p><strong>更新</strong>:<br>$$<br>h_{e_{i}}^{l+1} \leftarrow \sigma\left(\boldsymbol{W}^{l} \cdot h_{\mathcal{N}_{e_{i}}^{e}}^{l}\right)<br>$$<br>对邻居节点信息使用第$l$ 层的变换矩阵$\boldsymbol{W}^l$, 能获得更好的节点表示.</p><p><strong>损失函数</strong>:</p><p>基于GNN实体对齐方法的损失函数能使等价实体更近, 让不相关的实体更远:<br>$$<br>L=\sum_{\left(e_{i}, e_{j}\right) \in P} \mathop{\underline{\lVert\boldsymbol{h}_{e_{i}}-\boldsymbol{h}_{e_{j}}\rVert}}<br>\limits_\text {alignment}<br>+<br>\sum_{\left(e_{i}^{\prime}, e_{j}^{\prime}\right) \in P^{\prime}} \max \left(<br>\mathop{<br>\underline{\lVert\boldsymbol{h}_{e_{i}^{\prime}} - \boldsymbol{h}_{e_{j}^{\prime}}\rVert+\lambda}}\limits_{\text {apart}}, 0<br>\right)<br>$$</p><p>其中$\lambda$ 为间距, $(e_i^\prime, e_j^\prime)$ 代表从$(e_i, e_j)$ 中随机替换一个实体的负样本实体对. </p><p>有了Translation Based Models的基础, 这里能很容易的发现, 损失中的第一项$\lVert\boldsymbol{h}_{e_{i}}-\boldsymbol{h}_{e_{j}}\rVert$ 正巧和作者前面提到的Sharing Aligment一致, 而第二项$\lVert\boldsymbol{h}_{e_{i}^{\prime}} - \boldsymbol{h}_{e_{j}^{\prime}}\rVert+\lambda$恰巧和前面提到的Shape - Builder作用相同. 因此基于GNN的方法也可以视为作者提出的统一框架中的一种. 假设基于GNN的方法也属于作者提出的统一框架的一种, 那么经过GNN进行实体对齐后, 一定能保留量KG中实体对的相似性. 下面作者就要通过实验来证明该猜想.</p><h4 id="Visual-Experiment"><a href="#Visual-Experiment" class="headerlink" title="Visual Experiment"></a>Visual Experiment</h4><p>作者使用最简单的GCN - Align, 保留Triplet Loss, 将GCN从监督模型转化为<strong>自监督</strong>模型:<br>$$<br>L_{\text {apart}}=\sum_{\left(e_{i}^{\prime}, e_{j}^{\prime}\right) \in P^{\prime}} \max \left(\lambda-\lVert\boldsymbol{h}_{e_{i}^{\prime}}-\boldsymbol{h}_{e_{j}^{\prime}}\rVert_{1}, 0\right)<br>$$<br>用T - SNE对两个法语和英语的多个实体对齐后的结果进行<strong>降维可视化</strong>:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/relationalreflectionentityalignment5.jpg" style="zoom:50%;" /><p>经过GCN对齐后的实体分布确实有很大的<strong>相似性</strong>.</p><h4 id="Quantitative-Experiment"><a href="#Quantitative-Experiment" class="headerlink" title="Quantitative Experiment"></a>Quantitative Experiment</h4><p>光有可视化分析还不够, 作者希望进行量化实验. 作者提出了Shape Similarity来衡量两个Knowledge Graph中任意两实体构成实体对后的相对距离的<strong>差距</strong>:</p><p>$$<br>SS=\frac{\sum_{\left(e_i, \tilde{e_i}\right) \in P} \sum_{\left(e_j, \tilde{e_j}\right) \in P}dist(e_i, e_j) - dist(\tilde{e_i}, \tilde{e_j})}{\sum_{\left(e_i^{\prime}, \tilde{e_i}^{\prime}\right) \in P^{\prime}} \sum_{\left(e_j^{\prime}, \tilde{e_j}^{\prime}\right) \in P^{\prime}}dist(e_i^{\prime}, e_j^{\prime}) - dist(\tilde{e_i}^{\prime}, \tilde{e_j}^{\prime})}<br>$$</p><p>其中, $e_i, e_j \in G_1, \tilde{e_i}, \tilde{e_j} \in G_2$. $e_i, e_j$ 为$G_1$ 中的任意两实体构成的实体对, $\tilde{e_i}, \tilde{e_j}$ 为$G_2$ 中相应的实体构成的实体对. $({e_i}^{\prime}, \tilde{e_i}^{\prime}, {e_j}^{\prime}, \tilde{e_j}^{\prime})$ 代表$({e_i} , \tilde{e_i} , {e_j} , \tilde{e_j} )$ 中随机替换一个负例实体后的四元组. 故$P$ 代表有效的对齐实体对, $P^{\prime}$ 代表无效的负采样的负例实体对. </p><p>因此, 分子为对齐实体之间的相对距离差, 分母为随机实体对之间的距离差. 如果两个KG中的实体分布相似, 分子应该尽可能的小, 分母尽可能的大.</p><p>作者测试了随机分布, TransE, GCN(未训练), GCN对齐后的结果:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/relationalreflectionentityalignment6.jpg" style="zoom:50%;" /><p>这符合作者的预先设想, 随机和未训练的GCN对齐后的余弦距离都非常大, 接近于1. TransE效果比GCN - Align要稍差一些.</p><h3 id="Why-Linear-Transformation-Not-Work"><a href="#Why-Linear-Transformation-Not-Work" class="headerlink" title="Why Linear Transformation Not Work"></a>Why Linear Transformation Not Work</h3><p>基于前文提到的假设, 作者开始对标准线性变换在GNN中不Work的原因分析. </p><p>对于$\boldsymbol{W}$ 加以正交约束或不加约束实际上对应着Translation Based Model中的两种情况:</p><ul><li>当$\boldsymbol{W}$ 为正交矩阵或者单位矩阵$\boldsymbol{I}$ 时, 本质上就等价于Translation Based Model中的<strong>Sharing</strong> Alignment. </li><li>当$\boldsymbol{W}$ 不加以约束时, 本质上等价于Translation Based Model中的<strong>Mapping</strong> Alignment. 这可能会<strong>破坏</strong>掉数据集中的空间分布相似性.</li></ul><h4 id="Experiment-on-GCN-Align"><a href="#Experiment-on-GCN-Align" class="headerlink" title="Experiment on GCN - Align"></a>Experiment on GCN - Align</h4><p>作者使用最简单的GCN在实体对齐任务上, 对加以各类约束和不加约束的设置做了实验, 采用Loss来保证变换矩阵$\boldsymbol{W}$ 的<strong>正交</strong>性:<br>$$<br>L_{o}=\lVert\boldsymbol{W}^{T} \boldsymbol{W}-\boldsymbol{I}\rVert_{2}^{2}<br>$$</p><p>实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/relationalreflectionentityalignment7.jpg" style="zoom:50%;" /><p>在加以正交约束后, 将近涨了10个点. 这足以说明作者的假设成立. 更一般的, 作者发现, 使用更加简单的<strong>单位阵</strong>比正交矩阵的效果要稍好一些.</p><h4 id="Experiment-on-Complex-GNNs"><a href="#Experiment-on-Complex-GNNs" class="headerlink" title="Experiment on Complex GNNs"></a>Experiment on Complex GNNs</h4><p>作者认为对更加复杂的GNN方法做探究也是有必要的, 所以作者也测试了其他GNN方法在使用单位阵或正交阵作为$\boldsymbol{W}$ 的性能:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/relationalreflectionentityalignment8.jpg" style="zoom:50%;" /><p>对于更加复杂的方法, 加以约束仍然能取得更好的效果, 使用单位阵作为$\boldsymbol{W}$ 的结果总是比单纯正交约束要好一些. 作者猜测复杂的方法中所包含的变换矩阵更多, 使得正交约束的优化有些困难.</p><h3 id="Why-Advanced-KG-Embedding-Not-Work"><a href="#Why-Advanced-KG-Embedding-Not-Work" class="headerlink" title="Why Advanced KG Embedding Not Work"></a>Why Advanced KG Embedding Not Work</h3><p>作者将在链接预测上表现比较好的模型在实体对齐中做了测试:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/relationalreflectionentityalignment9.jpg" style="zoom:50%;" /><p>很多在链接预测中表现很好的KGE模型, 在实体对齐中效果很差, 至少比简单的TransE效果差17%, 而基于GNN的方法比GCN至少差3%. 作者将它们汇总在下表中:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/relationalreflectionentityalignment10.jpg" style="zoom:50%;" /><p>其中$\mid\mid$ 代表Concat, $\ast, \omega$ 分别代表卷积操作和卷积核. $d_i$ 代表实体$e_i$ 的度(GCN中需要用到度矩阵来衡量节点的重要性).</p><p>KGE方法本质上的思想都是<strong>将实体嵌入转换为特定的关系嵌入</strong>.</p><blockquote><p>对于这句话, 我认为可以这样理解, 在列出的模型中, 在训练时必然离不开关系$r$, 其实学习实体嵌入也是为了学习到关系$r$ 的表示, 因为关系的嵌入表示不可能在不依靠实体的情况下学习到. </p><p>在基于平移的模型中, 头实体$h$ 更像是<strong>锚点</strong>, 经过关系$r$ 的作用后能够确定尾实体$t$, 通过调整$r$ 能决定$t$ 的落点. 基于GNN的模型以节点为单位学习, 也将关系隐式表现在节点与邻居节点的聚合之间.</p><p>作者也指出, 尤其是RGCN是GCN和TransR的结合, KBAT是ConvE和GAT的结合.</p></blockquote><p>作者发现, 在最初的设计中, 所有的模型都没有对它们的变换矩阵加以约束, 从而<strong>违背</strong>了变换矩阵需要满足的正交结论. </p><blockquote><p>作者提到, 现在的方法很难在实际中做到正交. 例如TransR和RGCN不可能同时约束KG中非常多的关系. ConvE和KBAT在转换后的嵌入维度必须与输入维度保持一致, 否则会发生尺寸不匹配, 因此ConvE和KBAT的变换矩阵不能是方阵, <strong>更不可能是正交阵</strong>.</p></blockquote><h3 id="Key-Criteria-for-Transformation-Operation"><a href="#Key-Criteria-for-Transformation-Operation" class="headerlink" title="Key Criteria for Transformation Operation"></a>Key Criteria for Transformation Operation</h3><p>根据作者分析的问题, 作者提出了两种<strong>理想状态</strong>下的实体对齐变换标准, 对实体对齐操作加以<strong>约束</strong>.</p><h4 id="Relation-Differentiation"><a href="#Relation-Differentiation" class="headerlink" title="Relation Differentiation"></a>Relation Differentiation</h4><p>对于不同的关系类型, 作者希望将统一实体在不同关系下的表示变得不同:</p><p>$$<br>\varphi\left(\boldsymbol{h}_{e}, \boldsymbol{h}_{r_{1}}\right) \neq \varphi\left(\boldsymbol{h}_{e}, \boldsymbol{h}_{r_{2}}\right), \forall e \in E, \forall r_{1}, r_{2} \in R<br>$$</p><p>这一约束避免了模型学习到同实体在不同关系下的相同表示, 从而不能区分开关系之间的不同.</p><h4 id="Dimensional-Isometry"><a href="#Dimensional-Isometry" class="headerlink" title="Dimensional Isometry"></a>Dimensional Isometry</h4><p>同一KG下的两个实体转换进同一关系空间时, 它们之间的<strong>范数</strong>和<strong>相对位置</strong>应该保持不变:<br>$$<br>\begin{array}{c}<br>\lVert\boldsymbol{h}_{e}\rVert=\lVert\varphi\left(\boldsymbol{h}_{e}, \boldsymbol{h}_{r}\right)\rVert, \forall e \in E, \forall r \in R \\<br>\boldsymbol{h}_{e_{1}}^{T} \boldsymbol{h}_{e_{2}}=\varphi\left(\boldsymbol{h}_{e_{1}}, \boldsymbol{h}_{r}\right)^{T} \varphi\left(\boldsymbol{h}_{e_{2}}, \boldsymbol{h}_{r}\right), \forall e_{1}, e_{2} \in E, \forall r \in R<br>\end{array}<br>$$</p><ul><li><p>第一个约束保证了在经过关系变换后, 实体的Embedding范数与没变化时相同.</p></li><li><p>第二个约束保证了在经过关系变换后, 两实体的相对位置不发生变化(<strong>点积不变</strong>).</p></li></ul><h2 id="The-Proposed-Method"><a href="#The-Proposed-Method" class="headerlink" title="The Proposed Method"></a>The Proposed Method</h2><p>在本节中, 作者依托上节中提出的两个操作, 提出了一种<strong>基于GNN</strong>的新实体对齐方法<strong>RREA</strong>(<strong>R</strong>elational <strong>R</strong>eflection <strong>E</strong>ntity <strong>A</strong>lignment).</p><h3 id="Relational-Reflection-Transformation"><a href="#Relational-Reflection-Transformation" class="headerlink" title="Relational Reflection Transformation"></a>Relational Reflection Transformation</h3><p>令关系嵌入$\boldsymbol{h}_r$ 为<strong>法向量</strong>, 其垂直的超平面为$\boldsymbol{P}_r$, 使用与关系相关的反射矩阵$\boldsymbol{M}_r$ 来描述实体经过的变换:</p><p>$$<br>\boldsymbol{M}_{r}=\boldsymbol{I}-2 \boldsymbol{h}_{r} \boldsymbol{h}_{r}^{T}<br>$$</p><p>其中$\boldsymbol{h}_r$ 应该是归一化的. 并且, 非常容易能够证明反射矩阵$\boldsymbol{M}_r$ 的<strong>正交性</strong>:</p><p>$$<br>\begin{aligned}<br>\boldsymbol{M}_{r}^{T} \boldsymbol{M}_{r} &amp;=\left(\boldsymbol{I}-2 \boldsymbol{h}_{r} \boldsymbol{h}_{r}^{T}\right)^{T}\left(\boldsymbol{I}-2 \boldsymbol{h}_{r} \boldsymbol{h}_{r}^{T}\right) \\<br>&amp;=\boldsymbol{I}-4 \boldsymbol{h}_{r} \boldsymbol{h}_{r}^{T}+4 \boldsymbol{h}_{r} \boldsymbol{h}_{r}^{T} \boldsymbol{h}_{r} \boldsymbol{h}_{r}^{T}=\boldsymbol{I}<br>\end{aligned}<br>$$</p><p>因此, 只要不满足$\{\boldsymbol{h}_{r_i}\neq\boldsymbol{h}_{r_j} \forall{r_i, r_j} \in R \}$, 关系反射变换就能够满足在上一节中提出的两个理想标准.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/relationalreflectionentityalignment11.jpg" style="zoom:50%;" /><h3 id="Relational-Refection-Entity-Alignment"><a href="#Relational-Refection-Entity-Alignment" class="headerlink" title="Relational Refection Entity Alignment"></a>Relational Refection Entity Alignment</h3><p>下面作者正式介绍RREA. 模型需要输入实体嵌入矩阵$\boldsymbol{H}^{e} \in \mathbb{R}^{|E| \times d}$ 和关系嵌入矩阵$\boldsymbol{H}^{r} \in \mathbb{R}^{|R| \times d}$. </p><h4 id="Relational-Reflection-Aggregate-Layer"><a href="#Relational-Reflection-Aggregate-Layer" class="headerlink" title="Relational Reflection Aggregate Layer"></a>Relational Reflection Aggregate Layer</h4><p>节点$e_i$ 第$l$ 层的输出特征如下:</p><p>$$<br>\boldsymbol{h}_{e_{i}}^{l+1}=\operatorname{ReLU}\left(\sum_{e_{j} \in \mathcal{N}_{e_{i}}^{e}} \sum_{r_{k} \in R_{i j}} \alpha_{i j k}^{l} \boldsymbol{M}_{r_{k}} \boldsymbol{h}_{e_{j}}^{l}\right)<br>$$</p><p>其中$\mathcal{N}_{e_i}^e$ 为节点$e_i$ 的邻居集, $R_{ij}$ 为节点$e_i$ 到节点$e_j$ 之间的关系集. $\boldsymbol{M}_{r_{k}} \in \mathbb{R}^{d \times d}$ 为关系$r_k$ 的关系反射矩阵, 能够通过节点$e_i$ 和节点$e_j$ 之间的关系$r_k$ 直接使用关系反射变换得到. $\alpha^l_{ijk}$ 是节点$e_i$ 对其经过关系变换后的邻居节点$e_j$ 的注意力权重. </p><p>因为$\boldsymbol{M}_{r_{k}}$ 是<strong>正交阵</strong>, 所以它不像RGCN中的$\boldsymbol{W}_r$ 中含有$d^2$ 个自由度, 而是只含有$d$ 个自由度.</p><p>类似于<strong>GAT</strong>, 注意力权重$\alpha_{ijk}^l$ 由$\beta_{ijk}^l$ 得来:</p><p>$$<br>\alpha_{i j k}^{l}=\frac{\exp \left(\beta_{i j k}^{l}\right)}{\left.\sum_{e_{j} \in N_{e_{i}}^{e}} \sum_{r_{k} \in R_{i j}} \exp \left(\beta_{i j k}^{l}\right)\right)}<br>$$<br>$$<br>\beta_{i j k}^{l}=\boldsymbol{v}^{T}\left[\boldsymbol{h}_{e_{i}}^{l}||\boldsymbol{M}_{r_{k}} \boldsymbol{h}_{e_{j}}^{l}|| \boldsymbol{h}_{r_{k}}\right]<br>$$</p><p>其中$\boldsymbol{v} \in \mathbb{R}^{2d}$ 是可以学习的参数. </p><blockquote><p>$\mathcal{v}$ 的尺寸存疑.</p></blockquote><p>将所有层的输出Concat起来, 能得到实体$e_i$ 的最终特征输出$\boldsymbol{h}_{e_{i}}^{o u t}$:<br>$$<br>\boldsymbol{h}_{e_{i}}^{o u t}=\left[\boldsymbol{h}_{e_{i}}^{0}||\ldots||\boldsymbol{h}_{e_{i}}^{l}\right]<br>$$</p><p>这样能够保存<strong>全局信息</strong>, 而不是只以最后一层的输出作为特征.</p><h4 id="Dual-Aspect-Embedding"><a href="#Dual-Aspect-Embedding" class="headerlink" title="Dual - Aspect Embedding"></a>Dual - Aspect Embedding</h4><p>基于一些最近的研究, GNN只能包含<strong>拓扑结构信息</strong>, 缺少实体附近的<strong>关系信息</strong>, 因此作者直接将节点附近的关系信息一起<strong>Concat</strong>起来, 作为实体和关系的<strong>共同嵌入</strong>$\boldsymbol{h}_{e_{i}}^{M u l}$:<br>$$<br>\boldsymbol{h}_{e_{i}}^{M u l}=\left[\boldsymbol{h}_{e_{i}}^{o u t} || \frac{1}{\left|\mathcal{N}_{e_{i}}^{r}\right|} \sum_{r_{j} \in N_{e_{i}}^{r}} \boldsymbol{h}_{r_{j}}\right]<br>$$</p><p>其中$\mathcal{N}_{e_i}^r$ 是节点$e_i$ 的关系集. 这样就包含了GNN对节点本身抽取的<strong>节点特征</strong>和节点附近的<strong>关系特征</strong>.</p><h4 id="Alignment-Loss-Function-for-Training"><a href="#Alignment-Loss-Function-for-Training" class="headerlink" title="Alignment Loss Function for Training"></a>Alignment Loss Function for Training</h4><p>为了保证等价的实体在统一的空间中接近, 使用三元组损失:<br>$$<br>L=\sum_{\left(e_{i}, e_{j}\right) \in P} \max \left(d i s t\left(e_{i}, e_{j}\right)-\operatorname{dist}\left(e_{i}^{\prime}, e_{j}^{\prime}\right)+\lambda, 0\right)<br>$$<br>其中, $(e_i, e_j) \in P$ 是等价实体对, $e_i^\prime, e_j^\prime$ 是通过最近邻居采样得来的负样本实体对. </p><blockquote><p>注意, 这里的$e_i, e_j$ 和之前描述$SS$ 时的意义是不一样的. </p></blockquote><p>然后采用与GCN - Align相同的距离度量:<br>$$<br>\operatorname{dist}\left(e_{i}, e_{j}\right)=\lVert\boldsymbol{h}_{e_{i}}^{M u l}-\boldsymbol{h}_{e_{j}}^{M u l}\rVert_{1}<br>$$</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的实验参数设置请参考原论文.</p><h3 id="Baselines"><a href="#Baselines" class="headerlink" title="Baselines"></a>Baselines</h3><p>由于实体对齐现在还处于没有统一的阶段, 有一些研究认为数据集信息不足, 尝试引入<strong>额外信息</strong>, 所以这会导致不公平的比较. 作者基于现在所使用的数据, 将其分为三类:</p><ul><li>Basic: 只使用原始数据.</li><li>Semi - supervised: 引入半监督生成额外的结构化数据.</li><li>Textual: 除了结构化数据, 引入了实体名作为额外的输入特征.</li></ul><p>作者对RREA也制作了符合三类标准的三种版本, 以做公平比较.</p><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><h4 id="RREA-vs-Basic-and-Semi-supervised-Methods"><a href="#RREA-vs-Basic-and-Semi-supervised-Methods" class="headerlink" title="RREA vs. Basic and Semi-supervised Methods"></a>RREA vs. Basic and Semi-supervised Methods</h4><p>与前两种模型比较结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/relationalreflectionentityalignment13.jpg" style="zoom:50%;" /><p>在Hits@1上提升非常明显, 可以说作者的方法<strong>有效性</strong>得到了验证. 在Hits@10上提升不是很多, 主要的提升都集中在Hits@1上, 说明RREA能够更<strong>精确</strong>的挑选出合适的实体对. 主要是因为关系反射变换为实体构建了特定关系的嵌入, 能更好的捕获信息.</p><h4 id="RREA-vs-Textual-Methods"><a href="#RREA-vs-Textual-Methods" class="headerlink" title="RREA vs. Textual Methods"></a>RREA vs. Textual Methods</h4><p>与引入实体名的方法相比, 结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/relationalreflectionentityalignment12.jpg" style="zoom: 33%;" /><p>作者将提升归功于使用了MRAEA提出的无监督文本框架.</p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>消融实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/relationalreflectionentityalignment15.jpg" style="zoom:50%;" /><p>在模型中加入关系反射聚合和多重Embedding对模型提升都比较大, 引入了不同的信息.</p><h3 id="Robustness-Analysis"><a href="#Robustness-Analysis" class="headerlink" title="Robustness Analysis"></a>Robustness Analysis</h3><h4 id="Robustness-on-Pre-aligned-Ratio"><a href="#Robustness-on-Pre-aligned-Ratio" class="headerlink" title="Robustness on Pre - aligned Ratio"></a>Robustness on Pre - aligned Ratio</h4><p>作者希望能在低资源下表现良好, 因此在DBP15K中比较了三种基于GNN模型在不同数据量下的性能:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/relationalreflectionentityalignment14.jpg" style="zoom:50%;" /><p>RREA全面领先.</p><h4 id="Robustness-on-Hyper-parameter"><a href="#Robustness-on-Hyper-parameter" class="headerlink" title="Robustness on Hyper-parameter"></a>Robustness on Hyper-parameter</h4><p>为了探究RREA对超参数的鲁棒性, 作者在DBP15K上, 探究了RREA的超参数设置所带来的影响:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/relationalreflectionentityalignment16.jpg" style="zoom:50%;" /><p>无论是GNN层数还是间隔$\lambda$, 作者认为对模型的影响都比较有限, 模型随超参变化相对比较稳定.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>作者基于两个反直觉的现象, 提出了一种<strong>统一</strong>的实体对齐框架. 并强调了变换矩阵<strong>正交</strong>的重要性. </p><p>依据对两个问题的解释, 作者设计了一种能够解决反直觉问题的<strong>关系反射变换</strong>操作, 并提出了以此为基础的新方法<strong>RREA</strong>, 通过实验证明, 作者提出的方法在多种方法中属于优越的方法, 并且提升非常大.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实体对齐 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Realistic Re-evaluation of KGC Methods: An Experimental Study</title>
      <link href="/posts/53954.html"/>
      <url>/posts/53954.html</url>
      
        <content type="html"><![CDATA[<h1 id="Realistic-Re-evaluation-of-Knowledge-Graph-Completion-Methods-An-Experimental-Study"><a href="#Realistic-Re-evaluation-of-Knowledge-Graph-Completion-Methods-An-Experimental-Study" class="headerlink" title="Realistic Re-evaluation of Knowledge Graph Completion Methods: An Experimental Study"></a>Realistic Re-evaluation of Knowledge Graph Completion Methods: An Experimental Study</h1><p>本文是论文<a href="https://arxiv.org/abs/2003.08001" target="_blank" rel="noopener">Realistic Re-evaluation of Knowledge Graph Completion Methods: An Experimental Study</a> 的阅读笔记和个人理解. </p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者发现, 在FB15k和WN18中存在着大量的<strong>逆关系</strong>和<strong>重复关系</strong>, 表现出高度<strong>冗余</strong>, 从而导致<strong>Data Leakage</strong>. 作者希望能够<strong>系统性</strong>的去评估这些存在问题所带来的<strong>危害</strong>. 这些数据集上含有问题的关系导致了KGE模型在Link Prediction上的表现被高估了.</p><p>例如, 前人已经发现逆关系在FB15k和WN18中大量出现, 在去除逆关系前后对模型性能的影响非常大:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/realisticreevaluationofkge1.jpg" style="zoom: 33%;" /><h2 id="Evaluation-Framework"><a href="#Evaluation-Framework" class="headerlink" title="Evaluation Framework"></a>Evaluation Framework</h2><p>本节是对目前现存的KGE中评估框架的简单的介绍, 为后文作者所进行的所有实验做简单的铺垫, 对于一些KGE领域的常识在此不再赘述, 详细内容请读者自行查阅.</p><h3 id="Evaluation-DataSets"><a href="#Evaluation-DataSets" class="headerlink" title="Evaluation DataSets"></a>Evaluation DataSets</h3><p>现在KGE评估所使用的主流数据集是FB15k, FB15k - 237, WN18, WN18RR. YAGO在最近的论文中似乎用的比较少. 数据集中的实体, 关系个数以及它们所使用的数据划分如下所示:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/realisticreevaluationofkge2.jpg" style="zoom: 25%;" /><p>FB15k - 237, WN18RR, YAGO3 - 10 - DR是针对逆关系处理过后的数据集, 其实体或关系数据量都有不同程度的下降.</p><h3 id="Evaluation-Method-and-Measures"><a href="#Evaluation-Method-and-Measures" class="headerlink" title="Evaluation Method and Measures"></a>Evaluation Method and Measures</h3><p>现在常用的评估指标主要和<strong>搜索问题</strong>上相关的评估指标相重合.</p><p>首先, 要明确KGE模型主要使用<strong>链接预测</strong>作为评估模型的主要任务. 即对于完整的三元组$(h, r, t)$, 在丢失头实体$h$ 或者尾实体$t$ 的情况下, 需要通过问题$(h, r, ?)$ 或$(?, r, t)$ 信息来预测出丢失的部分. 从这个角度来看, 将Link Prediction用于知识图谱补全, 本身与一个搜索问题无异. </p><p>在搜索问题中常用的指标如下:</p><table><thead><tr><th>指标名</th><th>正方向</th><th>描述</th></tr></thead><tbody><tr><td>Hits@k</td><td>↑</td><td>概率最高的k个结果若正确则为命中</td></tr><tr><td>MR(Mean Rank)</td><td>↓</td><td>正确实体在所有实体中的排名, 并将它们相加求平均</td></tr><tr><td>MRR(Mean Reciprocal Rank)</td><td>↑</td><td>正确实体在所有实体中的排名的倒数, 并将它们相加求平均</td></tr></tbody></table><p>当然, 由于可能存在<strong>多个</strong>正确的头实体或尾实体答案, 上述没有经过任何过滤的排名评估计算方式被称为<strong>Raw</strong>, 如果将其余正确的答案从测试集中<strong>去除</strong>, 仅保留唯一的正确答案, 则称为<strong>Filtered</strong>. 那么上述评估指标也被相应的替换为FHits@k(↑), FMR(↓), FMRR(↑), 方向保持不变.</p><blockquote><p>该方法在TransE中被提出, 详见<a href="https://adaning.github.io/posts/53023.html#Evaluation-protocol">TransE</a>.</p></blockquote><h2 id="Inadequacy-of-Benchmarks-and-Evaluation-Measures"><a href="#Inadequacy-of-Benchmarks-and-Evaluation-Measures" class="headerlink" title="Inadequacy of Benchmarks and Evaluation Measures"></a>Inadequacy of Benchmarks and Evaluation Measures</h2><p>本文的核心工作在本节. 本节作者着重叙述了在FB15K和WN18中出现的种种问题. 主要存在的问题是<strong>逆关系</strong>和<strong>笛卡尔积关系</strong>. 逆关系其实已经被前人所提出并解决, 作者比较新颖的概念是提出了笛卡尔积关系.</p><h3 id="Identifying-the-Most-Probable-Freebase-Snapshot-for-Producing-FB15k"><a href="#Identifying-the-Most-Probable-Freebase-Snapshot-for-Producing-FB15k" class="headerlink" title="Identifying the Most Probable Freebase Snapshot for Producing FB15k"></a>Identifying the Most Probable Freebase Snapshot for Producing FB15k</h3><p>为了探寻FB15k中各类缺陷的<strong>根本原因</strong>, 作者需要观察Freebase原本的样貌. 但Freebase是一直在进行更新的, FB15k并未声明是在哪个时间节点进行创建. Freebase可能会由类似于<strong>快照</strong>的形式存储起来, 过一段周期对Freebase保存一次, 作者尝试搜索了与创建FB15k时相似的Snapshot版本, 有99.54%的三元组都出现在了FB15k中.</p><p>作者列举了一个关于长篇小说《A Room With A View》在Freebase中存储的真实样貌, 以便我们更好的来理解Freebase中的各种结构:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/realisticreevaluationofkge3.jpg" style="zoom: 50%;" /><p>能够看到, 在Freebase中存在着大量的<strong>逆关系</strong>, 几乎每一种关系都能找到它所对应的逆关系.逆关系是导致Data Leakage的主要原因, 模型通过<strong>关联逆关系对</strong>获得提升.</p><blockquote><p>注意, FB15k中来自Freebase的冗余逆关系是<strong>人为创建</strong>的, 它被添加了一个反向三元组, 用关系<code>reverse_property</code>显式表示出来.</p></blockquote><h4 id="CVT-and-Concatenated-Edges"><a href="#CVT-and-Concatenated-Edges" class="headerlink" title="CVT and Concatenated Edges"></a>CVT and Concatenated Edges</h4><p>在Freebase中, 有一类特殊的节点被称为CVT(Compound Value Type), 其发挥的作用更像<strong>中继节点</strong>, 以便能更好的处理<strong>多元关系</strong>. CVT能够作为更<strong>抽象</strong>的节点被多种二元关系所连接, 而三元组则以合并CVT所连接的两条边后的形式被创建. 例如<code>(Bafta Award For Best Film, award_category/nominees, CVT)</code>和<code>(CVT, award_nomination/ nomated_for, A Room With A View)</code>在奖项和作品之间形成三元组, 对这两种关系$r_1, r_2$ 的合并记为$r_1 . r_2$. CVT常用于更简单的标识某个事件所对应的多种属性.</p><p>例如, 关于<code>Obama</code>任职期限的<strong>多元关系</strong><code>government_position_held</code>包含了多个<strong>子二元关系</strong>, <code>offer_holder</code>, <code>office_position</code>, <code>from</code>, <code>to</code>等:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/realisticreevaluationofkge24.jpg" style="zoom: 50%;" /><blockquote><p>该图片取自<a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/44818.pdf" target="_blank" rel="noopener">From Freebase to Wikidata: The Great Migration</a>.</p></blockquote><p><strong>而在FB15k中, 将大多数的CVT节点合并掉了</strong>, 合并的边以<strong>三元组</strong>的形式出现在FB15k中.</p><h3 id="Data-Redundancy"><a href="#Data-Redundancy" class="headerlink" title="Data Redundancy"></a>Data Redundancy</h3><p><strong>Data Redundancy是导致Data Leakage的根本原因</strong>. 冗余的数据使得模型倾向于关联各种逆关系对, 从而获得在真实任务中不能获得的额外信息, 导致过拟合.</p><h4 id="Data-Leakage-due-to-Reverse-Triples"><a href="#Data-Leakage-due-to-Reverse-Triples" class="headerlink" title="Data Leakage due to Reverse Triples"></a>Data Leakage due to Reverse Triples</h4><p>前人已经发现, 在FB15k中含有大量的逆关系. 一般的, 对于逆三元组对$(h, r, t), (t, r^{-1}, h)$, 其中的$r$ 被称为逆关系. 实际上在Freebase中, 逆关系已经使用特殊的关系<code>reverse_property</code>来<strong>显式关联</strong>逆关系对. 例如, <code>(film/directed_by, reverse_property, director/film)</code>意味着<code>film/directed_by</code>和是一对逆关系. 如果是由CVT合并来的两关系$r_1. r_2$, 它们的逆关系理所当然的是$r_2^{-1} . r_1^{-1}$.</p><h5 id="FB15k"><a href="#FB15k" class="headerlink" title="FB15k"></a>FB15k</h5><p>在FB15k中, 训练集中的48w个三元组中, 有将近34w的三元组形成了17w的逆关系对, 在测试集中约有70.3%的三元组的逆关系能够在训练集中被找到. 这是非常严重的泄露了, 模型非常容易的将自己的训练目标转化为如何才能学习到更多的逆关系对, 从表面上优化在FB15k中的效果.</p><h5 id="WN18"><a href="#WN18" class="headerlink" title="WN18"></a>WN18</h5><p>在WN18中, 一共有18个关系, 其中有14个关系构成了7对逆关系对, 除此外还有三种<strong>对称关系</strong>. 在测试集和训练集中包含逆关系对的三元组非常多, 也导致了非常严重的泄露.</p><h4 id="Other-Redundant-Triples"><a href="#Other-Redundant-Triples" class="headerlink" title="Other Redundant Triples"></a>Other Redundant Triples</h4><p>作者指出, 除去表面上已经明确指出的逆关系, 在FB15k中还存在其他类型的<strong>语义冗余关系</strong>. 作者通过一种简单的衡量标准来判断关系对$(r_1, r_2)$ 中出现的两种关系$r_1, r_2$ 是否是冗余的. </p><p>假设$\left| r \right|$ 是关系$r$ 所对应的三元组实例个数, $T_r$ 代表关系$r$ 对应三元组实例中的头实体$h$ 和尾实体$t$ 对的集合, 即$T_r=\{(h, t) \mid r(h, t) \in \mathcal{G} \}$. 如果满足以下条件, 那么称$r_1$ 和$r_2$ 为<strong>重复关系</strong>:<br>$$<br>\frac{\left|T_{r_{1}} \cap T_{r_{2}}\right|}{\left|r_{1}\right|}&gt;\theta_{1} \text { and } \frac{\left|T_{r_{1}} \cap T_{r_{2}}\right|}{\left|r_{2}\right|}&gt;\theta_{2}<br>$$<br>其实就是非常简单的去看两种关系所对应的头尾实体对的<strong>重合度</strong>. 如果两种关系所对应的头尾实体对交集越多, 则证明其对应头尾实体的重合区域就越大, 两种关系也越有可能具有相似的语义, 冗余的可能性也就越大.</p><p>更多的, 对于关系$r_1$ 和逆关系$r_2^{-1}$ 也能由之推广而来. $T_r^{-1}$ 代表$T_r$ 的逆实体对, 即$T_r^{-1}=\{(t, h) \mid (h, t) \in T_r \}$. 同样的, 如果满足以下条件, 则称$r_1$ 和$r_2$ 为<strong>逆重复关系</strong>:<br>$$<br>\frac{\left|T_{r_{1}} \cap T_{r_{2}}^{-1}\right|}{\left|r_{1}\right|}&gt;\theta_{1} \text { and } \frac{\left|T_{r_{1}} \cap T_{r_{2}}^{-1}\right|}{\left|r_{2}\right|}&gt;\theta_{2}<br>$$<br>作者将$\theta_1, \theta_2$ 在FB15k中设置为0.8.</p><p>例如, 关系$r_1$ <code>football_position/players</code> 和关系$r_2$ <code>sports_position/players.football_roster_position/player</code>, 就是一对重复关系, 因为计算得出$\frac{\left|T_{r_{1}} \cap T_{r_{2}}\right|}{\left|r_{1}\right|}=0.87 , \frac{\left|T_{r_{1}} \cap T_{r_{2}}\right|}{\left|r_{2}\right|}=0.97$. 上述两种关系在下图中分别用红色, 绿色标记出:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/realisticreevaluationofkge4.jpg" style="zoom: 50%;" /><p>红色的关系记载了每个足球运动员整体的职业生涯所踢的位置, 绿色关系是通过CVT连接而来的, 代表特定时间球员在球队中所处的位置, 属于多元关系. 但实际上, 绝大多数的球员在职业生涯中只踢某一个特定的位置, 所以这两种关系大多数情况下是冗余的. 另一个类似的例子是$r_1$ 和关系$r_3$ <code>football_player/current_team . sports_team_roster/position</code>, 在图中用蓝色标出. $r_1, r_3$ 是逆重复的, 因为计算得出$\frac{\left|T_{r_{1}} \cap T_{r_{2}}^{-1}\right|}{\left|r_{1}\right|}=0.88 , \frac{\left|T_{r_{1}} \cap T_{r_{2}}^{-1}\right|}{\left|r_{2}\right|}=0.97$.</p><p>在上述讨论下, 作者将其他的冗余情况分为4种:</p><ol><li><strong>训练集</strong>中存在<strong>逆三元组</strong>.</li><li><strong>训练集</strong>中存在<strong>重复三元组</strong>或<strong>逆重复三元组</strong>.</li><li><strong>测试集</strong>中存在<strong>逆三元组</strong>.</li><li><strong>测试集</strong>中存在<strong>重复三元组</strong>或<strong>逆重复三元组</strong>.</li></ol><p>作者将上述四种情况用四位<strong>二进制码</strong>来表示, 并统计了FB15k中的测试集冗余情况:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/realisticreevaluationofkge5.jpg" style="zoom: 50%;" /><p>从中能够看出, 68%的三元组集中在<code>1000</code>, 即训练集中存在逆三元组. 没有出现作者所归纳出的四种问题的三元组<code>0000</code>也占了18%, 只在测试集中存在逆三元组的<code>0010</code>占了8%. 其余情况占少部分.</p><h3 id="Cartesian-Product-Relations"><a href="#Cartesian-Product-Relations" class="headerlink" title="Cartesian Product Relations"></a>Cartesian Product Relations</h3><p>笛卡尔积关系在之前的研究中并没有被提出过, 作者第一次发现了这种特殊的关系. 我们首先来回顾一下笛卡尔积. 集合$A=\{a_1, a_2\}, B=\{b_1, b_2, b_3\}$, 假设用$\times$ 来表示笛卡尔积, 那么有:<br>$$<br>\begin{aligned}<br>A\times B &amp;= \{(a_x, b_y)\mid x\ in (1, 2), y \in (1, 2, 3)\}\\<br>&amp;=\{(a_1, b_1), (a_1, b_2), (a_1, b_3), (a_2, b_1), (a_2, b_2), (a_2, b_3)\}<br>\end{aligned}<br>$$<br>如果将笛卡尔积用于<strong>三元组</strong>中, 那么对于笛卡尔积关系$r$, 其对应的头实体$h$ 和尾实体$t$ 总是符合笛卡尔积的运算关系, 即$h \in \mathcal{H}, t \in \mathcal{T}$, 关系$r$ 中的头尾关系实体对应该为$\mathcal{H} \times \mathcal{T}$, $\mathcal{H}$ 到$\mathcal{T}$ 中的每个实体都存在关系$r$.</p><p>例如, 关系<code>climate</code>, 通常以<code>(a, climate, b)</code> 的形式出现在现实生活中, 其中<code>a</code>为城市, <code>b</code>为月份. 但实际上, Link Prediction在这种三元组上做的预测<strong>毫无意义</strong>. 例如链接预测在​<code>climate</code>上的意义会转化为某城市在几月份是否有气候. </p><p>再假设一个场景, <code>Tokyo</code>与<code>CVT</code>相连, 边上的Label为<code>climate</code>, <code>January</code>与<code>CVT</code>相连, 边上的Label为<code>month</code>,  <code>34</code>与<code>CVT</code>相连, 边上Label为<code>average_min_temp_c</code>. 这个关系代表东京一月份的平均低温为34度, 但人们实际上在现实世界中更关心平均气温, 而不是将链接预测用于此预测一月是否有温度. 根本原因是现实世界的多重关系与链接预测任务将其<strong>简化</strong>为多重二元关系, 而<strong>多重二元关系在拆解开后不能恢复成多元关系</strong>.</p><p>此外, 负采样获得的三元组也有可能是正确的, 而笛卡尔积关系在这个问题上尤为突出.</p><p>在笛卡尔积关系大量出现在数据集中时, 会拉高整体模型的评估精度. 这种<strong>特殊性</strong>使得笛卡尔积与其他关系放在一起比较不太公平, 作者在论文中给出的建议是, <strong>将笛卡尔积关系和非笛卡尔积关系分别比较</strong>.</p><p>作者仍然尝试使用简单的方法来检测笛卡尔积关系, 对于关系$r$, 其头实体集$S_{r}=\{\mathrm{h} \mid \exists \ r(\mathrm{h}, \mathrm{t}) \in \mathcal{G}\}$, 尾实体集 $O_r = \{\mathrm{t} \mid \exists \ r(\mathrm{h}, \mathrm{t}) \in \mathcal{G}\}$, 若$|r| /\left(\left|S_{r}\right| \times\left|O_{r}\right|\right)$ 高于了某个阈值(之前设置的为0.8), 则认为该关系是笛卡尔积关系.</p><p>尽管笛卡尔积关系不像逆关系数量那么多, 但是笛卡尔积关系非常容易被模型学习到, 下图为FB15k - 237中部分笛卡尔积关系的FMRR:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/realisticreevaluationofkge6.jpg" style="zoom: 50%;" /><p>作者尝试证明自己的假设, 观察笛卡尔积关系所带来的影响. 如果笛卡尔积关系能被检测到, 则在链接预测时直接从已知的笛卡尔积头尾实体对的候选集中<strong>随机排序</strong>, 挑选合适的头尾实体.</p><blockquote><p>随机排序是我在<a href="https://github.com/idirlab/kgcompletion/blob/master/Cartesian-product/fb15k_cartesian_product.py" target="_blank" rel="noopener">代码</a>中看到的, 并不是很确定正确与否, 作者在论文中并没有明确指出.</p></blockquote><p>下图为单纯使用TransE在笛卡尔积关系上的表现和利用笛卡尔积特性后的模型表现:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/realisticreevaluationofkge7.jpg" style="zoom: 50%;" /><p>下图为上图中所有关系的具体含义:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/realisticreevaluationofkge8.jpg" style="zoom: 50%;" /><p>能从结果中观察到, 利用笛卡尔积特性的效果比单纯使用TransE要更好. 使用Freebase效果要比它的子集FB15k更强一些.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>FB15k - 237, WN18RR, YAGO3 - 10 - DR是三种去除逆关系后数据集, 基本上消除了论文中提到的逆关系带来Leakage的问题. </p><ul><li><p><strong>FB15k - 237</strong>中, 删除逆关系的过程与作者提出的过程本质上一致, 这就有可能存在误伤, 即删除了一些在语义上不冗余的关系, 没有考虑笛卡尔积关系.</p></li><li><p><strong>WN18RR</strong>中, 保留了<strong>对称关系</strong>(也产生了逆关系对), 数据集中的11种关系中, 有3种是自反关系.</p></li><li><p><strong>YAGO3 - 10 - DR</strong>是作者<strong>自制</strong>的数据集, 也做了移除逆关系的处理.</p></li></ul><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>作者将诸多流行的Baseline在多种数据集上用<strong>链接预测</strong>详细测试. 我在写的时候对原文中陈述的顺序进行了<strong>调整</strong>, 按照数据集将各种结果整合到一起, 方便阅读.</p><h4 id="FB15k-and-FB15k-237"><a href="#FB15k-and-FB15k-237" class="headerlink" title="FB15k and FB15k - 237"></a>FB15k and FB15k - 237</h4><p>FB15k, FB15k - 237上, 各类模型结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/realisticreevaluationofkge9.jpg" style="zoom: 50%;" /><p>不同颜色的结果代表不同的实验结果来源, 其中AMIE是<strong>基于规则</strong>的模型.</p><p>在将逆关系移除后, 所有的模型结果都发生了明显的<strong>退化</strong>.</p><p>作者进一步精确的分析了模型在数据集中各种关系的性能(以<strong>FMRR</strong>为指标), 将每个模型在测试集中相较于其他模型性能<strong>最好</strong>的<strong>关系所占百分比</strong>. 下图是FB15k - 237的热力图:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/realisticreevaluationofkge13.jpg" style="zoom: 50%;" /><p>为了更好地理解模型优缺点, 作者将上表中的关系<strong>分类</strong>对待, 根据前面得出的<strong>FMRR</strong>做出柱状图:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/realisticreevaluationofkge15.jpg" style="zoom: 50%;" /><p>下图为模型在每类别关系表现最优的占比:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/realisticreevaluationofkge16.jpg" style="zoom: 50%;" /><p>各模型在FB15k - 237区分类别的关系下的FHits@10:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/realisticreevaluationofkge17.jpg" style="zoom: 50%;" /><h4 id="WN18-WN18RR"><a href="#WN18-WN18RR" class="headerlink" title="WN18, WN18RR"></a>WN18, WN18RR</h4><p>本小节结构与上小节完全一致. WN18, WN18RR上, 各类模型结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/realisticreevaluationofkge10.jpg" style="zoom: 50%;" /><p>下图是WN18RR的热力图:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/realisticreevaluationofkge14.jpg" style="zoom: 50%;" /><p>因为WN18RR中含有的关系数量过少, 为了维持实验的<strong>稳健性</strong>, 所以作者并没有做出与FB15k - 237中类似的柱状图分析. 各模型在WN18RR区分类别的关系下的FHits@10:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/realisticreevaluationofkge18.jpg" style="zoom: 50%;" /><h4 id="YAGO3-10-YAGO3-10-DR"><a href="#YAGO3-10-YAGO3-10-DR" class="headerlink" title="YAGO3 - 10, YAGO3 - 10 - DR"></a>YAGO3 - 10, YAGO3 - 10 - DR</h4><p>本小节结构与上上小节也完全一致. YAGO3 - 10, YAGO3 - 10 - DR上, 各类模型结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/realisticreevaluationofkge19.jpg" style="zoom: 50%;" /><p>同样的, 在YAGO 3 - 10上作者也做了同样的可视化:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/realisticreevaluationofkge20.jpg" style="zoom: 50%;" /><p>下图为模型在每类别关系表现最优的占比:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/realisticreevaluationofkge21.jpg" style="zoom: 50%;" /><p>各模型在YAGO3 - 10区分类别的关系下的FHits@10:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/realisticreevaluationofkge22.jpg" style="zoom: 50%;" /><h4 id="Together"><a href="#Together" class="headerlink" title="Together"></a>Together</h4><p>作者假设删除了逆三元组和重复三元组后, 优于TransE的模型并没有体现出很好优势, </p><p>下图是在测试集的三元组某指标上优于TransE的模型中, 在训练集中含有逆或冗余三元组所占的百分比:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/realisticreevaluationofkge11.jpg" style="zoom: 50%;" /><p>这证实了在FB15k和WN18上, 大多数比TransE效果好的方法与逆关系或冗余三元组是强相关的, 并且在去掉它们之后, 会有相当大的性能损失.</p><p>作者进一步的分析了在<strong>改进后</strong>的数据集上, 不同模型在不同指标中取得最优的关系数量:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/realisticreevaluationofkge12.jpg" style="zoom: 50%;" /><p>从中能观察到, 在移除了包含逆关系的三元组后, 很多模型并不能取得比TransE好太多的效果.</p><p>最后, 作者汇总了各模型在各类数据集下的<strong>FHits@1</strong>:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/realisticreevaluationofkge23.jpg" style="zoom: 50%;" /><p>其中, Simple Model是作者提出的一个简单的基于统计信息的模型, 能够自动推断出两个关系能否形成逆关系对, 从而简单的推断出答案. 仅仅使用统计信息在FB15k和WN18上取得了相当好的成绩, 但在FB15k - 237和WN18RR上的成绩非常差, 进一步证明了之前数据集问题的存在性和危害.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>本篇论文是比较扎实的工作, 对现有常用的KGE数据集中存在的缺陷进行了系统性而全面的分析, 并测试了多种Baseline在数据集上的表现, 做了大量的可视化工作. 此外, 也证明了现有评估KGE的方法存在诸多缺陷.</p><p>对于逆关系, 作者仍然采取移除的方式处理. 对于比较新颖的笛卡尔积关系, 在其他的工作中并没有被提出.</p><p>如何合理的处理这些含有问题的关系是非常值得思考的事情, 有没有移除之外更好的办法? 这可能要追本溯源到KG中三元组的表示方法的诟病上.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Can We Predict New Facts with Open Knowledge Graph Embeddings?</title>
      <link href="/posts/57546.html"/>
      <url>/posts/57546.html</url>
      
        <content type="html"><![CDATA[<h1 id="Can-We-Predict-New-Facts-with-Open-Knowledge-Graph-Embeddings-A-Benchmark-for-Open-Link-Prediction"><a href="#Can-We-Predict-New-Facts-with-Open-Knowledge-Graph-Embeddings-A-Benchmark-for-Open-Link-Prediction" class="headerlink" title="Can We Predict New Facts with Open Knowledge Graph Embeddings? A Benchmark for Open Link Prediction"></a>Can We Predict New Facts with Open Knowledge Graph Embeddings? A Benchmark for Open Link Prediction</h1><p>本文是论文<a href="https://www.aclweb.org/anthology/2020.acl-main.209/" target="_blank" rel="noopener">Can We Predict New Facts with Open Knowledge Graph Embeddings? A Benchmark for Open Link Prediction</a>的阅读笔记和个人理解. </p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者指出, 虽然人们已经在<strong>特定领域</strong>的知识图谱上取得了成功, 但先前的方法都不是基于<strong>开放域</strong>的.</p><p>作者希望探讨是否有可能从<strong>开放知识图谱</strong>中通过<strong>非规范化的文本</strong>直接推断出新的事实, 即能否依据原始文本完成开放域的<strong>Link Prediction</strong>.</p><h2 id="Open-Knowledge-Graph"><a href="#Open-Knowledge-Graph" class="headerlink" title="Open Knowledge Graph"></a>Open Knowledge Graph</h2><p>开放知识图谱的三元组经常由<strong>文本三元组</strong>组成, 即$(\text { subject text, relation text, object text) }$, 由于文本信息非常嘈杂, 其中含有很多<strong>噪声</strong>, 因此同一种关系或者实体对应着非常多种不同的文本表述:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/predictnewfactfromopenkg1.jpg" style="zoom:33%;" /><p>例如, <code>NBC</code>这一实体可能对应着<code>{NBC - TV, NBC, NBC Television}</code>这一实体集合.</p><blockquote><p>对于同一实体的不同自然语言表述在原文中似乎被称为是Entity Mention, 但我没想好到底翻译成什么, 暂称实体集合或<strong>实体提及</strong>.</p></blockquote><p>能看到, 因为使用<strong>自然语言文本</strong>来表示三元组, 导致实体和关系非常<strong>不规范</strong>. 但是Open KG中通常包括着更多的文字表述, 也可以被捕获更多的信息.</p><p>Open KG<strong>不直接编码知识</strong>, 即使在文字描述中已经给出了实体本身, 在其中还可能含有大量的概念化知识.</p><p>在OKG中, 其结构应该是能<strong>自动构建</strong>的, 因为其涉及到自然语言的复杂性, 应该不需要实体和关系的词表.</p><h2 id="Open-Link-Prediction"><a href="#Open-Link-Prediction" class="headerlink" title="Open Link Prediction"></a>Open Link Prediction</h2><h3 id="Link-Prediction"><a href="#Link-Prediction" class="headerlink" title="Link Prediction"></a>Link Prediction</h3><p>Link Prediction在一般的KG中经常被用于测试模型的<strong>推断</strong>能力. </p><p>假设$\mathcal{E}$ 是实体$i, j$ 的集合, $\mathcal{R}$ 是关系$k$ 的集合, 那么知识图谱$\mathcal{T} \subseteq \mathcal{E} \times \mathcal{R} \times \mathcal{E}$. 如果从QA的角度来考虑, 查询头实体$i$ 和尾实体$j$ 的问题就分别被描述为$q_{h}=(?, k, j)$ 和 $q_{t}=(i, k, ?)$. 并且, 它们可能分别在训练集中<strong>单独</strong>出现过, 但并未以三元组的的形式一起出现.</p><p>例如<code>(NBC, headquarterIn, ?)</code>, 就是在询问NBC在哪个地方设立了总部.</p><h3 id="Open-Link-Prediction-1"><a href="#Open-Link-Prediction-1" class="headerlink" title="Open Link Prediction"></a>Open Link Prediction</h3><h4 id="Difference-Between-LP-and-OLP"><a href="#Difference-Between-LP-and-OLP" class="headerlink" title="Difference Between LP and OLP"></a>Difference Between LP and OLP</h4><p>在开放域知识图谱中, Link Prediction相应的被转化为Open Link Prediction:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/predictnewfactfromopenkg2.jpg" style="zoom:35%;" /><p>由于OKG自身的特性, 实体和关系被转化为实体提及(Entity Mention)和开放关系(Open Relation). 如果对应到QA中, 问题和答案可能都会有多种不同的表述. 甚至对于<strong>不同时间点</strong>, 对于同一实体都会产生不同的表述.</p><p>作者将每个实体提及和开放关系以非空的Token序列对待, 记Token的词表为$\mathcal{V}$. 记开放集$\mathcal{M}=\mathcal{V^+}$, 代表OKG中所观察到的元素集合, 即实体提及$i,j \in \mathcal{M}(\mathcal{E})$ 和开放关系$k \in \mathcal{M}(\mathcal{R})$. 故$\mathcal{T} \subset \mathcal{M} \times \mathcal{M} \times \mathcal{M}$.</p><p>在OLP中, 其任务目标是预测<strong>新且正确</strong>的问题$(i, k, ?)$ 或 $(?, k, j)$ 的答案. 答案来自于$\mathcal{M}(\mathcal{E})$, 但问题可能来自于$\mathcal{M}$ 中的任意的实体提及和开放关系.</p><p>例如, 对于问题<code>(&quot;NBC - TV&quot;, &quot;has office in &quot;, ?)</code>, 我们想要的答案是的<code>NewYorkCity</code>的实体提及<code>{&quot;New York&quot;, &quot;NYC&quot;, ...}</code>中的任意一条表述.</p><h4 id="Evaluation-Protocol"><a href="#Evaluation-Protocol" class="headerlink" title="Evaluation Protocol"></a>Evaluation Protocol</h4><h5 id="KGs-and-Entity-Ranking"><a href="#KGs-and-Entity-Ranking" class="headerlink" title="KGs and Entity Ranking"></a>KGs and Entity Ranking</h5><p>先回顾一下在普通KG模型在Link Prediction中的实体排名规则. </p><p>对于三元组$z=(i, j, k)$, 模型对问题$q_{h}(z)=(?, k, j)$ 或$q_{t}(z)=(i, k, ?)$ 进行排名时, 有两种设置:</p><ul><li><strong>Raw</strong>: 直接依据于正确的实体$j$ 或 $i$ 的排名.</li><li><strong>Filtered</strong>: 依据于$q_t(z)$ 或 $q_h(z)$ 只保留正确答案$j$ 或 $i$ 的排名. 即滤去了除$j$ 和$i$ 外的其他所有正确答案后的排名. </li></ul><h5 id="OKGs-and-Mention-Ranking"><a href="#OKGs-and-Mention-Ranking" class="headerlink" title="OKGs and Mention Ranking"></a>OKGs and Mention Ranking</h5><p>出于在OKG中所出现的种种的问题, 作者将一般KG中评估用的协议改进到了OKG中.</p><p>在OKG中, 问题可能会有多个等价的正确答案, 所以一定要考虑答案的实体提及. 相同的, 作者对OLP也提出两种设置:</p><ul><li><strong>Raw</strong>: 依据于正确答案提及的最高排名.</li><li><strong>Filtered</strong>: 滤去评估实体外其余正确答案后的所有实体提及, 然后再在评估实体的提及中选择最高排名.</li></ul><p>作者将上述流程用下图来概括:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/predictnewfactfromopenkg3.jpg" style="zoom: 40%;" /><ol><li>对于给定的问题, 对所有答案的实体提及进行排名.</li><li>将除评估实体外的其余正确答案的所有实体提及从排名中过滤掉, 并将其标记为”filtered”.</li><li>选择评估实体的提及中排名最高的作为其排名.</li></ol><p>综上, 根据作者提出的协议, 我们在评估时需要以下两方面数据准备:</p><ol><li>问题的正确实体提及的标注数据.</li><li>同一个实体的所有实体提及.</li></ol><p>根据这两个要求, 作者接下来自制了所需的数据集.</p><h2 id="Creating-the-Open-Link-Prediction-DataSet"><a href="#Creating-the-Open-Link-Prediction-DataSet" class="headerlink" title="Creating the Open Link Prediction DataSet"></a>Creating the Open Link Prediction DataSet</h2><p>在OKG中, 经常伴随着Test Leakage, 因此模型能够毫不费力的推断出真正的事实. 因此, 验证集和测试集的数据必须精心准备, 否则将出现模型能力的误判. 所以作者需要自己按照需求制作OLP的数据集.</p><h3 id="Source-DataSet"><a href="#Source-DataSet" class="headerlink" title="Source DataSet"></a>Source DataSet</h3><p>OLPBENCH是基于OPIEC的创建的.</p><p>三元组从维基百科抽取的过程如下所示:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/predictnewfactfromopenkg4.jpg" style="zoom:33%;" /><p>借助维基百科的<strong>超链接</strong>, 直接就完成了<strong>实体消歧</strong>, 即直接生成实体的所有实体提及.</p><p>作者将实体链接前后的数据展示在表中, 以方便大家了解这个过程:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/predictnewfactfromopenkg5.jpg" style="zoom: 33%;" /><p>在没有实体链接前, 实体的表述是不规范的, 关系表述也是不规范的. 在链接后, 生成了实体到提及之间的映射, 实体被唯一的确定了下来.</p><h3 id="Evaluation-data"><a href="#Evaluation-data" class="headerlink" title="Evaluation data"></a>Evaluation data</h3><p>作者认为, 验证集和测试集的数据需要满足很多严格的要求, 以此来解决OKG中出现的问题.</p><h4 id="Data-Quality"><a href="#Data-Quality" class="headerlink" title="Data Quality"></a>Data Quality</h4><p>评估数据可能是非常具有挑战性的, 因为其中含有非常多的<strong>噪声</strong>.</p><p>作者非常简单的通过规则来筛选关系. 不考虑长度小于3个Token的关系, 理由如下:</p><ol><li>短关系通常是长关系的一部分.</li><li>长关系更难以被普通的KG构建方法所捕捉</li><li>使用短关系抽取的实体注释噪声经常更多.</li></ol><h4 id="Human-Effort-for-Data-Creation"><a href="#Human-Effort-for-Data-Creation" class="headerlink" title="Human Effort for Data Creation"></a>Human Effort for Data Creation</h4><p>在Mention Ranking Protocol中, 作者引入了实体相关的知识来完成实体消歧的工作, 作者希望评估引入实体知识对<strong>模型选择</strong>的作用, 因此作者按照人工干预的程度对测试集和验证集进行划分, 并探究在它们基础上选择模型后的模型表现. 人工干预越多, 引入的实体知识就越多.</p><p>作者将验证集划分为以下三种:</p><ul><li><strong>Test and Valid - Linked</strong> data: 绝大多数都是人工制造的, 能根据实体提及找到正确的实体. 并可以使用Mention Ranking Protocol.</li><li><strong>Valid - Mention</strong> data: 引入了一部分人工. 没有使用实体链接, 但保证了数据与测试数据的相同分布. 如果目标领域可以使用NER的话, 验证集可能会自动通过NER生成.</li><li><strong>Valid - All</strong> data: 没有人工干预. 基本维持了原始文本的样子.</li></ul><h4 id="Training-Data"><a href="#Training-Data" class="headerlink" title="Training Data"></a>Training Data</h4><p>在普通的KG中, 人们经常通过删除逆关系来解决Test Leakage. 但在Open KG中, 因为自然文本的嘈杂, 更有可能通过<strong>文本</strong>中所包含的内容导致Test Leakage.</p><p>这种Leakage会导致数据集的制作和划分更加困难, 在评估模型时, 还需要某种方法判断模型是否在真正推断信息. 因此, 作者想更深入的量化在验证集中到底有多少问题能够不通过整个问题的信息来回答, 即只给出头实体或者关系是否能成功预测尾实体. 在不借助整个问题信息来回答问题时, 往往是借助<strong>统计信息</strong>来回答的, 即只回答<strong>最常共现</strong>的答案, 而不是真正在做推断.</p><p>例如:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/predictnewfactfromopenkg6.jpg" style="zoom: 40%;" /><p>图中左侧为传统KG中的Test Leakage, 逆关系②<code>hasCompany</code>, 模型通常能够很容易的利用其信息建立与①<code>headquaterIn</code> 的联系, 从而不加推断的给出答案.</p><p>图中右侧为OKG中的Test Leakage, 图中①和②直接就是相同的关系描述, 当然会导致Leakage. 而最上方③中的尾实体描述<code>New York&#39;s NBC</code> 也会导致①的Leakage. 最下方③的开放关系描述也会导致相同的Leakage.</p><p>作者尝试对OKG上的训练集移除Leakage划分了三种等级:</p><ul><li><p><strong>Simple Removal</strong>: 仅仅删除三元组$(i, k, j)$, 而三元组中的$i$ 和 $j$ 的提及全部被保留.</p></li><li><p><strong>Basic Removal</strong>: 删除三元组$(i, k, j)$ 和三元组$(j, k ,i)$, 并删除$i$ 和$j$ 的所有提及.</p></li><li><p><strong>Thorough Removal</strong>: 在BASIC Removal的基础上, 额外按照以下规则移除三元组:</p><ol><li>$(i, \ast, j)$ 和 $(j, \ast, i)$.</li><li>$(i, k +j, \ast)$ 和 $(\ast, k+i, j)$.</li><li>$(i+k+j, \ast, \ast)$ 和 $(\ast, \ast, i+k+j)$.</li></ol><p>基于规则移除更多的从自然语言的角度考虑了更严格的移除方式.</p></li></ul><h2 id="Open-Knowledge-Graph-Embeddings"><a href="#Open-Knowledge-Graph-Embeddings" class="headerlink" title="Open Knowledge Graph Embeddings"></a>Open Knowledge Graph Embeddings</h2><p>作者提出了一种用于开放知识图谱的KGE方法, 以便评估不同设定对模型所带来的影响.</p><p>因为开放知识图谱中实体和关系的<strong>多重表述性</strong>, 必须对实体和关系进行<strong>Token Level Modeling</strong>, 这样才能保证模型能处理任何长度的实体和关系输入. 我们默认Token以Token Embedding作为输入.</p><p>作者将模型分为<strong>关系模型</strong>和<strong>组合函数</strong>两部分:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/predictnewfactfromopenkg7.jpg" style="zoom: 40%;" /><p>实体和关系都是由多个Token组成的, $\mathcal{V}(\mathcal{E})^{+}$ 是从词表$\mathcal{V}(\mathcal{E})$ 中提取的非空序列, 若用$d, o \in \mathbb{N}_+$ 分别代表实体和关系的嵌入维度, 则实体嵌入函数$f$ 为$f: \mathcal{V}(\mathcal{E})^{+} \rightarrow \mathbb{R}^{d}$, 同理关系嵌入函数$g$ 为$g: \mathcal{V}(\mathcal{R})^{+} \rightarrow \mathbb{R}^{o}$. 那么在实体和关系分别嵌入完后, 使用关系打分函数$\mathcal{RM}$ 对这对开放三元组打分, $\mathcal{R} \mathcal{M}: \mathbb{R}^{d} \times \mathbb{R}^{o} \times \mathbb{R}^{d} \rightarrow \mathbb{R}$.</p><p>总结下, 对于给定的三元组$(i, k, j)$, $i, j \in \mathcal{V}(\mathcal{E})^{+}$, $k \in \mathcal{V}(\mathcal{R})^{+}$, 其得分可以被计算为:<br>$$<br>s(i, k, j)=\mathcal{R} \mathcal{M}(f(i), g(k), f(j))<br>$$</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Models-and-Training"><a href="#Models-and-Training" class="headerlink" title="Models and Training"></a>Models and Training</h3><h4 id="Prototpical-Model"><a href="#Prototpical-Model" class="headerlink" title="Prototpical Model"></a>Prototpical Model</h4><p>作者将COMPLEX作为最后的关系打分函数, 将LSTM作为组合函数$f$ 和$g$, COMPLEX + LSTM作为评估方法好坏的基准模型.</p><h4 id="Diagnostic-Models"><a href="#Diagnostic-Models" class="headerlink" title="Diagnostic Models"></a>Diagnostic Models</h4><p>作者希望能量化有多少问题是无需问题的全部信息就能够给出答案, 作者提出两个用于对比的模型:</p><ul><li><strong>Predict - With - Rel</strong>: 对于问题只需要根据关系就能给出答案. 例如对于问题$(i, k, ?)$ 只需要根据$(r, ?)$ 就能够进行作答. 考虑到有头实体和尾实体的区别, 使用两个打分函数对其进行建模, 将其打分函数记为:<br>$$<br>s_{t}(k, e)=g(k)^{T} f(j), \quad s_{h}(i, k)=f(i)^{T} g(k)<br>$$</li></ul><p>  其中$s_{t}: \mathbb{R}^{o} \times \mathbb{R}^{d} \rightarrow \mathbb{R}$, $s_{h}: \mathbb{R}^{d} \times \mathbb{R}^{o} \rightarrow \mathbb{R}$, 分别为$(i, k, ?)$ 和$(?, k, j)$ 所设计.</p><ul><li><strong>Predict - With - Ent</strong>: 与前者类似, 直接忽略关系, 只计算实体对$(i, j)$ 的分数, 打分函数被设计为:</li></ul><p>$$<br>s_{e}(i, j)=f(i)^{T} f(j)<br>$$</p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>作者在<strong>测试集</strong>上分别用三种验证集选择模型, 并使用了不同的模型和不同的Leakage Removal方式, 结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/predictnewfactfromopenkg8.jpg" style="zoom: 50%;" /><p>作者在<strong>验证集</strong>上比较了不同验证集对模型性能的影响:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/predictnewfactfromopenkg9.jpg" style="zoom:50%;" /><h4 id="Influence-of-Leakage"><a href="#Influence-of-Leakage" class="headerlink" title="Influence of Leakage"></a>Influence of Leakage</h4><p>从测试集结果能看出, 不同泄露移除方式下的COMPLEX - LSTM表现不同. Leakage移除越严格, 模型的性能就越差. 说明在训练集中确实存在着大量的Test Leakage, 在开放知识图谱中, 该现象十分严重.</p><h4 id="Influence-of-Non-Relational-Information"><a href="#Influence-of-Non-Relational-Information" class="headerlink" title="Influence of Non - Relational Information"></a>Influence of Non - Relational Information</h4><p>从测试集结果中看出, 所有的只使用Entity信息预测结果均为0, 而只使用关系进行预测, 仍然能够达到全部信息预测性能的20% ~ 25%.</p><h4 id="Effectiveness-of-Mention-Ranking"><a href="#Effectiveness-of-Mention-Ranking" class="headerlink" title="Effectiveness of Mention - Ranking"></a>Effectiveness of Mention - Ranking</h4><p>从验证集结果中能看出, 使用LINKED验证集的效果是最好的. 因为在三种验证集中, 只有LINKED能够使用实体到实体提及的映射, 即Mention Ranking Protocol, 这确实证明了OKG中引入实体知识消歧的重要性.</p><h4 id="Influence-of-Model-Selection"><a href="#Influence-of-Model-Selection" class="headerlink" title="Influence of Model Selection"></a>Influence of Model Selection</h4><p>从测试集结果中看出, 在THOROUGH设定下的COMPLEX + LSTM对于不同的验证集表现不同. 人工干预最多的LINKED性能最好, 但MENTION和ALL的表现其实相差无几, 没有人工干预的ALL的表现最差. 作者认为只使用包含实体提及的验证数据就足够了, 这样可以避免昂贵的实体消歧. 此外, 即使在验证集上LINKED和MENTION表现差距很大, 但实际上在测试集中并不能很好的体现出它们二者的差异, 作者认为使用自己所提出的协议来计算MRR并不能有益于模型选择.</p><h4 id="Overall-Performance"><a href="#Overall-Performance" class="headerlink" title="Overall Performance"></a>Overall Performance</h4><p>与COMPLEX在普通KG上的性能进行对比, 在OKG上的性能下降十分明显. 作者认为有以下四个原因;</p><ol><li>OKG中的文本噪声过多.</li><li>评估数据的难度非常大.</li><li>OKG非常的零散, 导致信息流被抑制.</li><li>问题不一定知道所有的正确答案.</li></ol><h4 id="Model-and-Data-Errors"><a href="#Model-and-Data-Errors" class="headerlink" title="Model and Data Errors"></a>Model and Data Errors</h4><p>作者将错误的Link Prediction分为三类:</p><ul><li>correct sense / wrong entity: 排名最高的提及在语义上是可以理解的, 但是不正确.</li><li>wrong sense: 预测的结果小方向上是不对的, 但大方向上正确, 还靠点边.</li><li>noise: 与预测结果完全不一致, 压根不沾边.</li></ul><p>100个采样收集的错误案例分类如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/predictnewfactfromopenkg10.jpg" style="zoom: 50%;" /><p>下面一栏中, 作者评估了在OPEIC中发生的提取错误三元组和概念性的事实. 从中看到, 模型的低性能大多并不是噪声所致. 有74%的预测都是正确的但是不常见的事实.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>作者根据开放知识图谱和普通知识图谱之间的特性差异提出了Open Link Prediction, 并提出了OKG中的评估协议. 作者建立了新的数据集OLPBENCH, 包含了实体到实体提及之间的映射.</p><p>最后, 作者全面的评估了OKG中模型选择, Leakage, 非关系信息所带来的影响. </p><p>本文仍然有诸多不理解的地方, 对OKG接触的还比较少, 等有新理解后会后续补充.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> OKG </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Generative Adversarial Zero-Shot Relational Learning for KGs</title>
      <link href="/posts/62547.html"/>
      <url>/posts/62547.html</url>
      
        <content type="html"><![CDATA[<h1 id="Generative-Adversarial-Zero-Shot-Relational-Learning-for-Knowledge-Graphs"><a href="#Generative-Adversarial-Zero-Shot-Relational-Learning-for-Knowledge-Graphs" class="headerlink" title="Generative Adversarial Zero-Shot Relational Learning for Knowledge Graphs"></a>Generative Adversarial Zero-Shot Relational Learning for Knowledge Graphs</h1><p>本文是论文<a href="https://arxiv.org/abs/2001.02332" target="_blank" rel="noopener">Generative Adversarial Zero-Shot Relational Learning for Knowledge Graphs</a>的阅读笔记和个人理解. 本论文涉及到大量关于GAN的内容, 我对GAN还不是很熟悉, 在论文中具体的内容也不展开讲了, 我会放在推荐阅读中. 其中涉及到的地方如果有错误欢迎指出.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者指出, 即使是现在的大规模知识图谱, 仍然有可能无法满足日益增长的<strong>扩展</strong>需求. </p><p>对于新加入的关系, 经典的KGE算法无法应对新加入的关系. 而获取人为标注的数据是非常困难的, 因此作者希望能通过<strong>Zero - Shot Learning</strong>来缓解数据缺失的问题. 更具体点, 从关系的<strong>文本</strong>中学习它们的语义特征, 来增强在没有见过任何样本的情况下对没见过的关系的认知能力. </p><p>作者提出了一种KGE<strong>范式</strong>, 能够把<strong>任意</strong>的KGE方法应用于此.</p><h2 id="Convert-into-Knowledge-Transfer-Problem"><a href="#Convert-into-Knowledge-Transfer-Problem" class="headerlink" title="Convert into Knowledge Transfer Problem"></a>Convert into Knowledge Transfer Problem</h2><p>作者将Zero Shot Learning转化为一个<strong>知识迁移</strong>问题, 作者将关注如何只用文本描述生成没见过的关系嵌入, 这样, 只要经过训练, 模型能对任意关系在不进行Fine Tune的情况下生成嵌入. 通过关系嵌入, 能够对没有见过的关系通过<strong>余弦相似度</strong>简单的识别.</p><p>最首要的问题就是将文本语义空间的信息<strong>迁移</strong>到KG语义空间, 对此作者采用GAN来做知识迁移, 作者提出的架构如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/ganzslforkg1.jpg" style="zoom: 33%;" /><p>对于模型已经见过的文本, 训练一个生成器(Generator), 能够从文本中生成相应的<strong>Fake</strong> Relation Embedding, 再针对该关系中所涉及到的头尾实体使用某种方法编码成<strong>Real</strong> Relation Embedding. 将真与假的Relation Embedding<strong>交替</strong>输入判别器(Discriminator), 让它来判断Relation Embedding到底是真的还是假的. </p><p>当Generator生成的数据能够<strong>以假乱真</strong>时, 生成器所生成的Relation Embedding就能直接<strong>近似</strong>的当做是未见过的关系的Relation Embedding, 在Link Prediction任务中, 便能轻松应对未见过的关系.</p><h2 id="Zero-Shot-Learning-in-KG"><a href="#Zero-Shot-Learning-in-KG" class="headerlink" title="Zero - Shot Learning in KG"></a>Zero - Shot Learning in KG</h2><p>有必要稍微说一下Zero - Shot Learning在Link Prediction的设定.</p><p>对于一个查询关系三元组$\left\{\left(e_{1}, r, e_{2}\right)\right\}$, 给定头实体和关系元组$(e_1, r)$, 假设其应该对应的候选尾实体$e_2^\prime \in C_{\left(e_{1}, r\right)}$, 其真正的尾实体是$e_2$. 模型应该能将真正尾实体$e_2$ 排在最高, 而其余候选集合的实体$e_2^{\prime}$ 应该排在后面. </p><p>在Zero - Shot Learning的设置下, 还应该有见过的关系集$R_{s}=\left\{r_{s}\right\}$ 和没有见过的关系集$R_{u}=\left\{r_{u}\right\}$, 显然$R_{s} \cap R_{u}=\emptyset$.</p><p>对于训练集, 所有关系都是<strong>见过</strong>的, 即:<br>$$<br>D_{s}=\left\{\left(e_{1}, r_{s}, e_{2}, C_{\left(e_{1}, r_{s}\right)}\right) \mid e_{1} \in E, r_{s} \in R_{s}, e_{2} \in E\right\}<br>$$<br>在测试集中, 所有关系都是<strong>没有见过</strong>的, 即:<br>$$<br>D_{u}=\left\{\left(e_{1}, r_{u}, e_{2}, C_{\left(e_{1}, r_{u}\right)}\right) \mid e_{1} \in E, r_{u} \in R_{u}, e_{2} \in E\right\}<br>$$<br>出于可行性, 作者将所有实体设置为闭集, 即测试集中出现的所有<strong>实体</strong>均在训练集中<strong>见过</strong>.</p><h2 id="Model-for-Zero-Shot-KG-Relational-Learning"><a href="#Model-for-Zero-Shot-KG-Relational-Learning" class="headerlink" title="Model for Zero-Shot KG Relational Learning"></a>Model for Zero-Shot KG Relational Learning</h2><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/ganzslforkg3.jpg" style="zoom: 33%;" /><p>该图在后文中讲解关于GAN的部分会再次出现.</p><p>作者提出的方法中, 核心问题是如何设计一种<strong>条件生成模型</strong>, 去学习从文本中生成高质量的关系嵌入. 在作者的框架中, 主要设计了三个组件:</p><ul><li><strong>Feature Encoder</strong>: 仅通过实体生成真实的Relation Embedding.</li><li><strong>Generator</strong>: 在文本表示下生成合理的Relation Embedding.</li><li><strong>Discriminator</strong>: 判断数据的来源真假, 并识别<strong>关系类别</strong>.</li></ul><h3 id="Feature-Encoder"><a href="#Feature-Encoder" class="headerlink" title="Feature Encoder"></a>Feature Encoder</h3><p>对于没见过的关系, 普通KGE方法是无法取得其Embedding的.</p><p>Feature Encoder使用的Embedding可以是任意KGE模型得来的Embedding, 这也就是为什么作者说这是一种Zero - Shot的使用范式, 能够应用于任何的KGE方法.</p><p>Feature Encoder应该是在GAN训练前<strong>预先训练好</strong>的, 训练生成器和判别器时, 其参数应该不变.</p><h4 id="Network-Architecture"><a href="#Network-Architecture" class="headerlink" title="Network Architecture"></a>Network Architecture</h4><p>出于大规模KG的复杂度, 考虑到实验的可行性, 对于每个实体对中的$e$, 作者只考虑到它们的一阶邻居$\mathcal{N}_{e}$:<br>$$<br>\mathcal{N}_{e}=\left\{\left(r^{n}, e^{n}\right) \mid\left(e, r^{n}, e^{n}\right) \in \mathcal{G}\right\}<br>$$<br><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/ganzslforkg2.jpg" style="zoom: 25%;" /></p><p>假设在原来的KGE模型中, 给出的邻居关系和实体Look Up Table Embedding分别为$v_{r^{n}}, v_{e^{n}}$, 其嵌入维度为$d$, 那么该实体$e$ 一阶邻居的所有信息$u_e$ 能被表示为:<br>$$<br>\begin{array}{l}<br>f_{1}\left(v_{r^{n}}, v_{e^{n}}\right)=W_{1}\left(v_{r^{n}} \oplus v_{e^{n}}\right)+b_{1} \\<br>u_{e}=\sigma\left(\frac{1}{\left|\mathcal{N}_{e}\right|} \sum_{\left(r^{n}, e^{n}\right) \in \mathcal{N}_{e}} f_{1}\left(v_{r^{n}}, v_{e^{n}}\right)\right)<br>\end{array}<br>$$<br>其中的$\sigma$ 是$\operatorname{tanh}$. $\oplus$ 代表Concat操作. 出于可行性, 作者<strong>限制</strong>了邻居的采样个数.</p><blockquote><p>将该实体对应在图中, 这个操作其实本质上就是<strong>图聚合</strong>.</p></blockquote><p>对于实体对$(e_1, e_2)$ 中的两个实体本身也需要被编码, 只考虑用前馈神经网络简单的进行编码即可:<br>$$<br>\begin{array}{l}<br>f_{2}\left(v_{e}\right)=W_{2}\left(v_{e}\right)+b_{2} \\<br>u_{e p}=\sigma\left(f_{2}\left(v_{e_{1}}\right) \oplus f_{2}\left(v_{e_{2}}\right)\right)<br>\end{array}<br>$$<br>最后将计算出的实体对编码和实体对中实体的邻居编码Concat起来, 得到$e_1, e_2$ 之间的关系表示$x_{(e_1, e_2)}$:<br>$$<br>x_{\left(e_{1}, e_{2}\right)}=u_{e_{1}} \oplus u_{e p} \oplus u_{e_{2}}<br>$$<br>上述过程中$W_1 \in R^{d \times 2d}, W_2 \in R^{d \times d}, b_1,b_2 \in R^d$ 均为可以学习的参数.</p><h4 id="Pretrained-Strategy"><a href="#Pretrained-Strategy" class="headerlink" title="Pretrained Strategy"></a>Pretrained Strategy</h4><p>作者指出, 预训练的关键是学习到了簇状结构数据的分布, 一般具有簇内的高相似度和簇外的低相似度. 对于每个关系$r_s$, 在每个Training Step中, 采样以下三种三元组:</p><ul><li>$\left\{e_{1}^{\star}, r_{s}, e_{2}^{\star}\right\}$: 直接<strong>无差别</strong>的随机从KG中选择与$r_s$ 相关的三元组, 称为<strong>参考三元组</strong>.</li><li>$\left\{e_{1}^{+}, r_{s}, e_{2}^{+}\right\}$: 从训练集中, 采样包含关系$r_s$ 的<strong>正例三元组</strong>.</li><li>$\left\{e_{1}^{+}, r_{s}, e_{2}^{-}\right\}$: 从其余的训练集中, 做替换尾实体的<strong>负采样</strong>, 称为<strong>负例三元组</strong>.</li></ul><p>通过Feature Encoder能生成参考三元组的真实关系表示$x_{(e_1^{\star}, e_2^{\star})}$, 然后分别计算参考三元组与正例负例三元组之间的<strong>余弦相似度</strong>:<br>$$<br>\begin{aligned}<br>score^+_\omega &amp;=\operatorname{cosine}(x_{(e_1^{\star}, e_2^{\star})}, x_{(e_1^{+}, e_2^{+})}) \\<br>score^-_\omega &amp;=\operatorname{cosine}(x_{(e_1^{\star}, e_2^{\star})}, x_{(e_1^{+}, e_2^{-})})<br>\end{aligned}<br>$$<br>那么最终目标就是<strong>最大化间隔</strong>:<br>$$<br>L_{\omega}=\max \left(0, \gamma+\operatorname{score}_{\omega}^{+}-\operatorname{score}_{\omega}^{-}\right)<br>$$</p><p>其中$\gamma$ 是间隔, $\omega$ 是模型中涉及到的所有可学习参数. 通过计算余弦相似度, Feature Encoder能尽可能的将实体之间的关系<strong>聚类</strong>, 从而生成到作者所说的<strong>簇状</strong>结构数据.</p><h3 id="Generative-Adversarial-Model"><a href="#Generative-Adversarial-Model" class="headerlink" title="Generative Adversarial Model"></a>Generative Adversarial Model</h3><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/ganzslforkg3.jpg" style="zoom: 50%;" /><h4 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h4><p>生成器的作用是从<strong>嘈杂</strong>的文本$T_r$ 中生成伪造的Relation Embedding. 但因为文本中经常伴随着非常多的停止词之类的<strong>无意义词语</strong>, 所以作者简单的<strong>去除停止词和标点</strong>, 并使用<strong>TF - IDF</strong>来分配词语的权重. 词向量直接使用<strong>Word2Vec</strong>将词语转化为稠密向量. 在Sentence Level Modeling上, 作者使用无视语序的<strong>词袋模型</strong>对句子建模. </p><blockquote><p>至于为什么作者没有在词向量上使用BERT, 在后文的实验中作者做了探究.</p></blockquote><p>Generator在生成数据时, 一般都需要加入一些<strong>噪声</strong>. 在这里, 作者加入高斯随机噪声$z \in R^Z$, $z$ 是从高斯分布$N(0, 1)$ 中采样的来的向量.</p><p>作者将高斯噪声$z$ 和TF - IDF后的句向量Concat起来, 经过两个FC层和一个Layer Norm, 最后得到生成器$G$ 通过文本生成的Relation Embedding $\tilde{x}_{r}$, 即$\tilde{x}_{r} \leftarrow G_{\theta}\left(T_{r}, z\right)$, 其中$\theta$ 是参数.</p><p> 对于GAN的训练方面, 为了避免<strong>模型崩溃</strong>并<strong>增强多样性</strong>(在推荐阅读中, 有解释GAN训练上出现的常见问题), 作者使用了WGAN中的Wasserstein Loss.</p><p>作者继续添加了分类损失, 其形式与Feature Encoder中使用的最大化间隔损失类似, 计算它分别与正例(在GAN训练时正例指$\tilde{x}_{r}$)和负例三元组之间的相似度, 但作者将<strong>簇中心</strong>$x_c^r$ 作为真实的关系表示:<br>$$<br>x_{c}^{r}=\frac{1}{N_{r}} \sum_{i=1}^{N_{r}} x_{\left(e_{1}, e_{2}\right)}^{i}<br>$$<br>其中$N_r$ 为涉及到关系$r_s$ 所有的三元组个数.</p><p>那么分类损失可以被写成:<br>$$<br>\begin{aligned}<br>score^+_\omega &amp;=\operatorname{cosine}(x_{c}^r, \tilde x_r) \\<br>score^-_\omega &amp;=\operatorname{cosine}(x_{c}^r, x_{(e_1^{+}, e_2^{-})}) \\<br>L_{cls}\left(G_{\theta}\left(T_{r}, z\right)\right)&amp;=\max \left(0, \gamma+\operatorname{score}_{\omega}^{+}-\operatorname{score}_{\omega}^{-}\right)<br>\end{aligned}<br>$$<br>最后, 作者还添加了Visual Pivot Regularization, 用于增加类内的区别. </p><p>生成器的损失函数如下:</p><p>$$<br>L_{G_{\theta}}=-\mathbb{E}_{z \sim p_{z}}\left[D_{\phi}\left(G_{\theta}\left(T_{r}, z\right)\right)\right] +L_{c l s}\left(G_{\theta}\left(T_{r}, z\right)\right)+L_{P}<br>$$<br>第一项是Wasserstein Loss, 第二项是Classification Loss, 第三项是Visual Pivot Regularization.</p><h4 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h4><p>判别器的作用是用于识别输入是否是伪造的. 生成器的结构比较简单, 只由一层使用Leaky ReLU的FC层, 接上Layer Norm组成, 最后结果分别用于判别真假和对关系分类, 损失函数如下:</p><p>$$<br>L_{D_{\phi}}=\mathbb{E}_{z \sim p_{z}}\left[D_{\phi}\left(G_{\theta}\left(T_{r}, z\right)\right)\right]-\mathbb{E}_{x \sim p_{\text {data}}}\left[D_{\phi}(x)\right]<br>+\frac{1}{2} L_{c l s}\left(G_{\theta}\left(T_{r}, z\right)\right)+\frac{1}{2} L_{c l s}(x)+L_{G P}<br>$$<br>第一二项仍然来自于Wasserstein Loss, 第三四项来源于分类, 分别对应着生成器的假数据$\tilde{x}_{r}$ 和Feature Encoder生成的真实数据$x_{(e_1, e_2)}$, 最后一项是梯度惩罚项.</p><blockquote><p>GAN相关的细节还不太懂, 不瞎解释了.</p></blockquote><h4 id="GAN-Training-Process"><a href="#GAN-Training-Process" class="headerlink" title="GAN Training Process"></a>GAN Training Process</h4><p>在讲解完生成器和判别器后, 作者将GAN训练流程总结如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/ganzslforkg4.jpg" style="zoom: 50%;" /><p>在GAN中, 通常采用多次更新判别器参数后只更新一次生成器的策略.</p><h3 id="Predicting-Unseen-Relations"><a href="#Predicting-Unseen-Relations" class="headerlink" title="Predicting Unseen Relations"></a>Predicting Unseen Relations</h3><p>在训练完GAN后, 生成器对于一个没有见过的关系描述文本$T_{r_u}$ 应该能给出合理的Relation Embedding, 即$\tilde{x}_{r_u} \leftarrow G_{\theta}\left(T_{r_u}, z\right)$. 在已知$(e_1, r_u)$ 的情况下, 我们应该根据生成器所生成的$\tilde{x}_{r_u}$ 与Feature Encoder产生的$x_{(e_1, e_2)}$ 的余弦相似度来评估计排名:<br>$$<br>\operatorname{score}_{\left(e_{1}, r_{u}, e_{2}\right)}=\operatorname{cosine}\left(\tilde{x}_{r_u}, x_{(e_1, e_2)}\right)<br>$$<br>但生成器会为生成的数据添加<strong>噪声</strong>, 为了尽可能弱化随机造成的影响, 作者采用生成多组数据最后取平均的方法计算得分: </p><p>$$<br>\operatorname{score}_{\left(e_{1}, r_{u}, e_{2}\right)}=\frac{1}{N_{\text {test}}} \sum_{i=1}^{N_{\text {test}}} \text {score}_{\left(e_{1}, r_{u}, e_{2}\right)}^{i}<br>$$</p><p>即生成任意数量$N_{test}$ 的Relation Embedding$\left\{\tilde{x}_{r_{u}}^{i}\right\}_{i=1,2, \dots, N_{\text {test}}}$, 最后取平均余弦相似度作为真正的得分.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>在本节中, 详细的参数设置请参考原论文.</p><h3 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h3><p>作者发现没有可用的ZS数据集, 所以作者自己根据需要制作了两个数据集:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/ganzslforkg5.jpg" style="zoom: 50%;" /><p>作者选取数据集的标准是, 满足<strong>大规模</strong>, 并含有<strong>关系文本描述</strong>.</p><h3 id="Baselines"><a href="#Baselines" class="headerlink" title="Baselines"></a>Baselines</h3><p>在非常普遍使用的KGE方法中, 它们基本<strong>不具备</strong>Zero - Shot的能力, 因为对于没有见过的关系, 在它们所存储的Embedding矩阵中是查询不到的, 所以作者用与Generator类似的方法为它们添加了Zero - Shot的能力: 使用一个与Generator类似的神经网络结构, 而不是直接为未知的关系生成随机的Relation Embedding. 在这个条件下, 模型能够直接根据文本信息生成未见过关系的Embedding, 然后继续结合它们原来的目标函数来调整Entity Embedding和与Generator类似的结构的参数.</p><p>例如, 在TransE中, Entity Embedding和Relation Embedding都是使用一个Look Up Table存储的:<br>$$<br>f_{\text {Trans} E}(\boldsymbol{h}, \boldsymbol{r}, \boldsymbol{t})=\left|v_{h}+v_{r}-v_{t}\right|_{1 / 2}<br>$$<br>在经过改造后, 它们的Relation Embedding不再通过Look Up Table给出, 而是通过类似生成器的结构$g$ 从文本$T_r$ 中生成:<br>$$<br>v_{r}=g\left(T_{r}\right)<br>$$<br>在对实体排名时, 也是按照它们原本的打分函数进行排名.</p><p>对于DisMult, ComplEx亦是如此. RESCAL都没有替换Relation Embedding的位置, 所以就不对它进行比较了.</p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>作者将改进后的Zero - Shot模型在自己制作的数据集NELL - ZS和Wiki - ZS上测试了它们Link Prediction的能力, 结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/ganzslforkg6.jpg" style="zoom: 33%;" /><p>其中ZSGAN代表使用了GAN训练框架, 即在之前的基础上加入了判别器, 并引入GAN相关的训练损失.下划线代表该结果在ZSGAN中是最棒的.</p><p>能从结果中观察到, 所有使用ZSGAN的模型都比未使用GAN的Baseline效果要好, 并且在这些模型中, DisMult体现出更好的能力.</p><h3 id="Analysis-of-Textual-Representations"><a href="#Analysis-of-Textual-Representations" class="headerlink" title="Analysis of Textual Representations"></a>Analysis of Textual Representations</h3><h4 id="Text-Descriptions-Analysis"><a href="#Text-Descriptions-Analysis" class="headerlink" title="Text Descriptions Analysis"></a>Text Descriptions Analysis</h4><p>作者将自制的两个数据集中的关系文本描述词频统计了出来:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/ganzslforkg7.jpg" style="zoom: 50%;" /><p>NELL - ZS的文本长度明显没有Wiki - ZS高.</p><p>在经过TF - IDF过滤后, 作者统计了TF - IDF &gt; 0.3的词语个数:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/ganzslforkg8.jpg" style="zoom: 50%;" /><p>在使用了TF - IDF后, 大多数重要的词语都落在$[2, 5]$ 区间内, 说明TF - IDF能够比较有效的降噪.</p><h4 id="Word-Representations"><a href="#Word-Representations" class="headerlink" title="Word Representations"></a>Word Representations</h4><p>作者其实尝试过现在非常流行的词向量表示BERT(Transformer Encoder):</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/ganzslforkg10.jpg" style="zoom: 50%;" /><p>但实际上效果不理想, 作者认为其中与以下原因有关:</p><ol><li>BERT引入了更多的<strong>句子级噪声</strong>, 在这个任务上似乎句子中的信息是无效的.</li><li>BERT产生的Word Embedding<strong>维度过高</strong>, 不利于GAN的训练.</li></ol><blockquote><p>我认为跟BERT的打开方式有一定的关联, Self - Attention本身就会根据句子中的其他内容来调整自身的表达, 跟TF - IDF的功能有一部分重复的地方.</p></blockquote><h3 id="Quality-of-Generated-Data"><a href="#Quality-of-Generated-Data" class="headerlink" title="Quality of Generated Data"></a>Quality of Generated Data</h3><p>作者为了评估生成器所生成的Relation Embedding的质量, 作者计算Feature Encoder生成的簇中心$x_r^c$ 和生成器生成的关系嵌入$\tilde x_r$ 之间的<strong>余弦相似度</strong>:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/ganzslforkg9.jpg" style="zoom: 50%;" /><p>能看到, 生成器所生成的关系嵌入的余弦相似度基本与模型的MRR和Hits@10表现成正比.</p><h3 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h3><p>作者尝试过类似CNN, LSTM对Sentence建模的方法, 但都失败了. 作者认为这些方法都引入了更多的参数, 从而不利于GAN的学习, 会导致GAN的过拟合. 所以对句子建模只是使用简单的使用词袋模型, 结合TF - IDF. 此外, 现在作者方法中的实体均为闭集, 将来会考虑到实体在开集中的处理.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>本文非常巧妙的想到用GAN结合KGE处理在未见过的关系上的问题. 作者通过使用Feature Encoder来构造真实的数据分布, 并通过一个简单的生成器来伪造数据, 当生成器能够很好的骗过判别器后, 直接使用生成器生成的Relation Embedding近似当做真实的Relation Embedding.</p><p>作者声称, 这是在KG领域的首个Zero - Shot关系学习方法.</p><p>抛开作者提出的方法本身不谈, 就作者在论文中提到关于GAN的内容来说, 有很大一部分是关于GAN的<strong>训练Trick</strong>. GAN虽然是一个非常好的idea. 但<strong>GAN的训练</strong>可能是一个很大的问题, 这点在实验结果中也多次提到, 在模型中引入的其他部分都有可能会对GAN的训练产生很大影响, 导致训练的不稳定.</p><h2 id="Recommended"><a href="#Recommended" class="headerlink" title="Recommended"></a>Recommended</h2><p>关于本文的:</p><ul><li><a href="https://www.bilibili.com/video/av541082248/" target="_blank" rel="noopener">AAAI2020丨知识图的生成性对抗式零样本关系学习</a></li><li><a href="https://zhuanlan.zhihu.com/p/112908641" target="_blank" rel="noopener">《基于对抗生成的Zero-Shot知识图谱关系学习》阅读笔记</a></li></ul><p>关于文中涉及到的GAN:</p><ul><li><a href="https://zhuanlan.zhihu.com/p/25071913" target="_blank" rel="noopener">令人拍案叫绝的Wasserstein GAN</a></li><li><a href="https://zhuanlan.zhihu.com/p/68120231" target="_blank" rel="noopener">提高GAN训练稳定性的9大tricks</a></li><li><a href="https://zhuanlan.zhihu.com/p/84617531" target="_blank" rel="noopener">Wasserstein距离学习笔记</a></li><li><a href="https://zhuanlan.zhihu.com/p/33752313" target="_blank" rel="noopener">通俗理解生成对抗网络GAN</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> GAN </tag>
            
            <tag> ZSL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>R-MeN: A Relational Memory-based Embedding Model</title>
      <link href="/posts/2954.html"/>
      <url>/posts/2954.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>Self - Attention: 详见<a href="https://adaning.github.io/posts/6744.html#Self-Attention">Transformer精讲</a>.</li></ul><p><strong>2020.12.14</strong>: 修正错误.</p></blockquote><h1 id="A-Relational-Memory-based-Embedding-Model-for-Triple-Classification-and-Search-Personalization"><a href="#A-Relational-Memory-based-Embedding-Model-for-Triple-Classification-and-Search-Personalization" class="headerlink" title="A Relational Memory-based Embedding Model for Triple Classification and Search Personalization"></a>A Relational Memory-based Embedding Model for Triple Classification and Search Personalization</h1><p>本文是论文<a href="https://arxiv.org/abs/1907.06080" target="_blank" rel="noopener">A Relational Memory-based Embedding Model for Triple Classification and Search Personalization</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者认为, 现在的KGE方法普遍在<strong>记忆有效的三元组问题</strong>上受限, 并不能<strong>有效的捕捉三元组之间的潜在依赖关系</strong>.</p><p>现在的KGE普遍都用Link Prediction评估模型的性能, 但在<strong>实际应用</strong>中, 经常以<strong>三元组分类</strong>和<strong>个性化搜索</strong>为主要目标. 所以作者希望通过记忆提升这两个场景中的KGE性能, 关系记忆能够编码关系三元组之间的潜在依赖关系.</p><blockquote><p>作者给出以下问题的定义:</p><ul><li>三元组分类: 判断给定的三元组是否<strong>有效</strong>.</li><li>个性化搜索: 针对用户给出的<strong>请求</strong>, 对系统给出的搜索结果进行<strong>重排</strong>.</li></ul></blockquote><h2 id="R-MeN"><a href="#R-MeN" class="headerlink" title="R - MeN"></a>R - MeN</h2><p>在R - MeN中, 作者考虑将三元组视为一个长度为3的<strong>时序序列</strong>看待, 通过<strong>Self - Attention</strong>对记忆<strong>循环交互</strong>.</p><p>整体流程是Embedding -&gt; Memory Attention interact -&gt; CNN decode:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rmen1.jpg" style="zoom: 50%;" /><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>作者假设三元组$(s, r, o)$ 的<strong>相对位置关系</strong>对于推断三元组是否有效是十分必要的, 因此需要在三元组输入之前对它们都加以位置编码, 用$\mathbf{v}_{s}, \mathbf{v}_{r}, \mathbf{v}_{o} \in \mathbb{R}^{d}$ 分别代表$s,r,o$ 的嵌入, 加上位置编码后, 再经过一次线性变换的三元组向量$\left\{\mathbf{x}_{1}, \mathbf{x}_{2}, \mathbf{x}_{3}\right\}$ 为:<br>$$<br>\begin{aligned}<br>\mathbf{x}_{1} &amp;=\mathbf{W}\left(\mathbf{v}_{s}+\mathbf{p}_{1}\right)+\mathbf{b} \\<br>\mathbf{x}_{2} &amp;=\mathbf{W}\left(\mathbf{v}_{r}+\mathbf{p}_{2}\right)+\mathbf{b} \\<br>\mathbf{x}_{3} &amp;=\mathbf{W}\left(\mathbf{v}_{o}+\mathbf{p}_{3}\right)+\mathbf{b}<br>\end{aligned}<br>$$<br>其中$\mathbf{W} \in \mathbb{R}^{k\times d}$ 为权重矩阵, $\mathbf{p}_{1}, \mathbf{p}_{2} \mathbf{p}_{3} \in \mathbb{R}^{d}$  分别它们的位置编码, 其中的$k$ 代表每个Memory Slot的维度大小(下文会提到). 在文中, 并没有提及位置编码的实现方式. </p><h3 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h3><p>作者假设有一个记忆$M \in \mathbb{R}^{N \times k}$, 它是由$N$ 行Memory Slot组成的, 每个Slot的大小为$k$. 用$M^{(t)}$ 代表$t$ 时刻的$M$ 矩阵, 那么$M_{i,:}^{(t)} \in \mathbb{R}^{k}$ 就代表其中第$i$ 个Memory Slot在$t$ 时刻存储的所有Memory.</p><p>那么每个Memory Slot中的记忆就一定会根据在$t$ 时刻的新输入$\mathbf{x}_t$ 来<strong>更新</strong>当前时刻的记忆$\hat{M}_{i,:}^{(t+1)}$.</p><p>如果使用跟Transformer中的<strong>多头</strong>类似的机制, 对每个Slot中的Memory进行分割:<br>$$<br>\hat{M}_{i,:}^{(t+1)}=\left[\hat{M}_{i,:}^{(t+1), 1} \oplus \hat{M}_{i,:}^{(t+1), 2} \oplus\right.\left.\ldots \oplus \hat{M}_{i,:}^{(t+1), H}\right]<br>$$<br>其中$\oplus$ 代表Concat操作, $H$ 代表总头数.</p><p>每个头的记忆$\hat{M}_{i,:}^{(t+1), h}$ 使用Self - Attention将过去时刻的记忆与当前时刻输入<strong>加权交互</strong>:<br>$$<br>\hat{M}_{i,:}^{(t+1), h}=\alpha_{i, N+1, h}\left(\mathbf{W}^{h, V} \mathbf{x}_{t}\right)+\sum_{j=1}^{N} \alpha_{i, j, h}\left(\mathbf{W}^{h, V} M_{j,:}^{(t)}\right)<br>$$<br>其中$\mathbf{W}^{h, V} \in \mathbb{R}^{n \times k}$ 是Value的投影矩阵, $n$ 是每个头的大小, 满足$k = nH$, 这样能保证Concat后的记忆矩阵中每个Slot的大小仍然是$k$ 维. $\left\{\alpha_{i, j, h}\right\}_{j=1}^{N}, \alpha_{i, N+1, h}$ 是注意力权重. 具体细节在下一小节介绍.</p><blockquote><p>三元组中的三个元素被分别输入后, 记忆将被<strong>清除</strong>.</p></blockquote><h3 id="Self-Attention-Interaction"><a href="#Self-Attention-Interaction" class="headerlink" title="Self - Attention Interaction"></a>Self - Attention Interaction</h3><p>与Self - Attention大同小异, 但在<strong>记忆存储机制</strong>的影响下, 归一化一定要包含<strong>过去记忆</strong>和<strong>当前时刻新输入</strong>两个部分:<br>$$<br>\begin{aligned}<br>\alpha_{i, j, h} &amp;=\frac{\exp \left(\beta_{i, j, h}\right)}{\sum_{m=1}^{N+1} \exp \left(\beta_{i, m, h}\right)} \\<br>\alpha_{i, N+1, h} &amp;=\frac{\exp \left(\beta_{i, N+1, h}\right)}{\sum_{m=1}^{N+1} \exp \left(\beta_{i, m, h}\right)}<br>\end{aligned}<br>$$<br>$\beta_{i, j, h}$ 是Attention得分, 也分别对应着过去记忆和当前时刻新输入两部分:<br>$$<br>\begin{aligned}<br>\beta_{i, j, h} &amp;=\frac{\left(\mathbf{W}^{h, Q} M_{i,:}^{(t)}\right)^{\top}\left(\mathbf{W}^{h, K} M_{j,:}^{(t)}\right)}{\sqrt{n}} \\<br>\beta_{i, N+1, h} &amp;=\frac{\left(\mathbf{W}^{h, Q} M_{i,:}^{(t)}\right)^{\top}\left(\mathbf{W}^{h, K} \mathbf{x}_{t}\right)}{\sqrt{n}}<br>\end{aligned}<br>$$<br>其中$\mathbf{W}^{h, Q} \in \mathbb{R}^{n \times k}$ , $\mathbf{W}^{h, K} \in \mathbb{R}^{n \times k}$ 分别是Query和Key的投影矩阵. 此外, 还在$\mathbf{x}_{t}$ 和$\hat{M}_{i,:}^{(t+1), h}$ 之间添加了Residual Connection和MLP, Gating, 具体添加方式作者未说明, 应该和关系记忆网络一致:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rmen7.jpg" style="zoom:33%;" /><p>图片来自于关系记忆网络的论文<a href="https://arxiv.org/abs/1806.01822" target="_blank" rel="noopener">Relational recurrent neural networks</a>. </p><p>对于输入$\mathbf{x}_t$ 的最后编码产生的结果记为$\mathbf{y}_t \in \mathbb{R}^k$.</p><h3 id="Score-Function"><a href="#Score-Function" class="headerlink" title="Score Function"></a>Score Function</h3><p>将之前得出的$\left\{\mathbf{y}_{1}, \mathbf{y}_{2}, \mathbf{y}_{3}\right\}$ Concat起来, 接着用一个基于CNN的Decoder来解码:<br>$$<br>f(s, r, o)=\max \left(\operatorname{ReLU}\left(\left[\mathbf{y}_{1}, \mathbf{y}_{2}, \mathbf{y}_{3}\right] \ast \mathbf{\Omega}\right)\right)^{\top} \mathbf{w}<br>$$<br>$\left[\mathbf{y}_{1}, \mathbf{y}_{2}, \mathbf{y}_{3}\right] \in \mathbb{R} ^ {k \times 3}$, $\Omega$ 代表大小为$\mathbb{R} ^ {m \times 3}$ 的卷积核集合, 其中$m$ 为卷积核的窗口大小, $\ast$ 代表卷积操作. $\mathbf{w} \in \mathbb{R}^{\left| \Omega \right|}$ 是一个权重向量, 能将所有卷积核卷积得到的结果变成标量(其实就是得分). $\operatorname{max}$ 在这里代表<strong>最大池化</strong>. 作者认为这样能捕捉Feature map中的最重要特征, 并减小参数量. </p><h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><p>采用Adam优化, 损失函数如下:<br>$$<br>\mathcal{L}=\sum_{(s, r, o) \in\left\{\mathcal{G} \cup \mathcal{G}^{\prime}\right\}} \log \left(1+\exp \left(-t_{(s, r, o)} \cdot f(s, r, o)\right)\right)<br>$$<br>其中$t_{(s, r, o)}$ 是一个符号函数:<br>$$<br>t_{(s, r, o)}=\left\{\begin{array}{l}<br>1 \text { for }(s, r, o) \in \mathcal{G} \<br>-1 \text { for }(s, r, o) \in \mathcal{G}^{\prime}<br>\end{array}\right.<br>$$<br>其中$\mathcal{G}$ 代表正确的知识图谱, $\mathcal{G}^{\prime}$ 代表被替换后污染的知识图谱(负例的知识图谱, 其中包含负例三元组).</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>作者主要面向实际应用中比较多的三元组分类和个性化搜索两个任务进行实验. 详细的模型参数设置请参考原论文.</p><h3 id="Triple-Classification"><a href="#Triple-Classification" class="headerlink" title="Triple Classification"></a>Triple Classification</h3><p>三元组分类的问题是给定一个三元组, 通过模型来判断它们是否有效. 作者在WN11和FB13数据集上测试了R - MeN的三元组分类性能. 结果取决于一个阈值$\theta_r$, 如果高于阈值则视为三元组有效, 否则无效. </p><p>虽然前面介绍的Memory可以包含多个Slots, 但通过实验后, 作者发现对于所有的数据集, Memory Slot为1的时候效果都是最好的. 所以在之后的实验中只考虑使用单一的Memory Slot. </p><blockquote><p>如果Memory Slot = 1, 关系记忆网络应该类似于GRU, 感觉有点退化的意思. 我始终没有想明白Single slot能work的理由.</p></blockquote><p>实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rmen2.jpg" style="zoom: 50%;" /><p>其中下划线代表效果次高的模型, 最下面一栏是借助了关系路径推理的模型.</p><p>R - MeN在这两个数据集上平均表现最好, 并且比TransE要好许多. </p><p>作者将TransE和R - MeN在WN11和FB13上针对关系的准确率进行了对比:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rmen3.jpg" style="zoom: 50%;" /><p>在WN11中, 对于一对一的关系<code>similar_to</code>, R - MeN比TransE有非常强的学习能力. 在<strong>作者给出的</strong>FB13关系准确率中, R - MeN比TransE要好.</p><h3 id="Search-Personalization"><a href="#Search-Personalization" class="headerlink" title="Search Personalization"></a>Search Personalization</h3><p>个性化搜索的问题被定义为根据用户给出的搜索请求, 目标是根据搜索系统给出的搜索结果进行重排. 这个搜索场景能被视为是三元组$(query, user, document)$, 所以R - MeN也能用于个性化搜索任务上.<br>作者在SEARCH17数据集上进行了实验, 结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rmen4.jpg" style="zoom: 25%;" /><p>KGE方法在个性化搜索任务上比传统方法要好一些, R - MeN显示出更好的性能.</p><h3 id="Effects-of-Hyper-Parameters"><a href="#Effects-of-Hyper-Parameters" class="headerlink" title="Effects of Hyper-Parameters"></a>Effects of Hyper-Parameters</h3><p>作者将多头的头大小$n$, 头的个数$H$ 在各个数据集上的性能进行对比:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rmen5.jpg" style="zoom: 80%;" /><p>在三个数据集上, 使用更大的头有利于性能提升. 而在头的个数上, WN11和FB13中使用多头效果要更好, 在SEARCH17中单头效果更好, 作者认为SEARCH17是单意图的, 所以通常用1个头的效果会比较好. 因为头越多, 注意力就越分散, 就越容易Over Fitting. </p><h3 id="Ablation-Analysis"><a href="#Ablation-Analysis" class="headerlink" title="Ablation Analysis"></a>Ablation Analysis</h3><p>在消融实验中, 作者尝试去掉R - MeN的位置编码, 不使用关系记忆网络:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rmen6.jpg" style="zoom: 50%;" /><p>在不使用关系记忆网络时, 打分函数直接就变为了接收最原始的实体和关系Embedding $\mathbf{v}_{s}, \mathbf{v}_{r}, \mathbf{v}_{o}$ 作为输入:<br>$$<br>f(s, r, o)=\max \left(\operatorname{ReLU}\left(\left[\mathbf{v}_{s}, \mathbf{v}_{r}, \mathbf{v}_{o}\right] \ast \mathbf{\Omega}\right)\right)^{\top} \mathbf{w}<br>$$<br>去掉位置编码后, 在SEARCH17上的表现退化比较大, WN11没有变化, FB13有一点点下降. 在不使用关系记忆网络时, 所有性能都有退化, 关系记忆网络体现出比较大的作用.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>作者将<strong>关系记忆网络</strong>用于KGE中, 并将三元组视为<strong>时序序列输入</strong>来看待. 并同时指出了KGE模型普遍采用Link Prediction来评估KGE模型的问题.</p><p>普通的KGE方法不能间接捕捉三元组内的联系, 只能用知识图谱中给出的实体间关系<strong>显式</strong>的学习它们之间的联系. 但通过关系记忆网络, 能够从三元组内的其他元素遗留下来的信息中学到一些潜在的联系.</p><p>循环记忆机制一直都有一个诟病: 自回归. 也就是当前时刻输入必须使用必须以上一个时刻的输出, 导致整个过程只能<strong>串行</strong>而不能并行. </p><blockquote><p>另外, 在使用Single Slot的时候, Memory可能就退化成类似GRU的结构了, 有点像Triple Level的RNN… 暂时还不知道这个看法的对错.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> Attention </tag>
            
            <tag> 记忆网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于轻量级卷积和动态卷积替代的注意力机制</title>
      <link href="/posts/40162.html"/>
      <url>/posts/40162.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>Depthwise Convolution: 详见<a href="https://adaning.github.io/posts/13629.html">深度可分离卷积与分组卷积</a>.</li><li>Attention: 详见<a href="https://adaning.github.io/posts/40071.html">Seq2Seq和Attention</a>.</li><li>Transformer: 详见<a href="https://adaning.github.io/posts/6744.html">Transformer精讲</a>.</li></ul></blockquote><p>本文是论文<a href="http://arxiv.org/abs/1901.10430" target="_blank" rel="noopener">PAY LESS ATTENTION WITH LIGHTWEIGHT AND DYNAMIC CONVOLUTIONS</a>的阅读笔记和个人理解. 因为本文的图片均来自于原论文或Reference中的文章, 我觉得列出的几篇文章都很好, 图片特别有助于讲解. 论文中还有大量我不了解的知识, 再有相关的东西再进行补充.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>Self - Attention虽然解决了长距离依赖问题, 但因为计算量大, 必须对文本长度进行限制. 牵扯到计算效率的问题, 自然而然的就想到高效而体积小的<strong>卷积</strong>. 作者希望用轻量级卷积实现类似Self - Attention的效果.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lightweightconv7.jpg" style="zoom: 67%;" /><p>左侧为Self - Attention的权重生成方式, 右侧为Dynamic Convolution的权重生成方式.</p><p>作者希望能通过仅通过每个时刻的输入就能生成注意力权重, 而非像Self - Attention一样依赖全局输入生成.</p><h2 id="Depthwise-Convolution-and-Convolution-1D"><a href="#Depthwise-Convolution-and-Convolution-1D" class="headerlink" title="Depthwise Convolution and Convolution 1D"></a>Depthwise Convolution and Convolution 1D</h2><p>Lightweight Convolution的核心是<strong>单个维度上的深度卷积</strong>(Depthwise Convolution). </p><h3 id="Convolution-in-1-Dimension"><a href="#Convolution-in-1-Dimension" class="headerlink" title="Convolution in 1 Dimension"></a>Convolution in 1 Dimension</h3><p>无论在多少Dimension的卷积中, Channel维对应的是输入数据相独立的”<strong>厚度</strong>“这个维度, 它必须能保留输入单元的原始信息, 以保证不同输入元素之间的交互.</p><p>例如Conv2d在CV中, 为了保留二维图片的<strong>平面位置信息</strong>, Channel维对应的是Depth维, 而Conv1d在NLP中, 为了保留一维序列的<strong>先后顺序信息</strong>, <strong>Channel维所对应的是词向量的Hidden维</strong>. 这里先入为主, </p><blockquote><p>我开始是Channel维没找对, 卡了很长时间, 希望大家在看的时候从这个角度先入为主.</p></blockquote><h3 id="Depthwise-Convolution"><a href="#Depthwise-Convolution" class="headerlink" title="Depthwise Convolution"></a>Depthwise Convolution</h3><p>深度卷积是一种对每个Channel分别卷积的卷积方式:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/dsconv3.jpg" style="zoom: 33%;" /><p>假设你已经具备了深度卷积的基础, 其数学表达如下:<br>$$<br>O_{i, c}=\operatorname{DepthwiseConv}\left(X, W_{c,:}, i, c\right)=\sum_{j=1}^{k} W_{c, j} \cdot X_{\left(i+j-\left\lceil\frac{k+1}{2}\right\rceil\right), c}<br>$$<br>其中$c$ 代表Channel, $k$ 为卷积核宽度, $i$ 为特征图中Token的位置. 那么$W_{c, :}$ 就代表指定Channel $c$ 的卷积核权重. </p><p>现在我们对已经嵌入好的序列输入做卷积. 如果我们使用的是<strong>标准卷积</strong>, 那么每次卷积的部分都必须<strong>贯穿Channel维</strong>, 即对每个Channel使用不同的参数:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lightweightconv1.jpg" style="zoom: 50%;" /><p>因为不同Channel上不共享相同的权重, 所以此时卷积核的参数量为$d_{in} \cdot d_{out} \cdot k$.</p><p>如果使用<strong>深度卷积</strong>, 那么每次卷积就只在每个Channel上单独进行:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lightweightconv2.jpg" style="zoom:50%;" /><p>如果$d_{in} = d_{out}=d$, 即<strong>维持卷积前后的Channel数不变</strong>, 这样参数一下就从$d^2k$ 降到了$dk$, 因为我们不用再做贯穿整个Channel的运算了.</p><p>如果我们拿深度卷积的式子举个例子, 当$i=2, c=5, k=3$ 时(即图中所对应的绿色卷积过程), 那么在特征图上第2行第5列的输出$O_{2, 5}$ 则为$O_{2, 5} =W_{5, 1:3}X_{1:3, 5}$.</p><blockquote><p>我开始不能理解为什么参数能下降$d$ 倍, 后来发现我一直都忽视了”<strong>代替Self - Attention</strong>“这个目的, 在Self - Attention前后, Channel是不变的, 即$d_{in}=d_{out}$. 因此作者说法无误.</p></blockquote><h2 id="Lightweight-Convolution"><a href="#Lightweight-Convolution" class="headerlink" title="Lightweight Convolution"></a>Lightweight Convolution</h2><p>轻量级卷积在深度卷积的基础上进一步改进, 它引入了<strong>多头共享权重</strong>机制, 使得参数进一步减少.</p><h3 id="Weight-sharing"><a href="#Weight-sharing" class="headerlink" title="Weight sharing"></a>Weight sharing</h3><p>为了进一步减少参数量, 作者将Channel维拆成$H$ 个头, 在同一个头覆盖的区域内<strong>仅使用一个头</strong>(或者说在同一个头内的每个Channel维上的卷积核参数相同):</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lightweightconv3.jpg" style="zoom: 50%;" /><p>例如上图中, 橙色区域的两个Channel共享同一个卷积核.</p><p>经过权重共享, 每个头所占有的Channel数为$\frac{d}{H}$, 这样参数量就缩小为了原来的$\frac{d}{H}$ 倍. 若有$H$ 个头, 参数量进一步从$d \cdot k$ 缩减到$H \cdot k$. </p><p>轻量级卷积的数学表达如下:</p><p>$$<br>\operatorname{LightConv}\left(X, W_{\lceil\frac{c H}{d}\rceil,:}, i, c\right)=\operatorname{ DepthwiseConv }\left(X, \operatorname{softmax}\left(W_{\lceil\frac{c H}{d}\rceil,:}\right), i, c\right)<br>$$</p><p>其中$\lceil\frac{c H}{d}\rceil$ 指的是$c$ 属于哪个头, $\frac{c}{d}$ 指的是Channel $c$ 在Channel维总深度$d$ 中所处的位置百分比. 因为一共就有$H$ 个头, 所以每个头占总Channel的$\frac{1}{H}$. 那么$\lceil\frac{c}{d} \div \frac{1}{H}\rceil$ 就能算出$c$ 的位置前面到底有几个头, 向上取整就是$c$ 所属的头位置.</p><p>输入时对卷积核Softmax归一化详见下一小节.</p><h3 id="Softmax-Normalization"><a href="#Softmax-Normalization" class="headerlink" title="Softmax Normalization"></a>Softmax Normalization</h3><p>我们最初的目标是通过卷积来代替自注意力机制, 所以仅仅通过卷积得到还不足够, 我们必须将其归一化形成<strong>权重</strong>, 才能够符合注意力权重的标准. 由于权重是对不同时间步分配的, 所以在卷积核大小$k$ 对卷积核<strong>内部进行</strong>权重归一化:</p><p>$$<br>\operatorname{softmax}(W)_{h, j}=\frac{\exp W_{h, j}}{\sum_{j^{\prime}=1}^{k} \exp W_{h, j^{\prime}}}<br>$$</p><p>其中$W \in \mathbb{R}^{H \times k}$, 为多头卷积核的权重矩阵. 通过Softmax归一化, 同一个卷积核中的参数只能得到<strong>固定</strong>的注意力权重, 因为我们目前仅仅是对卷积核权重这个固定参数上归一化, 而没有结合当前时刻的输入信息.</p><h3 id="Gated-Linear-Units"><a href="#Gated-Linear-Units" class="headerlink" title="Gated Linear Units"></a>Gated Linear Units</h3><p>GLU是一种应用在CNN上的一种<strong>门控机制</strong>, 于<a href="http://proceedings.mlr.press/v70/dauphin17a" target="_blank" rel="noopener">Language Modeling with Gated Convolutional Networks</a>中提出, 该结构能够提升CNN抽取高层抽象特征的能力, 其核心思想如下:</p><p>$$<br>h_{l}(\mathbf{X})=(\mathbf{X} \ast \mathbf{W}+\mathbf{b}) \otimes \sigma(\mathbf{X} \ast \mathbf{V}+\mathbf{c})<br>$$</p><p>其中$\ast$ 代表卷积操作, $\mathbf{X}$ 代表输入矩阵, $\mathbf{W}, \mathbf{V}$ 代表两卷积核, $\mathbf{b}, \mathbf{c}$ 代表两卷积偏置, $\otimes$ 代表逐元素点乘.</p><p> 在GLU中, 最基本的Block被定义为:<br>$$<br>\operatorname{GLU}(X) = X + \operatorname{CNN}(X) \otimes \operatorname{CNN}(X)<br>$$<br>残差连接和门控CNN就组成了最小的GLU单元.</p><blockquote><p>推荐阅读<a href="https://leimao.github.io/blog/Gated-Linear-Units/" target="_blank" rel="noopener">Gated Linear Units (GLU) and Gated CNN</a>.</p></blockquote><h3 id="Module"><a href="#Module" class="headerlink" title="Module"></a>Module</h3><p>轻量级卷积模块由<strong>Linear</strong>, <strong>GLU</strong>, <strong>LConv</strong>, <strong>Linear</strong>依次组成. 第一个Linear先将Token的Embedding维度从$d$ 投影映射到$2d$, 接着通过一个门控来调控输入的信息量, 再通过轻量级卷积, 最后再接一个Linear将维度调整回$d$:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lightweightconv8.jpg" style="zoom: 33%;" /><p>左图为Transformer中使用的点击缩放注意力, 右图为作者目前提出的轻量级卷积模块, 输入维度和输出维度是一致的.</p><h2 id="Dynamic-Convolution"><a href="#Dynamic-Convolution" class="headerlink" title="Dynamic Convolution"></a>Dynamic Convolution</h2><p>目前的轻量级卷积在不同时间步的权重都是<strong>固定</strong>的, 根本没有达到动态生成权重的效果, 基于轻量级卷积, 作者进一步提出<strong>动态卷积</strong>, 动态卷积对每个位置的权重都是<strong>动态</strong>的.</p><p>$$<br>\begin{aligned}<br>\operatorname{DynamicConv}(X, i, c)&amp;=\operatorname{LightConv}\left(X, f(X_{i})_{h,:}, i, c\right) \\<br>&amp;=\operatorname{ DepthwiseConv }\left(X, \operatorname{softmax}\left(f(X_{i})_{h,:}\right), i, c\right)<br>\end{aligned}<br>$$</p><p>其中$f$ 是一个简单的可学习的线性变换$W^Q$, 例如$f(X_{i})=\sum_{c=1}^{d} W_{h, j, c}^{Q} X_{i, c}$. 它能将当前时刻的$d$ 维的输入转化成$H\times k$ 维的注意力权重.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lightweightconv4.jpg" style="zoom: 50%;" /><p>简而言之, 就是利用某个时刻(实际上是Token)的全部Channel信息为当前时刻窗口内所有头的卷积核参数赋予权重, 而当前时刻Token的内容就相当于Self - Attention中的<strong>Query</strong>.</p><blockquote><p>注意力权重的生成只取决于<strong>当前时刻的输入</strong>, 而与前时刻和后时刻输入无关, 这是一个严重缺陷.</p></blockquote><p>下图依次为点积缩放注意力, 轻量级卷积模块, 动态卷积模块.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lightweightconv6.jpg" style="zoom: 33%;" /><p>动态卷积模块与轻量级卷积相比, 只是多增加了一个动态分配权重的Linear层.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>作者在机器翻译, 语言建模, 文本摘要任务上将Transformer中的自注意力机制换成轻量级卷积或动态卷积测试本方法的性能. 在Encoder中, 将自注意力模块替换, 在Decoder中将Masked自注意力模块替换.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lightweightconv5.jpg" style="zoom:50%;" /><p>针对<strong>具体任务</strong>作者进行了不同的调整, 详细配置请参照原论文.</p><p>作者提出, 参数量近似的情况下对比性能. 所以并没有采用6层的Block堆叠, 而是使用了7层. 这七层中, 卷积核大小依次为3, 7, 15, 31, 31, 31, 31.</p><blockquote><p>高层卷积核窗口设置的比较大, 我猜还是因为高层特征抽取中卷积的<strong>局部性限制</strong>问题. 在BERT的Attention可视化对高层能观察到, 除了一些特定层能很明显的体现出注意力差异, 其他高层基本上是均摊注意力权重, 所以卷积核需要更大范围的捕捉上下文特征相关性.</p></blockquote><h3 id="Machine-Translation"><a href="#Machine-Translation" class="headerlink" title="Machine Translation"></a>Machine Translation</h3><p>作者分别在newstest2014上测试了BLEU准确率:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lightweightconv9.jpg" style="zoom: 33%;" /><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lightweightconv10.jpg" style="zoom: 33%;" /><p>动态卷积与原生Transformer在参数量相同的情况下有较大提升.</p><h3 id="Model-Ablation"><a href="#Model-Ablation" class="headerlink" title="Model Ablation"></a>Model Ablation</h3><p>消融实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lightweightconv11.jpg" style="zoom: 33%;" /><p>感觉小Trick颇多, 权重共享在提高句子推断速度上有较大贡献. 在性能提升上每种Trick的贡献都差不多.</p><h3 id="Language-Modeling"><a href="#Language-Modeling" class="headerlink" title="Language Modeling"></a>Language Modeling</h3><p>在Google Billion Word上以困惑度为指标结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lightweightconv12.jpg" style="zoom: 33%;" /><p>动态卷积的参数比自注意力还稍多, 在测试集上取得了略胜自注意力一丢丢的成绩…</p><h3 id="Abstractive-Summarization"><a href="#Abstractive-Summarization" class="headerlink" title="Abstractive Summarization"></a>Abstractive Summarization</h3><p>作者在CNN - DailyMail中的文本摘要实验结果:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lightweightconv13.jpg" style="zoom: 33%;" /><p>轻量级卷积略胜一筹, 但其实也还是没节省多少参数.</p><p>从实验结果来看, 轻量级卷积在相同参数的情况下给出了一定的性能提升, 但并没有比较更少参数和其他模型的对比. 此外, 实验中的结果跟具体任务下的<strong>参数微调</strong>绝对是分不开的.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://qiita.com/koreyou/items/328fa92a1d3a7e680376" target="_blank" rel="noopener">論文紹介: Pay Less Attention with Lightweight and Dynamic Convolutions</a></li><li><a href="https://zhuanlan.zhihu.com/p/60482693" target="_blank" rel="noopener">Pay less attention with light-weight &amp;dynamic CNN</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg4MTEyMDk4Mg==&mid=2247484363&idx=1&sn=513a3f43902fc676f0e82f7136a94047&chksm=cf6b8072f81c096461d774eea90708121be418c53489b57e1951486b61f0c935bd69e948d482&mpshare=1&scene=1&srcid=1126GOjiRcL3TMhItmW6o196&sharer_sharetime=1606391708291&sharer_shareid=fd56e6671880039dc74b6cae5739dbd6&key=9520b16d9caa09008014ec82a8170b261af98843c13ece553a632234270e3c33bb2e29b1363f68b0b27df64710bdeadc29771bfb85bf331dff2c338dc3adb8a1794ae07bc06ea53a6088117fea040ab7da42919076085bc9bc06bb605be58f8aed3db8ef8b05c566bf1212d529e7685c07ef59850f6e244f20a2e229ba6b1b29&ascene=1&uin=MzExMTYwMjkzNw%3D%3D&devicetype=Windows+10+x64&version=6300002f&lang=zh_CN&exportkey=A%2B4JQqyuu2BEHy%2Fe%2B%2BFCG00%3D&pass_ticket=cOZYWOTfohJ2mVibAtIZzeO5jFOFGWxvNcr1jum%2BAYkYBwgP0NMOV0reW3wv3lkV&wx_header=0" target="_blank" rel="noopener">ICLR 2019 | 采用轻量化及动态卷积替代注意力机制</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
            <tag> Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KG-BERT: BERT for Knowledge Graph Completion</title>
      <link href="/posts/18273.html"/>
      <url>/posts/18273.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>BERT: 详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>.</li></ul></blockquote><p>本文是论文<a href="https://arxiv.org/abs/1909.03193" target="_blank" rel="noopener">KG-BERT: BERT for Knowledge Graph Completion</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>在先前的KGE方法中, 虽然它们能学到独特的实体关系表示, 但是却忽略了<strong>上下文</strong>. <strong>句法</strong>和<strong>语义</strong>信息在大规模文本数据中没有得到很好的利用, 它们仅仅使用了实体描述, 关系提及或者实体共现.</p><p>因为BERT在NLP中作为PLM取得的成果非常亮眼, 所以作者希望将它迁移到知识图谱补全任务中, 测试其在KGC中的性能. BERT是针对自然语言进行处理的, 作者简单的将实体和关系描述放入BERT, 使BERT能够获取KGC的能力, 称之为KG - BERT. 作者称, 这是<strong>第一项</strong>使用PLM对三元组进行建模的研究.</p><h2 id="KG-BERT"><a href="#KG-BERT" class="headerlink" title="KG - BERT"></a>KG - BERT</h2><p>既然BERT是基于自然语言的, 那么很容易就想到用实体和关系的<strong>描述</strong>或者它们的<strong>名字</strong>放入BERT, 然后获得通过某种训练方式得到三元组的表示. 作者设计了两种训练方式的KG - BERT, 这样能使它被运用到不同的知识图谱补全任务当中.</p><h3 id="KG-BERT-a"><a href="#KG-BERT-a" class="headerlink" title="KG - BERT(a)"></a>KG - BERT(a)</h3><p>在第一种方式中, 作者非常朴素的完全沿用了BERT的方法, 将实体, 关系<strong>描述</strong>或名字全部放入BERT中, 并用<code>[CLS]</code>处的隐态输出$C$ 来预测三元组是否正确, 该方式与BERT中的<strong>NSP任务</strong>完全一致. 第一种方式是针对<strong>三元组建模</strong>的.</p><p>例如, 三元组$(\text{SteveJobs}, \text{founded}, \text{AppleInc})$ 中的头实体$\text{SteveJobs}$ 可以表示为它的描述<code>Steven Paul Jobs was an American business magnate, entrepreneur and investor</code> 或者它的名字<code>Steve Jobs</code>, 而尾实体$\text{AppleInc}$ 可以表示为<code>Apple Inc. is an American multinational technology company headquartered in Cupertino, California</code>或者它的名字<code>Apple Inc</code>.</p><blockquote><p>在后续的研究中, NSP已经被证实会在NLP任务中带来<strong>副作用</strong>.</p></blockquote><p>在不同实体和关系之间用<code>[SEP]</code> 进行分隔, 并且每个Token的描述分别由Token本身的Embedding, Segment Embedding, Position Embedding组成. Segment Embedding因<strong>元素类型</strong>不同而不同, 头实体和尾实体都使用$e_A$ 作为Segment Embedding, 而关系采用$e_B$.</p><p>我们把<code>[CLS]</code>处的隐态输出$C$ 用来计算三元组的分类, 对于三元组$(h, r, t)$, 其打分函数为:<br>$$<br>\mathbf{s}_{\tau}=f(h, r, t)=\operatorname{sigmoid}\left(C W^{T}\right)<br>$$<br>其中$W$ 是变换矩阵, 和$C$ 乘完后可以获得三元组是否正确的概率$s_\tau$. </p><blockquote><p>在文中写到, $s_\tau$ 是一个二维向量, 包含$s_{\tau0}, s_{\tau1}$. $s_{\tau0} + s_{\tau1}=1$. 这是不是有点多余了… 其实只需要一个一维的$s_{\tau}$ 就足够了, 因为另一半可以用概率和为1算出来. </p></blockquote><p>现在对前面的模型描述进行总结, 整体结构如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kgbert1.jpg" style="zoom:33%;" /><p>其中优化用的损失函数为BCE:<br>$$<br>\mathcal{L}=-\sum_{\tau \in \mathbb{D}+\cup \mathbb{D}^{-}}\left(y_{\tau} \log \left(s_{\tau 0}\right)+\left(1-y_{\tau}\right) \log \left(s_{\tau 1}\right)\right)<br>$$<br>其中$y_\tau$ 是三元组是正例还是负例的标签, 在0和1之间取. $\mathbb{D}^{-}$ 代表负例, $\mathbb{D}^{+}$ 代表正例.</p><p>负样本仍然是<strong>负采样</strong>, 仅替换头实体和尾实体得来的:<br>$$<br>\mathbb{D}^{-}=\left\{\left(h^{\prime}, r, t\right) \mid h^{\prime} \in \mathbb{E} \wedge h^{\prime} \neq h \wedge\left(h^{\prime}, r, t\right) \notin \mathbb{D}^{+}\right\}<br>\cup\left\{\left(h, r, t^{\prime}\right) \mid t^{\prime} \in \mathbb{E} \wedge t^{\prime} \neq t \wedge\left(h, r, t^{\prime}\right) \notin \mathbb{D}^{+}\right\}<br>$$</p><h3 id="KG-BERT-b"><a href="#KG-BERT-b" class="headerlink" title="KG - BERT(b)"></a>KG - BERT(b)</h3><p>在第二种方式中, 作者只使用两个实体$h, t$ 的描述, 来预测它们之间的关系$r$. 在实验中, 作者发现这种结构在预测关系时效果要优于KG - BERT(a).</p><p>KG - BERT(b)采用<code>[CLS]</code> 处的隐态输出$C$ 后接一个分类矩阵来预测两实体之间的关系:<br>$$<br>\mathbf{s}_{\tau}^{\prime}=f(h, r, t)=\operatorname{softmax}\left(C W^{\prime T}\right)<br>$$<br>其中$W$ 为关系的分类矩阵, 多分类也将$\operatorname{sigmoid}$ 换成了$\operatorname{softmax}$. </p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kgbert2.jpg" style="zoom:33%;" /><p>负样本仍然来源于负采样, 只是对正例三元组的关系进行替换即可.</p><p>因为变更了任务, 损失函数不再使用BCE, 而是采用CE进行多分类:<br>$$<br>\mathcal{L}^{\prime}=-\sum_{\tau \in \mathbb{D}^{+}} \sum_{i=1}^{R} y_{\tau i}^{\prime} \log \left(s_{\tau i}^{\prime}\right)<br>$$<br>其中$y_{\tau i}^{\prime}$ 是关系的独热向量.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>在实验中, 作者希望探究KG - BERT的下述能力:</p><ul><li>模型能不能判断没见过的三元组的正确与否?(KG - BERT(a))</li><li>模型能不能根据给出的单个实体和关系描述预测出另一个实体?(Link Prediction)</li><li>模型能不能预测两个实体之间的关系?(KG - BERT(b))</li></ul><p>作者使用BERT - BASE初始化权重, 因为BASE比LARGE版本所受超参影响更小, 可选择的超参也很少. 其余参数设置详见原论文.</p><h3 id="Knowledge-Graph-Compeltion-Tasks"><a href="#Knowledge-Graph-Compeltion-Tasks" class="headerlink" title="Knowledge  Graph Compeltion Tasks"></a>Knowledge  Graph Compeltion Tasks</h3><h4 id="Triple-Classification"><a href="#Triple-Classification" class="headerlink" title="Triple Classification"></a>Triple Classification</h4><p>在三元组分类问题上, 作者在WN11和FB13做了实验:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kgbert3.jpg" style="zoom:33%;" /><p>KG - BERT效果非常明显, 该任务目标与其训练目标是一致的. 作者将其优秀的表现总结如下:</p><ol><li>输入中含有实体和关系的单词序列(使用了文本描述).</li><li>三元组分类与BERT训练时的NSP任务类似.</li><li>Token Vector结合了上下文, 在不同的三元组中描述往往是不同的, 因此不同三元组中的相同元素能获得不同表示.</li><li>Self - Attention很强.</li></ol><p>作者绘制了测试集准确率随训练集数据量提升的变化曲线:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kgbert4.jpg" style="zoom:33%;" /><p>WN11(左), FB13(右). KG - BERT从开始就优于其他模型, 得益于BERT强大的特征抽取能力.</p><h4 id="Link-Prediction"><a href="#Link-Prediction" class="headerlink" title="Link Prediction"></a>Link Prediction</h4><p>在链接预测上, 作者对多种模型在WN18RR和FB15k - 237上, 以及UMLS上做了测试:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kgbert5.jpg" style="zoom:33%;" /><p>KG - BERT(a)在MR上取得了很好的效果, 但是Hits@10的表现总比MR差很多, 作者对其的解释是KG - BERT虽然能避免实体和关系相关性很强的相似三元组, 但是没有对<strong>三元组本身</strong>进行<strong>明确</strong>的建模, 因此不好给定它们的准确排名.</p><blockquote><p>虽然作者在论文中没有明确写出模型输入数据的方法, 但大致能够猜到是对每个实体挨个替换实体描述.</p></blockquote><h4 id="Relation-Prediction"><a href="#Relation-Prediction" class="headerlink" title="Relation Prediction"></a>Relation Prediction</h4><p>作者在FB15k上测试了KG - BERT(b)关系预测的性能:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kgbert6.jpg" style="zoom: 50%;" /><p>碍于打分函数和模型方法的限制, 没有Conv系列的模型登场. KG - BERT在诸多模型中取得了最好的成绩.</p><h3 id="Attention-Visualization"><a href="#Attention-Visualization" class="headerlink" title="Attention Visualization"></a>Attention Visualization</h3><p>对<strong>注意力可视化</strong>能增强模型的可解释性, 也能一定程度上观察模型所学到的东西是否有效. 我认为这部分可以算作是本文的亮点之一.</p><p>作者从KG - BERT(a)取出第11层, 从WN18RR中取出的正例三元组<code>(twenty dollar bill NN 1,hypernym,note NN 6)</code>作为例子, 绘制Attention的可视化情况. 以头实体描述为<code>a United States bill worth 20 dollars</code>, 关系名<code>hypernym</code>, 尾实体描述<code>a piece of paper money</code>, 作为输入序列:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kgbert7.jpg" style="zoom: 50%;" /><p>第11层中<code>paper</code>, <code>money</code>具有很高的权重, <code>worth</code>, <code>20</code> 具有很低的权重. 抛开句子不谈, 模型很好的学到了<code>[SEP]</code>的作用, 因为它在不同的头之间多多少少分配了一些权重.</p><p>在KG - BERT(b)中, 以三元组<code>(20th century, /time/event/includes event, World War II)</code>为例:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kgbert8.jpg" style="zoom: 50%;" /><p>能看到b学到了类似a的模式. 但在这个例子中似乎每个头的注意力更为分散. KG - BERT(b)的目标是对实体进行关系预测, 所以对<code>[CLS]</code> 分配了更高的权重.</p><h3 id="Discussions"><a href="#Discussions" class="headerlink" title="Discussions"></a>Discussions</h3><p>作者提到, KG - BERT现在最大的问题还是<strong>计算成本</strong>太过高昂. 尤其是在链接预测的Evaluation时, 因为轮流替换实体描述花费了大量的时间. 作者认为可行的方法是使用像ConvE那样的<strong>1 - N Scoring</strong>或者采用更加轻量级的语言模型.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>KG - BERT是BERT最早在KG上的应用. 输入数据的方式也非常的简单, 符合我们的直觉, 效果也还不错. 从现在的眼光来看, 与KG相结合的BERT最好不要再使用之前的训练方式.</p><p>缺点是没有对三元组进行直接的建模, 计算成本比较高.</p><p>只是NSP任务在BERT上已经被证明会给BERT带来副作用, 如果要沿用KG - BERT的训练方式, 就需要对NSP任务在KG - BERT上的效果进行研究, 如果去掉NSP任务需要用什么任务来代替?</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> BERT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ConvE: Convolutional 2D Knowledge Graph Embeddings</title>
      <link href="/posts/42031.html"/>
      <url>/posts/42031.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>CNN</li></ul></blockquote><p>本文是论文<a href="https://arxiv.org/abs/1707.01476" target="_blank" rel="noopener">Convolutional 2D Knowledge Graph Embeddings</a>的阅读笔记和个人理解. 与之前在<a href="https://adaning.github.io/posts/59193.html">AcrE</a>中提到的ConvE不同, 本文重新对整篇论文进行叙述, 而非仅介绍论文中建模的部分.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>其实ConvE的出发点非常的简单, 就是之前的模型不够<strong>深</strong>, 有些简单. 因为之前使用的模型大多数采用矩阵映射, 内积等方式, 可能简单的模型能够处理小规模的KG, 想要提升性能就只能通过增大Embedding Dimension. 它们在<strong>大规模KG</strong>上不一定能获得良好的效果. 由于深度学习的兴起, 作者尝试将<strong>卷积</strong>引入到KGE领域, 使它能够在保证深度和模型复杂度的情况下能够处理大规模KG.</p><h2 id="ConvE"><a href="#ConvE" class="headerlink" title="ConvE"></a>ConvE</h2><p>ConvE其实就是将CNN移植到了KGE领域, CNN能在不引入过多参数的情况下, 高效而简单的提供多次交互.</p><h3 id="1D-Convolution-vs-2D-Convolution"><a href="#1D-Convolution-vs-2D-Convolution" class="headerlink" title="1D Convolution vs 2D Convolution"></a>1D Convolution vs 2D Convolution</h3><p>作者指出, 相较于1维卷积, 2维卷积有更强的<strong>表达能力</strong>(其实从直觉来说也是这样).</p><p>在做1维卷积时, 卷积核最多只能与左侧或右侧离得比较近的元素<strong>交互</strong>:<br>$$<br>\left(\begin{array}{lll}<br>\left.\left[\begin{array}{lll}<br>a &amp; a &amp; a<br>\end{array}\right] ;\left[\begin{array}{lll}<br>b &amp; b &amp; b<br>\end{array}\right]\right)=\left[\begin{array}{llllll}<br>a &amp; a &amp; a &amp; b &amp; b &amp; b<br>\end{array}\right]<br>\end{array}\right.<br>$$<br>但2维卷积不一样, 除了能够与邻近的左右元素交互, 还能与上下元素进行交互:<br>$$<br>\left(\left[\begin{array}{lll}<br>a &amp; a &amp; a \\<br>a &amp; a &amp; a<br>\end{array}\right] ;\left[\begin{array}{lll}<br>b &amp; b &amp; b \\<br>b &amp; b &amp; b<br>\end{array}\right]\right)=\left[\begin{array}{lll}<br>a &amp; a &amp; a \\<br>a &amp; a &amp; a \\<br>b &amp; b &amp; b \\<br>b &amp; b &amp; b<br>\end{array}\right]<br>$$<br>如果两种元素代表的意义不同, 那么交换它们的拼接方式还能进一步的<strong>提升</strong>交互次数:<br>$$<br>\left[\begin{array}{lll}<br>a &amp; a &amp; a \\<br>b &amp; b &amp; b \\<br>a &amp; a &amp; a \\<br>b &amp; b &amp; b<br>\end{array}\right]<br>$$<br>由于交换了Concat方式, a和b交错, 能够实现更多次交互. 所以作者认为, 2D卷积的表达能力比1D卷积要强, 因此不是单纯在1D的Embedding数据上做卷积, 而是尝试扩展到2D上做卷积.</p><blockquote><p>在<a href="https://adaning.github.io/posts/30287.html">InteractE</a>中, 使用了交错程度更大的<strong>棋盘式布局</strong>, 进一步提升了交互次数.</p></blockquote><h3 id="ConvE-Architecture"><a href="#ConvE-Architecture" class="headerlink" title="ConvE Architecture"></a>ConvE Architecture</h3><p>我们先看ConvE的整体流程的<strong>概括</strong>:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/conve1.jpg" style="zoom:67%;" /><p>然后再给出ConvE的<strong>打分函数</strong>:<br>$$<br>\psi_{r}\left(\mathbf{e}_{s}, \mathbf{e}_{o}\right)=f\left(\operatorname{vec}\left(f\left(\left[\overline{\mathbf{e}_{s}} ; \overline{\mathbf{r}_{r}}\right] \ast \omega\right)\right) \mathbf{W}\right) \mathbf{e}_{o}<br>$$<br>其中$\mathbf{e}_{s}, \mathbf{e}_{o}$ 分别代表头实体和尾实体的Embedding, $\overline{\mathbf{e}_{s}}, \overline{\mathbf{r}_{r}}$ 分别代表Reshape后的头实体和关系向量. $\omega$代表卷积核, $\mathbf{W}$ 代表投影矩阵. </p><p>接下来对ConvE的打分函数进行讲解, 请结合概括图来看. ConvE先通过Embedding的方式分别获得头实体表示$\mathbf{e}_{s}$ 和关系表示$\mathbf{r}_{r}$. 将头实体和关系表示先Concat起来, 然后将其Reshape到某一种尺寸, 此时头实体和关系的表示记为$\left[\overline{\mathbf{e}_{s}} ; \overline{\mathbf{r}_{r}}\right]$. 接着利用卷积抽取Reshape后的二维向量, 也就是对头实体和关系进行交互. 利用卷积抽取完信息后, 将所有的特征打平成一个一维向量, 通过投影矩阵$\mathbf{W}$ 投影到隐空间中, 然后与为尾实体表示$\mathbf{e}_{o}$ 做内积, 获得相似度, 即Logits. 这种方式通过内积来比较所获向量与尾实体的<strong>相似度</strong>, 越相似得分越高.</p><p>然后将Logits经过$\sigma$ 函数, 得到每个实体的概率:<br>$$<br>p=\sigma(\psi_{r}\left(\mathbf{e}_{s}, \mathbf{e}_{o}\right))<br>$$<br>优化时的损失函数采用BCE:<br>$$<br>\mathcal{L}(p, t)=-\frac{1}{N} \sum_{i}\left(t_{i} \cdot \log \left(p_{i}\right)+\left(1-t_{i}\right) \cdot \log \left(1-p_{i}\right)\right)<br>$$<br>$t$ 是尾实体的独热编码向量. 除此外还加入了Dropout, BatchNorm, 标签平滑等防止过拟合的手段.</p><p>这个损失函数很有说法, 因为在ConvE提取过Feature后, 能获得对所有实体相关的Logits, 这样就能<strong>对所有的尾实体同时打分</strong>, <strong>而不用考虑采样的问题</strong>. 在原文中这种打分方式被称为<strong>1 - N Scoring</strong>. </p><p>这种方式能极大地加快Evaluation的速度, 因为负采样只能对单一的三元组打分, 而这种方式能同时对所有的尾实体同时打分. 作者还测试了不对所有实体同时打分的情况, 例如只对10%的实体打分, 这种情况记为1 - 0.1 N Scoring, 虽然在正向传播和反向传播的速度减少了25%, 但训练速度慢了许多.</p><p>作者还指出, 这种思想能够应用于所有的1 - 1 Scoring Model.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="WN18RR"><a href="#WN18RR" class="headerlink" title="WN18RR"></a>WN18RR</h3><p>作者指出, 最常用的几个数据集例如FB15k, WN18都存在大量的<strong>互逆关系</strong>, 模型能从中掌握关系之间的对应情况, 而轻而易举的推出结果, 模型通过互逆关系<strong>偷懒</strong>, 而并非掌握推断能力. 已经有人对FB15k进行了修正, 推出FB15k -237, 作者也相应的对WN18进行了调整, 推出了<strong>WN18RR</strong>.</p><h3 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h3><p>作者对比了DistMult和ConvE在FB15k - 237上不同Embedding Size的参数量:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/conve2.jpg" style="zoom: 25%;" /><p>在相同参数量的情况下, ConvE能够腾出更多参数在Embedding Size上, 并且拥有比DistMult好的性能. </p><p>对于其他的超参设置, 作者在论文中指出, 大的卷积核效果不如多个小卷积核效果好(这点与CV中结论一致, 基本已经成常识了).</p><h3 id="Inverse-Model"><a href="#Inverse-Model" class="headerlink" title="Inverse Model"></a>Inverse Model</h3><p>虽然大家都已经知道在FB15k和WN18的测试集中存在大量的互逆关系, 但是从来没有人<strong>定量</strong>的对这个问题的<strong>严重性</strong>进行调查. 作者随意构造了一个<strong>基于规则的简单模型</strong>, 发现这个模型能随随便便在存在问题的数据集上取得非常棒的性能.</p><p>这个<strong>Inverse Model</strong>能在训练时自动从给定的两个关系对$r_1, r_2$ 中提取出来, 并检查它们是否是互逆关系. 在测试时, 模型能够自动的看三元组是否含有逆关系, 如果有的话则对最为匹配的$k$ 个逆关系进行排名, 然后从前$k$ 个排名中进行选择, 如果没有逆关系则随机从所有排名中选.</p><p>在后面的链接预测任务中, Inverse Model能够非常好的检查数据集是否存在大量的逆关系, 并评估这种情况的危害.</p><h3 id="Link-Prediction"><a href="#Link-Prediction" class="headerlink" title="Link Prediction"></a>Link Prediction</h3><p>在链接预测任务中, 作者分别在WN18, FB15k, WN18RR, FB15k - 237, YAGO3 - 10, Countries上做了实验.</p><p>WN18, FB15k:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/conve3.jpg" style="zoom: 33%;" /><p>Inverse Model轻而易举的超过了许多模型. </p><p>下面是去除了互逆关系的WN18RR和FB15k - 237:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/conve4.jpg" style="zoom: 33%;" /><p>Inverse Model的性能一落千丈, 同时ConvE在这两个数据集上表现都非常不错, FB15k - 237上表现超过所有Baseline, WN18RR和其他模型也没有差太多.</p><p>YAGO3 - 10, Countries:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/conve5.jpg" style="zoom: 33%;" /><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>作者对ConvE的组成因素挨个做了消融实验:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/conve7.jpg" style="zoom: 33%;" /><p>各种Dropout似乎对ConvE的影响很大, 1 - N Scoring的增益也不小, 标签平滑倒是不怎么重要.</p><h3 id="Indegree-and-PageRank"><a href="#Indegree-and-PageRank" class="headerlink" title="Indegree and PageRank"></a>Indegree and PageRank</h3><h4 id="Indegree"><a href="#Indegree" class="headerlink" title="Indegree"></a>Indegree</h4><p>作者指出, YAGO3 - 10, FB15k - 237与WN18RR相比都具有非常高的关系<strong>入度</strong>, 并且ConvE在它们上面表现非常好. 作者假设, 像ConvE这样更加<strong>深层次</strong>的模型能更好的对高入度的关系建模.</p><p>基于这个假设, 作者将低入度的WN18(low - WN18)和高入度的(high - FB15k)进行转换, 变为高入度的WN18(high - WN18)和低入度的(low - FB15k), 然后观察ConvE在上面的表现. 作者发现在low - FB15k上, ConvE的表现逊于DistMult, 在high - WN18上表现强于DistMult. 这支持了作者的假设.</p><h4 id="PageRank"><a href="#PageRank" class="headerlink" title="PageRank"></a>PageRank</h4><p><strong>PageRank</strong>是节点中心性的度量, PageRank越高, 节点的<strong>影响力</strong>就越大. 作者假设更深的模型在捕获约束上更有优势, 但更加难以优化.</p><p>作者仍然将ConvE和DistMult进行比对, 比较将DistMult换成ConvE后在不同数据集上Hits@10之间的差距:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/conve6.jpg" style="zoom: 33%;" /><p>基本规律是PageRank越高, 错误减少的就越多, ConvE就越强大. 这也佐证了作者的猜想.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>我认为作者在本文中的贡献如下:</p><ol><li>ConvE将卷积的方法引入了KGE中.</li><li>提供了去除互逆关系的WN18RR数据集.</li><li>提出了1 - N Score的思想, 大大加速Evaluation的过程. 而且它还能移植到其他1 - 1 Score模型上.</li><li>指出了卷积的高参数利用率, 为后续多种基于卷积的KGE方法打开了新的大门.</li></ol><p>本文不仅仅是简单的将卷积引入, 在实验的证明思路也比较值得学习.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度可分离卷积与分组卷积</title>
      <link href="/posts/13629.html"/>
      <url>/posts/13629.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>CNN: 详见<a href="https://adaning.github.io/posts/28799.html">卷积神经网络小结</a>.</li></ul></blockquote><p>本文着重介绍<strong>深度可分离卷积</strong>和<strong>分组卷积</strong>两种操作.</p><h2 id="深度可分离卷积"><a href="#深度可分离卷积" class="headerlink" title="深度可分离卷积"></a>深度可分离卷积</h2><p><strong>深度可分离卷积</strong>(Depthwise Separable Convolution)应用在MobileNet和Xception中. 似乎这二者的实现略有不同, 但二者的出发点都是通过深度可分离卷积来<strong>减少参数量</strong>.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/dsconv1.jpg" style="zoom: 33%;" /><blockquote><p>该图片出自<a href="https://zhuanlan.zhihu.com/p/28749411" target="_blank" rel="noopener">zhihu</a>(找不到最开始的原出处了).</p></blockquote><p>我们来举一个例子说明深度可分离卷积是怎样运算的.</p><p>如果我们使用四个标准的卷积对下面的RGB三通道图像数据$C \times H \times W$ 进行卷积操作, 卷积后的通道数(或者说特征图个数)取决于卷积核的个数, 在这里生成了四张特征图. </p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/dsconv2.jpg" style="zoom: 33%;" /><blockquote><p>本小节其余图片出自<a href="https://yinguobing.com/separable-convolution/" target="_blank" rel="noopener">卷积神经网络中的Separable Convolution</a>.</p></blockquote><p>卷积核在不同的通道上使用了不同的权重, 卷积核的大小可以看成是三维的$d_c \times k_h \times k_w$. 还有没有其他方法能达到等效或<strong>近似</strong>的效果呢? 在VGG中出现过类似的方法, 一个大卷积核可以被多个小卷积核<strong>近似表达</strong>. </p><p>$1\times 1$ 卷积能<strong>不改变特征图的大小</strong>, 利用这个特性, 深度可分离卷积将卷积拆成两步来做, 分别是<strong>深度卷积</strong>和<strong>分离卷积</strong>. 即使拆成两步, 最后能拿到的特征图大小也与标准卷积相同.</p><h3 id="深度卷积"><a href="#深度卷积" class="headerlink" title="深度卷积"></a>深度卷积</h3><p><strong>深度卷积</strong>(Depthwise Convolution)也可以称为<strong>逐通道卷积</strong>. 假设用$3\times 3 $ 大小的卷积核去卷积, 我们只对每个通道分别进行卷积, 而不是一起进行卷积:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/dsconv3.jpg" style="zoom: 33%;" /><p>逐通道卷积这一步中, 每个卷积核只有<strong>二维</strong>参数, 即$k_h \times k_w$ 个参数. 生成并能生成$C$ 张宽高变化后的特征图. 深度卷积只是单纯的<strong>分别运用</strong>了每个平面空间上的信息, 还没有将它们整合到一起, 即<strong>没有进行Channel维运算</strong>.</p><h3 id="分离卷积"><a href="#分离卷积" class="headerlink" title="分离卷积"></a>分离卷积</h3><p><strong>分离卷积</strong>(Separable Convolution)也称为<strong>逐点卷积</strong>. 其核心就是利用$1\times 1$卷积在不改变特征图大小情况下<strong>任意更改Channel数量</strong>的特性, 对Channel信息进行<strong>整合</strong>. $1\times1$ 卷积核有多少个, 就有多少特征图产生. 假设有$M$ 个$1\times1$ 卷积核, 就能生成$M$ 张宽高不变的特征图:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/dsconv4.jpg" style="zoom: 33%;" /><p>此时的$1 \times 1$ 卷积与标准卷积无异, 将<strong>贯穿</strong>于每一个Channel, 这样就完成了Channel维度上的<strong>信息整合</strong>. 经过深度卷积和分离卷积两步后, 其产生的特征图大小和使用四个$3\times3\times3$ 大小的标准卷积产生的结果无异.</p><h3 id="参数量分析"><a href="#参数量分析" class="headerlink" title="参数量分析"></a>参数量分析</h3><p>更一般的, 假设图像大小为$C \times H \times W$, 使用$N$ 个$K \times K$ 的卷积核. 那么标准卷积所使用的参数量$P_{std}$ 为:<br>$$<br>P_{std} = K \times K \times C \times N = K^2\times C \times N<br>$$<br>而深度可分离卷积的参数量$P_{ds}$ 由两部分组成:</p><ul><li><strong>Depthwise Convolution</strong>: 由于卷积核没有在Channel上的维度, 所以只使用了$K \times K \times N$ 个参数.</li><li><strong>Separable Convolution</strong>: 该部分是$1\times1$ 卷积的参数, 使用了$1\times1\times C \times N$ 个参数.</li></ul><p>综上, 深度可分离卷积参数量为:<br>$$<br>P_{ds} = K \times K \times N + 1\times1\times C \times N = (K^2+C) \times N<br>$$<br>故深度可分离卷积与标准卷积的<strong>参数比</strong>为:<br>$$<br>\frac{P_{ds}}{P_{std}} = \frac{(K^2+C)\times N}{K^2 \times C \times N} = \frac{1}{C} + \frac{1}{K^2}<br>$$<br>参数量大量的减少了, 效率也提高了(详见<a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="noopener">Mobilenet</a>).</p><h2 id="分组卷积"><a href="#分组卷积" class="headerlink" title="分组卷积"></a>分组卷积</h2><p><strong>分组卷积</strong>(Group Convolution)早在AlexNet中就出现了, AlexNet当时受算力的限制, 不能将整张图片直接读入GPU中, 所以对一张图片<strong>分割</strong>处理, 最后再整合起来:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/alexnet.jpg" style="zoom:67%;" /><blockquote><p>本节其余图片来源于<a href="https://blog.yani.ai/filter-group-tutorial/" target="_blank" rel="noopener">A Tutorial on Filter Groups (Grouped Convolution)</a>.</p></blockquote><p>假设我们要使用$c_2$ 个卷积核对$c_1\times H \times W$ 的图片进行卷积.</p><p>对于标准卷积, 使用的卷积核大小为$c_1\times h_1 \times w_1$:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/groupconv1.png" style="zoom:67%;" /><p>所产生的总参数量为$ c_1 \times h_1 \times w_1 \times c_2$.</p><p>如果将图片在Channel上分为$g$ 组, 分别只在$g$ 组上使用深度更小的卷积核分别进行卷积, 最后<strong>拼凑</strong>起来, 就称为了分组卷积:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/groupconv2.png" style="zoom:67%;" /><p>如图所示, 分组卷积所产生的总参数量为$g \times h_1 \times w_1 \times \frac{c_2}{g}$. 即标准卷积参数量的$\frac{1}{g}$ 倍.</p><blockquote><p>我能想到一个极端情况, 当分组数量$g$ 与Channel数相同, 即$g = c_1$ 时, 分组卷积就等价于前面说过的<strong>深度卷积</strong>(Depthwise Convolution). 因为只是最后将Channel维上的特征图Concat起来, 而没有进行<strong>交互</strong>, 所以在不同的Group之间, Channel维上的信息是没有得到整合的. 如果分组卷积只作为CNN的<strong>中间部件</strong>, 在后续的结构中可以使用其他整合的方式.</p></blockquote><h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/65377955" target="_blank" rel="noopener">理解分组卷积和深度可分离卷积如何降低参数量</a></li><li><a href="https://zhuanlan.zhihu.com/p/28749411" target="_blank" rel="noopener">变形卷积核、可分离卷积？卷积神经网络中十大拍案叫绝的操作。</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch实现: Transformer</title>
      <link href="/posts/63679.html"/>
      <url>/posts/63679.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>Pytorch基本操作</li><li>Transformer: 详见<a href="https://adaning.github.io/posts/6744.html">Transformer精讲</a></li></ul></blockquote><p>本文是Transfomrer的Pytorch版本实现. 实现的过程中非常考验<strong>维度控制</strong>的功底. 本文实现参考<a href="https://wmathor.com/index.php/archives/1455/" target="_blank" rel="noopener">Transformer 的 PyTorch 实现</a>, 我对其在个别地方进行了修改, 并对所有的数据<strong>全部</strong>加上了维度注释.</p><p>本文的代码已经放到了Colab上, 打开设置GPU就可以复现(需要科学上网). </p><p><a href="https://colab.research.google.com/drive/1OAXor9Io7pk64ilo-HoFS6RnH9ImbaaF?usp=sharing" target="_blank" rel="noopener"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="右键我在COLAB中打开!"></a></p><p>如果你不能科学上网, 应该看不到<code>Open in Colab</code>的图标.</p><p>在开头需要说明的是:</p><ul><li>网上的所有流传的代码, 一般都会把<code>batch_size</code>放在第0维. 因为我们基本上不对batch维做操作, 放在最前面来防止影响后面总需要使用<code>transpose</code>移动.</li><li>实际上最开始Transformer的Layer Norm位置全都加错了. 但是为了和论文保持一致, 先不调换LayerNorm的位置. 具体内容详见<a href="https://zhuanlan.zhihu.com/p/84614490" target="_blank" rel="noopener">香侬读 | Transformer中warm-up和LayerNorm的重要性探究</a>, 其中应该包含指出Post Layer Norm有问题的论文地址.</li><li>如果对Transformer不熟悉, 最好熟悉后再来看这篇文章.</li><li>注意<code>view</code>和<code>transpose</code>拆维度时不要乱了.</li></ul><h2 id="Preparing"><a href="#Preparing" class="headerlink" title="Preparing"></a>Preparing</h2><p>按照惯例, 先导包:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch <span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> torch <span class="token keyword">import</span> optim<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils <span class="token keyword">import</span> data <span class="token keyword">as</span> Data<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>因为后面需要用到一些关于Transformer的超参数, 所以在开头就先全部定义出来:</p><pre class="line-numbers language-python"><code class="language-python">d_model <span class="token operator">=</span> <span class="token number">512</span> <span class="token comment" spellcheck="true"># embedding size </span>max_len <span class="token operator">=</span> <span class="token number">1024</span> <span class="token comment" spellcheck="true"># max length of sequence</span>d_ff <span class="token operator">=</span> <span class="token number">2048</span> <span class="token comment" spellcheck="true"># feedforward nerual network  dimension</span>d_k <span class="token operator">=</span> d_v <span class="token operator">=</span> <span class="token number">64</span> <span class="token comment" spellcheck="true"># dimension of k(same as q) and v</span>n_layers <span class="token operator">=</span> <span class="token number">6</span> <span class="token comment" spellcheck="true"># number of encoder and decoder layers</span>n_heads <span class="token operator">=</span> <span class="token number">8</span> <span class="token comment" spellcheck="true"># number of heads in multihead attention</span>p_drop <span class="token operator">=</span> <span class="token number">0.1</span> <span class="token comment" spellcheck="true"># propability of dropout</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果你对Transformer足够熟悉, 看变量名和注释一定能看出来它们的含义, 它们依次是:</p><ul><li>d_model: Embedding的大小.</li><li>max_len: 输入序列的最长大小.</li><li>d_ff: 前馈神经网络的隐藏层大小, 一般是d_model的四倍.</li><li>d_k, d_v: 自注意力中K和V的维度, Q的维度直接用K的维度代替, 因为这二者必须始终相等.</li><li>n_layers: Encoder和Decoder的层数.</li><li>n_heads: 自注意力多头的头数.</li><li>p_drop: Dropout的概率.</li></ul><h2 id="Mask"><a href="#Mask" class="headerlink" title="Mask"></a>Mask</h2><p>Mask分为两种, 一种是因为在数据中使用了padding, 不希望pad被加入到注意力中进行计算的Pad Mask for Attention, 还有一种是保证Decoder自回归信息不泄露的Subsequent Mask for Decoder.</p><h3 id="Pad-Mask-for-Attention"><a href="#Pad-Mask-for-Attention" class="headerlink" title="Pad Mask for Attention"></a>Pad Mask for Attention</h3><p>为了方便, 假设<code>&lt;PAD&gt;</code>在字典中的Index是0, 遇到输入为0直接将其标为True.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_attn_pad_mask</span><span class="token punctuation">(</span>seq_q<span class="token punctuation">,</span> seq_k<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token triple-quoted-string string">'''  Padding, because of unequal in source_len and target_len.  parameters:  seq_q: [batch, seq_len]  seq_k: [batch, seq_len]  return:  mask: [batch, len_q, len_k]  '''</span>  batch<span class="token punctuation">,</span> len_q <span class="token operator">=</span> seq_q<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>  batch<span class="token punctuation">,</span> len_k <span class="token operator">=</span> seq_k<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># we define index of PAD is 0, if tensor equals (zero) PAD tokens</span>  pad_attn_mask <span class="token operator">=</span> seq_k<span class="token punctuation">.</span>data<span class="token punctuation">.</span>eq<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, 1, len_k]</span>  <span class="token keyword">return</span> pad_attn_mask<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> len_q<span class="token punctuation">,</span> len_k<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, len_q, len_k]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在Encoder和Decoder中使用Mask的情况可能各有不同:</p><ul><li><p>在Encoder中使用Mask, 是为了将<code>encoder_input</code>中没有内容而打上PAD的部分进行Mask, 方便矩阵运算.</p></li><li><p>在Decoder中使用Mask, 可能是在Decoder的自注意力对<code>decoder_input</code> 的PAD进行Mask, 也有可能是对Encoder - Decoder自注意力时对<code>encoder_input</code>和<code>decoder_input</code>的PAD进行Mask.</p></li></ul><h3 id="Subsequent-Mask-for-Decoder"><a href="#Subsequent-Mask-for-Decoder" class="headerlink" title="Subsequent Mask for Decoder"></a>Subsequent Mask for Decoder</h3><p>该Mask是为了防止Decoder的自回归信息泄露而生的Mask, 直接生成一个上三角矩阵即可:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_attn_subsequent_mask</span><span class="token punctuation">(</span>seq<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token triple-quoted-string string">'''  Build attention mask matrix for decoder when it autoregressing.  parameters:  seq: [batch, target_len]  return:  subsequent_mask: [batch, target_len, target_len]   '''</span>  attn_shape <span class="token operator">=</span> <span class="token punctuation">[</span>seq<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> seq<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> seq<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># [batch, target_len, target_len]</span>  subsequent_mask <span class="token operator">=</span> np<span class="token punctuation">.</span>triu<span class="token punctuation">(</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>attn_shape<span class="token punctuation">)</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, target_len, target_len] </span>  subsequent_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>subsequent_mask<span class="token punctuation">)</span>  <span class="token keyword">return</span> subsequent_mask <span class="token comment" spellcheck="true"># [batch, target_len, target_len] </span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其中, 用到了生成上三角的函数<code>np.triu</code>, 其用法为:</p><pre class="line-numbers language-python"><code class="language-python">np<span class="token punctuation">.</span>triu<span class="token punctuation">(</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">'''array([[0., 1., 1., 1.],       [0., 0., 1., 1.],       [0., 0., 0., 1.]])'''</span>np<span class="token punctuation">.</span>triu<span class="token punctuation">(</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">'''array([[1., 1., 1., 1.],       [0., 1., 1., 1.],       [0., 0., 1., 1.]])'''</span>np<span class="token punctuation">.</span>triu<span class="token punctuation">(</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">'''array([[1., 1., 1., 1.],       [1., 1., 1., 1.],       [0., 1., 1., 1.]])'''</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其中<code>k</code>能控制上三角的大小, 越大则上三角范围越小. 与之完全<strong>相反</strong>的函数是<code>np.tril</code>, 能够生成下三角矩阵.</p><h2 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h2><p>在Transformer中, 使用的是绝对位置编码, 用于传输给模型Self - Attention所不能传输的位置信息, 编码使用正余弦公式实现:<br>$$<br>\begin{aligned}<br>PE(pos, 2i)&amp; = \sin(pos / 10000^{\frac{2i}{d_{model}}}) \\<br>PE(pos, 2i+1)&amp; = \cos(pos / 10000^{\frac{2i}{d_{model}}})<br>\end{aligned}<br>$$<br>基于上述公式, 我们把它实现出来:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">PositionalEncoding</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">,</span> max_len<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    super<span class="token punctuation">(</span>PositionalEncoding<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span>p_drop<span class="token punctuation">)</span>    positional_encoding <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>max_len<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [max_len, d_model]</span>    position <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> max_len<span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [max_len, 1]</span>    div_term <span class="token operator">=</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span>                          <span class="token punctuation">(</span><span class="token operator">-</span>torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">10000</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> d_model<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [max_len / 2]</span>    positional_encoding<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>position <span class="token operator">*</span> div_term<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># even</span>    positional_encoding<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>position <span class="token operator">*</span> div_term<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># odd</span>    <span class="token comment" spellcheck="true"># [max_len, d_model] -> [1, max_len, d_model] -> [max_len, 1, d_model]</span>    positional_encoding <span class="token operator">=</span> positional_encoding<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># register pe to buffer and require no grads</span>    self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">'pe'</span><span class="token punctuation">,</span> positional_encoding<span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># x: [seq_len, batch, d_model]</span>    <span class="token comment" spellcheck="true"># we can add positional encoding to x directly, and ignore other dimension</span>    x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>pe<span class="token punctuation">[</span><span class="token punctuation">:</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>实现$1/ 10000^{\frac{2i}{d_{model}}}$ 时既可以像我写出的那样使用幂指运算, 也可以直接写出.</p><p><code>register_buffer</code>能够申请一个缓冲区中的<strong>常量</strong>, 并且它不会被加入到计算图中, 也就不会参与反向传播.</p><p>更多关于<code>register</code>在<code>parameter</code>和<code>buffer</code>上的区别请见<a href="https://zhuanlan.zhihu.com/p/89442276" target="_blank" rel="noopener">Pytorch模型中的parameter与buffer</a>.</p><h2 id="Feed-Forward-Neural-Network"><a href="#Feed-Forward-Neural-Network" class="headerlink" title="Feed Forward Neural Network"></a>Feed Forward Neural Network</h2><p>在Transformer中, Encoder或者Decoder每个Block都需要用一个前馈神经网络来添加<strong>非线性</strong>:<br>$$<br>\operatorname{FFN}(x)=\operatorname{ReLU}(xW_1+b_1)W_2 + b_2<br>$$<br>注意, 这里它们都是有偏置的, 而且这两个Linear可以用两个$1\times1$ 的卷积来实现:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">FeedForwardNetwork</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token triple-quoted-string string">'''  Using nn.Conv1d replace nn.Linear to implements FFN.  '''</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    super<span class="token punctuation">(</span>FeedForwardNetwork<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># self.ff1 = nn.Linear(d_model, d_ff)</span>    <span class="token comment" spellcheck="true"># self.ff2 = nn.Linear(d_ff, d_model)</span>    self<span class="token punctuation">.</span>ff1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_ff<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>ff2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>d_ff<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span>p_drop<span class="token punctuation">)</span>    self<span class="token punctuation">.</span>layer_norm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># x: [batch, seq_len, d_model]</span>    residual <span class="token operator">=</span> x    x <span class="token operator">=</span> x<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, d_model, seq_len]</span>    x <span class="token operator">=</span> self<span class="token punctuation">.</span>ff1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> self<span class="token punctuation">.</span>ff2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> x<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, seq_len, d_model]</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>layer_norm<span class="token punctuation">(</span>residual <span class="token operator">+</span> x<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>作为一个子层, 不要忘记Transformer中提到的Residual Connection和Layer Norm.</p><p>我选择用两个卷积代替Linear. 在<code>nn.Conv1d</code>中, 要求数据的规格为<code>[batch, x, ...]</code>, 我们是要对<code>d_model</code> 上的数据进行卷积, 所以还是需要<code>transpose</code>一下.</p><h2 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi - Head Attention"></a>Multi - Head Attention</h2><p>先说多头注意力, 因为多头注意力能够决定缩放点积注意力的输入大小. 作为一个子层, 其中的Residual Connection和Layer Norm是必须的.</p><p>多头注意力是多个不同的头来获取不同的特征, 类似于多个<strong>卷积核</strong>所达到的效果. 在计算完后通过一个Linear调整大小:<br>$$<br>\begin{aligned}<br>\operatorname{MultiHead}(Q, K, V) &amp;= \operatorname{Concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_h)W^O \\<br>\text{where } \text{head}_i &amp;= \operatorname{Attention}(QW^Q_i, KW^K_i, VW^V_i)<br>\end{aligned}<br>$$<br>多头注意力在Encoder和Decoder中的使用略有区别, 主要区别在于Mask的不同. 我们前面已经实现了两种Mask函数, 在这里会用到.</p><p>多头注意力实际上不是通过弄出很多大小相同的矩阵然后相乘来实现的, 只需要合并到一个矩阵进行计算:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n_heads<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    super<span class="token punctuation">(</span>MultiHeadAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># do not use more instance to implement multihead attention</span>    <span class="token comment" spellcheck="true"># it can be complete in one matrix</span>    self<span class="token punctuation">.</span>n_heads <span class="token operator">=</span> n_heads    <span class="token comment" spellcheck="true"># we can't use bias because there is no bias term in formular</span>    self<span class="token punctuation">.</span>W_Q <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_k <span class="token operator">*</span> n_heads<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>W_K <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_k <span class="token operator">*</span> n_heads<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>W_V <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_v <span class="token operator">*</span> n_heads<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_v <span class="token operator">*</span> n_heads<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>layer_norm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_Q<span class="token punctuation">,</span> input_K<span class="token punctuation">,</span> input_V<span class="token punctuation">,</span> attn_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''    To make sure multihead attention can be used both in encoder and decoder,     we use Q, K, V respectively.    input_Q: [batch, len_q, d_model]    input_K: [batch, len_k, d_model]    input_V: [batch, len_v, d_model]    '''</span>    residual<span class="token punctuation">,</span> batch <span class="token operator">=</span> input_Q<span class="token punctuation">,</span> input_Q<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># [batch, len_q, d_model] -- matmul W_Q --> [batch, len_q, d_q * n_heads] -- view --> </span>    <span class="token comment" spellcheck="true"># [batch, len_q, n_heads, d_k,] -- transpose --> [batch, n_heads, len_q, d_k]</span>    Q <span class="token operator">=</span> self<span class="token punctuation">.</span>W_Q<span class="token punctuation">(</span>input_Q<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> d_k<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, n_heads, len_q, d_k]</span>    K <span class="token operator">=</span> self<span class="token punctuation">.</span>W_K<span class="token punctuation">(</span>input_K<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> d_k<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, n_heads, len_k, d_k]</span>    V <span class="token operator">=</span> self<span class="token punctuation">.</span>W_V<span class="token punctuation">(</span>input_V<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> d_v<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, n_heads, len_v, d_v]</span>    attn_mask <span class="token operator">=</span> attn_mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, n_heads, seq_len, seq_len]</span>    <span class="token comment" spellcheck="true"># prob: [batch, n_heads, len_q, d_v] attn: [batch, n_heads, len_q, len_k]</span>    prob<span class="token punctuation">,</span> attn <span class="token operator">=</span> ScaledDotProductAttention<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>Q<span class="token punctuation">,</span> K<span class="token punctuation">,</span> V<span class="token punctuation">,</span> attn_mask<span class="token punctuation">)</span>    prob <span class="token operator">=</span> prob<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, len_q, n_heads, d_v]</span>    prob <span class="token operator">=</span> prob<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads <span class="token operator">*</span> d_v<span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, len_q, n_heads * d_v]</span>    output <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>prob<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, len_q, d_model]</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>layer_norm<span class="token punctuation">(</span>residual <span class="token operator">+</span> output<span class="token punctuation">)</span><span class="token punctuation">,</span> attn<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>提两个非常重要的点:</p><ol><li>在拆维度时不要破坏维度原来本身的意义.</li><li>虽然新版本已经有<code>reshape</code>函数可以用了, 但是仍然不要忘记, <code>transpose</code>后如果接<code>permute</code>或者<code>view</code>必须要加<code>contiguous</code>, 这是<strong>数据真实存储连续与否</strong>的问题, 请参见<a href="https://adaning.github.io/posts/42255.html">Pytorch之张量基础操作</a>中的<strong>维度变换</strong>部分.</li></ol><h2 id="Scaled-DotProduct-Attention"><a href="#Scaled-DotProduct-Attention" class="headerlink" title="Scaled DotProduct Attention"></a>Scaled DotProduct Attention</h2><p>Tranformer中非常重要的概念, 缩放点积注意力, 公式如下:<br>$$<br>\operatorname{Attention}(Q, K, V) = \operatorname{softmax}(\frac{QK^T}{\sqrt{d_k}})V<br>$$<br>实现起来非常简单, 只需要把Q, K两个矩阵一乘, 然后再缩放, 过一次Softmax, 再和V乘下:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">ScaledDotProductAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    super<span class="token punctuation">(</span>ScaledDotProductAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> Q<span class="token punctuation">,</span> K<span class="token punctuation">,</span> V<span class="token punctuation">,</span> attn_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''    Q: [batch, n_heads, len_q, d_k]    K: [batch, n_heads, len_k, d_k]    V: [batch, n_heads, len_v, d_v]    attn_mask: [batch, n_heads, seq_len, seq_len]    '''</span>    scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>Q<span class="token punctuation">,</span> K<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>d_k<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, n_heads, len_q, len_k]</span>    scores<span class="token punctuation">.</span>masked_fill_<span class="token punctuation">(</span>attn_mask<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1e9</span><span class="token punctuation">)</span>    attn <span class="token operator">=</span> nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span>scores<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, n_heads, len_q, len_k]</span>    prob <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>attn<span class="token punctuation">,</span> V<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, n_heads, len_q, d_v]</span>    <span class="token keyword">return</span> prob<span class="token punctuation">,</span> attn<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>masked_fill_</code>能把传进来的Mask为True的地方全都填充上某个值, 这里需要用一个很大的负数来保证$e^x \rightarrow 0$, 使得其在Softmax​ 中可以被忽略.</p><h2 id="Encoder-and-Decoder"><a href="#Encoder-and-Decoder" class="headerlink" title="Encoder and Decoder"></a>Encoder and Decoder</h2><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>先写出Encoder的每个Layer, 由多头注意力和FFN组成:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">EncoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    super<span class="token punctuation">(</span>EncoderLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>encoder_self_attn <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>ffn <span class="token operator">=</span> FeedForwardNetwork<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder_input<span class="token punctuation">,</span> encoder_pad_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''    encoder_input: [batch, source_len, d_model]    encoder_pad_mask: [batch, n_heads, source_len, source_len]    encoder_output: [batch, source_len, d_model]    attn: [batch, n_heads, source_len, source_len]    '''</span>    encoder_output<span class="token punctuation">,</span> attn <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder_self_attn<span class="token punctuation">(</span>encoder_input<span class="token punctuation">,</span> encoder_input<span class="token punctuation">,</span> encoder_input<span class="token punctuation">,</span> encoder_pad_mask<span class="token punctuation">)</span>    encoder_output <span class="token operator">=</span> self<span class="token punctuation">.</span>ffn<span class="token punctuation">(</span>encoder_output<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, source_len, d_model]</span>    <span class="token keyword">return</span> encoder_output<span class="token punctuation">,</span> attn<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>对于给定的<code>encoder_input</code>和<code>encoder_pad_pask</code>, Encoder应该能够完成整个Block(Layer)的计算流程. 然后实现整个Encoder:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Encoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    super<span class="token punctuation">(</span>Encoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>source_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>source_vocab_size<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>    self<span class="token punctuation">.</span>positional_embedding <span class="token operator">=</span> PositionalEncoding<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>    self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>EncoderLayer<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> layer <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_layers<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder_input<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># encoder_input: [batch, source_len]</span>    encoder_output <span class="token operator">=</span> self<span class="token punctuation">.</span>source_embedding<span class="token punctuation">(</span>encoder_input<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, source_len, d_model]</span>    encoder_output <span class="token operator">=</span> self<span class="token punctuation">.</span>positional_embedding<span class="token punctuation">(</span>encoder_output<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, source_len, d_model]</span>    encoder_self_attn_mask <span class="token operator">=</span> get_attn_pad_mask<span class="token punctuation">(</span>encoder_input<span class="token punctuation">,</span> encoder_input<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, source_len, source_len]</span>    encoder_self_attns <span class="token operator">=</span> list<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>      <span class="token comment" spellcheck="true"># encoder_output: [batch, source_len, d_model]</span>      <span class="token comment" spellcheck="true"># encoder_self_attn: [batch, n_heads, source_len, source_len]</span>      encoder_output<span class="token punctuation">,</span> encoder_self_attn <span class="token operator">=</span> layer<span class="token punctuation">(</span>encoder_output<span class="token punctuation">,</span> encoder_self_attn_mask<span class="token punctuation">)</span>      encoder_self_attns<span class="token punctuation">.</span>append<span class="token punctuation">(</span>encoder_self_attn<span class="token punctuation">)</span>    <span class="token keyword">return</span> encoder_output<span class="token punctuation">,</span> encoder_self_attns<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>对于整个Encoder, 直接将Token的Index传入Embedding中, 再添入位置编码, 之后就经过多层Transformer Encoder. 在传入Block前, 先需要计算Padding的Mask, 再将上层的输出作为下层输入依次迭代.</p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>其实实现了Encoder, Decoder的实现部分都是对应的. 先实现Decoder的Block:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">DecoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    super<span class="token punctuation">(</span>DecoderLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>decoder_self_attn <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>encoder_decoder_attn <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>ffn <span class="token operator">=</span> FeedForwardNetwork<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> decoder_input<span class="token punctuation">,</span> encoder_output<span class="token punctuation">,</span> decoder_self_mask<span class="token punctuation">,</span> decoder_encoder_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''    decoder_input: [batch, target_len, d_mdoel]    encoder_output: [batch, source_len, d_model]    decoder_self_mask: [batch, target_len, target_len]    decoder_encoder_mask: [batch, target_len, source_len]    '''</span>    <span class="token comment" spellcheck="true"># masked mutlihead attention</span>    <span class="token comment" spellcheck="true"># Q, K, V all from decoder it self</span>    <span class="token comment" spellcheck="true"># decoder_output: [batch, target_len, d_model]</span>    <span class="token comment" spellcheck="true"># decoder_self_attn: [batch, n_heads, target_len, target_len]</span>    decoder_output<span class="token punctuation">,</span> decoder_self_attn <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder_self_attn<span class="token punctuation">(</span>decoder_input<span class="token punctuation">,</span> decoder_input<span class="token punctuation">,</span> decoder_input<span class="token punctuation">,</span> decoder_self_mask<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Q from decoder, K, V from encoder</span>    <span class="token comment" spellcheck="true"># decoder_output: [batch, target_len, d_model]</span>    <span class="token comment" spellcheck="true"># decoder_encoder_attn: [batch, n_heads, target_len, source_len]</span>    decoder_output<span class="token punctuation">,</span> decoder_encoder_attn <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder_decoder_attn<span class="token punctuation">(</span>decoder_input<span class="token punctuation">,</span> encoder_output<span class="token punctuation">,</span> encoder_output<span class="token punctuation">,</span> decoder_encoder_mask<span class="token punctuation">)</span>    decoder_output <span class="token operator">=</span> self<span class="token punctuation">.</span>ffn<span class="token punctuation">(</span>decoder_output<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, target_len, d_model]</span>    <span class="token keyword">return</span> decoder_output<span class="token punctuation">,</span> decoder_self_attn<span class="token punctuation">,</span> decoder_encoder_attn<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>与Encoder相对应, 只不过因为多了一个Encoder - Decoder自注意力, 所以需要额外计算一个Encoder - Decoder的Mask. 然后写出整个Decoder:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Decoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    super<span class="token punctuation">(</span>Decoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>target_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>target_vocab_size<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>    self<span class="token punctuation">.</span>positional_embedding <span class="token operator">=</span> PositionalEncoding<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>    self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>DecoderLayer<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> layer <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_layers<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> decoder_input<span class="token punctuation">,</span> encoder_input<span class="token punctuation">,</span> encoder_output<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''    decoder_input: [batch, target_len]    encoder_input: [batch, source_len]    encoder_output: [batch, source_len, d_model]    '''</span>    decoder_output <span class="token operator">=</span> self<span class="token punctuation">.</span>target_embedding<span class="token punctuation">(</span>decoder_input<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, target_len, d_model]</span>    decoder_output <span class="token operator">=</span> self<span class="token punctuation">.</span>positional_embedding<span class="token punctuation">(</span>decoder_output<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, target_len, d_model]</span>    decoder_self_attn_mask <span class="token operator">=</span> get_attn_pad_mask<span class="token punctuation">(</span>decoder_input<span class="token punctuation">,</span> decoder_input<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, target_len, d_model]</span>    decoder_subsequent_mask <span class="token operator">=</span> get_attn_subsequent_mask<span class="token punctuation">(</span>decoder_input<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, target_len, target_len]</span>    decoder_encoder_attn_mask <span class="token operator">=</span> get_attn_pad_mask<span class="token punctuation">(</span>decoder_input<span class="token punctuation">,</span> encoder_input<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, target_len, source_len]</span>    decoder_self_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>gt<span class="token punctuation">(</span>decoder_self_attn_mask <span class="token operator">+</span> decoder_subsequent_mask<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>    decoder_self_attns<span class="token punctuation">,</span> decoder_encoder_attns <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>      <span class="token comment" spellcheck="true"># decoder_output: [batch, target_len, d_model]</span>      <span class="token comment" spellcheck="true"># decoder_self_attn: [batch, n_heads, target_len, target_len]</span>      <span class="token comment" spellcheck="true"># decoder_encoder_attn: [batch, n_heads, target_len, source_len]</span>      decoder_output<span class="token punctuation">,</span> decoder_self_attn<span class="token punctuation">,</span> decoder_encoder_attn <span class="token operator">=</span> layer<span class="token punctuation">(</span>decoder_output<span class="token punctuation">,</span> encoder_output<span class="token punctuation">,</span> decoder_self_mask<span class="token punctuation">,</span> decoder_encoder_attn_mask<span class="token punctuation">)</span>      decoder_self_attns<span class="token punctuation">.</span>append<span class="token punctuation">(</span>decoder_self_attn<span class="token punctuation">)</span>      decoder_encoder_attns<span class="token punctuation">.</span>append<span class="token punctuation">(</span>decoder_encoder_attn<span class="token punctuation">)</span>    <span class="token keyword">return</span> decoder_output<span class="token punctuation">,</span> decoder_self_attns<span class="token punctuation">,</span> decoder_encoder_attns<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>和Encoder相对应, 但Decoder和Encoder使用了两个不同的Embedding. 对于Mask, 可以把自回归Mask和Padding Mask用<code>torch.gt</code>整合成一个Mask, 送入其中.</p><h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>终于到了这一步, 虽然后面还有一些小小的工作, 但现在终于能看到Transformer的<strong>全貌</strong>了:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer.jpg" style="zoom: 50%;" /><p>里面有一个Encoder, 一个Decoder, 在Decoder端还需要加上投影层来分类:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Transformer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    super<span class="token punctuation">(</span>Transformer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> Encoder<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> Decoder<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>projection <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> target_vocab_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder_input<span class="token punctuation">,</span> decoder_input<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''    encoder_input: [batch, source_len]    decoder_input: [batch, target_len]    '''</span>    <span class="token comment" spellcheck="true"># encoder_output: [batch, source_len, d_model]</span>    <span class="token comment" spellcheck="true"># encoder_attns: [n_layers, batch, n_heads, source_len, source_len]</span>    encoder_output<span class="token punctuation">,</span> encoder_attns <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>encoder_input<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># decoder_output: [batch, target_len, d_model]</span>    <span class="token comment" spellcheck="true"># decoder_self_attns: [n_layers, batch, n_heads, target_len, target_len]</span>    <span class="token comment" spellcheck="true"># decoder_encoder_attns: [n_layers, batch, n_heads, target_len, source_len]</span>    decoder_output<span class="token punctuation">,</span> decoder_self_attns<span class="token punctuation">,</span> decoder_encoder_attns <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>decoder_input<span class="token punctuation">,</span> encoder_input<span class="token punctuation">,</span> encoder_output<span class="token punctuation">)</span>    decoder_logits <span class="token operator">=</span> self<span class="token punctuation">.</span>projection<span class="token punctuation">(</span>decoder_output<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, target_len, target_vocab_size]</span>    <span class="token comment" spellcheck="true"># decoder_logits: [batch * target_len, target_vocab_size]</span>    <span class="token keyword">return</span> decoder_logits<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> decoder_logits<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> encoder_attns<span class="token punctuation">,</span> decoder_self_attns<span class="token punctuation">,</span> decoder_encoder_attns<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>最后对logits的处理是<code>view</code>成了<code>[batch * target_len, target_vocab_size]</code>, 前面的大小并不影响我们一会用交叉熵计算损失.</p><h2 id="Input-Data"><a href="#Input-Data" class="headerlink" title="Input Data"></a>Input Data</h2><p>输入数据没什么好说的, 为了方便直接采用了硬编码的方式构造<code>word2index</code>, 这样我们的输入序列都被转换为了Token的index输入到Embedding层中, 自动转化为嵌入在低维空间的稠密向量:</p><p>Decoder的输入构造过程采用了<strong>Teaching Forcing</strong>, 保证了训练过程是可以保持<strong>并行</strong>的.</p><pre class="line-numbers language-python"><code class="language-python">sentences <span class="token operator">=</span> <span class="token punctuation">[</span>        <span class="token comment" spellcheck="true"># enc_input           dec_input         dec_output</span>        <span class="token punctuation">[</span><span class="token string">'ich mochte ein bier P'</span><span class="token punctuation">,</span> <span class="token string">'S i want a beer .'</span><span class="token punctuation">,</span> <span class="token string">'i want a beer . E'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token string">'ich mochte ein cola P'</span><span class="token punctuation">,</span> <span class="token string">'S i want a coke .'</span><span class="token punctuation">,</span> <span class="token string">'i want a coke . E'</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># Padding Should be Zero</span>source_vocab <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'P'</span> <span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'ich'</span> <span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'mochte'</span> <span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'ein'</span> <span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'bier'</span> <span class="token punctuation">:</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token string">'cola'</span> <span class="token punctuation">:</span> <span class="token number">5</span><span class="token punctuation">}</span>source_vocab_size <span class="token operator">=</span> len<span class="token punctuation">(</span>source_vocab<span class="token punctuation">)</span>target_vocab <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'P'</span> <span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'i'</span> <span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'want'</span> <span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'a'</span> <span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'beer'</span> <span class="token punctuation">:</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token string">'coke'</span> <span class="token punctuation">:</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token string">'S'</span> <span class="token punctuation">:</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token string">'E'</span> <span class="token punctuation">:</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token string">'.'</span> <span class="token punctuation">:</span> <span class="token number">8</span><span class="token punctuation">}</span>idx2word <span class="token operator">=</span> <span class="token punctuation">{</span>i<span class="token punctuation">:</span> w <span class="token keyword">for</span> i<span class="token punctuation">,</span> w <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>target_vocab<span class="token punctuation">)</span><span class="token punctuation">}</span>target_vocab_size <span class="token operator">=</span> len<span class="token punctuation">(</span>target_vocab<span class="token punctuation">)</span>source_len <span class="token operator">=</span> <span class="token number">5</span> <span class="token comment" spellcheck="true"># max length of input sequence</span>target_len <span class="token operator">=</span> <span class="token number">6</span><span class="token keyword">def</span> <span class="token function">make_data</span><span class="token punctuation">(</span>sentences<span class="token punctuation">)</span><span class="token punctuation">:</span>  encoder_inputs<span class="token punctuation">,</span> decoder_inputs<span class="token punctuation">,</span> decoder_outputs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    encoder_input <span class="token operator">=</span> <span class="token punctuation">[</span>source_vocab<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token keyword">for</span> word <span class="token keyword">in</span> sentences<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>    decoder_input <span class="token operator">=</span> <span class="token punctuation">[</span>target_vocab<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token keyword">for</span> word <span class="token keyword">in</span> sentences<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>    decoder_output <span class="token operator">=</span> <span class="token punctuation">[</span>target_vocab<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token keyword">for</span> word <span class="token keyword">in</span> sentences<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>    encoder_inputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>encoder_input<span class="token punctuation">)</span>    decoder_inputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>decoder_input<span class="token punctuation">)</span>    decoder_outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>decoder_output<span class="token punctuation">)</span>  <span class="token keyword">return</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>encoder_inputs<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>decoder_inputs<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>decoder_outputs<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>数据量非常的少, 所以等会的训练会根本不充分.</p><h2 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h2><p>制作一个Seq2Seq的数据集, 只需要按照Index返回Encoder的输出, Decoder的输入, Decoder的输出(label)就好:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Seq2SeqDataset</span><span class="token punctuation">(</span>Data<span class="token punctuation">.</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder_input<span class="token punctuation">,</span> decoder_input<span class="token punctuation">,</span> decoder_output<span class="token punctuation">)</span><span class="token punctuation">:</span>    super<span class="token punctuation">(</span>Seq2SeqDataset<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>encoder_input <span class="token operator">=</span> encoder_input    self<span class="token punctuation">.</span>decoder_input <span class="token operator">=</span> decoder_input    self<span class="token punctuation">.</span>decoder_output <span class="token operator">=</span> decoder_output  <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>encoder_input<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>  <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>encoder_input<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>decoder_input<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>decoder_output<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>对训练所需的所有东西进行定义:</p><pre class="line-numbers language-python"><code class="language-python">batch_size <span class="token operator">=</span> <span class="token number">64</span>epochs <span class="token operator">=</span> <span class="token number">64</span>lr <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">'cpu'</span><span class="token punctuation">)</span>model <span class="token operator">=</span> Transformer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span>ignore_index<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>encoder_inputs<span class="token punctuation">,</span> decoder_inputs<span class="token punctuation">,</span> decoder_outputs <span class="token operator">=</span> make_data<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span>dataset <span class="token operator">=</span> Seq2SeqDataset<span class="token punctuation">(</span>encoder_inputs<span class="token punctuation">,</span> decoder_inputs<span class="token punctuation">,</span> decoder_outputs<span class="token punctuation">)</span>data_loader <span class="token operator">=</span> Data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里有个<code>criterion = nn.CrossEntropyLoss(ignore_index=0)</code>, 其中<code>ignore_index=0</code>指的是PAD在计算交叉熵时不应该被包括进去(前面提到过PAD所对应的Index是0).</p><p>我们从定义好的数据集中取出数据到<code>device</code>, 然后用torch三件套:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token triple-quoted-string string">'''  encoder_input: [batch, source_len]  decoder_input: [batch, target_len]  decoder_ouput: [batch, target_len]  '''</span>  <span class="token keyword">for</span> encoder_input<span class="token punctuation">,</span> decoder_input<span class="token punctuation">,</span> decoder_output <span class="token keyword">in</span> data_loader<span class="token punctuation">:</span>    encoder_input <span class="token operator">=</span> encoder_input<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    decoder_input <span class="token operator">=</span> decoder_input<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    decoder_output <span class="token operator">=</span> decoder_output<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    output<span class="token punctuation">,</span> encoder_attns<span class="token punctuation">,</span> decoder_attns<span class="token punctuation">,</span> decoder_encoder_attns <span class="token operator">=</span> model<span class="token punctuation">(</span>encoder_input<span class="token punctuation">,</span> decoder_input<span class="token punctuation">)</span>    loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output<span class="token punctuation">,</span> decoder_output<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Epoch:'</span><span class="token punctuation">,</span> <span class="token string">'%04d'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'loss ='</span><span class="token punctuation">,</span> <span class="token string">'{:.6f}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>loss<span class="token punctuation">)</span><span class="token punctuation">)</span>    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Attention-Visualization"><a href="#Attention-Visualization" class="headerlink" title="Attention Visualization"></a>Attention Visualization</h2><p>这回有了自己造的Transformer, 经过了<strong>根本不完全的训练: )</strong>, 我们可以把它的Attention矩阵画出来看看:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> seaborn <span class="token keyword">as</span> sns<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token triple-quoted-string string">'''batch 1:[[1, 2, 3, 5, 0],[1, 2, 3, 4, 0]]'''</span>temp_batch <span class="token operator">=</span> <span class="token number">0</span>n_layers <span class="token operator">=</span> <span class="token number">4</span>plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span>n_heads <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">,</span> n_layers <span class="token operator">*</span> <span class="token number">3</span> <span class="token operator">+</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># encoder_attns: [n_layers, batch, n_heads, source_len, source_len]</span>i <span class="token operator">=</span> <span class="token number">0</span>tokens <span class="token operator">=</span> sentences<span class="token punctuation">[</span>temp_batch<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">for</span> layer <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">for</span> head <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_heads<span class="token punctuation">)</span><span class="token punctuation">:</span>    i <span class="token operator">+=</span> <span class="token number">1</span>    plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span>n_layers<span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> i<span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Layer:{}, Head:{}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>layer<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> head<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> i <span class="token operator">%</span> n_heads <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>      cbar<span class="token operator">=</span><span class="token boolean">True</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>      cbar<span class="token operator">=</span><span class="token boolean">False</span>    sns<span class="token punctuation">.</span>heatmap<span class="token punctuation">(</span>encoder_attns<span class="token punctuation">[</span>layer<span class="token punctuation">]</span><span class="token punctuation">[</span>temp_batch<span class="token punctuation">]</span><span class="token punctuation">[</span>head<span class="token punctuation">]</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cmap<span class="token operator">=</span><span class="token string">'YlGnBu'</span><span class="token punctuation">,</span>             xticklabels<span class="token operator">=</span>tokens<span class="token punctuation">,</span> yticklabels<span class="token operator">=</span>tokens<span class="token punctuation">,</span> cbar<span class="token operator">=</span>cbar<span class="token punctuation">,</span> vmin<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> vmax<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    plt<span class="token punctuation">.</span>xticks<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>yticks<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>最后两行<code>plt.xticks</code>和<code>plt.yticks</code>纯粹是为了<strong>方便注释掉</strong>, 才又写在了外面.</p><p><strong>不要对结果太在意</strong>, 因为<strong>训练是根本不完整的</strong>, 数据也才只有两条. 我只是想画出来看看每个头都大致学到了什么:</p><p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/pytorchtransformer1.jpg" alt=""></p><p>最右侧是Padding, 这一列的权重都被当做是0来计算. 在浅一些的层确实学到了不同Token对不同部分的权重. 再深一些的层基本都没有得到训练, 因为数据实在太少了. </p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Transformer </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KEPLER: Knowledge Embedding and Pre-trained Language Representation</title>
      <link href="/posts/52897.html"/>
      <url>/posts/52897.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>BERT(详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>)</li></ul></blockquote><h1 id="KEPLER-A-Unified-Model-for-Knowledge-Embedding-and-Pre-trained-Language-Representation"><a href="#KEPLER-A-Unified-Model-for-Knowledge-Embedding-and-Pre-trained-Language-Representation" class="headerlink" title="KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation"></a>KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation</h1><p>本文是论文<a href="http://arxiv.org/abs/1911.06136" target="_blank" rel="noopener">KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation</a> 的阅读笔记和个人理解.</p><blockquote><p>顺带吐槽一下这论文第一版和第二版差的也太大了…</p></blockquote><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者指出, PLM不能直接从文本中<strong>获取常识</strong>. 相反, KGE经常能获得知识图谱中实体和关系的有效表示, 却不能<strong>捕捉上下文</strong>. 基于这个简单的观察, 作者希望将常识注入PLM, 这样PLM不但能学到常识, 还能学到有效而丰富的信息<strong>表示</strong>. </p><p>并且在已经注入知识的PLM中, 作者还观察到以下问题:</p><ul><li>实体嵌入和语言分离, 不方便表示空间的对齐. KGE模型很少将KG结构作为输入, 并很少结合文本信息, 因此无法帮助PLM. </li><li>需要实体链接器, 在传播时容易出现错误.</li><li>与普通PLM相比, 查找实体的表示会带来额外的开销.</li></ul><p>而文本描述有与实体相关的丰富信息, 能够帮助文本语义空间与KG的符号空间对齐: </p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kepler1.jpg" style="zoom: 25%;" /><p>作者将实体结合其<strong>描述</strong>用PLM编码, 对KRL和PLM的目标进行<strong>联合优化</strong>. </p><h2 id="KEPLER"><a href="#KEPLER" class="headerlink" title="KEPLER"></a>KEPLER</h2><p>KEPLER(<strong>K</strong>nowledge <strong>E</strong>mbedding and <strong>P</strong>re - trained <strong>L</strong>anguag<strong>E</strong> <strong>R</strong>epresentation)是一个KGE和PLM表示<strong>统一</strong>的模型. 所以它包含了PLM和KE的联合优化目标. </p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kepler2.jpg" style="zoom: 50%;" /><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>作者使用了Transformer Encoder(其实就是BERT)作为<strong>文本编码器</strong>. 即Transformer Encoder将$N$ 个Token的序列$(x_1,\dots, x_N)$作为输入, 然后经过$L$ 层Transformer Encoder的堆叠计算, 得到$d$ 维的上下文表示$\mathbf{H}_i, 1\leq i \leq L$. 每层编码器$\mathrm{E}_i$由多头自注意力和前馈神经网络组成, 每层的Encoder表示记为:<br>$$<br>\mathbf{H}_i=\mathrm{E}_i(\mathbf{H}_{i-1})<br>$$<br>对于任意文本$\text{text}$, 作者希望将经过编码后的$\mathrm{[CLS]}$ 处的输出$\mathrm{E}_{[\mathrm{CLS}]} $ 做为文本的表示. </p><p>在KEPLER中, 作者和RoBERTa一样使用了<strong>BPE</strong>, 与之前出现的Knowledge - Enhanced Model相比较, 这里没有使用额外的<strong>实体连接器</strong>或者知识集成层. </p><h3 id="Knowledge-Embedding"><a href="#Knowledge-Embedding" class="headerlink" title="Knowledge Embedding"></a>Knowledge Embedding</h3><p>与其他KGE方法一样, KEPLER将实体和关系映射进一个$d$ 维的空间中, 并且使用打分函数训练. </p><p>但是KEPLER和普通的KGE方法又不一样, 它不再<strong>存储</strong>Embedding, 而是将实体结合它们本身的描述<strong>编码</strong>做为Embedding. 作者设计了两种结合实体描述的方法: </p><ul><li>只用<strong>实体描述</strong>. </li><li>使用<strong>实体描述</strong>和<strong>关系描述</strong>. </li></ul><h4 id="Using-Entity-Descriptions"><a href="#Using-Entity-Descriptions" class="headerlink" title="Using Entity Descriptions"></a>Using Entity Descriptions</h4><p>对于三元组$(h, r, t)$, 只使用三元组就是对头实体$h$ 的描述$\text{text}_h$和尾实体$t$ 的描述$\text{text}_t$ 分别进行编码, 然后再将关系$r$ 单独嵌入: </p><p>$$<br>\begin{aligned}<br>\mathbf{h} &amp;=\mathrm{E}_{[\mathrm{CLS}]}\left(\operatorname{text}_{h}\right) \\<br>\mathbf{t} &amp;=\mathrm{E}_{[\mathrm{CLS}]}\left(\operatorname{text}_{t}\right) \\<br>\mathbf{r} &amp;=\mathbf{T}_{r}<br>\end{aligned}<br>$$</p><p>其中$\mathbf{T}_r$ 代表关系$r$ 的Embedding权重. </p><h4 id="Using-Entity-and-Relation-Descriptions"><a href="#Using-Entity-and-Relation-Descriptions" class="headerlink" title="Using Entity and Relation Descriptions"></a>Using Entity and Relation Descriptions</h4><p>与只使用实体描述不一样, 因为BERT是可以对两段文字联合编码的, 所以这种方法可以将头实体描述和关系放在一起使用: </p><p>$$<br>\mathbf{h}_{r}=\mathrm{E}_{[\mathrm{CLS}]}\left(\operatorname{text}_{h, r}\right)<br>$$</p><p>具体一点, $\mathrm{text}_{h,r}$ 代表头实体$h$ 和其三元组关系$r$的描述, 在二者之间像BERT输入两段信息一样加上特殊的分隔符$[\mathrm{SEP}]$ 做为区分. 尾实体仍然单独输入进BERT. </p><blockquote><p>我认为该方法最少有两个问题: </p><ol><li>头实体获得关系描述加成(也有可能是污染), 尾实体没有被考虑到. </li><li>BERT捕捉上下文的能力被<strong>局限</strong>了, 仅能体现在<strong>描述</strong>的上下文当中, 并不能根据语境调整实体的表示. </li></ol></blockquote><h4 id="Konwledge-Embedding-Loss-and-Score-Function"><a href="#Konwledge-Embedding-Loss-and-Score-Function" class="headerlink" title="Konwledge Embedding Loss and Score Function"></a>Konwledge Embedding Loss and Score Function</h4><p>KE部分的损失如下: </p><p>$$<br>\mathcal{L}_{\mathrm{KE}} =-\log \sigma\left(\gamma-d_{r}(\mathbf{h}, \mathbf{t})\right)<br>-\sum_{i=1}^{n} \frac{1}{n} \log \sigma\left(d_{r}\left(\mathbf{h}_{\mathbf{i}}^{\prime}, \mathbf{t}_{\mathbf{i}}^{\prime}\right)-\gamma\right)<br>$$</p><p>其中$(h_i^\prime, r, t_i^\prime)$是负采样得到的样本, $\sigma$ 是Sigmoid函数, $\gamma$ 是间隔, $d_r$ 是打分函数, KEPLER沿用TransE的打分函数: </p><p>$$<br>d_{r}(\mathbf{h}, \mathbf{t})=\lVert\mathbf{h}+\mathbf{r}-\mathbf{t}\rVert_{p}<br>$$</p><p>作者使用的是一阶范数, 即$p=1$. </p><h3 id="Masked-Language-Modeling"><a href="#Masked-Language-Modeling" class="headerlink" title="Masked Language Modeling"></a>Masked Language Modeling</h3><p>在Masked Language Model上, KEPLER沿用了BERT和RoBERTa的<strong>MLM</strong>训练损失, 结构和Mask方式都<strong>没有发生任何变化</strong>. 在分类时仍然是在Encoder的最后一层输出$\mathbf{H}_{L,j}$ 后接上和<strong>字典</strong>等同大小的$W$ 路分类器. </p><p>因为RoBERTa的效果比较好, 所以直接用$\text{RoBERTa}_{\mathrm{BASE}}$ 的参数初始化. </p><h3 id="Training-Objectives"><a href="#Training-Objectives" class="headerlink" title="Training Objectives"></a>Training Objectives</h3><p>训练目标就是之前提到过的KGE部分和MLM部分之和: </p><p>$$<br>\mathcal{L}=\mathcal{L}_{\mathrm{KE}}+\mathcal{L}_{\mathrm{MLM}}<br>$$</p><p>这两部分目标是<strong>共享Encoder</strong>的, 在训练时可以采样<strong>不同类型</strong>的文本(倾向于优化KE或者MLM)作为训练数据. </p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h3><p>作者需要使用不同数据优化KE和MLM. </p><h4 id="KE-Objective-Wikidata5M"><a href="#KE-Objective-Wikidata5M" class="headerlink" title="KE Objective: Wikidata5M"></a>KE Objective: Wikidata5M</h4><p>因为作者需要大规模的KG, 并且必须包含对应的实体和关系<strong>描述</strong>, 最好还要支持Inductive Setting, 这样的数据集基本不存在, 所以作者自己根据<strong>Wikidata</strong>和<strong>Wikipedia</strong>构建了一个新的包含实体关系描述文本的大规模KG数据集<strong>Wikidata5M</strong>. </p><p>Wikidata5M比现在的常用数据集大得多, 并几乎涵盖了所有领域: </p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kepler3.jpg" style="zoom: 25%;" /><p>Wikidata5M中实体类型统计:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kepler4.jpg" style="zoom: 20%;" /><h5 id="DataSplit"><a href="#DataSplit" class="headerlink" title="DataSplit"></a>DataSplit</h5><p>数据可以按照两种设置进行划分: </p><ul><li><p><strong>Transductive Setting</strong>: 在绝大多数KG数据集中使用, 对于训练集, 验证集, 测试集<strong>共享所有实体</strong>, 但<strong>并不知道完整三元组</strong>.</p></li><li><p><strong>Inductive Setting</strong>: 训练集, 验证集, 测试集中, <strong>实体和三元组都不共享</strong>. 这更考验模型的<strong>推断</strong>能力, 也更困难. 但它更符合<strong>现实世界</strong>的应用情况. 具体数据集划分情况如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kepler5.jpg" style="zoom: 25%;" /></li></ul><h5 id="Benchmark"><a href="#Benchmark" class="headerlink" title="Benchmark"></a>Benchmark</h5><p>作者用之前常用的KGE模型测试了Wikidata5M的难度, 分别比较了它们在Wikidata5M上的MRR, MR, HITS@1, 3, 10.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kepler6.jpg" style="zoom: 33%;" /><p>确实非常难, 很多流行的数据集都没取得太好的效果. 因为Wikidata包括了各种不同类型的实体和关系, 作者也建议大家使用<strong>大规模数据集</strong>以确保模型能够被充分测试. </p><h4 id="MLM-Objective"><a href="#MLM-Objective" class="headerlink" title="MLM Objective"></a>MLM Objective</h4><p>作者使用优化MLM的数据集有: </p><ul><li>BookCorpus. </li><li>English WIkipedia. </li></ul><h3 id="Pre-Training-Settings"><a href="#Pre-Training-Settings" class="headerlink" title="Pre - Training Settings"></a>Pre - Training Settings</h3><p>关于参数设置我就不再提了, 详见原文. 主要叙述一下在后面实验经常比较的内容. </p><h4 id="KE-Settings"><a href="#KE-Settings" class="headerlink" title="KE Settings"></a>KE Settings</h4><p>对于KE的优化, 作者设计了三种设置: </p><ol><li><strong>KEPLER - Wiki</strong>: 用<strong>Wikidata5M</strong>训练KEPLER, 总使用描述的前512个Token. 当使用实体和关系一起作为输入时(Using Entity and Relation Descriptions), 模型被称为KEPLER - Wiki - rel. </li><li><strong>KEPLER - WordNet</strong>: 用<strong>WordNet</strong>训练KEPLER, 作者尝试将更多的语言知识融入进去, 或许会有益于NLP任务. WordNet中的关系数量相对来说非常少, 所以只采用实体描述. </li><li><strong>KEPLER - W + W</strong>: 联合训练Wikidata5M和WordNet. 损失函数相应的发生变化:<br>$$<br>\mathcal{L}=\mathcal{L}_{\mathrm{Wiki}}+ \mathcal{L}_{\mathrm{WordNet}}+\mathcal{L}_{\mathrm{MLM}}<br>$$</li></ol><h4 id="RoBERTa"><a href="#RoBERTa" class="headerlink" title="RoBERTa"></a>RoBERTa</h4><p>KEPLER是基于RoBERTa的, RoBERTa采用了更大的语料库进行训练. 为公平起见, 作者训练了<strong>RoBERTa*</strong>, 也是用RoBERTa的权重进行初始化, 但用相同的语料库, 并只对它的MLM目标继续优化. </p><blockquote><p>这样设计实验应该是为了凸显出加入KE Loss带来的变化. </p></blockquote><h3 id="NLP-Tasks"><a href="#NLP-Tasks" class="headerlink" title="NLP Tasks"></a>NLP Tasks</h3><p>在NLP任务中, 作者主要和其他Knowledge Enhanced Model在NLP任务上<strong>横向对比</strong>. </p><h4 id="Relation-Classification"><a href="#Relation-Classification" class="headerlink" title="Relation Classification"></a>Relation Classification</h4><h5 id="TACRED"><a href="#TACRED" class="headerlink" title="TACRED"></a>TACRED</h5><p>TACRED是人为标注用于关系分类的数据集. 在TACRED上表现如下: </p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kepler7.jpg" style="zoom: 25%;" /><p>KEPLER - Wiki表现很棒, 与RoBERTa*相比有很大进步, 但KEPLER - WordNet表现只比RoBERTa*好一点点. KEPLER - W + W和KEPLER - Wiki表现相当, 作者认为是WordNet限制了性能. </p><h4 id="FewRel"><a href="#FewRel" class="headerlink" title="FewRel"></a>FewRel</h4><p>FewRel是用于体现Few - Shot能力的关系分类数据集, 其2.0版本添加了更多的领域. 在FewRel上表现如下: </p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kepler8.jpg" style="zoom: 33%;" /><p>就作者给出的结果来看, KEPLER - Wiki表现很不错. 作者认为在FewRel1.0和2.0上的差异是因为2.0版本加入了医疗类的数据. </p><h4 id="Entity-Typing"><a href="#Entity-Typing" class="headerlink" title="Entity Typing"></a>Entity Typing</h4><p>KEPLER在OpenEntity上的表现如下: </p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kepler11.jpg" style="zoom: 20%;" /><p>从F1 Score上来看, KEPLER表现不错. </p><h4 id="GLUE"><a href="#GLUE" class="headerlink" title="GLUE"></a>GLUE</h4><p>GLUE(General Language Understanding Evaluation)是用来评判<strong>语言理解能力</strong>的测试, 这种任务不需要知识, 但需要<strong>理解能力</strong>. 作者通过GLUE尝试证明KEPLER对NLU任务表现没有退化. </p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kepler9.jpg" style="zoom: 33%;" /><p>在比较大的数据集上KEPLER表现相对正常, 在比较小的数据集上例如RTE, KEPLER退化的比较严重. 一般来说 , KEPLER对NLU没有明显副作用. </p><blockquote><p>即然RoBERTa*训练与KEPLER使用的是相同数据集, 那么按理说会因描述类的文本数据扩充而提升文本理解能力, 实际上却没有. 从这个角度来看, PTM和KE并不能给NLU任务带来增益, 甚至还会有<strong>减益</strong>. 应该还有更深层的原因. </p></blockquote><h3 id="KG-Tasks"><a href="#KG-Tasks" class="headerlink" title="KG Tasks"></a>KG Tasks</h3><p>作者在KG的Task上不能使用常用数据集, 因为它们都没有高质量的实体描述, 并且不支持Inductive Setting. </p><h4 id="Transductive-Setting"><a href="#Transductive-Setting" class="headerlink" title="Transductive Setting"></a>Transductive Setting</h4><p>在该设置中, 所有实体在所有阶段均是可见的, 但三元组不可见. 作者将其与TransE性能进行比较: </p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kepler10.jpg" style="zoom: 33%;" /><blockquote><p>用TransE当Baseline是不是有点太不公平了？TransE没有使用任何额外辅助信息, 实体关系描述肯定算引入辅助信息了. 作者在文中列出三条理由, 但我不是很认同. </p></blockquote><h4 id="Inductive-Setting"><a href="#Inductive-Setting" class="headerlink" title="Inductive Setting"></a>Inductive Setting</h4><p>在该设置中, 所有实体和三元组在各阶段都不共享. 作者将也引入实体描述的DKRL(也是作者提出的模型)作为Baseline进行比较: </p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kepler14.jpg" style="zoom: 33%;" /><p>KEPLER - Wiki - rel比KEPLER - Wiki要强大许多, 并且比DKRL提升巨大. </p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>KEPLER是一个<strong>多任务学习</strong>的模型, 作者希望探究对KEPLER性能提升的因素.<br>作者先将RoBERTa, RoBERTa*(仅使用MLM Loss), KEPLER - KE(仅使用KE Loss), KEPLER - Wiki在TACRED上进行测试: </p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kepler12.jpg" style="zoom: 25%;" /><p>相比于RoBERTa, RoBERTa*和KEPLER - KE性能下降了, 作者说这证明了KE Loss和MLM Loss都是必不可缺的.</p><p>作者希望进一步量化的去看看KEPLER到底学到了多少知识, 在TACRED中在<strong>对实体进行Mask</strong>(ME, Masked Entity)和<strong>只保留实体</strong>(Only Entity)的情况下, 重新对关系进行分类, 结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kepler13.jpg" style="zoom: 25%;" /><p>KEPLER - Wiki学习到一些知识, 比用同等数据训练出来的RoBERTa*效果要更好一些.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>作者分别从PLM和KGE两个方面总结了KEPLER的优点:</p><ul><li>KEPLER作为PLM, 将常识集成进了语言表示, 并且具有强大的语言理解能力, 并且增强了抽取知识的能力, 能够直接适应很多NLP任务. </li><li>KEPLER作为KEM, 能够使用丰富的文本信息, 能在文本描述的引导下预测从没见过的实体.</li></ul><p>在我看来, KEPLER其实很一般. 而且它有一些很明显的<strong>缺陷</strong>, 它没有很好地将实体描述中的相关做进一步<strong>扩展</strong>, 例如它没有很好地利用KG作为<strong>图</strong>的优势. 而且它的上下文提取能力获取的实体和关系表示是<strong>静态</strong>的, 并不能根据上下文改变实体的表达. </p><p>除去KEPLER本身外, 作者贡献了一个大规模附带文本描述的数据集. </p><p>最后, 就注入知识是否有益于NLU这个问题来说, 答案还是不明确. 从直觉的角度来说, KEPLER本身就能从上下文中利用BERT的结构学到一些语法知识, 在注入知识的情况下应该进一步提升NLU能力, 而现在很多工作实验结果则不然.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> NLP </tag>
            
            <tag> BERT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KGE预警论文两则</title>
      <link href="/posts/19912.html"/>
      <url>/posts/19912.html</url>
      
        <content type="html"><![CDATA[<p>本文是两篇KGE方向的预警论文的阅读笔记和个人理解. 预警类的工作其实是比较少见的, 对领域的发展也非常有指导意义.</p><blockquote><p><strong>2020.11.22</strong>: 更新Reciprocal Relation.</p><p><strong>2021.05.13</strong>: 修正Reciprocal Relation描述.</p></blockquote><h2 id="A-Re-evaluation-of-Knowledge-Graph-Completion-Methods"><a href="#A-Re-evaluation-of-Knowledge-Graph-Completion-Methods" class="headerlink" title="A Re - evaluation of Knowledge Graph Completion Methods"></a>A Re - evaluation of Knowledge Graph Completion Methods</h2><p>首先来看第一篇论文<a href="http://arxiv.org/abs/1911.03903" target="_blank" rel="noopener">A Re-evaluation of Knowledge Graph Completion Methods</a>.</p><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>这篇论文是在深度学习参与时代背景下提出的. 有相当多的深度学习方法被当做<strong>黑箱</strong>使用在Knowledge Embedding中. 例如普通的CNN, RNN, GNN, 到现在的Attention, 甚至是胶囊网络也有被用于KGE的研究. </p><p>作者敏锐的观察到, 虽然基于DL的方法有非常明显的提升, 但有些方法呈现出在<strong>不同数据集</strong>上的<strong>不一致性</strong>. 作者基于这一个问题, 深度剖析了<strong>基于卷积神经网络</strong>的几种方法呈现错误实验结果的原因.</p><h3 id="Observations"><a href="#Observations" class="headerlink" title="Observations"></a>Observations</h3><h4 id="Inconsistent-Improvements-on-Different-DataSet"><a href="#Inconsistent-Improvements-on-Different-DataSet" class="headerlink" title="Inconsistent Improvements on Different DataSet"></a>Inconsistent Improvements on Different DataSet</h4><p>基于DL的方法在FB15k - 237上的MRR表现十分良好, 但到WN18RR上就出现了问题:</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201120154015009.png" style="zoom:33%;" /><p>仔细看实验结果, 传统的建模方法例如RotatE, TuckER在两个数据集上相比ConvE是都有提升的, 只是提升的幅度不同. 而有些基于DL的方法在WN18RR上居然出现了<strong>退化</strong>的现象, 并且ConvKB的退化居然这么明显. 即使WN18RR是比较<strong>困难</strong>的数据集, 也不应该呈现<strong>大幅度</strong>的表现不一致.</p><h4 id="Score-Functions"><a href="#Score-Functions" class="headerlink" title="Score Functions"></a>Score Functions</h4><p>作者在FB15k - 237上做了个测试, 作者发现很多正确三元组和负采样得来的三元组居然具有<strong>相同Score</strong>:</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201120154042981.png" style="zoom:33%;" /><p>这意味着<strong>无论输入是什么</strong>, <strong>输出始终恒定</strong>(这不是离谱吗), 作者进一步在三种基于CNN的方法上做了对比:</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201120153725829.png" style="zoom: 33%;" /><p>从图中可以看到, 同是基于CNN的KGE方法, 刚才呈现退化的ConvKB和CapsE出现异常的次数非常多, 而一致提升的ConvE异常次数却非常少.</p><h3 id="Root-of-the-Problem"><a href="#Root-of-the-Problem" class="headerlink" title="Root of the Problem"></a>Root of the Problem</h3><p>经过作者的深入研究后, 发现居然是<strong>ReLU</strong>的锅:</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201120153853322.png" style="zoom: 33%;" /><p>可能模型输出的结果, 但最后通过ReLU激活的时候, 这些负数全都被<strong>过滤成零</strong>了, 这也就导致了许多三元组的得分完全一致. 该实验曲线与上一个实验趋势是保持一致的.</p><blockquote><p>其实这个现象被称为<strong>Dead Neuron</strong>, 因为ReLU将数据滤掉后, 会导致在BP时一整条链上的导数全为0, 也就不再更新梯度, 神经元的权重将得不到更新. 我想出的解决方案是使用Leaky ReLU等在负值域上有<strong>斜率</strong>的函数, 但同时激活函数作为NN中很重要的部分, 在更换后可能会对模型产生不可知的影响.</p></blockquote><h3 id="New-Evaluation-Protocol"><a href="#New-Evaluation-Protocol" class="headerlink" title="New Evaluation Protocol"></a>New Evaluation Protocol</h3><p>因为很多三元组的Score都相同, 导致最后在排名进行挑选时会出现<strong>不公平</strong>的现象. 假设正确的三元组是在相同得分的候选三元组中<strong>稳定分布</strong>的, 作者提出了三种选择策略:</p><ul><li>TOP: 将正确的三元组插入待预测插入到分数相同的候选三元组<strong>前</strong>.</li><li>BOTTOM: 将正确的三元组插入待预测插入到分数相同的候选三元组<strong>后</strong>.</li><li>RANDOM: 将正确的三元组<strong>随机</strong>插入待预测插入到分数相同的候选三元组中.</li></ul><blockquote><ol><li>作者认为这样做有效的原因是, 在Link Prediction中, 我们先计算所有负采样三元组的得分, 并将其排列, 最后再计算正确三元组得分, 将其放到合适的位置. 在选择三元组时, 选择排行<strong>最靠前</strong>的三元组. 如果调整了正确三元组的插入位置, 就能从一定程度上解决CNN模型性能虚高的问题.</li><li>在论文中这部分的观点我不是很认同, 我更倾向于是<strong>模型本身</strong>的问题而非<strong>评估协议</strong>的问题. 当然在Evaluation Protocol上动手也可以矫正实验结果, 但不能从根本上解决问题.</li></ol></blockquote><p>在FB15k -237上实验结果如下(作者也在WN18RR上做了实验, 结果一致):</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201120153927434.png" style="zoom:33%;" /><p>作者提到, 在原来的论文中, ConvE, RotatE, TuckER使用的协议是类似于RANDOM的方法, 而ConvKB, CapsE, KBAT使用的是TOP.</p><p>通过观察, 基于TOP的结果都显示出<strong>虚高</strong>的性能, 而且刚才问题最严重的的CapsE性能虚高最为明显. 基于BOTTOM的结果都<strong>虚低</strong>. 而RANDOM的结果相对来说要<strong>公平</strong>一些.</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>经过研究发现基于DL的KGE方法仍然存在一些问题, 在某些数据集上呈现出性能虚高的问题, 这些方法都具有<strong>误导性</strong>, 作者鼓励用文中的方法进行评估, 相对来说更加<strong>公平</strong>. 质疑类的论文价值很高, 但我认为<strong>作者提出的问题并没有从根本上解决</strong>.</p><h2 id="YOU-CAN-TEACH-AN-OLD-DOG-NEW-TRICKS-ON-TRAINING-KNOWLEDGE-GRAPH-EMBEDDINGS"><a href="#YOU-CAN-TEACH-AN-OLD-DOG-NEW-TRICKS-ON-TRAINING-KNOWLEDGE-GRAPH-EMBEDDINGS" class="headerlink" title="YOU CAN TEACH AN OLD DOG NEW TRICKS! ON TRAINING KNOWLEDGE GRAPH EMBEDDINGS"></a>YOU CAN TEACH AN OLD DOG NEW TRICKS! ON TRAINING KNOWLEDGE GRAPH EMBEDDINGS</h2><p>然后再来看第二篇论文<a href="https://openreview.net/forum?id=BkxSmlBFvr" target="_blank" rel="noopener">YOU CAN TEACH AN OLD DOG NEW TRICKS! ON TRAINING KNOWLEDGE GRAPH EMBEDDINGS</a>.</p><h3 id="Background-1"><a href="#Background-1" class="headerlink" title="Background"></a>Background</h3><p>随着KGE受到大家的重视, 越来越多的方法涌现出来, 并且在实验中SOTA. 但实际上大家击败的Baseline可能并不是该模型发挥的真正水平, 因为模型的性能是一定会与<strong>超参选择</strong>和<strong>训练</strong>有关. 许多研究人员将优秀的模型参数设置<strong>移植</strong>过来, 事实上该参数设置可能并不能在自己的模型上达到良好的效果. 除此外, 模型之间采用不同的方法导致<strong>很难横向对比</strong>也是一个很大的问题. </p><p>因此, 作者希望能够采用更大的超参搜索范围和更多种训练技巧, 来对这些模型的性能<strong>量化和总结</strong>.</p><p>以下是在论文中将要比较的模型, 粗体表示首次出现:</p><p>作者将对以下模型进行汇总, 其中粗体代表该方法首次在各类模型中使用:</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201120165535529.png" style="zoom:33%;" /><p>作者的研究只关注纯KGE模型, 不包括引入辅助信息的KGE模型.</p><h3 id="Models-Traning-Evaluation"><a href="#Models-Traning-Evaluation" class="headerlink" title="Models, Traning, Evaluation"></a>Models, Traning, Evaluation</h3><p>该部分中, 对于常见的领域类问题被我跳过了, 即论文中的前四点:</p><ul><li>Multi - relational link prediction.</li><li>Knowledge graph embeddings (KGE).</li><li>Evaluation.</li><li>KGE models.</li></ul><h4 id="Training-Type"><a href="#Training-Type" class="headerlink" title="Training Type"></a>Training Type</h4><p>现在常用的有三种训练类型, 分别是:</p><ul><li><strong>负采样</strong>: 将正样本中的任一元素随机替换作为负样本(有些模型中只替换头实体和尾实体).</li><li><strong>1 vs ALL</strong>: 打乱头和尾实体的位置, 由单个三元组生成全部负例(略存疑).</li><li><strong>K vs ALL</strong>: 批量构建头实体或尾实体非空的一个Batch, 如果在训练集中出现则为正例, 否则为负例. 这种方法在ConvE中首次出现, 在其中被作者称为1 - N Score.</li></ul><h4 id="Loss-Functions"><a href="#Loss-Functions" class="headerlink" title="Loss Functions"></a>Loss Functions</h4><p>一般也只有下列四种Loss:</p><ul><li>MSE(Mean Square Error).</li><li>MR(Margin Ranking), 也称为Hinge Loss.</li><li>BCE(Binary Cross Entropy).</li><li>CE(Cross Entropy).</li></ul><h4 id="Reciprocal-Relations"><a href="#Reciprocal-Relations" class="headerlink" title="Reciprocal Relations"></a>Reciprocal Relations</h4><p>对于Link Prediction任务来说, 对于同一关系, 分别预测头实体和尾实体的<strong>打分函数</strong>应该是不同的. </p><p>但实际上我们不用规定不同的头尾实体打分函数, 我们可以给同种关系以<strong>顺逆区分</strong>, 即每种关系使用两种不同的Embedding, 将所有问题都转化为预测尾实体, 然后打分函数共享相同的实体Embedding, 这样也能最大限度的节省计算开销.</p><p>该方法现在已经广泛的应用于各类KGE方法中, 可能会带来性能提升.</p><h4 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h4><p>现在一般使用的是L2正则化, 个别研究人员推荐使用L3正则. TransE使用归一化. 关于DL的模型可以使用Dropout, 例如ConvE. 在作者的研究中, 还考虑了L1正则.</p><h4 id="Hyperparameters"><a href="#Hyperparameters" class="headerlink" title="Hyperparameters"></a>Hyperparameters</h4><p>现在要考虑的超参有Batch Size, Learning Rate, 负采样个数, 实体和关系的正则权重等.</p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><h4 id="Experiments-Setup"><a href="#Experiments-Setup" class="headerlink" title="Experiments Setup"></a>Experiments Setup</h4><p>实验的设置非常琐碎, 但这是作者的主要工作.</p><ul><li><strong>Dataset</strong>: 现在最常用的两个数据集FB15k - 237和WN18RR.</li><li><strong>Models</strong>: 选用RESCAL, TransE, DistMult, ComplEx, ConvE.</li><li><strong>Evaluation</strong>: MRR和HITS@10.</li><li><strong>Hyperparameters</strong>: 考虑前面提到的三种Training Type(负采样, 1 vs ALL, K vs ALL), 提到的正则(None, L1, L2, L3, Dropout), 优化器(Adam, Adagrad), Embedding Size(128, 256, 512), 并分别对实体和关系建立权重用于Dropout和正则. 这是作者已知的最大范围的超参搜索空间.</li><li><strong>Training</strong>: 最大400Epochs. 每5个Epoch算一次MRR, 并且在有50个Epoch中模型MRR没有5%以上的提升, 则触发早停.</li><li><strong>Model Selection</strong>: 作者通过一个框架, 对于每个模型和数据集<strong>随机生成</strong>(即<strong>随机超参搜索</strong>)了30中不同的配置, 每种配置都包含不同的Training Type和Loss Function. 在随机搜索后用<strong>贝叶斯优化</strong>对参数进一步调优.</li></ul><p>作者给出了一张汇总表(摘自附录):</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201120193313355.png" style="zoom: 50%;" /><blockquote><p>Table 6 是随机搜索在FB15k - 237上的详细最优配置图, 是同级标题下的Impact of Hyperparameters -&gt; Best configurations (quasi-random search)中的第二幅图.</p></blockquote><h4 id="Comparison-of-Model-Performance"><a href="#Comparison-of-Model-Performance" class="headerlink" title="Comparison of Model Performance"></a>Comparison of Model Performance</h4><p>作者将搜索过后的模型(Ours)性能与之前发布的最初版模型(First), 以及最近的模型(Recent)和更大号的模型(Large)进行了比较:</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201120183435443.png" style="zoom:33%;" /><p>根据实验结果, 得到以下结论:</p><ol><li>经过重新超参搜索后的模型相比于原作者首次发布有了<strong>巨大提升</strong>.</li><li>在作者重新训练的这些模型之间, 性能差距逐渐<strong>缩小</strong>, 甚至发生<strong>逆转</strong>.</li><li>有些模型例如RESCAL, 相比于最近<strong>新发布</strong>的模型也<strong>没差多少</strong>.</li></ol><h4 id="Impact-of-Hyperparameters"><a href="#Impact-of-Hyperparameters" class="headerlink" title="Impact of Hyperparameters"></a>Impact of Hyperparameters</h4><h5 id="Anatomy-of-Search-Space"><a href="#Anatomy-of-Search-Space" class="headerlink" title="Anatomy of Search Space"></a>Anatomy of Search Space</h5><p>作者将搜索空间所有的模型的所有结果做了一张<strong>箱型图</strong>:</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201120183501722.png" style="zoom:33%;" /><p>从箱型图得到以下结论:</p><ol><li>不同类型的超参和训练方式给模型带来的影响巨大, 大多的模型性能<strong>均值</strong>和<strong>上限</strong>基本一致, 下限差的有点多. </li><li>TransE的结果不太好, 这是因为其他模型都有大概200种配置, 而TransE只有60种.</li><li>各个模型的表现在FB15k - 237上似乎要稳定一些, WN18RR上最优和最差差距很大.</li></ol><h5 id="Best-configurations-quasi-random-search"><a href="#Best-configurations-quasi-random-search" class="headerlink" title="Best configurations (quasi-random search)"></a>Best configurations (quasi-random search)</h5><p>作者给出了在随机搜索后得到最佳结果的配置(简略), 括号内是不使用该参数导致MRR减少的值:</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201120183522075.png" style="zoom:33%;" /><p>从表中发现不了什么规律, 每个模型所使用的最佳配置基本不同. 但<strong>随机搜索时CE性能似乎比较好</strong>.</p><p>各模型经过随机搜索在<strong>FB15k - 237</strong>上的详细最优配置(详细):</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201120193607208.png" style="zoom: 50%;" /><p>各模型经过随机搜索在<strong>WN18RR</strong>上的最优配置(详细):</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201120193632189.png" style="zoom:50%;" /><h5 id="Best-configurations-Bayesian-optimization"><a href="#Best-configurations-Bayesian-optimization" class="headerlink" title="Best configurations (Bayesian optimization)"></a>Best configurations (Bayesian optimization)</h5><p>各模型经过贝叶斯优化在各数据集上的最优配置:</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201120200139031.png" style="zoom: 33%;" /><p>贝叶斯优化后的效果比随机搜索的结果又稍稍<strong>提升</strong>了一点.</p><h5 id="Impact-of-Training-Type-and-Loss-Functions"><a href="#Impact-of-Training-Type-and-Loss-Functions" class="headerlink" title="Impact of Training Type and Loss Functions"></a>Impact of Training Type and Loss Functions</h5><p>作者在随机搜索上给出了不同训练技巧和不同损失函数的箱型图:</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201120183542089.png" style="zoom: 50%;" /><p>除了能体现出不同训练技巧和不同损失函数对不同模型的影响之外, 再次说明<strong>数据集</strong>也是一个非常重要的原因.</p><p>在图中还能看出来咖啡色组(<strong>1 vs ALL + CE</strong>)和粉色组(<strong>K vs ALL + CE</strong>)的上限非常高, 平均水平也不错. 这说明<strong>CE</strong>可能比其他损失函数要稍好些.</p><h3 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h3><p>这篇论文显示了参数配置在KGE模型上的重要性. 作者用了非常大的超参数搜索空间, 来说明有些模型的性能不一定像它在初始论文中看起来的表现一样. 在作者的更大范围的超参搜索和修改训练方式后, 模型之间的差距<strong>逐渐缩小</strong>, 甚至<strong>结果反转</strong>(有点NFL的意思).</p><p>本论文还有更多的结果在附录中, 主要是对更细粒度的组合做的箱型图, 其实都说明不了什么规律性的问题, 本来就没有规律可言. 给我们最大的启发就是KGE有些模型的性能需要<strong>重新审视</strong>, 对于不同的参数可能会出现截然不同的性能, 作者也鼓励使用<strong>更大参数搜索范围</strong>尽可能的将模型真实的性能展现出来, 方便大家做比较.</p><p>最后, 对作者的<strong>钻研精神</strong>和<strong>科研毅力</strong>致敬.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch实现: Skip-Gram</title>
      <link href="/posts/60645.html"/>
      <url>/posts/60645.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>Pytorch基本操作</li><li>Word2Vec</li></ul></blockquote><h1 id="Skip-Gram的Pytorch实现"><a href="#Skip-Gram的Pytorch实现" class="headerlink" title="Skip - Gram的Pytorch实现"></a>Skip - Gram的Pytorch实现</h1><p>本文用<strong>Pytorch</strong>实现了Skip - Gram, 它是Word2Vec的其中一种. 本文实现参考<a href="https://wmathor.com/index.php/archives/1435/" target="_blank" rel="noopener">PyTorch 实现 Word2Vec</a>, 如果理解上有困难, 另外推荐该博主更简单的实现版本<a href="https://wmathor.com/index.php/archives/1443/" target="_blank" rel="noopener">Word2Vec 的 PyTorch 实现(乞丐版)</a>, 以及其Word2Vec讲解<a href="https://wmathor.com/index.php/archives/1430/" target="_blank" rel="noopener">Word2Vec</a>.</p><p>本文的代码已经放到了Colab上, 打开设置GPU就可以复现(需要科学上网). </p><p><a href="https://colab.research.google.com/drive/1q2ne4eKD2ZZr7doxU1vyDUtfYI3WJVld?usp=sharing" target="_blank" rel="noopener"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="右键我在COLAB中打开!"></a></p><p>如果你不能科学上网, 应该看不到<code>Open in Colab</code>的图标.</p><h2 id="Preparing"><a href="#Preparing" class="headerlink" title="Preparing"></a>Preparing</h2><p>首先我们先导包:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils <span class="token keyword">import</span> data <span class="token keyword">as</span> tud<span class="token keyword">from</span> torch <span class="token keyword">import</span> optim<span class="token keyword">from</span> collections <span class="token keyword">import</span> Counter<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>Counter</code>会应用于等会为字典计数.</p><p>设置GPU:</p><pre class="line-numbers language-python"><code class="language-python">device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">'cpu'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Current Device:"</span><span class="token punctuation">,</span> device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>如果在GPU可用的情况下, <code>device = &#39;cuda&#39;</code>, 否则<code>device = cpu</code>.</p><p>定义其他参数:</p><pre class="line-numbers language-python"><code class="language-python">MAX_VOCAB <span class="token operator">=</span> <span class="token number">10000</span>window <span class="token operator">=</span> <span class="token number">3</span>negative_sample <span class="token operator">=</span> <span class="token number">15</span>hidden <span class="token operator">=</span> <span class="token number">128</span>batch_size <span class="token operator">=</span> <span class="token number">256</span>epochs <span class="token operator">=</span> <span class="token number">2</span>lr <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token comment" spellcheck="true"># set random seed to ensure result is reproducible</span><span class="token keyword">def</span> <span class="token function">set_random</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">import</span> random  np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">1116</span><span class="token punctuation">)</span>  torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">1116</span><span class="token punctuation">)</span>  random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">1116</span><span class="token punctuation">)</span>set_random<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>MAX_VOCAB</code> 是最大词表中的单词. <code>window</code>指的是除去中心词后, 窗口中<strong>单侧</strong>的词数. <code>negative_sample</code>指的是对于每个窗口中除去中心词的其他词, 进行多少次负采样, 即总共负采样<code>window * 2 * negative_sample</code>个单词.</p><h2 id="Getting-Information-from-Text"><a href="#Getting-Information-from-Text" class="headerlink" title="Getting Information from Text"></a>Getting Information from Text</h2><p>导入文件, 并初始化词表, 以及后续需要用到的参数.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">with</span> open <span class="token punctuation">(</span><span class="token string">'./drive/My Drive/Colab Notebooks/text8.train.txt'</span><span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>  text <span class="token operator">=</span> f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>text <span class="token operator">=</span> text<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># We can only use MAX_VOCAB - 1 words we use &lt;UNK> as a word.</span>vocab <span class="token operator">=</span> dict<span class="token punctuation">(</span>Counter<span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">.</span>most_common<span class="token punctuation">(</span>MAX_VOCAB <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># the count of &lt;UNK> is text length - other words' count</span>vocab<span class="token punctuation">[</span><span class="token string">'&lt;UNK>'</span><span class="token punctuation">]</span> <span class="token operator">=</span> len<span class="token punctuation">(</span>text<span class="token punctuation">)</span> <span class="token operator">-</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>list<span class="token punctuation">(</span>vocab<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># save the mapping pair of word to index</span>word2idx <span class="token operator">=</span> <span class="token punctuation">{</span>word<span class="token punctuation">:</span> i <span class="token keyword">for</span> i<span class="token punctuation">,</span> word <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>vocab<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">}</span>idx2word <span class="token operator">=</span> <span class="token punctuation">{</span>i<span class="token punctuation">:</span> word <span class="token keyword">for</span> i<span class="token punctuation">,</span> word <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>vocab<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">}</span>word_count <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>count <span class="token keyword">for</span> count <span class="token keyword">in</span> vocab<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>word_freqs <span class="token operator">=</span> word_count <span class="token operator">/</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>word_count<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># refer to original paper</span>word_freqs <span class="token operator">=</span> word_freqs <span class="token operator">**</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token operator">/</span><span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>数据集下载地址: 链接: <a href="https://pan.baidu.com/s/1j52-cQiIvHpbTGW312f4aw" target="_blank" rel="noopener">https://pan.baidu.com/s/1j52-cQiIvHpbTGW312f4aw</a> 提取码: af3p</p></blockquote><h2 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h2><p>创建一个专门给Embedding用的数据集:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">EmbeddingDataset</span><span class="token punctuation">(</span>tud<span class="token punctuation">.</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">,</span> word2idx<span class="token punctuation">,</span> word_freqs<span class="token punctuation">)</span><span class="token punctuation">:</span>    super<span class="token punctuation">(</span>EmbeddingDataset<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>text_encoded <span class="token operator">=</span> <span class="token punctuation">[</span>word2idx<span class="token punctuation">.</span>get<span class="token punctuation">(</span>word<span class="token punctuation">,</span> word2idx<span class="token punctuation">[</span><span class="token string">'&lt;UNK>'</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> word <span class="token keyword">in</span> text<span class="token punctuation">]</span>    self<span class="token punctuation">.</span>text_encoded <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>self<span class="token punctuation">.</span>text_encoded<span class="token punctuation">)</span>    self<span class="token punctuation">.</span>word2idx <span class="token operator">=</span> word2idx    self<span class="token punctuation">.</span>word_freqs <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>word_freqs<span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>text_encoded<span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>    center_word <span class="token operator">=</span> self<span class="token punctuation">.</span>text_encoded<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># get words in window exception center word</span>    pos_idx <span class="token operator">=</span> <span class="token punctuation">[</span>i <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>idx <span class="token operator">-</span> window<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span>i <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>idx<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> idx <span class="token operator">+</span> window <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>    pos_idx <span class="token operator">=</span> <span class="token punctuation">[</span>i <span class="token operator">%</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>text_encoded<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> pos_idx<span class="token punctuation">]</span>    pos_words <span class="token operator">=</span> self<span class="token punctuation">.</span>text_encoded<span class="token punctuation">[</span>pos_idx<span class="token punctuation">]</span>    neg_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>self<span class="token punctuation">.</span>word_freqs<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    neg_mask<span class="token punctuation">[</span>pos_words<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>    neg_words <span class="token operator">=</span> torch<span class="token punctuation">.</span>multinomial<span class="token punctuation">(</span>neg_mask<span class="token punctuation">,</span> negative_sample <span class="token operator">*</span> pos_words<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># check if negative sample failure exists</span>    <span class="token keyword">if</span> len<span class="token punctuation">(</span>set<span class="token punctuation">(</span>pos_words<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span> set<span class="token punctuation">(</span>neg_words<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Need to resample.'</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> center_word<span class="token punctuation">,</span> pos_words<span class="token punctuation">,</span> neg_words<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我们需要自行实现的函数包括<code>__init__</code>, <code>__len__</code>, <code>__getitem__</code>. </p><p>这个数据集创建好后, 等会就可以用<code>torch.utils.data</code>中的<code>Dataloader</code>进行加载了.</p><blockquote><p>注意, 如果设定的负采样数比较大, 千万不要采用如下代码:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">while</span> len<span class="token punctuation">(</span>set<span class="token punctuation">(</span>pos_words<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span> set<span class="token punctuation">(</span>neg_words<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span> neg_words <span class="token operator">=</span> torch<span class="token punctuation">.</span>multinomial<span class="token punctuation">(</span>self<span class="token punctuation">.</span>word_freqs<span class="token punctuation">,</span> negative_sample <span class="token operator">*</span> pos_words<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Negative sample false"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>这样会导致负采样次数大量增加. 我开始就纳闷为什么训练速度这么慢, 后来发现是采样会重复很多次.</p><p>因为对于我们的训练来说, 负采样时根据单词出现的概率采样, <strong>有非常大概率采样到窗口中已经出现的词</strong>, 这样在计算Loss时会出现问题. 正确的做法应该是像我写的一样, 将单词的概率做一份<strong>拷贝</strong>, 然后将窗口词和中心词在拷贝中的概率<strong>全部置零</strong>, 然后再采样, 这样即使是使用<code>torch.multionmial</code>, 也不会采样到出现在窗口内的词.</p></blockquote><h2 id="Skip-Gram"><a href="#Skip-Gram" class="headerlink" title="Skip - Gram"></a>Skip - Gram</h2><p>然后定义Word2Vec的模型, 直接把损失函数放到里面了:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Word2Vec</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> hidden<span class="token punctuation">)</span><span class="token punctuation">:</span>    super<span class="token punctuation">(</span>Word2Vec<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>vocab_size <span class="token operator">=</span> vocab_size    self<span class="token punctuation">.</span>hidden <span class="token operator">=</span> hidden    <span class="token comment" spellcheck="true"># we use two embedding between input word and other words in window</span>    self<span class="token punctuation">.</span>in_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>self<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden<span class="token punctuation">)</span>    self<span class="token punctuation">.</span>out_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>self<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden<span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_labels<span class="token punctuation">,</span> pos_labels<span class="token punctuation">,</span> neg_labels<span class="token punctuation">)</span><span class="token punctuation">:</span>    input_embedding <span class="token operator">=</span> self<span class="token punctuation">.</span>in_embedding<span class="token punctuation">(</span>input_labels<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, hidden]</span>    pos_embedding <span class="token operator">=</span> self<span class="token punctuation">.</span>out_embedding<span class="token punctuation">(</span>pos_labels<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, window * 2, hidden]</span>    neg_embedding <span class="token operator">=</span> self<span class="token punctuation">.</span>out_embedding<span class="token punctuation">(</span>neg_labels<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, window * 2 * k, hidden]</span>    input_embedding <span class="token operator">=</span> input_embedding<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, hidden, 1] must be the same dimension when use torch.bmm</span>    pos_dot <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>pos_embedding<span class="token punctuation">,</span> input_embedding<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, window * 2, 1]</span>    neg_dot <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>neg_embedding<span class="token punctuation">,</span> <span class="token operator">-</span>input_embedding<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, window * 2 * k, 1]</span>    pos_dot <span class="token operator">=</span> pos_dot<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, window * 2]</span>    neg_dot <span class="token operator">=</span> neg_dot<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, window * 2 * k]</span>    pos_loss <span class="token operator">=</span> F<span class="token punctuation">.</span>logsigmoid<span class="token punctuation">(</span>pos_dot<span class="token punctuation">)</span><span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    neg_loss <span class="token operator">=</span> F<span class="token punctuation">.</span>logsigmoid<span class="token punctuation">(</span>neg_dot<span class="token punctuation">)</span><span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    loss <span class="token operator">=</span> neg_loss <span class="token operator">+</span> pos_loss    <span class="token keyword">return</span> <span class="token operator">-</span>loss  <span class="token keyword">def</span> <span class="token function">get_input_embedding</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># get weights to build an application for evaluation</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>in_embedding<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在对Tensor进行操作时, 一定要<strong>时刻追踪Tensor维度的变换意义</strong>.</p><p>在我参考的博客中, 直接将Loss写到模型中了, 虽然结果都一样, 但我个人不建议这样做, 如果有一份可以复用的代码模板, 还是需要进行一些改动.</p><h2 id="Training-and-Save"><a href="#Training-and-Save" class="headerlink" title="Training and Save"></a>Training and Save</h2><p>对我们前面定义的类进行<strong>实例化</strong>, 同时定义优化器:</p><pre class="line-numbers language-python"><code class="language-python">dataset <span class="token operator">=</span> EmbeddingDataset<span class="token punctuation">(</span>text<span class="token punctuation">,</span> word2idx<span class="token operator">=</span>word2idx<span class="token punctuation">,</span> word_freqs<span class="token operator">=</span>word_freqs<span class="token punctuation">)</span>dataloader <span class="token operator">=</span> tud<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span>word2vec <span class="token operator">=</span> Word2Vec<span class="token punctuation">(</span>MAX_VOCAB<span class="token punctuation">,</span> hidden<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>word2vec<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Step in one epoch:{}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>len<span class="token punctuation">(</span>dataloader<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>训练时可以用<strong>tqdm</strong>来对剩余时间进行评估:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> time <span class="token keyword">import</span> time<span class="token keyword">from</span> tqdm<span class="token punctuation">.</span>notebook <span class="token keyword">import</span> tqdmstart <span class="token operator">=</span> time<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">for</span> step<span class="token punctuation">,</span> <span class="token punctuation">(</span>input_label<span class="token punctuation">,</span> pos_label<span class="token punctuation">,</span> neg_label<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>tqdm<span class="token punctuation">(</span>dataloader<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    input_label <span class="token operator">=</span> input_label<span class="token punctuation">.</span>long<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    pos_label <span class="token operator">=</span> pos_label<span class="token punctuation">.</span>long<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    neg_label <span class="token operator">=</span> neg_label<span class="token punctuation">.</span>long<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 3 step in torch</span>    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>    loss <span class="token operator">=</span> word2vec<span class="token punctuation">(</span>input_label<span class="token punctuation">,</span> pos_label<span class="token punctuation">,</span> neg_label<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> step <span class="token operator">%</span> <span class="token number">1000</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">and</span> step <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>        end <span class="token operator">=</span> time<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"epoch:{}, step:{}, loss:{}, in time:{:.2f}s"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>epoch<span class="token punctuation">,</span> step<span class="token punctuation">,</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> end <span class="token operator">-</span> start<span class="token punctuation">)</span><span class="token punctuation">)</span>        start <span class="token operator">=</span> time<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我最终的训练Loss是在18 ~ 19左右波动. 训练总时长在COLAB一小时左右.</p><p>保存一下模型:</p><pre class="line-numbers language-python"><code class="language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>word2vec<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'./drive/My Drive/embedding-{}.th'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>hidden<span class="token punctuation">)</span><span class="token punctuation">)</span>embedding_weights <span class="token operator">=</span> word2vec<span class="token punctuation">.</span>get_input_embedding<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>注意, 如果对模型进行保存, 也同时要保存<code>word2idx</code>. 因为Word2Vec本质上是<strong>查表</strong>, 除了存储模型权重, 还需要存储单词到表(权重)索引的映射关系<code>word2idx</code>.</p><p>这里的<code>embbeding_weights</code>从GPU上拿下来, 等会做一个小检测, 看看我们训练的Word2Vec效果怎么样.</p><h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><p>选取与其相似度最高的十个词来检测一下Word2Vec的训练效果:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> scipy<span class="token punctuation">.</span>spatial<span class="token punctuation">.</span>distance <span class="token keyword">import</span> cosine<span class="token keyword">def</span> <span class="token function">find_nearest</span><span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">:</span>    index <span class="token operator">=</span> word2idx<span class="token punctuation">[</span>word<span class="token punctuation">]</span>    embedding <span class="token operator">=</span> embedding_weights<span class="token punctuation">[</span>index<span class="token punctuation">]</span>    cos_dis <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>cosine<span class="token punctuation">(</span>e<span class="token punctuation">,</span> embedding<span class="token punctuation">)</span> <span class="token keyword">for</span> e <span class="token keyword">in</span> embedding_weights<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> <span class="token punctuation">[</span>idx2word<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> cos_dis<span class="token punctuation">.</span>argsort<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token keyword">for</span> ie_words <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'two'</span><span class="token punctuation">,</span> <span class="token string">'man'</span><span class="token punctuation">,</span> <span class="token string">'computers'</span><span class="token punctuation">,</span> <span class="token string">'machine'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'word:{} is similar to {}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>ie_words<span class="token punctuation">,</span> find_nearest<span class="token punctuation">(</span>ie_words<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>控制台输出:</p><pre><code>word:two is similar to [&#39;two&#39;, &#39;three&#39;, &#39;four&#39;, &#39;five&#39;, &#39;zero&#39;, &#39;six&#39;, &#39;seven&#39;, &#39;one&#39;, &#39;eight&#39;, &#39;nine&#39;]word:man is similar to [&#39;man&#39;, &#39;woman&#39;, &#39;young&#39;, &#39;god&#39;, &#39;men&#39;, &#39;person&#39;, &#39;girl&#39;, &#39;soul&#39;, &#39;goddess&#39;, &#39;son&#39;]word:computers is similar to [&#39;computers&#39;, &#39;computer&#39;, &#39;devices&#39;, &#39;hardware&#39;, &#39;machines&#39;, &#39;applications&#39;, &#39;systems&#39;, &#39;components&#39;, &#39;electronic&#39;, &#39;computing&#39;]word:machine is similar to [&#39;machine&#39;, &#39;machines&#39;, &#39;device&#39;, &#39;program&#39;, &#39;memory&#39;, &#39;computer&#39;, &#39;engine&#39;, &#39;ibm&#39;, &#39;computers&#39;, &#39;programming&#39;]</code></pre><p>效果其实还不错, 基本上基于平移的规则, 我们给定一个词, 都能找到与其表面语义近似的词.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Word2Vec </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
      <link href="/posts/24649.html"/>
      <url>/posts/24649.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>BERT(详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>)</li></ul></blockquote><h1 id="RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach"><a href="#RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach" class="headerlink" title="RoBERTa: A Robustly Optimized BERT Pretraining Approach"></a>RoBERTa: A Robustly Optimized BERT Pretraining Approach</h1><p>本文是论文<a href="https://arxiv.org/abs/1907.11692" target="_blank" rel="noopener">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a>的阅读笔记和个人理解. RoBERTa已经被广泛的应用于各类由BERT衍生的模型参数初始化, 可以视为是完全体形态的BERT.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者在文章中指出, 训练是一个非常重要的过程, 但BERT在发布时并没有得到很好的训练, 导致其性能看起来比现在的<strong>自回归语言</strong>模型性能要略差(例如XLNet). 但实际上, 对BERT应用一些<strong>训练技巧</strong>对提升BERT性能影响是非常大的. 因此, 作者重新对BERT施加了一些训练技巧, 使得BERT的<strong>性能</strong>得到了进一步提升, 并且具有更强的<strong>鲁棒性</strong>.</p><h2 id="RoBERTa"><a href="#RoBERTa" class="headerlink" title="RoBERTa"></a>RoBERTa</h2><p>RoBERTa(<strong>R</strong>obustly <strong>o</strong>ptimized <strong>BERT</strong> <strong>a</strong>pproach). RoBERTa只是应用了更好的训练技巧, 因此整体结构是没有发生任何变化的. 如果对BERT的结构不熟悉, 建议回顾BERT的知识.</p><h3 id="Dynamic-Masking"><a href="#Dynamic-Masking" class="headerlink" title="Dynamic Masking"></a>Dynamic Masking</h3><p>作者总结出三种Mask的方法:</p><ul><li><strong>纯静态Mask</strong>: 就是BERT中使用的Mask, 在<strong>数据预处理</strong>阶段就进行, 每个Epoch所Mask同一句中的Token位置都是相同的.</li><li><strong>改进一点的静态Mask</strong>: 将每个Sentence都<strong>重复</strong>N次, 这样可能在预处理阶段能得到N种不同的Mask. 因为扩大了每个Epoch的数据量, 训练的Epoch要是原来的1/N倍.</li><li><strong>动态Mask</strong>: 每个Sentence给BERT之前<strong>动态</strong>Mask, 即生成一种新的Mask方式. 这样每个Epoch拿到的Mask基本上是不同的.</li></ul><p>从Mask的方法上来看, 动态Mask并没有引入太多的计算花费, 但是却大大提升了训练时句子的<strong>多样性</strong>. 为了证明其有效性, 作者将上述三种Mask方式的性能将BERT(Base)在SQuAD上的F1 Score, MNLI - m和SST - 2上的ACC做了比较:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/roberta1.jpg" style="zoom: 33%;" /><p>其中reference来自XLNet中给出的BERT(Base)数据. 每种方式都采用了5轮的随机初始化.</p><p>从中能看出, 改进后的静态Mask与原版Mask性能相仿, 动态Mask要比改进后的静态Mask稍好一点. 但动态Mask又不会引入太高的时间开销, 这样的增益还是很划算的.</p><h3 id="Training-without-NSP"><a href="#Training-without-NSP" class="headerlink" title="Training without NSP"></a>Training without NSP</h3><p>在BERT中, 训练时候采用了预测Mask和NSP两种训练任务. NSP(Next Sentence Prediction)任务是它随机的将两段连续或毫不相关的文档拼在一起, 然后用<code>[CLS]</code>位置的输出预测两段文字是否来自于统一文章(或者说连续不连续).</p><p>其实在早一点的多篇论文中指出, NSP任务虽然在BERT中被假设非常重要, 但实际上NSP任务会导致BERT的<strong>退化</strong>. 越来越多的人开始<strong>质疑</strong>NSP任务的必要性. 作者为了观察各种训练方式之间的差异, 设计了如下四种训练方式:</p><ol><li><strong>Segment - Pair + NSP</strong>: 与训练BERT时的方案无异. <strong>每次输入两段来自同一文档或多个文档的内容</strong>. 内容总长度必须少于512个Token.</li><li><strong>Sentence - Pair + NSP</strong>: <strong>每次只输入两个来自同一文档或多个文档的句子</strong>. 每次输入的序列长度肯定小于512个Token, 所以用<strong>增大Batch Size</strong>的方式来让这种方式的总Token数与Segment - Pair + NSP总Token数相近.</li><li><strong>Full - Sentences</strong>: <strong>全部输入可能来自于同一文档或多个文档的连续句子</strong>, 直到填满为止. 序列长度最多512个Token. 在切换不同文档时, 在之间加上特殊的分隔符. 不采用NSP任务.</li><li><strong>Doc - Sentences</strong>: <strong>全部输入来自同一文档的句子</strong>, 即只从一篇文档中对连续句子采样, 如果文档的内容少于512个Token, 则动态增大Batch Size使得其与Full - Sentences总Token数量相近.</li></ol><p>实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/roberta2.jpg" style="zoom: 33%;" /><p>作者得出了以下结论:</p><ol><li>观察方式1和方式2, 使用单个句子明显的损失了下游任务的性能. 可能模型并不能从句子中学习到长范围的依赖.</li><li>观察方式1和方式4, 即BERT的原始训练方式与不适用NSP的训练方式, 移除NSP能稍微增强下游任务的性能.</li></ol><p>但是由于表现最好的Doc - Sentences需要动态调整Batch Size, 作者还是采用了Full - Sentences作为后文实验方式.</p><h3 id="Bigger-bigger-and-bigger"><a href="#Bigger-bigger-and-bigger" class="headerlink" title="Bigger, bigger, and bigger"></a>Bigger, bigger, and bigger</h3><h4 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h4><p>XLNet用了126G的数据, 当时BERT训练只用了十几个G的数据, 所以对比是很不公平的. RoBERTa在数据上不能落后, 一口气用了160G的数据. 分别由以下部分组成:</p><table><thead><tr><th align="left">数据集名称</th><th align="center">大小(GB)</th><th>说明</th></tr></thead><tbody><tr><td align="left">BookCorpus</td><td align="center">16</td><td>BERT训练时候用的数据</td></tr><tr><td align="left">CC - NEWS</td><td align="center">76</td><td>过滤后的新闻类数据</td></tr><tr><td align="left">OpenWebText</td><td align="center">38</td><td>根据网友点赞数从URL中提取的帖子文本</td></tr><tr><td align="left">Stories</td><td align="center">31</td><td>故事类数据</td></tr><tr><td align="left"><strong>总计</strong></td><td align="center"><strong>161</strong></td><td></td></tr></tbody></table><p>更大量的数据对BERT提升是巨大的, 使之能够与XLNet相对公平的进行比较.</p><blockquote><p>从<strong>辩证</strong>的角度来说, 更大量的数据也会使模型的<strong>偏见</strong>和<strong>歧视性</strong>更强. 在数据上的偏见消除是非常重要的, 目前来说没有太好的解决方案.</p></blockquote><h4 id="Batch-Size"><a href="#Batch-Size" class="headerlink" title="Batch Size"></a>Batch Size</h4><p>先前有大量实验表明, 适量增大Batch Size有益于模型的收敛, 更能使训练稳定, 并有助于提高模型的性能. 更大的Batch Size也能帮助快速训练. 作者做了增大Batch Size对性能影响的实验, 保证Batch Size和Step的积不变, 即<strong>维持相同的计算开销</strong>, 实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/roberta3.jpg" style="zoom: 33%;" /><p>适量增大Batch Size确实有助于提高模型的性能, 但考虑到更大Batch Size在<strong>训练速度</strong>上带来的优势, 作者只采用了8K的Batch Size, 而非效果最好的2K Batch Size.</p><p>本小节叙述一些无关紧要的参数调整. 除去峰值Learning Rate, 和Warmup的次数, RoBERTa延续了BERT的<strong>原始参数</strong>. 考虑到RoBERTa采用了更大的Batch Size, 所以将Adam中的Beta2从0.999 换为了0.98.</p><p>RoBERTa不会随机的将短句注入, 并且前90%的训练中不会使用缩短的序列, 只使用全长序列.</p><p>RoBERTa还采用了<strong>BPE</strong>缩小词表, 进一步提升了性能.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>作者从增大数据量和增长训练时间两个角度做了实验:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/roberta4.jpg" style="zoom: 50%;" /><p>从逐步添加训练技巧的流程来看, RoBERTa在和BERT使用十几G数据的情况下提升非常大, 也和XLNet在用13G时的性能不分高下, 证明了RoBERTa改进的<strong>正确性</strong>. 逐渐增大训练数据量和训练时长(这里是通过Step调整), RoBERTa<strong>逐渐碾压</strong>了XLNet.</p><p>然后将RoBERta与GLUE排行榜中其他的模型也进行了实验:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/roberta5.jpg" style="zoom: 50%;" /><p>RoBERTa作者提到, 对于GLUE有两种Fine - Tune的方式:</p><ul><li>单任务, 对每个GLUE任务分别进行Fine - Tune, 并且只用相应任务的训练数据.</li><li>多任务, 在测试集上进行比较. 但与排行榜上其他的模型不同, 其他模型对多任务进行Fine - Tune, RoBERTa只对单任务进行Fine - Tune.</li></ul><p>从实验结果中来看, RoBERTa比没训练好的BERT提升相当的大.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>作者实际上在RoBERTa中主要做了四件事:</p><ol><li>用更大的Batch Size, 更多的Data, 更长的训练时间. 就是更大.</li><li>废除NSP的训练目标, 这个非常重要.</li><li>将静态Mask换为动态Mask.</li><li>用更长序列训练(不怎么重要).</li></ol><p>严格意义上来说, RoBERTa才是BERT的完全体. 这提供给大家一个非常好的预训练基准, 而且在其他论文中也鼓励用RoBERTa而不是BERT进行比较, 因为BERT的训练是不够充分的.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> BERT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Integrating Image-Based and Knowledge-Based Representation Learning</title>
      <link href="/posts/13721.html"/>
      <url>/posts/13721.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>AlexNet(详见<a href="https://adaning.github.io/posts/38085.html">卷积神经网络发展史</a>)</li><li>Attention(详见<a href="https://adaning.github.io/posts/40071.html">Seq2Seq和Attention</a>)</li><li>TransE(详见<a href="https://adaning.github.io/posts/53023.html">TransE: Translating Embeddings for Modeling Multi-relational Data</a>)</li></ul></blockquote><h1 id="Integrating-Image-Based-and-Knowledge-Based-Representation-Learning"><a href="#Integrating-Image-Based-and-Knowledge-Based-Representation-Learning" class="headerlink" title="Integrating Image-Based and Knowledge-Based Representation Learning"></a>Integrating Image-Based and Knowledge-Based Representation Learning</h1><p>本文是论文<a href="https://ieeexplore.ieee.org/abstract/document/8689107/" target="_blank" rel="noopener">Integrating Image-Based and Knowledge-Based Representation Learning</a>的阅读笔记和个人理解. 这篇论文是刘志远老师&lt;知识图谱与深度学习&gt;中2.8节提到的模型.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>研究人员发现, 语言的理解和生成是由大脑<strong>不同位置</strong>的区域负责的, 这些区域对应了许多现实生活中的事物.</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201113210117233.png"  style="zoom: 25%;" /><p>有时, 图片之间也能暗含关系:</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201113210138922.png" style="zoom: 25%;" /><p>即若A是B的一部分, 我们的视觉也认为A的周围应该有B.</p><p>作者认为, 我们理解世界是通过<strong>Knowledge Base</strong>(实体和关系的结构化三元组), 和<strong>Image Representation</strong>(通过Deep Convolutional Networks). 所以作者研究了基于<strong>图片</strong>的知识表示模型IKRL Model(<strong>I</strong>mage - <strong>B</strong>ased Knowledge <strong>R</strong>epresentation <strong>L</strong>earning Model). </p><p>在先前的KRL方法中, 只使用了KG中的关系信息. 然而KG中的结构化信息经常<strong>过于简单</strong>, 或者<strong>不完整</strong>, 会限制知识表示在下游任务中的表现. KRL是允许向表示中添加实体图像信息的, 而基于结构和图片的表示能够从<strong>多方面</strong>表示实体.</p><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>受到大脑的启发, 通过编码和图像两种手段来对实体进行表示, $\mathbf{h}_S, \mathbf{t}_S$ 是<strong>基于结构化的表示(SBR)</strong>的头实体和尾实体, $\mathbf{h}_I, \mathbf{t}_I$ 是<strong>基于图片的表示(IBR)</strong>的头实体和尾实体.</p><h3 id="Joint-Energy-Function"><a href="#Joint-Energy-Function" class="headerlink" title="Joint Energy Function"></a>Joint Energy Function</h3><p>我们将基于结构化的表示(SBR)和基于图片的表示(IBR), 融合起来, 形成一个IKRL Model, <strong>联合能量函数</strong>如下:<br>$$<br>E(h, r, t)=E_{S S}+E_{S I}+E_{I S}+E_{I I}<br>$$</p><p>联合能量函数中有四项, 这四项分别是:</p><ul><li>$E_{S S}=\lVert\mathbf{h}_{S}+\mathbf{r}-\mathbf{t}_{S}\rVert$: 和<strong>TransE</strong>的能量函数一模一样.</li><li>$E_{II}=\lVert\mathbf{h}_{I}+\mathbf{r}-\mathbf{t}_{I}\rVert$: 与TransE的能量函数也一样, 但是是<strong>图片版本</strong>的.</li><li>$E_{SI}=\lVert\mathbf{h}_{S}+\mathbf{r}-\mathbf{t}_{I}\rVert$, $E_{IS}=\lVert\mathbf{h}_{I}+\mathbf{r}-\mathbf{t}_{S}\rVert$: 这两项希望能将SBR和IBR投入<strong>相同语义空间</strong>.</li></ul><p>其中实体向量$\mathbf{h}_S, \mathbf{h}_I, \mathbf{t}_S, \mathbf{t}_I$ 都是<strong>归一化</strong>过的, 关系$\mathbf{r}$ 不是, TransE论文中曾提到关系的归一化对学习到关系没太大影响. 但是请注意, 在这里SBR和IBR共用同一个关系向量$r$, 关系的表示是<strong>不能从图像中直接学到</strong>的. 共享关系向量能够作为两种实体表示之间的<strong>转换</strong>, 也能方便它们嵌入到同一语义空间中.</p><p>模型结构的概览图如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201113210155434.png" style="zoom: 50%;" /><p>因为考虑到融入了IBR, 所以每个实体的多个图片都被送入Image Encoder中, 然后在通过注意力机制让模型考虑每张图片的重要性, 然后SBR和IBR联合学习整个能量函数.</p><h3 id="Image-Encoder"><a href="#Image-Encoder" class="headerlink" title="Image Encoder"></a>Image Encoder</h3><p>图像对IKRL来说是非常重要的, 因为图像能够从外观等多个方面刻画实体. 此外, <strong>多张图片</strong>可能从不同角度提供同一个实体的不同特性, 我们用$I_{k}=\left\{\mathrm{img}_{1}^{(k)}, \mathrm{img}_{2}^{(k)}, \ldots, \mathrm{img}_{n}^{(k)}\right\}$ 来表示多张图片.</p><p>既然涉及到图像, 那么比较成熟的方案肯定是用<strong>CNN</strong>提取视觉特征了, 用CNN对每张图像构建特征表示. </p><p>Image Encoder由<strong>图像表示模块</strong>和<strong>图像投影模块</strong>组成:</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201113210231408.png" style="zoom: 25%;" /><h4 id="Image-Representation-Module"><a href="#Image-Representation-Module" class="headerlink" title="Image Representation Module"></a>Image Representation Module</h4><p>图像表示模块主要依赖于CNN对特征进行抽取, 将实体在<strong>图像空间</strong>中进行表示. 作者在这里只使用ALexNet(确实比较老), AlexNet是一个只由5层Conv层和2层全连接层组成的神经网络:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/alexnet.jpg" style="zoom: 50%;" /><p>将图像Reshape成$224\times 224$ 的大小, 然后接上5层Conv层, 2层全连接层就得到了实体的图像表示.</p><blockquote><p>这里使用AlexNet肯定不是最好的选择, 我猜测作者出于<strong>实验性目的</strong>才使用了AlexNet.</p></blockquote><h4 id="Image-Projection-Module"><a href="#Image-Projection-Module" class="headerlink" title="Image Projection Module"></a>Image Projection Module</h4><p>因为IBR和SBR不在同一个语义空间中, 所以需要用一次变换将它们投入相同的实体空间中:<br>$$<br>\mathbf{p}_{i}=\mathbf{M} \cdot f\left(\mathrm{img}_{i}\right)<br>$$<br>$\mathbf{p}_i$ 指图片表示, $f(\cdot)$ 表示神经网络的输出. 当然, 这个投影矩阵$\mathbf{M}$ 在每个实例之间<strong>共享</strong>.</p><h3 id="Attention-Based-Multi-Instance-Learning"><a href="#Attention-Based-Multi-Instance-Learning" class="headerlink" title="Attention Based Multi - Instance Learning"></a>Attention Based Multi - Instance Learning</h3><p>Attention在IKRL中起到了非常大的作用. 因为Attention让更多信息性的图片对IKRL有更多的贡献. Attention在IKRL中被用于<strong>多实例学习</strong>, 因为绝大多数实体都有不止一张不同的图片, 但视觉信息经常伴随着<strong>噪声</strong>, 所以相当有必要对实体所对应的图片进行<strong>选择</strong>. 论文后面实验会多次说明这一点.</p><p>细想一下, 其实我们也是这样的, 我们善用注意力去选择表征实例, 而滤掉不相关的实例.</p><p>在IKRL中, 实例级别的注意力能将每个实例与实体进行<strong>匹配</strong>, 得到实例对实体的<strong>权重</strong>:<br>$$<br>\operatorname{att}\left(\mathbf{p}_{i}^{(k)}, \mathbf{e}_{S}^{(k)}\right)=\frac{\exp \left(\mathbf{p}_{i}^{(k)} \cdot \mathbf{e}_{S}^{(k)}\right)}{\sum_{j=1}^{n} \exp \left(\mathbf{p}_{j}^{(k)} \cdot \mathbf{e}_{S}^{(k)}\right)}<br>$$</p><p>其中$\mathbf{e}_{S}^{(k)}$ 代表第$k$ 个实体的SBR.</p><p>在获取了权重后, 对图像表示<strong>加权求和</strong>, 得到IBR:<br>$$<br>\mathbf{e}_{I}^{(k)}=\sum_{i=1}^{n} \frac{\operatorname{att}\left(\mathbf{p}_{i}^{(k)}, \mathbf{e}_{S}^{(k)}\right) \cdot \mathbf{p}_{i}^{(k)}}{\sum_{j=1}^{n} \operatorname{att}\left(\mathbf{p}_{j}^{(k)}, \mathbf{e}_{S}^{(k)}\right)}<br>$$<br>除了上述普通的Attention外, 作者还用两种方法与之进行比较:</p><ul><li>如果对每张实例分配相同权重, 称为$\text{AVG}$.</li><li>如果只选权重最大的实例作为$\mathbf{p}_{i}^{(k)}$, 称为$\text{MAX}$.</li></ul><p>在后续的实验中, 会比较这三种方法之间的性能.</p><h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><p>作者使用最大化间隔的Hinge Loss来训练:</p><p>$$<br>L=\sum_{(h, r, t) \in T} \sum_{\left(h^{\prime}, r^{\prime}, t^{\prime}\right) \in T^{\prime}} \max \left(\gamma+E(h, r, t)-E\left(h^{\prime}, r^{\prime}, t^{\prime}\right), 0\right)<br>$$</p><p>其中$\gamma$ 代表间隔. 与现在的其他KRL模型训练一样, 都有<strong>负采样</strong>:<br>$$<br>T^{\prime}=\left\{\left(h^{\prime}, r, t\right) \mid h^{\prime} \in E\right\} \cup\left\{\left(h, r, t^{\prime}\right) \mid t^{\prime} \in E\right\}<br>\cup\left\{\left(h, r^{\prime}, t\right) \mid r^{\prime} \in R\right\}, \quad(h, r, t) \in T<br>$$<br>但这里的负采样不光替换实体, 而是随机替换三元组中的<strong>实体</strong>和<strong>关系</strong>.</p><h3 id="Optimization-and-Implementation-Details"><a href="#Optimization-and-Implementation-Details" class="headerlink" title="Optimization and Implementation Details"></a>Optimization and Implementation Details</h3><p>IKRL模型中, 所有的参数为$\theta=(\mathbf{E}, \mathbf{R}, \mathbf{W}, \mathbf{M})$, $\mathbf{E}$ 代表SBR的Embedding, 包括$\mathbf{h}_s, \mathbf{t}_s$. $\mathbf{R}$ 代表关系嵌入. $\mathbf{W}$ 代表神经网络的参数, $\mathbf{M}$ 是投影矩阵的参数. 作者使用SGD来优化模型, $\mathbf{E}, \mathbf{R}$ 直接用TransE的参数初始化, $\mathbf{M}$ 随机初始化.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细试验参数设置请参考原论文, 不过我认为本论文是一篇<strong>实验性</strong>的工作, 参数设置上没有太多意义. SBR的实体和关系Embedding维度$d_s=50$, 并且最多为每个实体使用10张图片.</p><h3 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h3><p>因为没有现成的Image Based Knowledge Dataset, 所以作者团队自己搞了一个WN9 - IMG. 这个数据集先从WN18中抽出一部分三元组, 然后从ImageNet中抽出来一部分图片, 组成了基于图片的数据集. 这个数据集中只有9种关系, 6555个实体. 不同关系的数据集分布如下所示:</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201113210532097.png" style="zoom: 25%;" /><p>8类实体数量如下所示:</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201113210614662.png"  style="zoom: 25%;" /><h3 id="Entity-Prediction"><a href="#Entity-Prediction" class="headerlink" title="Entity Prediction"></a>Entity Prediction</h3><p>在训练时, 仍然使用IBR和SBR<strong>混合</strong>的方式进行训练. 只是在<strong>测试</strong>时对使用的信息进行改动:</p><ul><li>SBR: 在测试时只使用基于结构的表示.</li><li>IBR: 在测试时只使用基于图像的表示.</li><li>UNION: 二者都可以使用.</li></ul><p>在与TransE和TransR的比较中, IKRL的表现非常好:</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201113210715845.png" style="zoom: 25%;" /><p>从结果中得到以下结论:</p><ul><li>无论是哪种IKRL, 全面碾压TransE和TransR, 证明了实体图像中丰富的视觉信息能帮助更深入的理解实体.</li><li>相比于只使用SBR训练(TransE), 融合了IBR的表示训练有极大的提升. 因为IKRL能通过能量函数中的两种表示进行混合项训练, 也间接地学习到了一部分图像信息, 这也是为什么测试时只使用SBR就能得到很大提升.</li><li>在MR上显示出的效果提升比较大. 作者认为MR是更多关注Embedding在空间上的<strong>整体效果</strong>, 而Hits@10对错误比例更加敏感. IKRL因为融入了图片信息, 能从图片中间接的发现KG中没有体现的直接关系, 可以利用图片的中发现的<strong>潜在关系</strong>.</li><li>IKRL是基于TransE进行训练的, 但仍然比TransR效果好, 说明IKRL有更好的<strong>鲁棒性</strong>, 能更好的用在基于平移模型的改进模型上.</li></ul><blockquote><p>我有些惊讶, 仅仅只使用IBR带来的效果居然这么好. 是不是说明了模型在训练阶段使用了SBR后, 在不同任务上SBR测试阶段的意义是不同的? 可能有些任务不需要SBR?</p></blockquote><p>作者还对Attention的三种方式的效果进行了比较:</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201113210928871.png" style="zoom: 33%;" /><p>MAX最差, 这是因为单一图片提供的视觉信息有限. AVG其次, 虽然考虑到了所有实例, 但不可避免的引入了噪声. 普通的Attention是最好的, 它能够根据图片的质量来对实例进行<strong>筛选</strong>.</p><h3 id="Triple-Classification"><a href="#Triple-Classification" class="headerlink" title="Triple Classification"></a>Triple Classification</h3><p>在三元组分类任务上, 作者对每种关系依据验证集设置一个阈值$\delta_r$, 用阈值来对三元组分类是否正确进行分类. 例如当$\lVert\mathbf{h}+\mathbf{r}-\mathbf{t}\rVert&gt;\delta_r$ 时, 分类错误, 反之分类正确. 其他模型同样按照自己的评分函数分类判断.</p><p>在不同的Attention种类下, 结果如下所示:</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201113211050295.png" style="zoom: 50%;" /><p>仍然是与Entity Prediction相似的结论. 融入视觉信息的表示是一件非常重要的事情, 并同时证明了Attention保证了图像的质量, 充分利用实体的多样性, 增强了模型的<strong>鲁棒性</strong>.</p><h3 id="Representation-Analysis"><a href="#Representation-Analysis" class="headerlink" title="Representation Analysis"></a>Representation Analysis</h3><h4 id="SBR-and-IBR"><a href="#SBR-and-IBR" class="headerlink" title="SBR and IBR"></a>SBR and IBR</h4><p>作者分别基于SBR和IBR计算了数据集中所有类别的实体之间的<strong>协方差</strong>:</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201113211018302.png" style="zoom: 67%;" /><p>从中不难发现, IBR能对不同类别的实体相<strong>区分</strong>(不同类之间相关性差), SBR将少数类别的实体相<strong>连接</strong>(有些类相关性强). 但在IBR中, Plant和Object似乎具有很高的相关性.</p><p>IBR更容易<strong>基于外观</strong>区分实体, SBR更善于<strong>基于功能</strong>区分实体, 如下图:</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201113211141244.png" style="zoom: 50%;" /><p>左侧”sport”从图像上来看并不相似, 但功能相似. 右侧”aritifact”从图像上看上去非常相似, 但功能不相似.</p><p>作者分别对SBR和IBR进行了PCA降维成2维后绘制出实体在坐标系中的位置, 并用RSA做可解释性分析:</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201113211120889.png" style="zoom:67%;" /><p>SBR(左), IBR(中), 主成分可解释性(右). IBR能够明显的将各关系的实体分开, SBR效果就差一些. 从数据可解释性来看, 除了随维度增大可解释性增强, 但从表示类型来看, IBR也是始终要强于IBR的. </p><h4 id="Relation-Analysis"><a href="#Relation-Analysis" class="headerlink" title="Relation Analysis"></a>Relation Analysis</h4><p>作者将关系也做PCA和RSA:</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201113211207797.png" style="zoom: 50%;" /><p>能很清楚的看到, 反义关系往往处于某个主成分正交轴的<strong>对立面</strong>. RSA图中看到, 在主成分没有达到8时可解释性已经收敛, 说明目前学到的关系复杂度是比需求要大的.</p><h3 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h3><p>作者在这里对案例进行了分析.</p><h4 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h4><p>首先是对Attention对<strong>实例筛选</strong>的结果进行了可视化:</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201113211241891.png" style="zoom: 50%;" /><p>例如”cycling”这个实体, 真正的骑行图像被赋予了高注意力, 而没有自行车的图像被赋予低注意力. “typewirter”中, 整体打印机的图片被赋予高注意力, 而打印机局部细节的图片被赋予低注意力. 在”riding”中, 人类骑马的图片被赋予高注意力, 马群自己走的照片被赋予低注意力.</p><p>这证明了注意力能自动从图像中学习知识表示, 减少低质量图片的噪声.</p><h4 id="Semantic-Translation-in-Image-Representation-Space"><a href="#Semantic-Translation-in-Image-Representation-Space" class="headerlink" title="Semantic Translation in Image Representation Space"></a>Semantic Translation in Image Representation Space</h4><p>然后, 作者发现, 在跨模态的图像 - 知识空间中, 像Word2Vec一样, 也含有<strong>语义平移</strong>规则:</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/image-20201113211321972.png" style="zoom: 33%;" /><p>在图像的表示中, 柜子和抽屉的差, 与钢琴和琴键之间的差大致相等, 表示出”属于”的关系. 这体现了编码的<strong>语义规则</strong>性.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>IKRL介绍了一种<strong>基于图像</strong>的知识表示方法. 将<strong>基于结构化的表示(SBR)</strong>和<strong>基于图片的表示(IBR)</strong>融合在了一起, 并用<strong>Attention</strong>做图片的自动过滤, 提高模型的<strong>性能</strong>和<strong>鲁棒性</strong>.</p><p>我认为, 这篇文章比较有价值的有以下几个部分:</p><ul><li>论文中提出的<strong>多实例学习</strong>思路确实不错, 或许多模态都可以用多实例学习作为接口, 将实体与不同模态之间的数据进行自动对齐和筛选.</li><li>做了很多的<strong>分析类实验</strong>, 虽然使用的模型都是最原始最简单的, 但这些<strong>可视化探究</strong>都是非常有价值的, 我认为相当多的可视化都<strong>得益于图像</strong>.</li><li>提供了基于图像的知识数据集<strong>WN9 - IMG</strong>.</li></ul><p>我认为的缺点有:</p><ul><li>在处理SBR时只用了TransE, 在处理IBR时只用了非常早的AlexNet. 那么在对关系建模时就肯定会遇到对<strong>多关系</strong>建模的痛点. 但想必这篇论文也只是一次<strong>尝试和探索</strong>, 而非追求性能.</li><li>没有与更多的KRL方法进行对比, 但想必在后续肯定会有相关工作.</li></ul><p>我认为实验还揭示了另一个点, 或许在某些任务上<strong>融入语言特征</strong>并不能起到很大的作用. 比如本论文提到的模型, 如果只用IBR, 也能取得相当好的效果. 当然不排除作者提出的KRL模型没有对三元组使用其他方法优化的因素. SBR确实能够在某些任务上使模型完成<strong>更复杂</strong>的任务, 并与IBR<strong>互补</strong>. 但从本论文的结果来看, <strong>并不能很大幅度的提升性能</strong>.</p><p>希望<strong>脑科学和神经科学</strong>能快快进步, 给深度学习发展带来更多动力和想法.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> 多模态 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TransE: Translating Embeddings for Modeling Multi-relational Data</title>
      <link href="/posts/53023.html"/>
      <url>/posts/53023.html</url>
      
        <content type="html"><![CDATA[<h1 id="TransE-Translating-Embeddings-for-Modeling-Multi-relational-Data"><a href="#TransE-Translating-Embeddings-for-Modeling-Multi-relational-Data" class="headerlink" title="TransE: Translating Embeddings for Modeling Multi-relational Data"></a>TransE: Translating Embeddings for Modeling Multi-relational Data</h1><p>本文是论文<a href="https://proceedings.neurips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html" target="_blank" rel="noopener">Translating Embeddings for Modeling Multi-relational Data</a>的阅读笔记和个人理解. 这篇论文是一篇比较早的论文了, 2012年Knowledge graph这个概念被谷歌提出, 2013年这篇论文就发表了, 并且大家也对它认可度很高, 几乎之后的所有关于KGE的论文中都会出现以它为Baseline的实验.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>在有向图中, 可以允许有多元关系数据的存在, 例如图中的边可以被表示为$ (head, label, tail) $, 即$(h, \ell, t)$. 这点经常体现在各种知识库中, 比如Google的知识图谱.</p><p>在先前的Knowledge Embedding方法中, 要么具有很<strong>高复杂度</strong>, 要么有<strong>高计算成本</strong>.<br>作者阐述, 基于平移的模型主要来源于下列两个Motivation:</p><ul><li>最主要的Motivation是在很多KBs中, <strong>层次化表示</strong>都非常的常见, 那么平移一种很自然的转换表示法. 比如树的自然表示就可以用兄弟关系和父子关系来投射到一个二维坐标系中表示. 所以可以尝试可以把三元组<strong>嵌入到低维空间</strong>中.</li><li>类似与<strong>Word2vec</strong>, 作者观察到Word Embedding能够捕获一些文本中的一对一关系, 例如国家和城市之间的”省会”关系, 在嵌入空间上就是平移.</li></ul><h2 id="Translation-Based-Model"><a href="#Translation-Based-Model" class="headerlink" title="Translation - Based Model"></a>Translation - Based Model</h2><p>TransE的思想其实是非常简单的. 对于三元组$(h, \ell, t)$, TransE希望能够在空间中有$h+\ell \approx h$, 即$h+\ell$ 离$t$ 最近, 离其他的尾实体$\ell^{\prime}$非常远. 用$d(h+\ell^{\prime}, t)$ 代表二者间不相似的程度, 其中使用的不相似度度量$d$ 可以是L1范数, 也可以是L2范数等等.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transe2.jpg" style="zoom: 33%;" /><h3 id="TransE-Algorithm"><a href="#TransE-Algorithm" class="headerlink" title="TransE Algorithm"></a>TransE Algorithm</h3><p>其实明白了TransE的思想, 算法的流程也就不难明白. 算法伪代码如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transe1.jpg" style="zoom:50%;" /><p>基本分为如下几步:</p><ol><li>用均匀分布初始化关系和实体矩阵.</li><li>进入循环, 每次循环开始都对实体$e$ <strong>归一化</strong>.</li><li>从三元组中采样出一个Minibatch来.</li><li>对每个Batch中的三元组进行替换, 主要是替换头实体或者尾实体生成负样本, 将打乱后的数据(负样本)和原来正确的三元组(正样本)和放到一个集合中.</li><li>对集合中的样本按照损失函数梯度对Embedding进行更新.</li></ol><p>与SVD一样, 采用<strong>最大化间隔</strong>作为损失函数:<br>$$<br>\mathcal{L}=\sum_{(h, \ell, t) \in S} \sum_{\left(h^{\prime}, \ell, t^{\prime}\right) \in S_{(h, \ell, t)}^{\prime}}\left[\gamma+d(\boldsymbol{h}+\boldsymbol{\ell}, \boldsymbol{t})-d\left(\boldsymbol{h}^{\prime}+\boldsymbol{\ell}, \boldsymbol{t}^{\prime}\right)\right]_{+}<br>$$<br>其中$\gamma$ 是超参数, 代表<strong>间隔</strong>. $[x]_+$ 代表取$x$ 的正的部分, 其实就是ReLU. </p><p>这个损失函数的意义是, 希望能够最小化正例三元组在空间中的距离, 最大化负例三元组在空间中的距离.</p><p>其中$S^{\prime}$ 来自正确三元组替换头实体或尾实体的集合:<br>$$<br>S_{(h, \ell, t)}^{\prime}=\left\{\left(h^{\prime}, \ell, t\right) \mid h^{\prime} \in E\right\} \cup\left\{\left(h, \ell, t^{\prime}\right) \mid t^{\prime} \in E\right\}<br>$$</p><h3 id="Dissimilarity-Function"><a href="#Dissimilarity-Function" class="headerlink" title="Dissimilarity Function"></a>Dissimilarity Function</h3><p>正如我们之前所说, 不相似度度量可以使用L1或L2范数, 这里作者直接使用L2范数:<br>$$<br>d(\boldsymbol{h}+\boldsymbol{\ell}, \boldsymbol{t})=    \lVert\boldsymbol{h}    \rVert_{2}^{2}+<br>\lVert\ell\rVert_{2}^{2}+<br>\lVert\boldsymbol{t}\rVert_{2}^{2}-2\left(\boldsymbol{h}^{T} \boldsymbol{t}+\boldsymbol{\ell}^{T}(\boldsymbol{t}-\boldsymbol{h})\right)<br>$$<br>那么$h+\ell$ 与$t$ 越近, 不相似度就越低(越相似), 反之不相似度就越高(越不相似). 因为每个Batch都对实体做归一化, 所以还有$\lVert\boldsymbol{h}\rVert_{2}^{2}=\lVert\boldsymbol{t}\rVert_{2}^{2}=1$.</p><h3 id="Some-Explain"><a href="#Some-Explain" class="headerlink" title="Some Explain"></a>Some Explain</h3><p>在优化时使用SGD, 并且在训练过程中, 作者提出的算法只对每个Batch更新时的实体进行了归一化, 关系没有做要求, 在此作者没有说明原因.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="DataSets"><a href="#DataSets" class="headerlink" title="DataSets"></a>DataSets</h3><p>因为是比较早的KGE方法, 作者只在WN, FB15k, FB1M上进行了对比.</p><h3 id="Evaluation-protocol"><a href="#Evaluation-protocol" class="headerlink" title="Evaluation protocol"></a>Evaluation protocol</h3><p>评估阶段的协议非常重要, 直到现在, 很多模型也都遵循着TransE的协议. </p><p>对每个测试三元组移除头实体, 并用其他实体轮流替换, 按照Loss升序排列, 最后再计算正确的三元组Loss. 然后对每个尾实体做相同的操作. </p><p>可能在替换后, 三元组是<strong>仍然正确</strong>的. 如果不做任何<strong>过滤</strong>的操作, 这个被替换后仍然正确的三元组将被记为错误, 这样就可能会<strong>低估</strong>模型能力. 所以应该将已经在训练集, 验证集, 测试集中出现的三元组从替换后的三元组中剔除, 从而保证在替换头实体或尾实体后三元组一定是错误的. 这种方法被称为<strong>Filtered setting</strong>, 没有经过过滤的称为raw, 该方法在其他很多论文中都出现过.</p><h3 id="Link-Prediction"><a href="#Link-Prediction" class="headerlink" title="Link Prediction"></a>Link Prediction</h3><p>在实体链接上的结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transe3.jpg" style="zoom:50%;" /><p>结果分为了两类, 有些指标在Raw和Filtered上表现差距非常大, 说明Filtered Setting还是非常有必要的.</p><p>在和Baseline模型相比较时都显示出TransE的强大.</p><p>之后作者又在FB15k上做了区分类别的实体链接实验:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transe4.jpg" style="zoom:50%;" /><p>能够很清楚的看出, TransE在一对一的建模能力上要好于其他Baseline模型, 并且在预测头实体时一对多, 预测尾实体时多对一时本身的效果比较好, 但不及于其他模型.</p><h3 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h3><p>作者将预测尾实体时的一些案例放了出来, 其中粗体表示测试集中的正确元组, 斜体表示训练集中预测正确的元组.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transe5.jpg" style="zoom:50%;" /><p>虽然在最高位的不是最佳答案, 但也说明了TransE能够表示一些<strong>常识</strong>.</p><h3 id="Generalization-Ability"><a href="#Generalization-Ability" class="headerlink" title="Generalization Ability"></a>Generalization Ability</h3><p>在FB15k上, 作者随机选择了40种关系作为FB15k - 40rel, 并将其于三元组全部作为FB15k - rest. 作者只让所有Embedding模型学习这四十个关系所对应的数据, 即只学习FB15k - 40rel, 并对FB15k - rest进行连接预测, 尝试观察TransE的<strong>泛化能力</strong>:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transe6.jpg" style="zoom:50%;" /><p>左图是模型在MR上的表现, 右图是Hit@10的表现.</p><p>结果表明, TransE是<strong>学习速度最快</strong>的方法, 即使只有少量三元组, TransE表现仍然不错. 随着知识的数量增大, TransE仍然有<strong>进步空间</strong>.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Trans它为KGE世界打开了新的大门, 之后衍生了例如TransR, TransH等不少基于平移的模型. 并且参数量和计算量都不算特别多. 但TransE也有非常多的<strong>缺点</strong>, 这些问题包括但不限于:</p><ul><li>一对多建模存在<strong>竞争</strong>, 即比如$(Writer, Compose, Novel)$这个三元组, 一名作者可以写多部小说, 作品之间是不同的实体. 在TransE中, $Writer + Compose$只能尽可能指向某一部作品, 如果有多部作品就会导致竞争. 多对多同理, 肯定更不擅长了.</li><li>不能完成<strong>对称关系</strong>的建模, 例如”朋友”这种关系. 在TransE中, 假设小明和小李是朋友, 用$(Ming, Friend, Li)$ 来表示, 小李被表示为$Ming + Friend$. 但是小明却不能被表示为$Li + Friend$.</li><li>不能完成<strong>自反关系</strong>的建模, 例如$(h, r, h)$ 的关系在TransE中完全没有办法表示.</li><li>没有考虑<strong>语义关系</strong>.</li></ul>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CoKE: Contextualized Knowledge Graph Embedding</title>
      <link href="/posts/42304.html"/>
      <url>/posts/42304.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>Self - Attention</li><li>BERT</li></ul><p><strong>2020.11.17</strong>: 解决了标签泄露的疑惑.</p><p><strong>2021.04.09</strong>: 修正可视化实验的描述.</p></blockquote><h1 id="CoKE-Contextualized-Knowledge-Graph-Embedding"><a href="#CoKE-Contextualized-Knowledge-Graph-Embedding" class="headerlink" title="CoKE: Contextualized Knowledge Graph Embedding"></a>CoKE: Contextualized Knowledge Graph Embedding</h1><p>本文是论文<a href="https://arxiv.org/abs/1911.02168" target="_blank" rel="noopener">CoKE: Contextualized Knowledge Graph Embedding</a> 的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者指出, 在先前的Embedding方法都是<strong>静态</strong>的, <strong>忽略了实体和关系在不同图之间的上下文所对应的真正含义</strong>. 在不同的上下文中, 实体和关系的含义经常是不同的, 例如下图:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/coke1.jpg" style="zoom: 33%;" /><p>假设有两个子图, 代表政治关系的子图(左侧蓝色)和代表家庭关系子图(右侧橘色), 它们都指向同一个实体<code>Barack Obama</code>, 在不同子图中它们所代表的表示应该是不同的, 因为在政治和家庭领域中, <code>Barack Obama</code>应该具有不同的含义, 因此Embedding需要在不同语境中被<strong>动态</strong>表示. 必须根据<strong>上下文语义信息</strong>来判断应该采取怎样的表示. 相比于静态表示法, 结合上下文语义信息的表示法有更丰富而灵活的Embedding.</p><p>除此外还有一个重要原因, 作者发现<strong>实体和关系很少孤立出现</strong>, 它们更多的出现伴随丰富的上下文, 甚至以边, 路径, 子图的方式出现, 这就为利用上下文提供了极大地便利.</p><p><strong>CoKE</strong>(<strong>Co</strong>ntextualized <strong>K</strong>nowledge Graph <strong>E</strong>mbedding)的设计初衷便是一种结合语境的动态知识表示法.</p><h2 id="CoKE"><a href="#CoKE" class="headerlink" title="CoKE"></a>CoKE</h2><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/coke2.jpg" style="zoom: 33%;" /><h3 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h3><p>对于Knowledge Graph中给定的三元组结构$(s, r, o)$, 两个实体之间可能在<strong>给定的上下文</strong>中对应着两种情况:</p><ul><li><strong>Edge</strong>:$s \rightarrow r \rightarrow o$, 即实体到实体只经过一跳, 用一种关系的<strong>边</strong>就能表示, 这是一种在知识图谱中最基本的方式. 例如$\text{BarackObama}\rightarrow\text{HasChild}\rightarrow\text{SashaObama}$.</li><li><strong>Path</strong>: $s \rightarrow r_{1} \rightarrow \cdots \rightarrow r_{k} \rightarrow o$, 即实体到实体需要用一系列关系构成的<strong>路径</strong>来表示, 由于包含了多跳信息, 这种表示往往伴随着更强的<strong>推理性</strong>. 例如$\text{BarackObama}\rightarrow\text{HasChild}\stackrel { (\text {Sasha}) }\rightarrow\text {LivesIn} \stackrel { \text {(US)} } \rightarrow\text{OffcialLanguage}\rightarrow\text{English}$.</li></ul><p>CoKE的目标就是根据丰富的图结构上下文学习到实体和关系的<strong>动态自适应性</strong>表示.</p><h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p>CoKE也采用了现在大家一致认为表现最好的<strong>Transformer Encoder(BERT)</strong> 架构完成Embedding. 其实模型的结构非常简单, 就是Transformer Encoder.</p><p>对于给定的输入序列$X=\left(x_{1}, x_{2}, \cdots, x_{n}\right)$,  在输入到Transformer Encoder前, 只需要对Token Emedding额外加上<strong>位置编码</strong>:<br>$$<br>\mathbf{h}_{i}^{0}=\mathbf{x}_{i}^{\mathrm{ele}}+\mathbf{x}_{i}^{\mathrm{pos}}<br>$$<br>需要注意的是, 这里没有对Entity和Relation的Embedding加以区分, 直接统一使用Word Embedding.</p><p>然后就对Transformer Encoder进行<strong>堆叠</strong>:<br>$$<br>\mathbf{h}_{i}^{\ell}=\text { Transformer }\left(\mathbf{h}_{i}^{\ell-1}\right), \quad \ell=1,2, \cdots ,L<br>$$<br>其中$\ell$ 代表Transformer的堆叠层数. 论文这里还吹了一波Transformer的双向捕捉上下文能力和其他优点等等. 通过多层堆叠处理后, 得到的表示$\left\{\mathbf{h}_{i}^{L}\right\}_{i=1}^{n}$ 是<strong>对输入自适应</strong>的.</p><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>CoKE也用给Token打<code>[Mask]</code>的方式训练模型, 作者设计了一种给出子图上下文情况下的实体预测任务.</p><p>但是与Masked Language Model的Mask方式不同, 并不是对Sequence 随机Mask, 而是只对边(路径)中的<strong>实体</strong>进行Mask, 这样每次预测的任务类似于<strong>问答</strong>的任务. 并且这样还有一个好处, 在执行很多下游任务(例如实体链接, 路径查询问答)时, 这种方式能够完全的避免BERT遇到的<strong>Train - Test discripency</strong>.</p><h4 id="Double-Entity-Mask"><a href="#Double-Entity-Mask" class="headerlink" title="Double Entity Mask"></a>Double Entity Mask</h4><p>Double Entity Mask这个名字是我自己起的, 因为它分别Mask了两个不同的实体并创建了两个不同的实例.</p><p>针对我们在Problem Formulation中提到的, 在KG中对于给定的两个实体可能有两种情况, 分别是边和路径. 边可以看做是一种特殊的路径, 所以作者采用<strong>相同策略</strong>对它们进行Mask, 虽然形式上没有区别, 但实际上对模型有着<strong>不同的影响</strong>.</p><ul><li>对于边$s \rightarrow r \rightarrow o$, 将产生两个实例, 分别是$? \rightarrow r \rightarrow o$ 和 $s \rightarrow r \rightarrow ?$. 要把实体预测出来, 这样的问题是在问答任务中的<strong>单跳</strong>问题. 例如$\text{BarackObama}\rightarrow\text{HasChild}\rightarrow?$, 就是在询问”<em>Who is the child of Barack Obama?</em>“.</li><li>对于路径$s \rightarrow r_{1} \rightarrow \cdots \rightarrow r_{k} \rightarrow o$, 也映射成两个实例, 分别预测头实体$s$ 和尾实体$o$. 它可以被看做是问答任务中的<strong>多跳</strong>问题. 例如$\text{BarackObama}\rightarrow\text{HasChild}\rightarrow\text {LivesIn} \rightarrow\text{OffcialLanguage}\rightarrow?$, 就是在询问”<em>What is the official language of the country where Barack Obama’s child lives in?</em>“.</li></ul><p>将上面二者统一, 对于给定的输入序列$X=\left(x_{1}, x_{2}, \cdots, x_{n}\right)$, 创建两个训练实例, 一个用<code>[MASK]</code> 替换掉$x_1$ 来让模型预测头实体$s$, 与之相对的另一个用<code>[MASK]</code> 替换$x_n$, 让模型预测尾实体$o$. 然后将替换后的序列加上位置编码, 一股脑放进Transformer Encoder, 最后得到最终隐态$\mathbf{h}_{1}^{L}$ 和$\mathbf{h}_{n}^{L}$, 然后用它来预测被Mask的实体.</p><blockquote><p>我之前以为这样会产生标签泄露, 实际上并<strong>不会产生标签泄露</strong>. 假设原三元组是$(h, r, t)$, 那么CoKE产生的实例是$(?, r, t)$ 和$(h, r, ?)$. 这时模型预测的分别是头实体和尾实体在相同关系下的概率分布. </p><p>与WN18和WN18RR的关系不一样, WN18RR是在WN18的基础上, 在训练集去除类似于$(t, r^{-1}, h)$ 这样的三元组. 也就是说在WN18中存在$(h, r, ?)$ 和$(t, r^{-1}, ?)$ 如果模型对<strong>顺逆关系</strong>做关联, 逆关系能直接泄露出另一端的实体嵌入.</p></blockquote><h4 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h4><p>和BERT一致, 在最后分类的时候用前馈神经网络和Softmax来做实体的预测:<br>$$<br>\begin{array}{l}<br>\mathbf{z}_{1}=\text { Feedforward }\left(\mathbf{h}_{1}^{L}\right), \mathbf{z}_{n}=\text { Feedforward }\left(\mathbf{h}_{n}^{L}\right) \\<br>\mathbf{p}_{1}=\operatorname{softmax}\left(\mathbf{E}^{\mathrm{ele}} \mathbf{z}_{1}\right), \mathbf{p}_{n}=\operatorname{softmax}\left(\mathbf{E}^{\mathrm{ele}} \mathbf{z}_{n}\right)<br>\end{array}<br>$$<br>其中$\mathbf{E}^{\mathrm{ele}}$ 是和实体Embedding<strong>共享权重</strong>的分类权重矩阵.</p><p>模型的概览图如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/coke2.jpg" style="zoom: 50%;" /><p>左图是对边的Mask, 右图是对关系的Mask. 它们都是通过Transformer Encoder最后时刻的隐态来确定被Mask掉的目标实体.</p><h4 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h4><p>因为以分类为目标, 所以用<strong>交叉熵</strong>为损失函数;<br>$$<br>\mathcal{L}(X)=-\sum_{t} y_{t} \log p_{t}<br>$$<br>训练时还使用了<strong>标签平滑</strong>, 设定实体标签所对应的$y_t=\epsilon$, 其他实体$y_{t}=\frac{1-\epsilon}{V-1}$.</p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Link-Prediction"><a href="#Link-Prediction" class="headerlink" title="Link Prediction"></a>Link Prediction</h3><p>链接预测上使用了FB15k, WN18和剔除包含反关系的FB15k-237, WN18RR.</p><p>CoKE只允许最大输入序列长度为3, 即<strong>只使用三元组</strong>, 不使用上下文. 结果直接使用了<strong>filtered setting</strong>, 对头实体和尾实体分别Mask再预测, 用MRR, H@n来测试性能. 详细的参数设置和训练Trick请见原论文.</p><blockquote><p>Filtered setting最早出现在TransE的论文中, 为了避免在替换实体后三元组仍然正确, 从而将正确的答案判为错误, 看低模型性能. 避免的方法是从替换后的实体中剔除在训练集, 验证集, 测试集中已经出现过的三元组.</p></blockquote><h4 id="Main-Result"><a href="#Main-Result" class="headerlink" title="Main Result"></a>Main Result</h4><p>FB15k, WN18:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/coke3.jpg" style="zoom: 33%;" /><p>上面一组是没有使用上下文的Embedding方法, 下面一组是结合上下文或路径的方法. CoKE在这里只使用了三元组, 应该从属于第一组. 在FB15k上表现良好, WN18上与最佳结果差距不大.</p><p>FB15k-237, WN18RR:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/coke4.jpg" style="zoom: 33%;" /><p>仍然表现不错, 除了WN18RR上与RotatE差距较大.</p><p>这组只使用三元组的CoKE证明它在<strong>单跳推理</strong>上的表现很不错.</p><h4 id="Parameter-Efficiency"><a href="#Parameter-Efficiency" class="headerlink" title="Parameter Efficiency"></a>Parameter Efficiency</h4><p>作者还在这比较了一下CoKE的参数效率, 当然这是<strong>建立在预测效果之上</strong>:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/coke5.jpg" style="zoom: 33%;" /><p>我觉得都已经上Transformer了, 就别再考虑啥参数效率了吧, 很大一个原因是它只使用了<strong>256</strong>维的Embedding, 并且每层只用了4个头.</p><h3 id="Path-Query-Answering"><a href="#Path-Query-Answering" class="headerlink" title="Path Query Answering"></a>Path Query Answering</h3><p>这个任务与两实体间需要多跳, 且有一个实体被Mask掉情况相同. 作者使用Random Walk从WordNet和FreeBase生成数据. 最多通过连续$k$ 跳从头实体$s$ 到达尾实体$o$. 在本任务中, 限制输入序列长为7, 即两实体间最多有5跳.</p><h4 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h4><p>作者将正确的答案$o$ 和不正确的答案$\mathcal{N}$ 的概率分布按降序排序, 并计算在$o$ 后不正确的答案占结果的总比例, 记为MQ, 范围是0到1, 1为最佳. 是按结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/coke6.jpg" style="zoom: 33%;" /><p>关于问答相关的内容还不是很了解. 但能看到随着Path长度的增加, 性能逐渐提升. 到$k\leq3$ 时, 性能就已经全面领先于作者列出的方法了. 在本任务中CoKE作者证明了具有多跳推理能力.</p><h4 id="Further-Analysis"><a href="#Further-Analysis" class="headerlink" title="Further Analysis"></a>Further Analysis</h4><p>作者想检测CoKE是否不但拥有多跳推理能力, 还同时经过多跳推理而强化了单跳推理能力. 作者设计了一个实验, 在训练CoKE时使用不同路径长的数据, 但在测试时仅让CoKE预测三元组(长度为1). 结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/coke7.jpg" style="zoom: 33%;" /><p>结果显示, CoKE随着训练时可使用的数据路径长的增加, 单跳推理能力也会增加. 证明了<strong>多跳推理的训练有利于单跳推理能力的提升</strong>.</p><h3 id="Visual-Illustration"><a href="#Visual-Illustration" class="headerlink" title="Visual Illustration"></a>Visual Illustration</h3><p>作者通过T - SNE探究了CoKE对上下文语境利用的情况.</p><p>从FB15k中以实体<code>TheKingsSpeech</code>为例, 将所有三元组收集起来. 将在不同语境下的<code>TheKingsSpeech</code> Mask掉(无论是头还是尾), 然后用已经训练好的CoKE获取它的Final Hidden State $\mathbf{h}_n^L$, 再用<strong>T - SNE</strong>降维可视化.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/coke8.jpg" style="zoom: 50%;" /><p>先看左图, <code>TheKingsSpeech</code>的表示形式在不同的关系之间表示是不同的, 大概有几种不同的关系. 作者不同关系使用不同颜色区分. 例如<code>award winning work/award winner</code> 和<code>award nominated work/award nominee</code>具有<strong>几乎相同的表示</strong>, 都聚在左上角. 这说明CoKE具有了结合上下文信息的能力.</p><p>另外, 对于顺关系$(s, r, o)$的逆关系$\left(o, r^{-1}, s\right)$, 例如<code>film/genre</code>和<code>film genre/films in this genre</code>, 因为<code>TheKingsSpeech</code>在<strong>顺逆关系</strong>中从属于<strong>不同的头尾位置</strong>, 但是它们居然获得了相同的表示, 说明CoKE对<strong>逆关系</strong>能够很好的识别.</p><p>右图也是CoKE对顺逆关系学习的探究, 这回作者直接将每组互逆关系对用相同的颜色和正三角倒三角表示, 结果显示几乎所有相同颜色的三角都聚到了一起. 但右侧有两堆不同颜色的三角聚到了一起, 分别是<code>JoelCoen</code>和<code>EthanCoen</code>, 这两人被称为科恩兄弟, 共同创作. 这表明了CoKE对<strong>关系的区分粒度</strong>.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>CoKE的思路其实很不错, 考虑到了已出现两个实体之间的单跳和多跳关系, 采用了目前效果最好的Transformer Encoder(BERT)作为特征抽取器.</p><p>同时该思考, 如果有某些暗含的背景因素没有出现在两实体中, 是否有可能对这两实体或关系的表示产生影响? 比如我们得到的路径是$A \rightarrow r_{1} \rightarrow \cdots \rightarrow r_{k} \rightarrow B$, 但是有某个隐含的背景条件或是实体有$C\rightarrow A$, 能对$A, B$的表示产生影响, 这种情况应该如何处理? 其实已经有方法给出了这个问题的解决方案.</p><p>值得一提的是, 作者在<strong>探究实验</strong>上的设计是很有意思的, 首先是想到了证明多跳能强化单跳的推理能力, 其次就是使用T - SNE来可视化探究CoKE对上下文的利用和识别能力.</p><p>最后, 如果对我提出的思考感兴趣, 或是对结合上下文Embedding的相关内容感兴趣, 欢迎阅读我写的<a href="https://adaning.github.io/posts/28100.html">CoLAKE: Contextualized Language and Knowledge Embedding</a>, 我认为CoLAKE比CoKE要进步了不少.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ConvKB: A Novel Embedding Model for KB Completion Based on CNN</title>
      <link href="/posts/52280.html"/>
      <url>/posts/52280.html</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>2021.3.15</strong>: 指出权重共享并没有出现在源码中.</p><p>本文前置知识:</p><ul><li>ConvE</li><li>Conv1d</li></ul></blockquote><h1 id="A-Novel-Embedding-Model-for-Knowledge-Base-Completion-Based-on-Convolutional-Neural-Network"><a href="#A-Novel-Embedding-Model-for-Knowledge-Base-Completion-Based-on-Convolutional-Neural-Network" class="headerlink" title="A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network"></a>A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network</h1><p>本文是论文<a href="https://www.aclweb.org/anthology/N18-2053/" target="_blank" rel="noopener">A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network</a>的阅读笔记和个人理解. 本文篇幅比较短, 模型比较简单.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者认为ConvE只考虑了局部不同维度的关系, 而没有考虑全局相同维度的关系, 这就需要一种方式在实体和关系之间捕获全局关系和过渡特性.</p><p>我个人的理解就是, 一般KGE都是用Link Prediction去训练模型, 只使用了头实体和关系的信息, 去预测尾实体. 但ConvKB能够同时使用头实体, 尾实体, 关系的信息.</p><h2 id="ConvKB"><a href="#ConvKB" class="headerlink" title="ConvKB"></a>ConvKB</h2><p>ConvKB的方法是非常简单的.</p><p>假设头实体$h$, 关系$r$, 尾实体$t$, 先将$(h, r, t)$ 通过$k$ 维的Embedding, 得到$\left(\boldsymbol{v}_{h}, \boldsymbol{v}_{r}, \boldsymbol{v}_{t}\right)$, 并将其转为矩阵$\boldsymbol{A}=\left[\boldsymbol{v}_{h}, \boldsymbol{v}_{r}, \boldsymbol{v}_{t}\right] \in \mathbb{R}^{k\times 3}$. 作者使用多个$1\times 3$ 的Conv1d来捕捉三者<strong>同个维度上的全局信息</strong>, 其卷积核为$\omega$, 通过卷积运算能产生特征图$\boldsymbol{v}=\left[v_{1}, v_{2}, \ldots, v_{k}\right] \in \mathbb{R}^{k}$:<br>$$<br>v_{i}=g\left(\boldsymbol{\omega} \cdot \boldsymbol{A}_{i,:}+b\right)<br>$$<br>其中$b$ 为偏置项, $g$ 为激活函数, 这里用的是ReLU.</p><p>设$\Omega$ 为所有卷积核的集合, $\tau$ 为卷积核的数量, 则有$\tau =|\Omega|$. 那么将所有的卷积核扫描完后, 能产生$\tau$ 个大小为$k \times 1$ 的向量, 将他们Concat起来, 大小为$\mathbb{R}^{\tau k \times 1}$. 然后用一个权重向量$\mathbf{w}$ 来和Concat后的向量做点积, 得到分数.</p><p>看图总结一下, 图中取的$\tau=3$, $k=4$:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/convkb1.jpg" style="zoom: 50%;" /><h3 id="Score-Function"><a href="#Score-Function" class="headerlink" title="Score Function"></a>Score Function</h3><p>打分函数为:<br>$$<br>f(h, r, t)=\operatorname{concat}\left(g\left(\left[\boldsymbol{v}_{h}, \boldsymbol{v}_{r}, \boldsymbol{v}_{t}\right] \ast \boldsymbol{\Omega}\right)\right) \cdot \mathbf{w}<br>$$<br>其中$\ast$ 代表卷积操作, $\Omega$ 和$\mathbf{w}$ 是<strong>参数共享</strong>, $\mathbf{w} \in \mathbb{R}^{\tau k \times 1}$. <strong>打分函数的输出越小证明三元组越可靠</strong>.</p><blockquote><p>虽然作者说可以”权重共享”, 但思考一下维度变换其实根本没法这么做(除非$k=3$).</p><p>并且作者<a href="https://github.com/daiquocnguyen/ConvKB" target="_blank" rel="noopener">开源代码</a>也没有关于权重共享的任何内容.</p></blockquote><p>作者还提到, 如果只使用一个卷积核$\omega=[1, 1, -1]$, 固定$b=0$, 令激活函数$g(x)=|x|$ 或 $g(x)=x^2$, 并使$\mathbf{w} = 1$, <strong>ConvKB将退化为TransE</strong>, 即$|h+r-t|$.</p><h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><p>损失函数为:<br>$$<br>\begin{array}{c}<br>\mathcal{L}=\sum_{(h, r, t) \in\left\{\mathcal{G} \cup \mathcal{G}^{\prime}\right\}} \log \left(1+\exp \left(l_{(h, r, t)} \cdot f(h, r, t)\right)\right)<br>+\frac{\lambda}{2}||\mathbf{w}||_{2}^{2}<br>\end{array}<br>$$<br>$l_{(h,r,t)}$ 是一个符号:<br>$$<br>\ l_{(h, r, t)}=\left\{\begin{array}{l}<br>1 \text { for }(h, r, t) \in \mathcal{G} \\<br>-1 \text { for }(h, r, t) \in \mathcal{G}^{\prime}<br>\end{array}\right.<br>$$<br>$\mathcal{G}$ 代表知识图谱, $\mathcal{G’}$ 代表从知识图谱中生成的<strong>无效</strong>三元组集合.</p><p>损失函数大致的意思是, 模型能否根据ConvKB来区分<strong>三元组是否正确</strong>. 所以其实它的本质是一个<strong>二分类交叉熵</strong>.</p><p>后面加了正则项防止过拟合.</p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>作者把ConvKB在WN18RR和FB15k-237上做了<strong>链接预测</strong>的实验, 结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/convkb2.jpg" style="zoom: 50%;" /><p>能看到, 相比于ConvE, ConvKB有相当大的提升, 作者还吐槽了ConvE在某些指标上甚至还没有TransE好.</p><p>一些超参数的设置细节不再列举了, 原文说的比较详细.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>ConvKB没有使用Reshape, 不像ConvE那样提取Embedding间的局部信息, 而是通过Conv1d来提取包含尾实体的全局信息. 但实验列出的指标没有Hit@1和Hit@3, 不知道ConvKB在这两个指标上是否表现得也像Hit@10一样好. </p><blockquote><p>另外, 我认为在$(h, r, t)$ 的每个相同Dimension上不一定有实质的联系, 可能是交错的, 所以可以考虑用Permutation之类的方式来尽可能消除这种可能性. 或者通过某些方式让CNN提取交错Dimension位置上的特征.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>InteractE: Improving Convolution-based KGE by Increasing Feature Interactions</title>
      <link href="/posts/30287.html"/>
      <url>/posts/30287.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>ConvE</li><li>Depth - wise Convolution</li></ul><p><strong>2020.11.14</strong>: 对实验进行部分补充.</p></blockquote><h1 id="InteractE-Improving-Convolution-based-Knowledge-Graph-Embeddings-by-Increasing-Feature-Interactions"><a href="#InteractE-Improving-Convolution-based-Knowledge-Graph-Embeddings-by-Increasing-Feature-Interactions" class="headerlink" title="InteractE: Improving Convolution-based Knowledge Graph Embeddings by Increasing Feature Interactions"></a>InteractE: Improving Convolution-based Knowledge Graph Embeddings by Increasing Feature Interactions</h1><p>本文章是论文<a href="http://arxiv.org/abs/1911.00219" target="_blank" rel="noopener">InteractE: Improving Convolution-based Knowledge Graph Embeddings by Increasing Feature Interactions</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>Link Prediction能够根据已有的事实对缺失的链接进行推断, 依此缓解<strong>KG不完整</strong>的问题.</p><p>虽然已经有基于卷积的Embedding方法, 例如ConvE, ConvKB等, 但作者认为虽然卷积增加了头实体和关系之间的Embedding交互, 但<strong>交互次数</strong>仍然不够, 因此抽取特征的能力被<strong>限制</strong>了.</p><p>从名字上就能看出来, InteractE的主要思路就是通过一切手段让<strong>交互最大化</strong>, 从而实现更好的表达.</p><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><h3 id="Reshape-Function"><a href="#Reshape-Function" class="headerlink" title="Reshape Function"></a>Reshape Function</h3><p>下面定义头实体和关系的$d$ 维Embedding分别为$\boldsymbol{e}_{s}=\left(a_{1}, \ldots, a_{d}\right), \boldsymbol{e}_{r}=\left(b_{1}, \ldots, b_{d}\right)$, 参数矩阵为$w$ 的卷积核大小为$k$, 并假设输入的矩阵$N$ 大小为$m \times n=2d$, 那么当卷积核对矩阵进行扫描时覆盖的区域$M_k$ 就应该有$M_k \subseteq N$, 且$M_k = N_{i:i+k, j:j+k}$. 我们将$\phi$ 记为Reshape Function, $\phi\left(\boldsymbol{e}_{s}, \boldsymbol{e}_{r}\right)$ 就代表了头实体和关系的Embedding后Reshape.</p><p>作者汇总了三种Reshape方法, 都非常好理解:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/interacte2.jpg" style="zoom: 50%;" /><ul><li>Stack: 就是简单的<strong>堆叠</strong>, 这种方法被ConvE中使用, 记为$\phi_{s t k}$.</li><li>Alternate: 将头实体和关系的Embedding<strong>按行交错</strong>排列, 记为$\phi_{a l t}^{\tau}$.</li><li><strong>Chequer(本文使用)</strong>: 像棋盘一样的对二者的Embedding按元素交错排列, 即相邻的元素一定与自己的类型不同, 记为$\phi_{c h k}$.</li></ul><h3 id="Interaction"><a href="#Interaction" class="headerlink" title="Interaction"></a>Interaction</h3><p>作者给出了交互精确的定义. 假设$M_{k} \subseteq \phi\left(\boldsymbol{e}_{s}, \boldsymbol{e}_{r}\right)$, $x,y$ 是$M_k$ 中的两个元素, 那么就能根据三元组$\left(x, y, M_{k}\right)$ 给出交互的定义. </p><p>如果$x,y$ 分别来自于$\boldsymbol{e}_{s},\boldsymbol{e}_{r}$, 则称该交互是<strong>异质</strong>(heterogeneous)的, 记为$\mathcal{N}_{h e t}(\phi, k)$. 反之, 如果来自于相同的Embedding, 则称该交互是<strong>同质</strong>(homogeneous)的, 记为$\mathcal{N}_{\text {homo}}(\phi, k)$.</p><p>对于Reshape Function $\phi$, 卷积核大小为$k$, 其中的元素交互次数定义为$\mathcal{N}(\phi, k)$, 如果有Padding, 可以将$\phi$ 替换为加上Padding的Reshape Function $\Omega(\phi)$.</p><p>下面用一个例子来说明交互的定义. 在一次$3\times 3$ 的卷积中, 子矩阵为$M_3$, 有5个元素来自$\boldsymbol{e}_{s}$, 4个元素来自$\boldsymbol{e}_{r}$. 那么异质交互的次数为:<br>$$<br>\mathcal{N}_{h e t}=2(5 \times 4)=40<br>$$<br>其中, $5\times 4$是来自$\boldsymbol{e}_{s}$ 和来自$\boldsymbol{e}_{r}$ 元素的组合次数, 前面的2代表每次组合相对两个元素分别有一次交互.</p><p>同理, 同质交互次数为:<br>$$<br>\mathcal{N}_{\text {homo}}=2\left[\left(\begin{array}{l}<br>5 \\<br>2<br>\end{array}\right)+\left(\begin{array}{l}<br>4 \\<br>2<br>\end{array}\right)\right]=32<br>$$<br>同质交互和异质交互的和是一个常数:<br>$$<br>\mathcal{N}_{h e t}(\phi, k)+\mathcal{N}_{h o m o}(\phi, k)=2\left(\begin{array}{c}<br>k^{2} \\<br>2<br>\end{array}\right)<br>$$</p><h2 id="InteractE"><a href="#InteractE" class="headerlink" title="InteractE"></a>InteractE</h2><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/interacte1.jpg" style="zoom: 50%;" /><p>在ConvE的基础上, InteractE基于三种方式来<strong>最大化</strong>头实体与关系Embedding间的交互次数:</p><ul><li>Feature Permutation</li><li>Checked Reshaping</li><li>Circular Convolution</li></ul><h3 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h3><h4 id="Feature-Permutation"><a href="#Feature-Permutation" class="headerlink" title="Feature Permutation"></a>Feature Permutation</h4><p>InteractE使用<strong>随机排列</strong>的方式对头实体Embedding $e_s$ 和关系Embedding $e_r$ 打乱. 如果随机打乱$t$ 次, 那么结果集为$\mathcal{P}_{t}=\left[\left(e_{s}^{1}, e_{r}^{1}\right) ; \ldots ;\left(e_{s}^{t}, e_{r}^{t}\right)\right]$, 在Reshape后只是因为随机排列导致不同排列中对应位置的元素不同, 但仍然保证相邻两元素的<strong>类型不同</strong>, 记为$\phi\left(e_{s}^{i}, e_{r}^{i}\right)$.</p><p>通过多次随机排列, 能保证对Embedding上的每个Dim都是公平的.</p><h4 id="Checkered-Reshaping"><a href="#Checkered-Reshaping" class="headerlink" title="Checkered Reshaping"></a>Checkered Reshaping</h4><p>$\phi_{chk}(\cdot)$ 能<strong>最大化异质交互</strong>次数, 我们对所有随机排列后的Embedding进行Reshape, 即:<br>$$<br>\phi\left(\mathcal{P}_{t}\right)=\left[\phi\left(e_{s}^{1}, e_{r}^{1}\right) ; \ldots ; \phi\left(e_{s}^{t}, e_{r}^{t}\right)\right]<br>$$</p><h4 id="Circular-Convolution"><a href="#Circular-Convolution" class="headerlink" title="Circular Convolution"></a>Circular Convolution</h4><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/interacte3.jpg" style="zoom: 50%;" /><p>其实从图上来看, 循环卷积不难理解, 只是当充分的利用了Padding的区域, 不再将其填充为0, 而是按照循环卷积(右图)的方式对其进行循环卷积的Padding, 利用其他的位置对原来填充0的位置进行有效的填补. 为了更好地描述该过程, 写出两种卷积Padding的矩阵, 标准卷积Padding(左), 循环卷积Padding(右):<br>$$<br>\begin{array}{cc}<br>\begin{bmatrix}<br>0 &amp; 0 &amp; 0 \\<br>0 &amp; x_{11} &amp; x_{12} \\<br>0 &amp; x_{21} &amp; x_{22}<br>\end{bmatrix} &amp;<br>\begin{bmatrix}<br>x_{44} &amp; x_{41} &amp; x_{42} \\<br>x_{14} &amp; x_{11} &amp; x_{12} \\<br>x_{24} &amp; x_{21} &amp; x_{22}<br>\end{bmatrix}<br>\\<br>\text{Standard Conv} &amp; \text{Circular Conv}<br>\end{array}<br>$$<br>若写出循环卷积其数学形式, 如下:<br>$$<br>[\boldsymbol{I} \star \boldsymbol{w}]_{p, q}=\sum_{i=-\lfloor k / 2\rfloor}^{\lfloor k / 2\rfloor} \sum_{j=-\lfloor k / 2\rfloor}^{\lfloor k / 2\rfloor} \boldsymbol{I}_{[p-i]_{m},[q-j]_{n}} \boldsymbol{w}_{i, j}<br>$$</p><p>在做循环卷积的时候, 使用的是Depth - wise Convolution. 也就是将随机排列Reshape后的Tensor集合$\phi\left(\mathcal{P}_{t}\right)$ 在Channel维度上Stack起来, 然后对每个Channel<strong>单独</strong>进行卷积.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/depthwiseconv.jpg" style="zoom:50%;" /><p>作者在这里还强调, 直接引入Depth维度上的卷积核<strong>参数共享</strong>能取得更好的效果.</p><p>引入循环卷积, 进一步的增加了二者的交互性.</p><h4 id="Score-Function"><a href="#Score-Function" class="headerlink" title="Score Function"></a>Score Function</h4><p>打分函数如下:</p><p>$$<br>\psi(s, r, o)=g\left(\operatorname{vec}\left(f\left(\phi\left(\mathcal{P}_{k}\right) \circledast \boldsymbol{w}\right)\right) \boldsymbol{W}\right) \boldsymbol{e}_{o}<br>$$</p><p>其中$\circledast$ 代表循环卷积, $\operatorname{vec}(\cdot)$ 代表Concat(其实就是Flatten), $e_{o}$ 代表尾实体的Embedding, $\boldsymbol{W}$ 代表变换矩阵. $f$ 和$g$ 分别代表$\text{ReLU}$ 和$\sigma$ 函数. 在训练时使用标准的<strong>二分类交叉熵</strong>作为损失函数, 并使用<strong>标签平滑</strong>. </p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Performance-Comparison"><a href="#Performance-Comparison" class="headerlink" title="Performance Comparison"></a>Performance Comparison</h3><p>这部分的实验意在比较InteractE与现有方法的性能比较, 下面是InteractE在三个数据集上的Link Prediction效果:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/interacte4.jpg" style="zoom: 67%;" /><blockquote><p>有一件非常有意思的事情, InteractE与AcrE(Serial)表现出几乎相同的性能, 而且在WN18RR上的表现也不好. 但AcrE的参数量却远少于InteractE.</p><p>我个人猜测是AcrE中使用的空洞卷积在一定程度上能与InteractE中的棋盘Reshape和循环卷积等效.</p></blockquote><h3 id="Effect-of-Feature-Reshaping-and-Circular-Convolution"><a href="#Effect-of-Feature-Reshaping-and-Circular-Convolution" class="headerlink" title="Effect of Feature Reshaping and Circular Convolution"></a>Effect of Feature Reshaping and Circular Convolution</h3><p>作者列举了三种Reshape的方式, 并做了它们分别与标准卷积核循环卷积的性能对比实验:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/interacte5.jpg" style="zoom: 67%;" /><p>从实验结果能看出, 除了使用$\tau=1$ 的交替排列方式, 在其他所有对照组种, 循环卷积的性能均高于标准卷积. 并且, $\tau$ 被定义为是$e_s, e_r$ 间重复的交替次数, $\tau$ 越小, 异质交互次数就越多, 随着其减小, 效果明显提升. </p><h3 id="Effect-of-Feature-Permutations"><a href="#Effect-of-Feature-Permutations" class="headerlink" title="Effect of Feature Permutations"></a>Effect of Feature Permutations</h3><p>作者选用了多种Feature Permutation, 将它们在三种不同的数据集上对比:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/interacte6.jpg" style="zoom: 50%;" /><p>YAGO3-10的实体量要远大于FB15k-237和WN18RR. 当超过某个<strong>阈值</strong>时, 排列数量将不再影响性能, 甚至变得多余.</p><h3 id="Evaluation-on-different-Relation-Types"><a href="#Evaluation-on-different-Relation-Types" class="headerlink" title="Evaluation on different Relation Types"></a>Evaluation on different Relation Types</h3><p>作者对比了RotatE, ConvE, InteractE在不同类别上的头尾预测效果:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/interacte7.jpg" style="zoom: 50%;" /><p>RotatE更善于处理1-1的简单一些的关系, InteractE更善于捕捉1-N, N-N的复杂关系, 佐证了交互带来的作用.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>InteractE通过尽可能<strong>最大化</strong>头实体和关系Embedding的<strong>交互</strong>来获得更好的嵌入表示. 其他Embedding方法不同的是, InteractE从<strong>Permutation</strong>和<strong>Reshape</strong>两个独特的角度处理了Embedding的问题. </p><p>但不知道作者有没有考虑过其他的Encoding Model能进一步增强Embedding之间的交互, 或者是尝试使用更复杂的Reshape方式, 在卷积时扩大交互范围(相应的开销也会提高).</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> 循环卷积 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CoLAKE: Contextualized Language and Knowledge Embedding</title>
      <link href="/posts/28100.html"/>
      <url>/posts/28100.html</url>
      
        <content type="html"><![CDATA[<h1 id="CoLAKE-Contextualized-Language-and-Knowledge-Embedding"><a href="#CoLAKE-Contextualized-Language-and-Knowledge-Embedding" class="headerlink" title="CoLAKE: Contextualized Language and Knowledge Embedding"></a>CoLAKE: Contextualized Language and Knowledge Embedding</h1><blockquote><p>本文前置知识:</p><ul><li>BERT</li><li>Self - Attention</li></ul><p><strong>2020.11.11</strong>: 想通了CoLAKE在训练时最关键的部分.</p><p><strong>2020.11.22</strong>: 在读完KEPLER后, 重温一遍CoLAKE, 更新实验部分.</p></blockquote><p>本文是论文<a href="http://arxiv.org/abs/2010.00309" target="_blank" rel="noopener">CoLAKE: Contextualized Language and Knowledge Embedding</a>的阅读笔记和个人理解. 这篇论文的很多工作与<strong>KEPLER</strong>相似, 建议先阅读<a href="https://adaning.github.io/posts/52897.html">我对KEPLER的讲解</a>, 再来看CoLAKE.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>这两年关于KGE的PTM很火爆, 论文开头便指出了现在将知识注入的PTM的劣势, 这些现存的嵌入一般都是空洞的, 静态的, 不灵活的. 这些PTM都有一些共同的<strong>短板</strong>:</p><ul><li>实体嵌入是<strong>单独</strong>训练的, 然后再使用到PTM中, 导致<strong>知识嵌入</strong>和<strong>语言嵌入</strong>不是同时嵌入的, 即没有真正做到<strong>联合嵌入</strong>.</li><li>在做实体嵌入时, 很少能全部的捕捉到丰富的<strong>上下文</strong>信息, 导致模型的性能被预训练的实体嵌入所<strong>限制</strong>.</li><li>预训练的实体嵌入是<strong>静态</strong>的, 当知识图谱发生轻微变化时(例如添加一个新的实体), 需要<strong>重新训练</strong>.</li></ul><p>基于上述缺点, 作者提出了CoLAKE, 这是一种能够根据<strong>上下文</strong>, 实现<strong>语言</strong>和<strong>知识</strong>的<strong>联合嵌入</strong>的<strong>Masked Language Model</strong>.</p><h2 id="CoLAKE"><a href="#CoLAKE" class="headerlink" title="CoLAKE"></a>CoLAKE</h2><p>CoLAKE(<strong>Co</strong>ntextualized <strong>L</strong>anguage <strong>a</strong>nd <strong>K</strong>nowledge <strong>E</strong>mbedding), CoLAKE能根据知识上下文和语言上下文来<strong>动态</strong>的表示实体. 对于每个实体, 将该实体与知识相<strong>关联</strong>的部分作为子图, 视为该实体的上下文. CoLAKE能动态访问不同的常识, 根据<strong>背景知识</strong>来更好的帮助模型理解实体在上下文中的含义, 而并非只关注实体本身.</p><blockquote><p>题外话:</p><p>其实我第一次听CoLAKE这个名字是在前两天举办的YSSNLP上, 邱锡鹏老师在预训练模型的演讲里提的. 我当时感觉哈利波特那个图(下面第一张图就是)好像在哪见过, 后来确实在邱老师9月份演讲的PDF里找到了, 只不过没标模型的名字而已. </p><p>当时还有听众提出了一个问题: 外部知识应该如何引入影响来语义的呢?</p><p>邱老师虽然没给出具体的方案, 但指出了大致的一条思路: Token的表示可能会根据外部知识的<strong>影响</strong>或<strong>变化</strong>, 从而改变它的表示, 这样能获得更<strong>精确</strong>的语义表示.</p><p>读完这篇10月份发的论文才明白, 其实邱老师说的是CoLAKE.</p></blockquote><h3 id="Word-Knowledge-Graph"><a href="#Word-Knowledge-Graph" class="headerlink" title="Word - Knowledge Graph"></a>Word - Knowledge Graph</h3><p>WK Graph(Word - Knowledge Graph)是为了处理<strong>异构</strong>的<strong>语言</strong>和<strong>知识图谱</strong>而引入的, 这种结构能将它们<strong>统一</strong>在同一个数据结构下. 知识在知识图谱中的存储方式是<strong>三元组</strong>, 而语言的存储方式一般是<strong>无结构化的文本数据</strong>. Transformer的<strong>自注意力</strong>机制可以看做是基于单词的<strong>全连接图</strong>, 所以图是表示知识和语言更<strong>通用</strong>的结构.</p><h4 id="WK-Graph-Example"><a href="#WK-Graph-Example" class="headerlink" title="WK Graph Example"></a>WK Graph Example</h4><p>下面来举一个例子体会WK Graph的作用, 在本节先关注这张图片的左侧:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/colake1.jpg" style="zoom: 67%;" /><p>像ERNIE, KnowBERT之类的模型的实体嵌入和语言嵌入是<strong>半上下文联合</strong>的, 而CoLAKE是<strong>全上下文联合</strong>嵌入. 那么模型在CoLAKE中如何理解下面这两个句子呢?</p><blockquote><ol><li>Harry Potter points his wnad at Lord Voldemort.</li><li>“You have Lily’s hazel eyes”, he told Harry Potter.</li></ol></blockquote><p>CoLAKE能在KG中搜索<code>Harry Potter</code>相关的知识, 得到<code>(Harry Potter, enemy of, Lord Voldemort)</code>和<code>(Harry Potter, mother, Lily Poter)</code>这两个三元组在图中的表示. 然后用前者来帮助理解句子1, 后者帮助理解句子2. 这样就使得知识能够在不同的上下文中更灵活的运用.</p><p>右侧说明了Word - Knowledge Graph的结构. <strong>内圈</strong>是原文中多个单词<strong>全连接</strong>形成的<strong>Word Graph</strong>, 而<strong>外圈</strong>是经过知识扩展过的<strong>Knowledge Subgraph</strong>. 二者以相同的实体为桥接处, 将Word Graph和Knowledge Subraph<strong>拼接</strong>就形成了Word - Knowledge Graph.</p><h4 id="Graph-Construction"><a href="#Graph-Construction" class="headerlink" title="Graph Construction"></a>Graph Construction</h4><p>本节主要说明WK Graph是如何生成的.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/colake2.jpg" style="zoom:67%;" /><p>生成图的方式其实没有想象中的复杂. 我们先通过实体连接器将KG中能被找到的实体作为<strong>锚点</strong>(Anchor Nodes), 然后将基于KG延伸出的Subgraph在这个锚点的基础上进行<strong>扩展</strong>. 锚点在图中用红色的虚线外轮廓标识. 延伸的过程有利于将Knowledge Subgraph和Word Graph的实体嵌入进同一个空间, 这也就是一直在强调的<strong>联合嵌入</strong>.</p><p>在Word Graph中有三种类型的节点, 分别是Word Nodes, Entity Nodes, Relation Nodes, 也就分别对应着图中黄色, 蓝色, 绿色的节点.</p><p>注意观察图中的节点编号, 在锚点的Knowledge Subgraph中, 锚点连接的实体和关系节点的编号是<strong>依赖于锚点</strong>的, 这种方式也称为<strong>Soft - Position Index</strong>, 在之后还会提到.</p><p>CoLAKE只使用了与锚点相邻的<strong>15</strong>个随机关系和实体作为Subgraph, 然后并入WK Graph中.</p><h3 id="CoLAKE-Architecture"><a href="#CoLAKE-Architecture" class="headerlink" title="CoLAKE Architecture"></a>CoLAKE Architecture</h3><p>CoLAKE的整个结构基于BERT. 所以使用的Basic Blcok是Transformer Encoder. 如果对BERT不了解的建议参考我以前写的<code>&lt;ELMo, GPT, BERT&gt;</code>, 对Transformer不理解的请参照<code>&lt;Transformer精讲&gt;</code>, 看完后理解起来会好一些.</p><p>话不多说, 直接看结构图:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/colake3.jpg" style="zoom:67%;" /><p>请观察CoLAKE与BERT的<strong>不同点</strong>. 并且这幅图画的非常严谨, 在右侧的Knowledge Graph并不是全连接的, 左侧的Word Graph是全连接的, Token Embedding的颜色与之前Word Graph颜色也是相对应的.</p><h4 id="Embedding-Layer"><a href="#Embedding-Layer" class="headerlink" title="Embedding Layer"></a>Embedding Layer</h4><p>CoLAKE对BERT的Embedding做了改动. 在BERT中采用的Encoding是Token Encoding, <strong>Segment Encoding</strong>, Position Encoding. 而CoLAKE中使用的是Token Encoding, <strong>Type Encoding</strong>, Position Encoding. </p><h5 id="Token-Embedding"><a href="#Token-Embedding" class="headerlink" title="Token Embedding"></a>Token Embedding</h5><p>对于Token的Embedding不必多说, CoLAKE与BERT都还是使用<strong>查表</strong>的方式, 但CoLAKE是单词, 实体, 关系<strong>分别查找</strong>:</p><ul><li>对于单词的Embedding, CoLAKE使用了<strong>BPE</strong>, 能减小词表, 这已经是一种常见的Subword技巧. </li><li>对于实体和关系, 我们分别建两张表, 直接学习实体和关系的<strong>独立</strong>表示.</li></ul><h5 id="Type-Encoding"><a href="#Type-Encoding" class="headerlink" title="Type Encoding"></a>Type Encoding</h5><p>CoLAKE不涉及到Segment的问题, 所以将Segment Encoding替换成了Type Encoding. 在Word Graph中有三种类型的节点, 分别是Word Nodes, Entity Nodes, Relation Nodes. 所以要对这三种类型<strong>分别</strong>加以编码.</p><h5 id="Position-Encoding"><a href="#Position-Encoding" class="headerlink" title="Position Encoding"></a>Position Encoding</h5><p>与BERT相比, CoLAKE多了Knowledge Subgraph, 所以使用了<strong>Soft - Position Index</strong>的方式对Knowledge Subgraph中的节点进行位置编码.</p><blockquote><p>Soft - Position Index: 允许<strong>重复</strong>的Position出现, 将以锚点作为头实体的三元组记为$&lt;h, r, t&gt;$, 若锚点头实体在句子中的位置为$x$, 则关系$r$ 位置记为$x+1$, 尾实体位置记为$x+2$. 这样能够保证三元组的<strong>位置连续</strong>.</p></blockquote><h4 id="Masked-Transformer-Encoder"><a href="#Masked-Transformer-Encoder" class="headerlink" title="Masked Transformer Encoder"></a>Masked Transformer Encoder</h4><p>对于图中的节点矩阵$\mathbf{X}$ , 附加<strong>Mask</strong>的注意力机制如下:<br>$$<br>\begin{aligned}<br>\mathbf{Q}, \mathbf{K}, \mathbf{V} &amp;=\mathbf{X} \mathbf{W}^{Q}, \mathbf{X} \mathbf{W}^{K}, \mathbf{X} \mathbf{W}^{V} \\<br>\mathbf{A} &amp;=\frac{\mathbf{Q} \mathbf{K}^{\top}}{\sqrt{d_{k}}} \\<br>\operatorname{Attn}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) &amp;=\operatorname{Softmax}(\mathbf{A}+\mathbf{M}) \mathbf{V}<br>\end{aligned}<br>$$<br>Mask矩阵$\mathbf{M}$ 可以通过以下方式求得:<br>$$<br>\mathbf{M}_{i j}=\left\{\begin{array}{ll}<br>\quad 0 &amp; \text { if } x_{i} \text { and } x_{j} \text { are connected } \\<br>-\inf &amp; \text { if } x_{i} \text { and } x_{j} \text { are disconnected}<br>\end{array}\right.<br>$$</p><blockquote><p>认真看一下, 这个Mask的作用可不是Transformer Decoder中的Mask, 它是取决于<strong>节点矩阵</strong>的, 只是为了能让节点只能看到<strong>邻近一跳</strong>的信息.</p></blockquote><h3 id="Pre-Training-Objective"><a href="#Pre-Training-Objective" class="headerlink" title="Pre - Training Objective"></a>Pre - Training Objective</h3><p>CoLAKE也是一个Masked Language Model, 所以它也是通过对句子中的Token随机Mask, 然后从词表中基于上下文将Mask的Token预测出来. 将BERT的Mask模式迁移到图中, 只是将Mask Token变为Mask Node, 我们仍然是对15%的Node进行随机选中, 但选中后的操作<strong>略有不同</strong>, 下面将BERT和CoLAKE做个对比:</p><table><thead><tr><th>概率</th><th>BERT</th><th>CoLAKE</th></tr></thead><tbody><tr><td>80%</td><td>将被选中的Token替换为<code>[Mask]</code></td><td>将被选中的Node替换为<code>[Mask]</code></td></tr><tr><td>10%</td><td>将被选中的Token替换为<strong>任意词</strong></td><td>将被选中的Node替换为<strong>与原节点同类的节点</strong></td></tr><tr><td>10%</td><td>不做任何替换</td><td>不做任何替换</td></tr></tbody></table><p>在WK Graph中有三大类节点, 被Mask后所对应的意义是不同的:</p><ul><li><p>Masking Word Nodes: 与BERT的情况是一致的, 但CoLAKE在预测Word时除了依赖上下文还能依赖<strong>知识</strong>做出预测, 因为锚点与Word是全连接的关系, 而锚点受到知识的影响.</p></li><li><p>Masking Entity Nodes: 如果被Mask的实体是<strong>锚点</strong>, 那么则依靠它的上下文进行预测. 如果不是锚点, 那么CoLAKE的目标就是KGE.</p></li><li><p>Masking Relation Nodes: 如果Mask的是两锚点之间的关系,  那么目标就与<strong>关系抽取</strong>一致. 否则, 目标就是预测两实体之间的关系, 这与传统的KGE方法相似.</p></li></ul><p>然而, 在预训练时预测被Mask的锚点可能比较简单, 模型很容易就能用外部知识而不是依赖上下文完成这个人物, 因此在预训练时会<strong>丢掉50%</strong>的锚点邻居.</p><blockquote><p>在Mask做训练时, 从Knowledge Embedding的角度来看待, CoLAKE的方式有点像<strong>多任务训练</strong>. 因为Mask了不同属性的节点会导致CoLAKE所利用的信息不同, 执行任务的初始条件也不同.</p><p>我开始对Mask掉锚点后仍然能够利用知识库中的内容表示疑惑, 我开始认为在不知道锚点的情况下, 知识库中所存在的知识应该是不能成功进行链接的. 后来我从另一个角度出发, 作为人类, <strong>我们常已知了句子中的上下文和锚点相关的知识(除去锚点本身), 我们完全可以根据其他与锚点相关的属性和实体来猜出锚点是什么</strong>, 这样就完全说得通了.</p></blockquote><h3 id="Model-Training"><a href="#Model-Training" class="headerlink" title="Model Training"></a>Model Training</h3><p>CoLAKE通过对三种不同类型的节点进行训练, 采用交叉熵作为损失函数. 但是因为知识图谱中的实体数量实在是太庞大了, 所以实现CoLAKE有两个问题:</p><ul><li><p>实体数量较大, 训练输入时几乎不可能在GPU上维护一个entity embedding矩阵, 作者给出的解决方案是把entity embedding放在CPU上, 把模型的其他部分放在多张GPU上, GPU上的训练进程从CPU中读出entity embedding同时向CPU写入对应的梯度, CPU上的进程根据收集的梯度对entity embedding进行异步地更新.</p></li><li><p>实体数量过于庞大导致Softmax十分耗时, 所以简单的采用<strong>负采样</strong>解决这个问题.</p></li></ul><p>该部分来自<a href="https://zhuanlan.zhihu.com/p/263012775" target="_blank" rel="noopener">CoLAKE: 同时预训文本和知识</a>. 问题1隐射出当前KG和NLP发展可能还是受算力的制约, 相较与CV, NLP需要更多的计算来表达信息.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="DataSet-and-Implementation-Details"><a href="#DataSet-and-Implementation-Details" class="headerlink" title="DataSet and Implementation Details"></a>DataSet and Implementation Details</h3><p>CoLAKE主要用了两个数据集:</p><ul><li>English Wikipedia.</li><li>Wikidata5M(出自<strong>KEPLER</strong>).</li></ul><p>CoLAKE使用英文的维基百科作为预训练数据, 使用了Hugging Face的Transformer来实现, 和RoBERTa一样使用BPE, 直接使用了<strong>RoBERTa</strong>的权重初始化, 在1e-4的学习率下只训练了一轮.</p><h3 id="Knowledge-Driven-Tasks"><a href="#Knowledge-Driven-Tasks" class="headerlink" title="Knowledge Driven Tasks"></a>Knowledge Driven Tasks</h3><p>在<strong>知识驱动</strong>型任务上, 作者主要将CoLAKE与同样的注入知识的PLM进行<strong>横向对比</strong>, 其次与不注入知识的RoBERTa和BERT<strong>纵向对比</strong>, CoLAKE表现相当不错:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/colake5.jpg" style="zoom: 25%;" /><p>左侧Open Entity对应的任务为实体分类, 右侧FewRel对应的任务为关系抽取.</p><h3 id="Knowledge-Probing"><a href="#Knowledge-Probing" class="headerlink" title="Knowledge Probing"></a>Knowledge Probing</h3><p>LAMA(<strong>LA</strong>nguage <strong>M</strong>odel <strong>A</strong>nalysis) probe任务的目的是定性的测量模型到底存储了多少<strong>常识</strong>. 作者希望通过LAMA来观察CoLAKE对知识的掌握程度, 为公平起见, 作者还对所有模型只使用词汇之间有交集的部分.通过对Mask掉的知识进行预测, 取得它们的P@1, 实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/colake6.jpg" style="zoom: 50%;" /><p>BERT的表现倒是很亮眼. 作者认为K - Adapter比CoLAKE在两个数据集上效果好的原因是基于RoBERTa的LARGE版本, 并且使用了LAMA的一部分数据进行训练. CoLAKE总体而言的表现比RoBERTa要好非常多.</p><blockquote><p>BERT比使用更多数据的RoBERTa效果好太多了, 这与提出LAMA的论文<a href="https://arxiv.org/abs/1909.01066" target="_blank" rel="noopener">Language Models as Knowledge Bases?</a>结论一致.</p></blockquote><h3 id="Language-Understanding-Tasks"><a href="#Language-Understanding-Tasks" class="headerlink" title="Language Understanding Tasks"></a>Language Understanding Tasks</h3><p>目前的许多研究表明, 注入知识后, PLM的NLU能力可能会<strong>退化</strong>. 作者在GLUE常用数据集上对RoBERTa, KEPLER, CoLAKE做了实验: </p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/colake7.jpg" style="zoom: 50%;" /><p>从实验结果来看, CoLAKE并没有发生很严重NLU能力退化, 与KEPLER相比, 真的只是比RoBERTa差一点点, KEPLER就退化的比较严重.</p><blockquote><p>从这个实验来看, NLU能力是和<strong>语言知识</strong>相关的, 如果给模型灌输一些<strong>世界知识</strong>, 原来存储语言知识的部分可能会被<strong>干扰</strong>, 可能是引入了许多的噪声, 我们或许还没有很好的掌握如何运用这些知识.</p><p>我认为将KEPLER和CoLAKE放在一起对比, 一定是CoLAKE的WK Graph起到了某种作用, 能够保留模型对语言模型的理解能力. 沿着这个思路, <strong>图</strong>或<strong>网</strong>的结构是非常重要的. 如果对CoLAKE的WK Graph进一步改进, 或许能够提升模型的NLU能力.</p></blockquote><h3 id="Word-Knowledge-Graph-Completion"><a href="#Word-Knowledge-Graph-Completion" class="headerlink" title="Word - Knowledge Graph Completion"></a>Word - Knowledge Graph Completion</h3><p>因为CoLAKE加入了 Word - Knowledge Graph的结构, 它本质上已经变为了一个预训练好的GNN. 作者希望利用这个特性, 来测试CoLAKE对结构和语义特征的建模能力. 只要在FewRel上做关系抽取, 就能使其对关系进行补全.</p><p>与KEPLER相同, 这里作者给出了两种设置:</p><ul><li><strong>Transductive setting</strong>: 对于每个样本, 两个实体$h, t$, 和它们的关系$r$, 可能分别在训练, 验证, 还是测试中出现过. 但它们的整体表示三元组$(h, r, t)$ 却没有在训练数据中出现.</li><li><strong>Inductive setting</strong>: 对于每个样本, 至少有一个实体在训练阶段是模型没见过的. 这更考验模型的推断能力. 即将两个实体中至少一个在训练阶段没见过的实体进行Mask, 然后利用其邻居节点来预测该节点的表示.</li></ul><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/colake4.jpg" style="zoom: 50%;" /><p>实验结果如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/colake8.jpg" style="zoom: 25%;" /><p>CoLAKE无论在Transductive Setting还是Inductive Setting上都比Baseline强大非常多.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>CoLAKE有如下特点(或说主要贡献):</p><ul><li>它是一个<strong>Masked Language Model</strong>, 并能将上下文知识表示和上下文语言表示<strong>联合嵌入</strong>.</li><li>由于<strong>Word - Knowledge Graph</strong>的存在, 它能够轻松地将<strong>异构</strong>的知识信息和语言信息<strong>融合</strong>.</li><li>因为它本质上是一个预训练的<strong>图神经网络</strong>, 所以具有结构感知能力, 并且易于<strong>扩展</strong>.</li></ul><p>CoLAKE真的是和我理想中将<strong>外部知识注入PTM</strong>的方式非常相似了.</p><p>CoLAKE提供了一个统一的数据结构. 这样非常有利于将来其他形式的数据注入到其中, 当然注入的方式可能是一个值得研究的问题.</p><p>顺带一提, 从这些Knowledge Enriched PTM来看, BERT非常受大家的青睐, 说明BERT与XLNet相比更为<strong>简洁</strong>, 容易被大家所理解和接受.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> BERT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AcrE: Atrous Convolution and Residual Embedding</title>
      <link href="/posts/59193.html"/>
      <url>/posts/59193.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识: </p><ul><li>膨胀卷积(空洞卷积)</li><li>残差连接</li></ul></blockquote><h1 id="Knowledge-Graph-Embedding-with-Atrous-Convolution-and-Residual-Learning"><a href="#Knowledge-Graph-Embedding-with-Atrous-Convolution-and-Residual-Learning" class="headerlink" title="Knowledge Graph Embedding with Atrous Convolution and Residual Learning"></a>Knowledge Graph Embedding with Atrous Convolution and Residual Learning</h1><p>本文是论文<a href="https://arxiv.org/abs/2010.12121" target="_blank" rel="noopener">Knowledge Graph Embedding with Atrous Convolution and Residual Learning</a>的阅读笔记和个人理解. 由Pytorch实现的源代码已经放到上<a href="https://github.com/neukg/AcrE" target="_blank" rel="noopener">Github</a>, 这是NEU KG组的论文! 刚发我就看完了. 该论文已经被<strong>COLING</strong>收录. 该论文是一篇KGE方向的论文, 用极简结构实现了非常好的性能, 并在多个常用数据集上SOTA.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>在当今各类KGE模型大红大紫的情况下, 模型的<strong>复杂度</strong>和<strong>表达能力</strong>得不到很好的<strong>权衡</strong>. 最近流行的例如基于深度神经网络的嵌入模型, 基于图神经网络的嵌入模型, 都有非常高的<strong>耗时</strong>和模型<strong>复杂度</strong>, 导致不能在一些实时的场景下灵活运用. AcrE的目标就是实现<strong>简单</strong>, <strong>高效</strong>的知识嵌入, 同时兼具了参数量少, 计算量低的特征.</p><h2 id="ConvE"><a href="#ConvE" class="headerlink" title="ConvE"></a>ConvE</h2><p>本节作为背景知识为AcrE铺垫, 取自<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/17366/15884" target="_blank" rel="noopener">Convolutional 2D Knowledge Graph Embeddings</a>. 主要介绍KGE中的ConvE方法. ConvE只不过是将卷积使用在了KGE上, 有卷积基础的应该能够猜到使用方法, 所以不会细说.</p><p>得益于CNN的<strong>权重共享</strong>, 参数量非常少, 因此很高效, 并且很简单, 注意这个特点.</p><h3 id="1D-Convolution-VS-2D-Convolution"><a href="#1D-Convolution-VS-2D-Convolution" class="headerlink" title="1D Convolution VS 2D Convolution"></a>1D Convolution VS 2D Convolution</h3><p>作者指出, 相较于1维卷积, 2维卷积有更强的<strong>表达能力</strong>(其实从直觉来说也是这样).</p><p>在做1维卷积时, 卷积核最多只能与左侧或右侧离得比较近的元素<strong>交互</strong>:<br>$$<br>\left(\begin{array}{lll}<br>\left.\left[\begin{array}{lll}<br>a &amp; a &amp; a<br>\end{array}\right] ;\left[\begin{array}{lll}<br>b &amp; b &amp; b<br>\end{array}\right]\right)=\left[\begin{array}{llllll}<br>a &amp; a &amp; a &amp; b &amp; b &amp; b<br>\end{array}\right]<br>\end{array}\right.<br>$$<br>但2维卷积不一样, 除了能够与邻近的左右元素交互, 还能与上下元素进行交互:<br>$$<br>\left(\left[\begin{array}{lll}<br>a &amp; a &amp; a \\<br>a &amp; a &amp; a<br>\end{array}\right] ;\left[\begin{array}{lll}<br>b &amp; b &amp; b \\<br>b &amp; b &amp; b<br>\end{array}\right]\right)=\left[\begin{array}{lll}<br>a &amp; a &amp; a \\<br>a &amp; a &amp; a \\<br>b &amp; b &amp; b \\<br>b &amp; b &amp; b<br>\end{array}\right]<br>$$<br>如果两种元素代表的意义不同, 那么交换它们的拼接方式还能进一步的<strong>提升</strong>交互次数:<br>$$<br>\left[\begin{array}{lll}<br>a &amp; a &amp; a \\<br>b &amp; b &amp; b \\<br>a &amp; a &amp; a \\<br>b &amp; b &amp; b<br>\end{array}\right]<br>$$<br>由于交换了Concat方式, a和b交错, 能够实现更多次交互.</p><h3 id="ConvE-Architecture"><a href="#ConvE-Architecture" class="headerlink" title="ConvE Architecture"></a>ConvE Architecture</h3><p>直接看图:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/conve1.jpg" style="zoom:67%;" /><p>整体流程也是非常简单, 将头实体和关系先Embedding, 然后再Reshape到一个合适的尺寸, 然后就用卷积来提取特征, 用全连接将其投影到与Embedding大小相同的隐空间中, 最后将隐空间的映射和尾实体的Embedding做相似度比较.</p><p>按照描述, 打分函数为:<br>$$<br>\psi_{r}\left(\mathbf{e}_{s}, \mathbf{e}_{o}\right)=f\left(\operatorname{vec}\left(f\left(\left[\overline{\mathbf{e}_{s}} ; \overline{\mathbf{r}_{r}}\right] \ast \omega\right)\right) \mathbf{W}\right) \mathbf{e}_{o}<br>$$<br>其中$\mathbf{e}_{s}, \mathbf{e}_{o}$ 分别代表头实体和尾实体的Embedding, $\overline{\mathbf{e}_{s}}, \overline{\mathbf{r}_{r}}$ 分别代表Reshape后的头实体和关系向量. $\omega$代表卷积核, $\mathbf{W}$ 代表投影矩阵. 这种方式通过内积来比较所获向量与尾实体的<strong>相似度</strong>, 越相似得分越高.</p><p>然后将该得分经过$\sigma$ 函数, 得到每个实体的概率:<br>$$<br>p=\sigma(\psi_{r}\left(\mathbf{e}_{s}, \mathbf{e}_{o}\right))<br>$$<br>使用二分类交叉熵进行优化:<br>$$<br>\mathcal{L}(p, t)=-\frac{1}{N} \sum_{i}\left(t_{i} \cdot \log \left(p_{i}\right)+\left(1-t_{i}\right) \cdot \log \left(1-p_{i}\right)\right)<br>$$<br>$t$ 是尾实体的独热编码向量. 除此外还加入了Dropout, BatchNorm, 标签平滑等防止过拟合的手段.</p><h2 id="AcrE"><a href="#AcrE" class="headerlink" title="AcrE"></a>AcrE</h2><p>AcrE(<strong>A</strong>trous <strong>C</strong>onvolution and <strong>R</strong>esidual <strong>E</strong>mbedding), 在ConvE的基础上主要做了两点改动, 也就是我们开头所需的前置知识, 空洞卷积和残差连接.</p><h3 id="Atrous-Convolution"><a href="#Atrous-Convolution" class="headerlink" title="Atrous Convolution"></a>Atrous Convolution</h3><p>Atrous Convolution也称<strong>空洞卷积</strong>或<strong>膨胀卷积</strong>. 由于空洞卷积只是作为一种CNN的变体形式的卷积, 因此它的机制在此不做过多讨论.</p><p> 空洞卷积相较于普通卷积, 有了一个新的参数”<strong>膨胀率</strong>“. 它指的是在卷积下, 每个卷积核元素之间的距离 - 1. 空洞卷积能在<strong>不引入额外参数</strong>的情况下获得更大的<strong>感受野</strong>, 如下图:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/dilatedconv.jpg" style="zoom: 33%;" /><p>上图取自<a href="https://arxiv.org/abs/1511.07122" target="_blank" rel="noopener">Multi-Scale Context Aggregation by Dilated Convolutions</a>.</p><table><thead><tr><th align="center"></th><th align="center">最左侧</th><th align="center">正中间</th><th align="center">最右侧</th></tr></thead><tbody><tr><td align="center">卷积核</td><td align="center">(3, 3)</td><td align="center">(3, 3)</td><td align="center">(3, 3)</td></tr><tr><td align="center">感受野</td><td align="center">$3 \times 3$</td><td align="center">$7 \times 7$</td><td align="center">$15 \times 15$</td></tr><tr><td align="center">膨胀率</td><td align="center">1</td><td align="center">2</td><td align="center">4</td></tr></tbody></table><blockquote><p>空洞卷积常在语义分割和图像重建上使用, 与之相对的, 使用空洞卷积也会带来一些<strong>弊端</strong>, 如有需求请自行查询相关内容.</p></blockquote><p>在AcrE中, 它用来解决CNN在连续重复的下采样和池化而导致<strong>特征图分辨率丢失</strong>问题. 个人认为, 由于空洞卷积扩大了感受野, 能进一步增加实体和关系Embedding之间的交互, 以此将头实体和关系更<strong>紧密</strong>的联系到一起.</p><h3 id="Residual-Connection"><a href="#Residual-Connection" class="headerlink" title="Residual Connection"></a>Residual Connection</h3><p>引入残差连接主要的原因有两个:</p><ul><li><strong>梯度爆炸</strong>和<strong>梯度消失</strong>.</li><li>多次卷积导致的<strong>原始信息丢失</strong>问题.</li></ul><p>在ConvE中, 作者没有使用残差连接. 关于残差连接, 不懂可以去我之前写的<code>&lt;卷积神经网络发展史&gt;</code>中了解.</p><h3 id="AcrE-Architecture"><a href="#AcrE-Architecture" class="headerlink" title="AcrE Architecture"></a>AcrE Architecture</h3><p>AcrE有两种结构, 分别是<strong>串行</strong>(Serial)结构和<strong>并行</strong>(Parallel)结构. 无论哪种都使用了空洞卷积和残差连接. 但无论哪种结构都必须要有<strong>标准卷积</strong>的作用, 膨胀卷积虽然会提供更大的感受野, 但也有可能会因膨胀丧失局部信息.</p><h4 id="2D-Embedding-Representation"><a href="#2D-Embedding-Representation" class="headerlink" title="2D Embedding Representation"></a>2D Embedding Representation</h4><p>在ConvE中已经提到, 1D卷积没有2D卷积的表达能力强, 而且2D卷积能更多的增强头实体和关系间的交互, 所以都使用的是2D卷积. AcrE中, 首先要说一下KG中的三元组在2D中的表示方法, 可以视作是<strong>预处理</strong>. 对于三元组$&lt;h, r, t&gt;$, $\mathbf{h}, \mathbf{r}, \mathbf{t}$分别代表头实体, 关系和尾实体. </p><p>若$\tau$ 代表<strong>Reshape</strong>操作, $\mathbf{e}$ 代表实体的Embedding, $\mathbf{r}$ 代表关系的Embedding, $[;]$ 代表Concat, 则2D中的嵌入表示方法是$\tau([\mathbf{e};\mathbf{r}])$. </p><h4 id="Serial-AcrE-Model"><a href="#Serial-AcrE-Model" class="headerlink" title="Serial AcrE Model"></a>Serial AcrE Model</h4><p>在串行AcrE中, Embedding由一系列串行的卷积动作和最后的Flatten.</p><p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/acre1.jpg" alt=""></p><h5 id="Standard-Convolution-based-Learning"><a href="#Standard-Convolution-based-Learning" class="headerlink" title="Standard Convolution based Learning"></a>Standard Convolution based Learning</h5><p>在串行AcrE中, 将Reshape后的Embedding先经过标准卷积, 得到结果$\mathbf{C}_{0}$:<br>$$<br>\mathbf{C}_{0}^{i}=\omega_{0}^{i} \star \tau([\mathbf{e} ; \mathbf{r}])+\mathbf{b}_{0}^{i}<br>$$<br>其中$\star$ 代表卷及操作, $\omega_{0}^{i}$ 代表第i个卷积核, $b_0^i$ 代表第i个偏置. 假设有$F$ 个卷积核, 则$\mathbf{C}_{0}=\left[\mathbf{C}_{0}^{1}: \mathbf{C}_{0}^{2}: \mathbf{C}_{0}^{3}: \ldots: \mathbf{C}_{0}^{F}\right]$, 之后该卷积结果会经过一系列的空洞卷积. </p><blockquote><p>关于池化, 这里必须要提一下. </p><p>在CV任务中是卷积和池化交替使用, 因为其信息量通常是<strong>冗余</strong>的, 池化能够减小特征图的尺寸, 相当于求平均或最大值的效果, 池化有利于后面卷积抽取更关键的特征. </p><p>但目前在NLP相关的任务中, 信息非常复杂, 使用池化直接就导致了信息的<strong>损失</strong>. 并且, 因为我们是想将头实体和尾实体还有关系Embedding进一个同维度的空间下的, 如果使用池化会导致<strong>维度变化</strong>. </p><p>最后指出, 在实验中加入池化并没有很大程度的影响性能.</p></blockquote><h5 id="Atrous-Convolution-based-Learning"><a href="#Atrous-Convolution-based-Learning" class="headerlink" title="Atrous Convolution based Learning"></a>Atrous Convolution based Learning</h5><p>“空洞卷积大致是什么”这个问题在前面已经解决了, 其实就是在卷积核各元素之间插入一些小洞. 对于给定的输入向量$\mathbf{x}$, 长度为$K$的卷积核向量$\mathbf{w}$, 在空洞卷积下的输出$\mathbf{y}$ 由如下方式得来:<br>$$<br>y_{i}=\sum_{k=1}^{K} x_{i+l \times k} \times w_{k}<br>$$<br>其中$l$ 是膨胀率, 标准卷积的膨胀率为1.</p><p>在串行AcrE中, 卷积是一个接着一个<strong>串行</strong>的:<br>$$<br>\mathbf{C}_{\mathbf{t}}=\omega_{\mathbf{t}} \star \mathbf{C}_{t-1}+\mathbf{b}_{\mathbf{t}}<br>$$<br>$\mathbf{C}_{t-1}$ 代表上个卷积的输出结果, $\omega_{\mathbf{t}}$ 和$\mathbf{b}_{\mathbf{t}}$ 分别是卷积核和偏置向量.</p><h5 id="Feature-Vector-Generation"><a href="#Feature-Vector-Generation" class="headerlink" title="Feature Vector Generation"></a>Feature Vector Generation</h5><p>在串行的AcrE中, 不同种卷积一个接一个的执行, 每个卷积都能从之前抽取不同的实体和关系交互. 但越多的卷积使用, 就会导致越多的原始信息丢失, 这就导致了模型在学习时<strong>忘记</strong>了抽取出的特征到底有没有用. 同时, 为了缓解梯度爆炸和梯度消失, 在这使用残差连接来<strong>弥补</strong>原来丢失的信息. 在残差连接后, 紧接着使用<strong>ReLU</strong>做激活函数, 并做<strong>Flatten</strong>:<br>$$<br>\mathbf{o}=\text {Flatten}\left(\operatorname{ReLU}\left(\mathbf{C}_{T}+\tau([\mathbf{e} ; \mathbf{r}])\right)\right)<br>$$<br>其中$\mathbf{C}_{T}$ 是最后一个空洞卷积的输出, $T$ 是空洞卷积的次数.</p><h4 id="Parallel-AcrE-Model"><a href="#Parallel-AcrE-Model" class="headerlink" title="Parallel AcrE Model"></a>Parallel AcrE Model</h4><p>并行AcrE分别执行卷积后聚合.</p><p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/acre2.jpg" alt=""></p><p>注意, 在并行结构图中所示的<strong>卷积类型</strong>是不同的! 使用不同膨胀率的卷积核.</p><h5 id="Results-Integration"><a href="#Results-Integration" class="headerlink" title="Results Integration"></a>Results Integration</h5><p>与串行形态的AcrE不一样, 并行形态下, 2D Embedding分别用不同形式的卷积计算, 最后以<strong>某种形式</strong>聚合到一起.<br>$$<br>\mathbf{C}=\mathbf{C}_{\mathbf{0}} \oplus \mathbf{C}_{\mathbf{1}} \oplus \ldots \oplus \mathbf{C}_{\mathbf{T}}<br>$$<br>其中$\mathbf{C}_{\mathbf{0}}$ 是标准卷积, $\mathbf{C}_{\mathbf{i}}$ 是第i个空洞卷积. $\oplus$ 意味着某种聚合操作, 可以是基于Concat的操作, 也可以是基于加的操作等等. 在<strong>实验</strong>中会比较这两种方式的性能差异.</p><h5 id="Feature-Vector-Generation-1"><a href="#Feature-Vector-Generation-1" class="headerlink" title="Feature Vector Generation"></a>Feature Vector Generation</h5><p>与串行AcrE相仿, 将每个卷积结果聚合, 再加上残差的信息, 然后用ReLU, 再经过一次变换, 最后Flatten.<br>$$<br>\mathbf{c}=\text {Flatten}\left(\mathbf{W}_{\mathbf{1}} \operatorname{ReLU}(\mathbf{C}+\tau([\mathbf{e} ; \mathbf{r}]))\right)<br>$$<br>其中$\mathbf{W_1}$ 是变化矩阵, 这是比串行结构多出来的地方.</p><blockquote><p>我个人认为并行结构下的AcrE与Inception(详见<a href="https://adaning.github.io/posts/38085.html">卷积神经网络发展史</a>)中的<strong>多尺度</strong>是相同的道理, 通过不同的<strong>膨胀率</strong>实现了对实体和关系向量多个角度的抽取, 最后Concat到一起, 每个不同感受野的膨胀卷积都能提供不同的信息.</p></blockquote><h3 id="Score-Function-and-Loss-Function"><a href="#Score-Function-and-Loss-Function" class="headerlink" title="Score Function and Loss Function"></a>Score Function and Loss Function</h3><h4 id="Score-Function"><a href="#Score-Function" class="headerlink" title="Score Function"></a>Score Function</h4><p>打分函数其实与ConvE相似, 将输出向量先经过变换矩阵加上偏置, 再用点积比较与尾实体的<strong>相似度</strong>:<br>$$<br>\psi(h, r, t)=(\mathbf{o} \mathbf{W}+\mathbf{b}) \mathbf{t}^{\top}<br>$$<br>其中$\mathbf{W}, \mathbf{b}$ 分别是变化矩阵和偏置向量. 接着用$\sigma$ 函数获得所有候选实体的概率:<br>$$<br>p(t \mid h, r)=\operatorname{sigmoid}(\psi(h, r, t))<br>$$</p><h4 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h4><p>这点和ConvE一样, 使用了交叉熵作为损失函数:<br>$$<br>\mathcal{L}=-\frac{1}{N} \sum_{i=1}^{N}\left[t_{i} \log p\left(t_{i} \mid h, r\right)+\left(1-t_{i}\right) \log \left(1-p\left(t_{i} \mid h, r\right)\right)\right]<br>$$<br>其中$\mathbf{t}$ 是尾实体的独热编码. </p><h3 id="Other-Details"><a href="#Other-Details" class="headerlink" title="Other Details"></a>Other Details</h3><p>和ConvE一样, BatchNorm和卷积的向性比较好, 所以也使用了BatchNorm. 包括标签平滑啊之类的trick也都从ConvE沿用了下来. </p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>在实验的代码中, 只使用了三个卷积核. 有一个标准卷积核和两个不同膨胀率的膨胀卷积核.</p><h3 id="Settings"><a href="#Settings" class="headerlink" title="Settings"></a>Settings</h3><h4 id="DataSets"><a href="#DataSets" class="headerlink" title="DataSets"></a>DataSets</h4><p>我们将其他KGE方法在如下六个常用数据集中对比:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/acre3.jpg" style="zoom:50%;" /><h4 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h4><p>实验部分均采用<strong>Link Prediction</strong>的任务将其他KGE方法与AcrE进行对比. Link Prediction是基于已知的某端实体和关系对未知的另一端实体进行预测的任务. 即对于三元组$&lt;h,r,t&gt;$, 已知$&lt;h,r&gt;$ 预测$t$, 或已知$&lt;t,r&gt;$ 预测$h$. 并使用Hit@k, MRR作为指标进行对比.</p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><h4 id="Benchmark-Datasets-Experiment"><a href="#Benchmark-Datasets-Experiment" class="headerlink" title="Benchmark Datasets Experiment"></a>Benchmark Datasets Experiment</h4><p>在DB100k中的实验结果, 上面的内容全部取自SEEK的论文<a href="https://arxiv.org/pdf/2005.00856.pdf" target="_blank" rel="noopener">SEEK: Segmented Embedding of Knowledge Graphs</a>:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/acre4.jpg" style="zoom:50%;" /><p>还有在其他五个数据集上的表现:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/acre5.jpg" style="zoom:67%;" /><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/acre6.jpg" style="zoom:67%;" /><p>只是在WN18RR上没有取得太好的效果, 在其他数据集上均SOTA或在平均性能上超过其他KGE方法.</p><p>实验的细节分了<strong>区分头尾</strong>的预测和<strong>按类别</strong>预测两种.</p><p>下面是<strong>区分头尾</strong>的预测:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/acre7.jpg" style="zoom:67%;" /><p>直接SOTA.</p><p>下面是<strong>按类别</strong>预测:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/acre8.jpg" style="zoom:67%;" /><h4 id="Ablation-Experiment"><a href="#Ablation-Experiment" class="headerlink" title="Ablation Experiment"></a>Ablation Experiment</h4><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/acre9.jpg" style="zoom:67%;" /><p>消融实验主要进行了如下工作:</p><ol><li>将<strong>串行</strong>和<strong>并行</strong>的AcrE对比, 发现一般情况下串行的AcrE都比并行AcrE性能要<strong>差</strong>.</li><li>将<strong>带有</strong>残差连接的AcrE和<strong>去掉</strong>残差连接的AcrE对比, 发现去掉后性能有<strong>明显下降</strong>, 说明了残差连接对AcrE非常重要.</li><li>将并行用Concat方式聚合的AcrE和并行用Add方式聚合的AcrE对比, 发现<strong>Concat</strong>效果要好一些.</li></ol><h4 id="Parameters-Comparison"><a href="#Parameters-Comparison" class="headerlink" title="Parameters Comparison"></a>Parameters Comparison</h4><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/acre10.jpg" style="zoom:67%;" /><p>ConvE的参数虽然比AcrE要少, 但在前面的对比实验中知道效果并没有AcrE好. AcrE的参数量要<strong>远少于</strong>除了ConvE的所有模型. 并且并行结构比串行结构的模型要稍大一些, 因为最后<strong>多一次变换</strong>.</p><p>另外, 调参的部分碍于篇幅问题, 没有在论文中贴出. 能够选择的参数范围非常小, 还是比较好调的, 参数在源码中都可以找到.</p><p>关于计算效率的优势, 因为涉及到其他训练的参数影响, 没法提供一个非常公平的环境去验证. 但作为ConvE的变体, 运行时间的复杂度应该与ConvE相仿.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>该论文简单易懂, 只看嵌入方式的<strong>图示</strong>和<strong>表格</strong>中给出的实验结果就能够把握文章重点. 尤其是最后的<strong>消融实验</strong>体现出了<strong>残差连接</strong>起到的作用. AcrE本身结构就简单到了<strong>令人发指</strong>的地步, 非常不可思议它甚至能达到一些高复杂度模型的结果… </p><p>在实验部分将那些没有在指定数据集上给出实验结果的论文的Embedding方法全都跑了一遍, 并做了<strong>大量的对比实验</strong>证明AcrE的有效性. 并且这是<strong>第一次</strong>将<strong>不同形式的卷积</strong>用到了<strong>KGE</strong>上的研究工作.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> 空洞卷积 </tag>
            
            <tag> 残差连接 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer-XL与XLNet</title>
      <link href="/posts/35276.html"/>
      <url>/posts/35276.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>Transformer(Masked Self - Attention和FFN)</li><li>BERT(与XLNet做对比)</li><li>Seq2Seq(AutoRegressive &amp; AutoEncoding)</li></ul><p><strong>2020.10.26</strong>: 更新了Transformer - XL的隐态更新维度分析.</p></blockquote><h1 id="Transformer-XL与XLNet"><a href="#Transformer-XL与XLNet" class="headerlink" title="Transformer - XL与XLNet"></a>Transformer - XL与XLNet</h1><p>本文是Transformer - XL和XLNet论文的阅读笔记和个人理解(实际上还包括Vanilla Transformer).</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>这两篇论文都没有从Transformer开始, 而是从<a href="https://arxiv.org/abs/1808.04444" target="_blank" rel="noopener">Character-Level Language Modeling with Deeper Self-Attention</a>开始, 该模型利用了Self - Attention在更深层次的语言模型上进行字符级建模, 因内容比单词级更长, 所以提出了直接将内容按照固定大小划分为<strong>Segment</strong>处理的方法. 该在文中被称为Vanilla Transformer, 被多次提及. 分段的方式伴随着多种缺点, Vanilla Transformer对Segment的划分没有进行任何优化. 而Transformer - XL对该模型存在的问题进行了优化, 而XLNet将Transformer - XL和BERT的优势结合到了一起.</p><h2 id="Vanilla-Transformer"><a href="#Vanilla-Transformer" class="headerlink" title="Vanilla Transformer"></a>Vanilla Transformer</h2><p> 中使用的Transformer被称为Vanilla Transformer(普通Transformer). 我们只需要知道它将一个Masked 多头Self - Attention和一个由两层全连接构成的FFN称为一个Transformer Layer, 然后堆叠多层Transformer Layer. 训练时使用<strong>语言模型</strong>的方式, 采用<strong>字符级</strong>数据. 它能够堆叠的非常深, 达到了64层, 其实它也是通过加深模型的方式来增长前文依赖能力的.</p><p>如果没有Transformer基础建议看我写过的<code>&lt;Transformer精讲&gt;</code>. 关于Vanilla Transformer, 其他具体内容如果想了解可以去看原论文.</p><p>在Transformer中通常需要设定一个固定的长度, 如果输入序列长小于固定长则进行<strong>填充</strong>. 因为使用字符级数据, 所以经常会出现大于序列长的情况. 它采用分段处理, 引入辅助函数训练. </p><p>与Transformer一样, 训练阶段, 模型能够一次处理<strong>整段数据</strong>:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformerxl1.jpg" style="zoom: 50%;" /><p>评估阶段, 模型需要按照自回归每次前进一个Token:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformerxl2.jpg" style="zoom: 67%;" /><h2 id="Transformer-XL"><a href="#Transformer-XL" class="headerlink" title="Transformer - XL"></a>Transformer - XL</h2><p>Transformer - XL出自论文<a href="https://arxiv.org/abs/1901.02860" target="_blank" rel="noopener">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a>. 其中XL指的是extra - long. 作者主要通过Segment - Level Recurrence的方式来缓解Transformer中的内容长度限定问题, 其衍生问题由Relative Positional Encoding解决. </p><p>在Vanilla Transformer中的分段导致了两个问题: </p><ol><li><strong>依赖</strong>的最大长度被固定长这个参数严格的限制了. </li><li>在划分时很有可能会<strong>截断上下文的含义</strong>, 导致不同的Segment之间考虑不到上下文联系, 这被称为上下文碎片化(Context framentation). </li></ol><p>不同段之间的梯度传播会被分段而<strong>截断</strong>:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformerxl5.jpg" style="zoom: 50%;" /><p>这两个缺陷是Transformer - XL改进的动机. </p><h3 id="Segment-Level-Recurrence-with-State-Reuse"><a href="#Segment-Level-Recurrence-with-State-Reuse" class="headerlink" title="Segment - Level Recurrence with State Reuse"></a>Segment - Level Recurrence with State Reuse</h3><p>最首要的想法就是建立前后Segment之间的<strong>联系</strong>, 想办法将前面Segment的信息传到后面.</p><p>作者直接以Segment为单位进行操作, 每次都将<strong>同层</strong>上个Segment的隐态<strong>暂存</strong>下来, 而并非是对隐态重新计算. 但<strong>不保存梯度</strong>, 因为我们只是用同层的上个Segment隐态做为一种记忆信息对当前Segment产生影响, 并不需要进行反向传播. 储存的隐态数量尽可能多的.</p><p>$$<br>\begin{array}{l}<br>\widetilde{\mathbf{h}}_{\tau+1}^{n-1}=\left[\mathrm{SG}\left(\mathbf{h}_{\tau}^{n-1}\right) \circ \mathbf{h}_{\tau+1}^{n-1}\right] \\<br>\mathbf{q}_{\tau+1}^{n}, \mathbf{k}_{\tau+1}^{n}, \mathbf{v}_{\tau+1}^{n}=\mathbf{h}_{\tau+1}^{n-1} \mathbf{W}_{q}^{\top}, \widetilde{\mathbf{h}}_{\tau+1}^{n-1} \mathbf{W}_{k}^{\top}, \widetilde{\mathbf{h}}_{\tau+1}^{n-1} \mathbf{W}_{v}^{\top} \\<br>\mathbf{h}_{\tau+1}^{n}=\text { Transformer-Layer }\left(\mathbf{q}_{\tau+1}^{n}, \mathbf{k}_{\tau+1}^{n}, \mathbf{v}_{\tau+1}^{n}\right)<br>\end{array}<br>$$</p><p>其中$n$ 代表层数, $\tau$ 代表Segment号. $\mathrm{SG}(\cdot)$ 代表不求梯度, 不算作反向传播的一部分. 注意上面的式子, $\mathbf{k}_{\tau+1}^{n}$ 和 $\mathbf{v}_{\tau+1}^{n}$ 都是由当前Segment隐态和上个Segment隐态Concat后的$\widetilde{\mathbf{h}}_{\tau+1}^{n-1}$ 生成的, 而$\mathbf{q}_{\tau+1}^{n}$ <strong>仅由当前Segment隐态$\mathbf{h}_{\tau+1}^{n-1}$影响</strong>. 每个Segment都做相同的动作, 因此称该方法为Segment - Level Recurrence.</p><p>在训练时仍然每次前进一段:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformerxl3.jpg" style="zoom: 50%;" /><p>与Vanilla Transformer不同的是, 因为能够直接<strong>复用</strong>整个Segment的信息, 能够极大加快Evaluation的速度, 所以每次前进Segment个Token:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformerxl6.jpg" style="zoom: 50%;" /><blockquote><p>如果对前进Segment个Token有些模糊, 还是要理解作者是以整个Segment为单位进行操作的, 即使是进行自回归运算, 也是直接生成Segment个输出, 并输入进下个Segment.<br>$$<br>\mathbf{h}_{\tau}=f\left(\mathbf{h}_{\tau-1}, \mathbf{E}_{\mathbf{s}_{\tau}}+\mathbf{U}_{1: L}\right)<br>$$<br>如果在对前一个Segment的隐态和当前时刻隐态做Concat这个操作有疑惑, 我们来分析一下在这个计算过程中维度的变化. 请记住一个结论: <strong>Transformer隐态输出大小取决于Query的序列长度</strong>.</p><p>假设前一时刻隐态$\mathbf{h}_{\tau}^{n-1}$ 和当前时刻隐态$\mathbf{h}_{\tau+1}^{n-1}$ 的Size为$L\times d$, $W_q^T$, $W_v^T$, $W_k^T$ 的Size为$d_k \times d$. 在做完Concat后, 拼接后隐态$\widetilde{\mathbf{h}}_{\tau+1}^{n-1}$ Size为$2L \times d$, 看一下Attention分数的计算:<br>$$<br>\displaylines{<br>\mathbf{q}_{\tau+1}^{n}, \mathbf{k}_{\tau+1}^{n}, \mathbf{v}_{\tau+1}^{n}=\mathbf{h}_{\tau+1}^{n-1} \mathbf{W}_{q}^{\top}, \widetilde{\mathbf{h}}_{\tau+1}^{n-1} \mathbf{W}_{k}^{\top}, \widetilde{\mathbf{h}}_{\tau+1}^{n-1} \mathbf{W}_{v}^{\top} \\<br>\text{Attention Score}_{\tau+1}^n = \frac{\mathbf{q}_{\tau+1}^{n}{\mathbf{k}_{\tau+1}^{n}}^T}{\sqrt{d_k}}\mathbf{v}_{\tau+1}^{n}<br>}<br>$$<br>重复一遍之前的话, $\mathbf{k}_{\tau+1}^{n}$ 和 $\mathbf{v}_{\tau+1}^{n}$ 都是由当前Segment隐态和上个Segment隐态Concat后的$\widetilde{\mathbf{h}}_{\tau+1}^{n-1}$ 生成的, 而$\mathbf{q}_{\tau+1}^{n}$ <strong>仅由当前Segment隐态$\mathbf{h}_{\tau+1}^{n-1}$影响</strong>.</p><p>在求得分时, $\mathbf{q}_{\tau+1}^{n}$ 的维度是$L \times d_k$, $\mathbf{k}_{\tau+1}^{n}$ 和的$\mathbf{v}_{\tau+1}^{n}$维度都是$2L \times d$, 求完$\mathbf{q}_{\tau+1}^{n} \cdot {\mathbf{k}_{\tau+1}^{n}}^T$ 后维度是$L\times 2L$, 然后与$\mathbf{v}_{\tau+1}^{n}$ 再相乘, 维度变成$L\times d_k$. 乘完后的计算维度是与Transformer<strong>保持一致</strong>的.</p></blockquote><p>并且还能更好的联系前文信息, 每次能够依赖的最大长度也从$\text{Segment Length}$ 提升到$\text{Layer} \times \text{Segment Length}$的倍数:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformerxl4.jpg" style="zoom: 67%;" /><h3 id="Relative-Positional-Encoding"><a href="#Relative-Positional-Encoding" class="headerlink" title="Relative Positional Encoding"></a>Relative Positional Encoding</h3><p>虽然已经利用Segment - Level Recurrence来解决固定长问题, 但在计算<strong>位置编码</strong>时候还没有彻底将<strong>段递归</strong>的思想融合进去. </p><h4 id="Absolute-Positional-Encoding"><a href="#Absolute-Positional-Encoding" class="headerlink" title="Absolute Positional Encoding"></a>Absolute Positional Encoding</h4><p>在Transformer中, 使用的是绝对位置编码: </p><p>$$<br>\begin{aligned}<br>\mathbf{A}_{i, j}^{\mathrm{abs}}&amp;= \left[ \mathbf{W}_{q}\left( \mathbf{E}_{x_{i}}+\mathbf{U}_i \right)\right]^{\top} \mathbf{W}_{k}\left(\mathbf{E}_{x_j}+\mathbf{U}_j\right)\\<br>&amp;=\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{E}_{x_{j}}}_{(a)}+\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{U}_{j}}_{(b)}<br>+\underbrace{\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{E}_{x_{j}}}_{(c)}+\underbrace{\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{U}_{j}}_{(d)}<br>\end{aligned}<br>$$</p><p>这个式子是由Transformer中的Attention拆来的, 其中$\mathbf{W}$ 代表权重矩阵, $\mathbf{E}$ 代表某个Token的Embedding, $\mathbf{U}$ 代表位置编码.</p><p>这样的位置编码在同一个Segment中是生效的, 但是对于不同的Segment就无法区分两个Segment中的同一对应位置, 即<strong>绝对位置不同</strong>, 但<strong>位置编码相同</strong>. 因此作者提出使用相对位置编码来解决该问题. </p><h4 id="Relative-Positional-Encoding-1"><a href="#Relative-Positional-Encoding-1" class="headerlink" title="Relative Positional Encoding"></a>Relative Positional Encoding</h4><p>作者先是说明了位置编码的作用: </p><blockquote><p>Conceptually, the positional encoding gives the model a <strong>temporal clue</strong> or “<strong>bias</strong>“ about how information should be gathered.</p></blockquote><p>概念上来说位置编码告诉模型一些如何收集信息的时序线索或者”偏置”. </p><blockquote><p>when a query vector $q_{τ,i}$ attends on the key vectors $k_{τ,≤i}$, it does not need to know the absolute position of each key vector to identify the temporal order of the segment. Instead, it suffices to know the relative distance between each key vector $k_{τ,j}$ and itself $q_{τ,i}$, i.e. $i−j$.</p></blockquote><p>但是在进行查询的时候, 每个key的绝对位置实际上通常是<strong>无关紧要</strong>的, 但与query的相对位置却非常关键. 也就是说我只要知道Query和Key的距离就足够了, 根本不关心它们到底处于原文中的哪个具体位置. 所以可以引入相对位置编码来告诉模型Token之间的相对位置关系, 从而代替更为复杂的绝对位置编码. </p><p>$$<br>\mathbf{A}_{i, j}^{\mathrm{rel}} =\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k, E} \mathbf{E}_{x_{j}}}_{(a)}+\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k, R} \mathbf{R}_{i-j}}_{(b)}<br>+\underbrace{u^{\top} \mathbf{W}_{k, E} \mathbf{E}_{x_{j}}}_{(c)}+\underbrace{v^{\top} \mathbf{W}_{k, R} \mathbf{R}_{i-j}}_{(d)}<br>$$</p><p>主要关注两个<strong>替换点</strong>:</p><ul><li>所有与Key对应的绝对位置$j$ 相关的项$\mathbf{U}_j$ 应该被替换为相对位置项$\mathbf{R}_{i-j}$.</li><li>由于Query总出于同一位置, 所有与Query对应的绝对位置$i$ 相关的项$\mathbf{U}_i^{\top}\mathbf{W}_q^{\top}$ 也应该被替换掉, 在这里替换成两个可训练参数$u$ 和$v$ .</li></ul><p>其中$\mathbf{R}_{i-j}$的编码方式与Transformer中的<strong>正余弦</strong>位置编码方式相同. </p><p>对于相对位置的四项, 作者做了汇总, 并给它们下了直观含义:</p><blockquote><p>term (a) represents contentbased addressing, term (b) captures a contentdependent positional bias, term (c) governs a global content bias, and (d) encodes a global positional bias.</p></blockquote><table><thead><tr><th align="center">项数</th><th align="center">绝对位置表示</th><th align="center">相对位置表示</th><th>相对位置含义</th></tr></thead><tbody><tr><td align="center">a</td><td align="center">$\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{E}_{x_{j}}$</td><td align="center">$\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k, E} \mathbf{E}_{x_{j}}$</td><td>基于内容的寻址</td></tr><tr><td align="center">b</td><td align="center">$\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{U}_{j}$</td><td align="center">$\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k, R} \mathbf{R}_{i-j}$</td><td>依赖内容的位置偏置</td></tr><tr><td align="center">c</td><td align="center">$\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{E}_{x_{j}}$</td><td align="center">$u^{\top} \mathbf{W}_{k, E} \mathbf{E}_{x_{j}}$</td><td>全局内容偏置</td></tr><tr><td align="center">d</td><td align="center">$\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{U}_{j}$</td><td align="center">$v^{\top} \mathbf{W}_{k, R} \mathbf{R}_{i-j}$</td><td>全局位置偏置</td></tr></tbody></table><blockquote><p>我开始很难理解(c)和(d)的含义到底是如何观察出来的. 如果从映射和查询的角度去理解的话, 可以看作是权重矩阵左侧的内容做了一次Embedding, 投影进潜在空间, 然后与权重矩阵右侧的内容计算得分. </p><p>个人理解还是有些模糊, 可解释性不够好.</p></blockquote><p>在加上相对位置编码后, 整个过程就完善了, 形成一个闭环:<br>$$<br>\begin{aligned}<br>\widetilde{\mathbf{h}}_{\tau}^{n-1}=&amp;\left[\mathrm{SG}\left(\mathbf{m}_{\tau}^{n-1}\right) \circ \mathbf{h}_{\tau}^{n-1}\right] \\<br>\mathbf{q}_{\tau}^{n}, \mathbf{k}_{\tau}^{n}, \mathbf{v}_{\tau}^{n}=&amp; \mathbf{h}_{\tau}^{n-1} \mathbf{W}_{q}^{n \top}, \widetilde{\mathbf{h}}_{\tau}^{n-1} \mathbf{W}_{k, E}^{n}, \widetilde{\mathbf{h}}_{\tau}^{n-1} \mathbf{W}_{v}^{n \top} \\<br>\mathbf{A}_{\tau, i, j}^{n}=&amp; \mathbf{q}_{\tau, i}^{\top} \mathbf{k}_{\tau, j}^{n}+\mathbf{q}_{\tau, i}^{n} \mathbf{W}_{k, R}^{n} \mathbf{R}_{i-j}<br>+u^{\top} \mathbf{k}_{\tau, j}+v^{\top} \mathbf{W}_{k, R}^{n} \mathbf{R}_{i-j} \\<br>\mathbf{a}_{\tau}^{n}=&amp; \text { Masked-Softmax }\left(\mathbf{A}_{\tau}^{n}\right) \mathbf{v}_{\tau}^{n} \\<br>\mathbf{o}_{\tau}^{n}=&amp; \text { LayerNorm }\left(\operatorname{Linear}\left(\mathbf{a}_{\tau}^{n}\right)+\mathbf{h}_{\tau}^{n-1}\right) \\<br>\mathbf{h}_{\tau}^{n}=&amp; \text { Positionwise-Feed-Forward }\left(\mathbf{o}_{\tau}^{n}\right)<br>\end{aligned}<br>$$<br>其中$\mathbf{m}_{\tau}^{n-1}$ 指的是第$\tau$ 个Segment时GPU缓存中的所有第$n-1$ 层的隐态.</p><blockquote><p>推荐阅读:</p><ul><li><a href="http://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html" target="_blank" rel="noopener">Transformer-XL: Unleashing the Potential of Attention Models</a> 是谷歌官方发布的Blog, 里面有动图.</li></ul></blockquote><h2 id="XLNet"><a href="#XLNet" class="headerlink" title="XLNet"></a>XLNet</h2><p>XLNet出自论文<a href="http://arxiv.org/abs/1906.08237" target="_blank" rel="noopener">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a>.</p><p>虽然BERT在各类任务上的出色表现, 但BERT是一个<strong>自编码</strong>(Auto Encoding)模型, 相较与<strong>自回归</strong>(Auto Regressive)模型, 自编码模型在面对<strong>生成式</strong>的任务是天生具有劣势的(我们后面再提). XLNet尝试在自回归的基础上将BERT身上的优点吸取过来.</p><p>但是怎么才能在自回归模型中引入双向捕捉上下文的能力呢? 在先前的任务中, AR模型一般只能通过Forward和Backward两次单向编码来尝试捕捉上下文, 但并没有双向捕捉上下文效果来的理想, 下游任务又非常需要这种双向建模的能力. 自回归和自编码的模型<strong>各有优劣</strong>, 很多时候特点既是一种<strong>优势</strong>, 也是一种<strong>限制</strong>. AR模型受限于自身结构, 不能从结构上改变. 作者巧妙的想到了利用<strong>打乱Token之间的顺序</strong>来获取上下文. </p><h3 id="Why-not-BERT"><a href="#Why-not-BERT" class="headerlink" title="Why not BERT?"></a>Why not BERT?</h3><h4 id="Disadvantage-of-BERT"><a href="#Disadvantage-of-BERT" class="headerlink" title="Disadvantage of BERT"></a>Disadvantage of BERT</h4><p>XLNet的作者指出了BERT的两个<strong>致命缺点</strong>:</p><ol><li><p>被BERT使用的人工制造符号譬如<code>[MASK]</code>完全贯穿于BERT的<strong>预训练</strong>过程, 但在用<strong>实际数据</strong>做Fine - tune时根本没有, 导致了预训练和微调之间的<strong>差异</strong>. 我记得在BERT中, 作者提到了缓解这个问题的方案, 但不能从根本上解决这个问题:</p><blockquote><p>在Fine tune的时候不可能对单词进行Mask, 这样就会导致预训练和微调的不匹配, 为缓解这种问题, 并非总是将选中的单词Mask, 而是在每个句子中, 有15%的词会被选中, 在选中单词后有三种可能性:</p><ol><li><strong>80%</strong>的概率将被选中的单词替换为<code>[MASK]</code>.</li><li><strong>10%</strong>的概率将被选中的单词替换为<strong>随机词</strong>.</li><li><strong>10%</strong>的概率对被选中的单词<strong>不进行替换</strong>.</li></ol></blockquote></li><li><p>BERT假设被Mask掉的Token与句子中剩下的内容是独立的, 但在实际任务中普遍有远程依赖.</p><blockquote><p>比如”New York is a city.” 假设Mask了”New”和”York”两个词语, 那么在已知”is a city”的情况下, 就不太可能预测准确.</p></blockquote></li></ol><h4 id="Comparsion-of-AutoEncoding-and-AutoRegressive"><a href="#Comparsion-of-AutoEncoding-and-AutoRegressive" class="headerlink" title="Comparsion of AutoEncoding and AutoRegressive"></a>Comparsion of AutoEncoding and AutoRegressive</h4><p>假设你真的不知道AE和AR是什么意思, 我们用大白话给AE何AR重新下个定义吧:</p><ul><li><p>自回归模型(AR): 将上一时刻的输出作为输入, 对当前时刻输出进行预测.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/xlnet2.jpg" style="zoom: 67%;" /></li><li><p>自编码模型(AE): 把完整的内容破坏掉一部分作为输入, 来预测被破坏掉的那部分.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/xlnet3.jpg" style="zoom: 67%;" /></li></ul><blockquote><p>上述图片出自NLPCC 2020中科大讯飞和HIT - SCIR做的报告&lt;Revisiting Pre-Trained Models for Natural Language Processing&gt;.</p></blockquote><p>对给定的文本输入序列$\mathbf{x}=\left[x_{1}, \cdots, x_{T}\right]$, $e(x)$ 代表Token $x$ 的Embedding.</p><p>自回归模型的任务目标是最大似然该函数:<br>$$<br>\max _{\theta}\quad \log p_{\theta}(\mathbf{x})=\sum_{t=1}^{T} \log p_{\theta}\left(x_{t} \mid \mathbf{x}_{&lt;t}\right)=\sum_{t=1}^{T} \log \frac{\exp \left(h_{\theta}\left(\mathbf{x}_{1: t-1}\right)^{\top} e\left(x_{t}\right)\right)}{\sum_{x^{\prime}} \exp \left(h_{\theta}\left(\mathbf{x}_{1: t-1}\right)^{\top} e\left(x^{\prime}\right)\right)}<br>$$<br>但是对于AE模型来说(主要以BERT作为对比对象), 由于会选中15%的Token做Mask, 假设被污染的Token用$\hat{\mathbf{x}}$ 表示, 自编码模型的任务目标是从<strong>带噪声</strong>的$\hat{\mathbf{x}}$ 中重建$\overline{\mathbf{x}}$:<br>$$<br>\max _{\theta}\quad \log p_{\theta}(\overline{\mathbf{x}} \mid \hat{\mathbf{x}}) \approx \sum_{t=1}^{T} m_{t} \log p_{\theta}\left(x_{t} \mid \hat{\mathbf{x}}\right)=\sum_{t=1}^{T} m_{t} \log \frac{\exp \left(H_{\theta}(\hat{\mathbf{x}})_{t}^{\top} e\left(x_{t}\right)\right)}{\sum_{x^{\prime}} \exp \left(H_{\theta}(\hat{\mathbf{x}})_{t}^{\top} e\left(x^{\prime}\right)\right)}<br>$$</p><p>其中当Token $x_t$ 被Mask时$m_t=1$, 其他时候为0. $H_\theta$ 是Transformer将长度为$T$ 的文本序列$\mathbf{x} $ 映射成隐态向量$H_{\theta}(\mathbf{x})=\left[H_{\theta}(\mathbf{x})_{1}, H_{\theta}(\mathbf{x})_{2}, \cdots, H_{\theta}(\mathbf{x})_{T}\right]$ .</p><p>注意这两个目标函数, 首先条件不同, 其次是隐态表示不同. AR模型中因为只能看到前文的信息, 而AE模型能够看到全部的上下文信息. 将二者比较后, 作者得出三个结论:</p><ul><li><p><strong>独立假设</strong>: 这里强调了在自编码任务目标中的$\approx$, BERT假设在给定$\hat{\mathbf{x}}$ 的情况下被Mask的词是相互独立的. 而AR模型不需要独立性假设.</p></li><li><p><strong>输入噪声</strong>: 在预训练时会出现特殊的人造Token, Fine - tune时没有, 导致不匹配. 而AR模型不对输入引入噪声, 所以不会遇到该问题.</p></li><li><p><strong>上下文依赖</strong>: AR模型只能捕捉到位置位于前面的信息$h_\theta(\mathbf{x_{1:t-1}})$, BERT能更好的捕捉双向上下文信息$H_\theta(\mathbf{x})_t$.</p><blockquote><p>对于双向捕捉可以参考<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>.</p></blockquote></li></ul><p>作者观察到的这三点非常重要, 作为模型的改进方向.</p><p>此外, AR模型的特性能带来一个AE模型不能带来的优势, 这也是BERT为捕捉双向上下文付出的代价. 我们考虑一个场景:</p><ul><li>BERT要对<code>[New, York, is, a, city]</code> 中的<code>[New, York]</code> 进行预测, 作为AE模型对<code>[New, York]</code> 做了Mask.</li><li>XLNet要对<code>[is, a, city, New, York]</code> 中的<code>[New, York]</code> 进行预测, 作为AR模型, 需要先预测出<code>[New]</code>, 再利用<code>[New]</code> 的信息预测<code>[York]</code>.</li></ul><p>那么这两种方式产生的信息量是绝对不一样的:<br>$$<br>\begin{aligned}<br>\mathcal{J}_{\mathrm{BERT}} &amp;=\log p(\text { New } \mid \text { is a city })+\log p(\text { York } \mid \text { is a city }) \\<br>\mathcal{J}_{\mathrm{XLNet}} &amp;=\log p(\mathrm{New} \mid \text { is a city })+\log p(\text { York } \mid \mathrm{New}, \text { is a city })<br>\end{aligned}<br>$$<br>很明显, XLNet因为是AR模型, 永远都能学到更多的依赖对. 至于为什么可以在二者输入不一样的情况下进行比较, 在读完下一小节后你就会恍然大悟.</p><blockquote><p>在附录中还有更多的比对, 更详细的内容请自行阅读原论文.</p></blockquote><h3 id="Permutation-Language-Model-Based-on-AutoRegressive-Model"><a href="#Permutation-Language-Model-Based-on-AutoRegressive-Model" class="headerlink" title="Permutation Language Model Based on AutoRegressive Model"></a>Permutation Language Model Based on AutoRegressive Model</h3><p>由于AR语言模型比较特殊, 不好实现像BERT那样双向捕捉上下文的效果. 作者提出用<strong>置换</strong>的方法, 既能保留AR模型的优势, 又允许模型捕捉上下文. </p><p>对于给定长度为$T$ 的序列$\mathbf{x}$, 总共有$T!$ 种不同的排列方式. 如果模型参数能通过多次随机的句子排列<strong>共享</strong>, 那么AR模型就能将所有位置上的信息结合到一起.</p><p>假设$\mathcal{Z}_{T}$ 代表长度为$T$ 的序列下标, $z_t$ 代表第$t$ 个元素,  $\mathbf{z}_{&lt;t}$ 代表置换后的前$t-1$ 个元素.</p><p>沿着之前AR模型的目标, 现在给置换语言模型(PLM)的基本优化目标重新下个定义吧:<br>$$<br>\max _{\theta} \mathbb{E}_{\mathbf{z} \sim \mathcal{Z}_{T}}\left[\sum_{t=1}^{T} \log p_{\theta}\left(x_{z_{t}} \mid \mathbf{x}_{\mathbf{z}&lt;t}\right)\right]<br>$$<br>我们的目标只是单纯的从原来有序的输入序列变成了<strong>位置随机重排</strong>后的序列, 这样在做自回归时就能够看到原本不属于该位置的元素, 这样就能双向捕捉上下文.</p><blockquote><p>作者在这里强调, 仅生成一个置换后的新下标序列, 而输入序列的原文顺序是不变的. 沿用与原始输入相对应的位置编码, 并依靠<strong>Attention</strong>来实现对下标的置换. 因为在<strong>FineTuning</strong>期间文本的顺序是正常而<strong>不发生置换</strong>的.</p></blockquote><p>作者给出了一张对置换部分的阐述图, 假设当前要对$x_3$ 进行预测, 在置换语言模型中只能通过Attention来关注置换后序列在$3$ 前的输入: </p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/xlnet1.jpg" style="zoom: 67%;" /><p>例如左上角, 分解顺序为$3 \rightarrow 2 \rightarrow 4 \rightarrow 1$ 时, 只能对之前的隐态$\text{mem}$ 做Attention. 右上角, 分解顺序为$2 \rightarrow 4 \rightarrow 3 \rightarrow 1$ 时, 只能对置换顺序在$3$ 前的$2$ 和 $4$ 做Attention. 下面两幅图同理.</p><blockquote><p>是不是有些巧妙? 对, 置换的方法确实比较巧妙, 但是请思考一个问题, 假设置换后的顺序为$2 \rightarrow 4 \rightarrow 3 \rightarrow 1$, 马上就要预测$x_3$ 了, 由于打乱了顺序, 模型怎才能知道它要预测的是$x_3$ 还是 $x_1$ 呢?</p></blockquote><h3 id="Two-Stream-Self-Attention-for-Target-Aware-Representations"><a href="#Two-Stream-Self-Attention-for-Target-Aware-Representations" class="headerlink" title="Two - Stream Self - Attention for Target - Aware Representations"></a>Two - Stream Self - Attention for Target - Aware Representations</h3><h4 id="Target-Aware"><a href="#Target-Aware" class="headerlink" title="Target - Aware"></a>Target - Aware</h4><p>由于采用了置换的方式, 所以按照Transformer的方法处理目标可能会遇到一些问题.</p><p>如果我们沿用之前Transformer的方式, $h_{\theta}\left(\mathbf{x}_{\mathbf{z}&lt;t}\right)$ 代表$\mathbf{X}_{\mathbf{Z}&lt;t}$ 的隐态, 来计算下一个单词的分布:<br>$$<br>p_{\theta}\left(X_{z_{t}}=\right.\left.x \mid \mathbf{x}_{\mathbf{z}&lt;t}\right)=\frac{\exp \left(e(x)^{\top} h_{\theta}\left(\mathbf{x}_{\mathbf{z}&lt;t}\right)\right)}{\sum_{x^{\prime}} \exp \left(e\left(x^{\prime}\right)^{\top} h_{\theta}\left(\mathbf{x}_{\mathbf{z}&lt;t}\right)\right)}<br>$$<br>由于我们之前<strong>打乱</strong>了原有的句子顺序, 模型完全不知道该预测哪个Token了. 对于未知的所有Token, 它们有可能不在原来的位置上, 也有可能在原来的位置上. 换句话说, 如果使用Transformer的方法, 下一个要预测的Token分布与它原来的<strong>位置</strong>就没有关联了. </p><p>因此必须将下一个要预测的Token通过某种方式<strong>显式</strong>的告诉模型, “哦它才是打乱顺序后我该预测的下一个Token”, 这也就是论文中说的<strong>Target - Aware</strong>:<br>$$<br>p_{\theta}\left(X_{z_{t}}=x \mid \mathbf{x}_{z&lt;t}\right)=\frac{\exp \left(e(x)^{\top} g_{\theta}\left(\mathbf{x}_{\mathbf{z}&lt;t}, z_{t}\right)\right)}{\sum_{x^{\prime}} \exp \left(e\left(x^{\prime}\right)^{\top} g_{\theta}\left(\mathbf{x}_{\mathbf{z}&lt;t}, z_{t}\right)\right)}<br>$$<br>$g_{\theta}\left(\mathbf{x}_{\mathbf{z}&lt;t}, z_{t}\right)$ 代表加入目标位置信息的表示方法.</p><h4 id="Two-Stream-Self-Attention"><a href="#Two-Stream-Self-Attention" class="headerlink" title="Two - Stream Self - Attention"></a>Two - Stream Self - Attention</h4><p>因为置换把要预测的Token的<strong>位置信息</strong>直接搞掉了, 所以必须通过其他方式提供位置信息. 我们必须符合以下两个点的限制:</p><ol><li>预测$x_{z_t}$ 时, $g_{\theta}\left(\mathbf{x}_{\mathbf{z}&lt;t}, z_t\right) $ 不能$z_t$ 处的内容信息$x_{z_t}$, 只能使用$z_t$ 处的位置信息.</li><li>在预测一个位置上$j&gt;t$ 的Token$x_{z_j}$ 时, $g_{\theta}\left(\mathbf{x}_{\mathbf{z}&lt;t}, z_t\right) $ 应该将$x_{z_t}$ 编码来提供全部的上下文信息.</li></ol><p>基于这两点规则, XLNet采用<strong>双流注意力</strong>机制, 其中一种注意力提供<strong>上下文</strong>信息, 另一种提供<strong>位置</strong>信息.</p><ul><li>$h_\theta\left(\mathbf{x}_{\mathbf{z}\leq t}\right)$, 记为$h_{z_t}$, 表示包括$x_{z_t}$ 在内的编码的上下文信息, 与标准的Transformer相同, 被称作为<strong>内容流</strong>(Content Stream).</li><li>$g_\theta\left(\mathbf{x}_{\mathbf{z}_&lt;t}, z_t\right)$, 记为$g_{z_t}$, 表示仅包括$\mathbf{x}_{\mathbf{z} &lt; t}$ 在内的上下文信息和$z_t$ 的位置信息, 并没有$x_{z_t}$ 处的位置信息, 被称之为<strong>请求流</strong>(Query Stream).</li></ul><h5 id="Content-Stream-and-Query-Stream"><a href="#Content-Stream-and-Query-Stream" class="headerlink" title="Content Stream and Query Stream"></a>Content Stream and Query Stream</h5><p>下面就来以论文中的图为例子, 看一下具体的双流注意力在场景中的结构.</p><p>第一层的Query Stream被定义为一个可训练的向量, 记作$g_i^{(0)}=w$, 设置Content Stream是对应的Embedding, 记作$h_i^{(0)}=e(x_i)$. Self - Attention层记为$m=1,\dots,M$. </p><p>XLNet的每一层与Transformer - XL相同, 包括FFN, Layer Norm, 残差连接, 多头注意力… 这些细节都暂时省略掉, 到后面我们会串起来看.</p><p>先来看看最熟悉的Self - Attention, 这与Transformer中是保持一致的, 不要忘记它也被称为内容流:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/xlnet4.jpg" style="zoom: 50%;" /><p>Content Stream能使用<strong>包括其本身在内的所有上下文信息</strong>. 为了存储上下文信息, 所以Q依赖于上层对应位置的Content Stream, K和V均依赖于所有输入的隐态:<br>$$<br>h_{z_{t}}^{(m)} \leftarrow \text { Attention }\left(\mathrm{Q}=h_{z_{t}}^{(m-1)}, \mathrm{KV}=\mathbf{h}_{\mathrm{z} \leq t}^{(m-1)} ; \theta\right), \quad\left(\text {content stream: use both } z_{t} \text { and } x_{z_{t}}\right)<br>$$<br>下面来看看XLNet中独有的请求流$g_\theta$:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/xlnet5.jpg" style="zoom: 50%;" /><p>因为只用于<strong>发起请求</strong>, 请求流的Q只能由上一层的Query Stream生成, 而K和V不能使用当前位置的内容信息, 所以只能由<strong>其他位置</strong>的上下文信息决定:<br>$$<br>g_{z_{t}}^{(m)} \leftarrow \text { Attention }\left(\mathrm{Q}=g_{z_{t}}^{(m-1)}, \mathrm{KV}=\mathbf{h}_{\mathrm{z}&lt;t}^{(m-1)} ; \theta\right), \quad \text { (query stream: use } z_{t} \text { but cannot see } \left.x_{z_{t}}\right)<br>$$<br>对比一下Query Stream和Content Stream, 二者的不同点主要在<strong>Q的依赖来源</strong>和能否使用<strong>上层当前位置的内容信息</strong> $h_{z=t}^{(m-1)}$:<br>$$<br>\begin{aligned}<br>&amp;g_{z_{t}}^{(m)} \leftarrow \text { Attention }\left(\mathrm{Q}=g_{z_{t}}^{(m-1)}, \mathrm{KV}=\mathbf{h}_{\mathrm{z}&lt;t}^{(m-1)} ; \theta\right), \quad \text { (query stream: use } z_{t} \text { but cannot see } \left.x_{z_{t}}\right)\\<br>&amp;h_{z_{t}}^{(m)} \leftarrow \text { Attention }\left(\mathrm{Q}=h_{z_{t}}^{(m-1)}, \mathrm{KV}=\mathbf{h}_{\mathrm{z}&lt;t}^{(m-1)} ; \theta\right), \quad\left(\text {content stream: use both } z_{t} \text { and } x_{z_{t}}\right)<br>\end{aligned}<br>$$</p><h5 id="Permutation-Attention-Mask"><a href="#Permutation-Attention-Mask" class="headerlink" title="Permutation Attention Mask"></a>Permutation Attention Mask</h5><p>其实我们也不用做真正意义上的置换, 只需要将Attention中的<strong>Mask机制</strong>结合起来使用就好, 如下图:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/xlnet6.jpg" style="zoom: 50%;" /><p>图中的白点代表被Mask, 红点代表可以被Attend.</p><p>置换后的顺序为$3\rightarrow2\rightarrow4\rightarrow1$ 时, 请求流是不能看到当前正在预测的Token的内容信息的, 所以请求流矩阵<strong>主对角线</strong>一定都是被Mask的. 因为第一个要预测的就是位置$3$ 的内容, 所以矩阵第三行全都被Mask. 在预测出位置$3$ 后的内容后, 在请求流矩阵第二行的第三列就变得已知了. 在预测位置$4$ 和$1$ 时同理.</p><p>当搞懂了请求流矩阵后, 再看内容流矩阵, 完全符合我们之前对这两种流的定义: <strong>内容流矩阵不过是请求流矩阵主对角线没被Mask而已</strong>, 因为请求流的要求是当前要预测的内容不可知, 内容流却只是存储上下文, 没有这种限制.</p><h5 id="Particial-Prediction"><a href="#Particial-Prediction" class="headerlink" title="Particial Prediction"></a>Particial Prediction</h5><p>在实验过程中, 发现如果对所有置换后的序列输入进行预测, 会导致<strong>收敛缓慢</strong>, 难以优化. 作者只去预测置换后序列的<strong>最后几个</strong>Token, 这样置换后序列$\mathbf{z}$ 就被分成了两个部分, 分别是分割点$c$ 前的$\mathbf{z}_{\leq c}$, 和<strong>需要预测</strong>的分割点后的序列$\mathbf{z}_{&gt;c}$. 根据这个变化, XLNet的优化目标变为:<br>$$<br>\max _{\theta} \mathbb{E}_{\mathbf{z} \sim \mathcal{Z}_{T}}\left[\log p_{\theta}\left(\mathbf{x}_{\mathbf{z}&gt;c} \mid \mathbf{x}_{\mathbf{z} \leq c}\right)\right]=\mathbb{E}_{\mathbf{z} \sim \mathcal{Z}_{T}}\left[\sum_{t=c+1}^{|\mathbf{z}|} \log p_{\theta}\left(x_{z_{t}} \mid \mathbf{x}_{\mathbf{z}&lt;t}\right)\right]<br>$$</p><p>关于分割点, 作者假设了超参数$K$, 设$|\mathbf{z}| /(|\mathbf{z}|-c) \approx K$. 有$\frac{1}{K}$ 个Token需要被预测, 不需要预测的Token所对应的Query Stream是不需要被计算的, 这样能够节省计算资源.</p><h5 id="Integrate-Attention-into-Transformer-Backbone"><a href="#Integrate-Attention-into-Transformer-Backbone" class="headerlink" title="Integrate Attention into Transformer Backbone"></a>Integrate Attention into Transformer Backbone</h5><p>我们现在可以把除了双流Attention以外的结构加进来了, 仍然也都是Transformer中的组件, 只是位置编码采用的是<strong>相对位置编码</strong>(这里的相对位置编码和Transformer - XL还不一样, 后面会提及).</p><p>对于时间步$t=1,\dots,T$, 将含有相对位置编码的Attention记为$\text{RelAttn}$, 将Position - wise的Feed Forward记为$\text{PosFF}$, 整个双流Attention<strong>更新</strong>过程如下:<br>$$<br>\begin{array}{l}<br>\hat{h}_{z_{t}}^{(m)}=\text { LayerNorm }\left(h_{z_{t}}^{(m-1)}+\operatorname{RelAttn}\left(h_{z_{t}}^{(m-1)},\left[\tilde{\mathbf{h}}^{(m-1)}, \mathbf{h}^{(m-1)}_{\mathbf{z}_{\leq t}}\right]\right)\right) \\<br>h_{z_{t}}^{(m)}=\text { LayerNorm }\left(\hat{h}_{z_{t}}^{(m)}+\operatorname{PosFF}\left(\hat{h}_{z_{t}}^{(m)}\right)\right) \\<br>\hat{g}_{z_{t}}^{(m)}=\text { LayerNorm }\left(g_{z_{t}}^{(m-1)}+\operatorname{RelAttn}\left(g_{z_{t}}^{(m-1)},\left[\tilde{\mathbf{h}}^{(m-1)}, \mathbf{h}_{\mathbf{z}&lt;t}^{(m-1)}\right]\right)\right) \\<br>g_{z_{t}}^{(m)}=\text { LayerNorm }\left(\hat{g}_{z_{t}}^{(m)}+\operatorname{PosFF}\left(\hat{g}_{z_{t}}^{(m)}\right)\right)<br>\end{array}<br>$$</p><h3 id="Incorporating-Ideas-From-Transformer-XL"><a href="#Incorporating-Ideas-From-Transformer-XL" class="headerlink" title="Incorporating Ideas From Transformer - XL"></a>Incorporating Ideas From Transformer - XL</h3><p>因为Transformer - XL是最近颇有成效的AR模型, 所以作者尝试将它的两种最关键的技术<strong>相对编码</strong>和<strong>段级递归</strong>引入, 与XLNet结合. 作者引入了两种相对编码, Transformer - XL一模一样的<strong>相对位置编码</strong>和结合段问题添加的<strong>相对段编码</strong>.</p><h4 id="Segement-Level-Recurrence"><a href="#Segement-Level-Recurrence" class="headerlink" title="Segement Level Recurrence"></a>Segement Level Recurrence</h4><p>其实和Transformer - XL中的段级递归一样. 假设现在有两段长序列$\mathbf{s}$, $\tilde{\mathbf{x}}=\mathbf{s}_{1: T}$ 与 $\mathbf{x}=\mathbf{s}_{T+1: 2 T}$, 其置换后的序列分别是$\tilde{\mathbf{z}}=\text{Permutation}([1\cdots T])$ 和 $\mathbf{z}=\text{Permutation}([T+1\cdots 2T])$. 那么计算第二段$\mathbf{x}$的内容流可以按照如下方式更新:<br>$$<br>h_{z_{t}}^{(m)} \leftarrow \text { Attention }\left(\mathrm{Q}=h_{z_{t}}^{(m-1)}, \mathrm{KV}=\left[\tilde{\mathbf{h}}^{(m-1)}, \mathbf{h}_{\mathbf{z}_{\leq t}}^{(m-1)}\right] ; \theta\right)<br>$$<br>将Memory中缓存的隐态$\tilde{\mathbf{h}}^{(m-1)}$ 和$\mathbf{x}$ 产生的隐态$\mathbf{h}_{\mathbf{z}_{\leq t}}^{(m-1)}$ Concat起来, 一起计算K和V.</p><h4 id="Multiple-Segments"><a href="#Multiple-Segments" class="headerlink" title="Multiple Segments"></a>Multiple Segments</h4><p>XLNet与BERT一样, 在预训练阶段也是随机选择了两段连续或不连续的上下文合并到一起, 作为一个序列的两个Segment输入进XLNet中, 但只对连续内容使用Segment Memory. 两段之间采用的符号也和BERT类似, <code>[CLS, A, SEP, B, SEP]</code>.</p><h4 id="Relative-Segment-Encoding"><a href="#Relative-Segment-Encoding" class="headerlink" title="Relative Segment Encoding"></a>Relative Segment Encoding</h4><p>注意, 这是<strong>相对段编码</strong>而并非相对位置编码. XLNet中的段编码方式与BERT中的绝对段编码方式不同, XLNet采用相对段编码.</p><p>对于给定的序列位置$i$ 和 $j$, 当这两个位置都位于同一个Segment时, 令$s_{ij}=s_+$, 否则$s_{ij}=s_-$. 其中$s_+$ 和$s_-$ 是每个注意力头<strong>分别</strong>学习的. 这种编码只关注<strong>两个Token是否位于同个Segment内</strong>, 而并<strong>不考虑来自于哪个特定的Segment</strong>, 这个角度来看与相对编码的思想是保持一致的.</p><p>在计算注意力时, $i$ 对$j$ 的注意力是这样计算的:<br>$$<br>a_{i j}=\left(\mathbf{q}_{i}+\mathbf{b}\right)^{\top} \mathbf{s}_{i j}<br>$$<br>$q_i$ 是标准注意力计算来的Query Vector, $\mathbf{b}$ 是一个可学习的参数, 最后将这个注意力$a_{ij}$和正常的注意力<strong>相加</strong>得到最终的注意力.</p><p>相对编码增强了模型的泛化能力, 并提供了完成多段输入任务的可能性.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Transformer - XL结合了<strong>段循环</strong>和<strong>注意力机制</strong>, 缓解了Transformer受限于输入长度的问题, 也节省了计算资源的开销, 减少了不必要的计算. 并在段循环的基础上做出相对位置编码的改进.</p><p>XLNet用置换的方法将AE模型引以为傲的双向捕捉上下文能力移植到AR模型上, 将Transformer - XL的特性也组装到自己的模型中.</p><p>从大家对XLNet的看法来说, 它是<strong>饱受争议</strong>的. 虽然根据所给出的实验数据来说, XLNet能够取得比较好的效果, 但是对比BERT使用的13G数据, XLNet使用了158G数据, 而BERT才仅仅使用了13G数据, 到底是模型本身的方法导致效果好, 还是数据堆上去的, 我个人认为很难说. 后人基于XLNet所作出的后续研究非常少. 相反, BERT的魔改模型都快一窝了. </p><p>我记得前段时间还有人说XLNet完爆BERT, 大家吵得不可开交. 具体见<a href="https://zhuanlan.zhihu.com/p/74812464" target="_blank" rel="noopener">XLNet团队：只要公平对比，BERT毫无还手之力</a>(可能也有点标题党的嫌疑).</p><p>另外吐槽一下, 因为XLNet整篇论文特别碎, 也很乱, 所以才一直拖着, 毕竟写这篇博客实在是太费劲了. 虽然明确的指出了BERT确实存在的问题, 但XLNet解决方案不是很优雅, 感觉像一堆零件<strong>拼凑</strong>到了一起.</p><p>虽然XLNet诟病繁多, 但XLNet为AR模型实现双向上下文捕捉能力提供了可能, 这种带领大家突破BERT框架的思维模式还是很重要的..</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ELMo, GPT, BERT</title>
      <link href="/posts/3996.html"/>
      <url>/posts/3996.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文的前置知识:</p><ul><li>RNN</li><li>Transformer</li><li>Language Model</li></ul></blockquote><h1 id="ELMo-GPT-BERT"><a href="#ELMo-GPT-BERT" class="headerlink" title="ELMo, GPT, BERT"></a>ELMo, GPT, BERT</h1><p>本文是对ELMo, GPT, BERT三个模型的结构介绍以及个人理解, 多图预警.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>由于NLP领域的<strong>数据标注</strong>难度非常大, 成本很高, 所以必须要采用<strong>无监督或半监督</strong>的方法来在数据集不够充足的情况下提高模型的<strong>通用表示能力</strong>. 从Word2Vec(2013)到Glove(2014), 再到ELMo(2017), GPT-1(2018), BERT(2018), GPT-2(2019), GPT-3(2020)… 前人已经在预训练领域做了不少的探索. 由于NLP领域在自然语言的表示上非常困难, 特别注重于<strong>感知</strong>和<strong>理解</strong>, 现有的算法对机器要求又比较高, 所以NLP的发展进度比CV是滞后一些的, 在CV上早就出现了预训练模型, 也就不难理解为什么会在预训练上摸索了.</p><p>之所以把ELMo, GPT, BERT这三个模型放在一起说, 是因为BERT作为NLP中的又一个<strong>里程碑</strong>(并不是因为BERT模型本身创新点有多少, 而是它代表本阶段的所有技术融合)将这三个模型进行了对比. 这三个模型像是NLP近四年在<strong>预训练领域</strong>的探索过程的缩影, 从ELMo出现以来, NLP的ImageNet时代就来临了. 除此外, 它们三个都与<strong>单词结合上下文表示</strong>有关. 但无论哪种预训练模型, 都离不开强大的数据集和高算力支撑.</p><p>本文的图片来自于:</p><ul><li><p><a href="https://jalammar.github.io/illustrated-bert/" target="_blank" rel="noopener">The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)</a></p></li><li><p><a href="https://jalammar.github.io/illustrated-gpt2/" target="_blank" rel="noopener">The Illustrated GPT-2 (Visualizing Transformer Language Models)</a></p></li><li><p><a href="https://jalammar.github.io/how-gpt3-works-visualizations-animations/" target="_blank" rel="noopener">How GPT3 Works - Visualizations and Animations</a></p></li><li><p>ELMo, GPT, BERT的原论文.</p></li><li><p>芝麻街那几个小人的图片来源出自网络.</p><p>其余图片的具体来源不再注明是来自于哪篇文章的, 请自行到对应模型的文章查找. 上述三篇文章均来自<code>jalammar</code>, 选择他的文章做图片来源的理由太简单了, 并不是因为内容准确(自然语言不具有像数学语言一样的精确性), 而是因为颜值高. </p></li></ul><p>当然Word2vec我认为没必要再细说了, 现在所有的词嵌入都是基于Word2vec的, Word2vec更像是一个”<strong>死</strong>“的稠密向量, 单词嵌入后就是唯一的表示, 不能根据其在句子中的位置和上下文关系而改变其含义, 这就为处理<strong>一词多义</strong>问题添加了难度.</p><h2 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h2><p>ELMo全称是<strong>Embeddings from Language Models</strong>, 出自<a href="https://arxiv.org/abs/1802.05365" target="_blank" rel="noopener">Deep contextualized word representations</a>. </p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/elmo.jpg" style="zoom: 25%;" /><p>上面这个小人就是芝麻街里的角色ELMo. ELMo是一种基于Embedding的高级词向量表示. </p><p>前面说过, Word2vec具有局限性, 不能够表示一个单词的多种意思. 作者认为一个好的词向量有两个优点:</p><ol><li>能表示复杂的单词特性, 例如语义和语法信息.</li><li>词向量能适应多种语境有不同的体现, 即结合语境的一词多义.</li></ol><p>ELMo也是基于这两个点出发, 尝试通过更深的方法来实现基于<strong>上下文</strong>来表示单词的模型, 能够学习到更复杂的语义特征, 所以也说它是更高级的词向量. </p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/elmo1.png" style="zoom: 33%;" /><p>在上图中, ELMo指出对于不同的语义, 具有不同的词向量.</p><p>ELMo在执行不同的Task时或不同的语境时, 同一个词得到的词嵌入也可能是不同的.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/elmo2.png" style="zoom:50%;" /><h3 id="Bidirectional-Language-Models"><a href="#Bidirectional-Language-Models" class="headerlink" title="Bidirectional Language Models"></a>Bidirectional Language Models</h3><p>虽然ELMo自称是双向语言模型, 但实则不然. 先抛出这个结论, 在本小节结束的时候再点出.</p><p>论文中指出, 高层LSTM能够捕捉结合语境的<strong>词意</strong>信息, 而底层LSTM能捕捉<strong>语法</strong>上的特征, 把它们结合起来, 非常有利于下游任务的执行, 这也恰恰就是作者认为的优秀的词向量的特性.</p><p>ELMo采用<strong>无监督</strong>的方法, 利用<strong>语言模型</strong>的特点, 对句子本身的单词进行移位预测, 依赖串行结构以当前时刻以前的单词预测本时刻单词输出的性质也叫作<strong>自回归</strong>(Transformer的<strong>Decoder</strong>也是自回归结构). 文本数量非常庞大, 无需任何标签就能从中学习.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/elmo3.png" style="zoom: 50%;" /><p>如图中, LSTM在已知前三个单词”let’s”, “stick”, “to”的情况下需要预测”improvisation”.</p><p>另外, 引入从左向右和从右向左的两个<strong>单向LSTM</strong>能更好的结合上下文, 获得更复杂的语义表示. 对于多层的LSTM, 每层输出的隐态被作为下一层的输入继续传递.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/elmo4.png" style="zoom:50%;" /><p>这两个单向而反向的LSTM的权重将在预训练完成后被保存, 在正式预测时派上用场.</p><p>对于上述过程, 我们用数学语言来描述. 对于序列中的$N$ 个Token, $\left(t_{1}, t_{2}, \ldots, t_{N}\right)$, 第$k$ 个Token前的序列被描述为$\left(t_{1}, \ldots, t_{k-1}\right)$, 那么根据语言模型, 序列$\left(t_{1}, t_{2}, \ldots, t_{N}\right)$ 的概率就应该表示为:<br>$$<br>p\left(t_{1}, t_{2}, \ldots, t_{N}\right)=\prod_{k=1}^{N} p\left(t_{k} \mid t_{1}, t_{2}, \ldots, t_{k-1}\right)<br>$$<br>如果把上面的方向叫做<strong>Forward</strong>, 相应的, 与之相反的方向就称为<strong>Backward</strong>, 描述如下:<br>$$<br>p\left(t_{1}, t_{2}, \ldots, t_{N}\right)=\prod_{k=1}^{N} p\left(t_{k} \mid t_{k+1}, t_{k+2}, \ldots, t_{N}\right)<br>$$<br>对与语言模型, 普遍采用<strong>极大似然</strong>来调整两个单向LSTM的参数, 只要最大化向前和向后的对数概率就好:<br>$$<br>\begin{array}{l}<br>\sum_{k=1}^{N}\left[\log p\left(t_{k} \mid t_{1}, \ldots, t_{k-1} ; \Theta_{x}, \overrightarrow{\Theta}_{L S T M}, \Theta_{s}\right)\right.<br>\left.+\log p\left(t_{k} \mid t_{k+1}, \ldots, t_{N} ; \Theta_{x}, \overleftarrow{\Theta}_{L S T M}, \Theta_{s}\right)\right]<br>\end{array}<br>$$<br>其中$\overrightarrow{\Theta}_{L S T M}$ 和 $\overleftarrow{\Theta}_{L S T M}$ 分别代表两个LSTM的参数, $\Theta_{x}$ 代表Token的表示, $\Theta_{s}$ 代表Softmax层在两个LSTM维护的参数. 在论文中提到, 在某些任务中加入L2正则$\lambda \lVert\mathbf{w}\rVert _{2}^{2}$ 效果可能会更好.</p><p>再强调一遍, 这两个LSTM是单向工作的:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/elmo7.png" style="zoom: 67%;" /><h3 id="ELMo-Architecture"><a href="#ELMo-Architecture" class="headerlink" title="ELMo Architecture"></a>ELMo Architecture</h3><p>在ELMo中, 并非直接采用Word2vec作为输入, 而是在Embedding后采用了字符级的CNN作为替代输入. 作者认为字符级CNN是一种更加<strong>不敏感</strong>的上下文表示, 也有可能Word2vec本身就限制了多语义的表达.</p><p>如果输入用$\mathbf{x}_{k}^{L M}$ 表示, 一共有$L$ 层LSTM, 在论文中采取$L = 2$. 而Forward的LSTM最终的隐态输出为$\overrightarrow{\mathbf{h}}_{k, j}^{L M}$, Backward输出为$\overleftarrow{\mathbf{h}}_{k, j}^{L M}$, 那么$k$ 应该能被$2L+1$ 个参数表示, 称这个参数集合为$R_k$, 并将两个LSTM的隐态合并后有:<br>$$<br>\begin{aligned}<br>\mathbf{h}_{k, j}^{L M}&amp;=\left[\overrightarrow{\mathbf{h}}_{k, j}^{L M} ; \overleftarrow{\mathbf{h}}_{k, j}^{L M}\right] \\<br>R_{k} &amp;=\left\{\mathbf{x}_{k}^{L M}, \overrightarrow{\mathbf{h}}_{k, j}^{L M}, \overleftarrow{\mathbf{h}}_{k, j}^{L M} \mid j=1, \ldots, L\right\} \\<br>&amp;=\left\{\mathbf{h}_{k, j}^{L M} \mid j=0, \ldots, L\right\}<br>\end{aligned}<br>$$<br>为了更好的解释ELMo对下游任务是如何运作的, 将$R_k$和所有参数都表示为一个向量:<br>$$<br>\mathbf{E} \mathbf{L M o}_{k}=E\left(R_{k} ; \mathbf{\Theta}_{e}\right)<br>$$<br>对下游任务, ELMo对不同的Task采用了不同的权重.<br>$$<br>\mathbf{E} \mathbf{L} \mathbf{M} \mathbf{o}_{k}^{t a s k}=E\left(R_{k} ; \Theta^{t a s k}\right)=\gamma^{t a s k} \sum_{j=0}^{L} s_{j}^{t a s k} \mathbf{h}_{k, j}^{L M}<br>$$<br>$\text { s }^{\text {task}}$ 是经过Softmax标准化后的权重, $\gamma^{\text {task}}$ 是一个缩放因子. 对于每层LSTM, 它的标准化权重都是分别计算的, 最终是将所有层LSTM的隐态分别加以Softmax, 再相加求和, 这样对于不同的Task, ELMo就能学习到词向量在面对不同任务时的不同线性组合.</p><p>如下图, 最终的ELMo向量是由两个LSTM拼接后的向量和每层LSTM在经过Softmax后得到的权重加权求和而成的.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/elmo5.png" style="zoom:50%;" /><p>下面是ELMo在面对不同任务时Softmax所学习到的权重情况的可视化, 确实证明了ELMo对不同的任务下, 关注的权重不同, 对同样的词语有不同的表达:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/elmo6.jpg" style="zoom: 67%;" /><blockquote><ul><li>SRL: Semantic role labeling.</li><li>Coref: Coreference resolution.</li><li>SNLI: Textual entailment.</li><li>SQuAD: Question Answer.</li><li>SST-5: Sentiment analysis.</li></ul></blockquote><h3 id="How-to-Use-ELMo"><a href="#How-to-Use-ELMo" class="headerlink" title="How to Use ELMo"></a>How to Use ELMo</h3><p>在使用ELMo的时候, 要冻结两个LSTM的参数. 最简单的用法就是把LSTM<strong>最后一层</strong>的隐态输出作为词向量使用, 也可以老老实实将每层的LSTM隐态输出都加权求和. 具体怎么使用, 还是要结合<strong>任务复杂程度</strong>来确定.</p><p>作者在论文中提到, 可以将$\mathbf{E} \mathbf{L} \mathbf{M} \mathbf{o}_{k}^{t a s k}$ 和 输入$\mathbf{x}_{k}$ 一起concat起来, 即$\left[\mathbf{x}_{k} ; \mathbf{E} \mathbf{L} \mathbf{M} \mathbf{o}_{k}^{t a s k}\right]$, 加强表示, 然后作为词向量送入RNN, 也可以将$\mathbf{x}_{k}$ 替换为$\mathbf{h}_{k}$, 和ELMo向量concat起来, 即$\left[\mathbf{h}_{k} ; \mathbf{E} \mathbf{L} \mathbf{M} \mathbf{o}_{k}^{t a s k}\right]$.</p><p>总而言之, <strong>怎么用都行</strong>.</p><h3 id="Fake-Bidirectional-Language-Model"><a href="#Fake-Bidirectional-Language-Model" class="headerlink" title="Fake Bidirectional Language Model"></a>Fake Bidirectional Language Model</h3><p>现在来说说小节开始时遇到的那个问题:</p><p>ELMo所谓的双向实际上是通过<strong>两个单向且反向的LSTM</strong>实现的, 并不是直接使用双向LSTM, ELMo并非是原论文中所称的”deep bidirectional language model (biLM)”, 这点在BERT论文中曾有提到过:</p><blockquote><ul><li><p>BERT uses masked language models to enable pre-trained <strong>deep bidirectional</strong> representations. This is also in <strong>contrast</strong> to Peters et al. (2018a), which uses a shallow concatenation of <strong>independently</strong> trained left-to-right and right-to-left LMs.</p></li><li><p>Similar to ELMo, their model is feature-based and <strong>not deeply bidirectional</strong>.</p></li></ul></blockquote><p>当你了解BERT后, 会发现作者虽然一直在强调双向语言模型, 但ELMo并不是真正意义上的双向语言模型.</p><p><strong>那么为什么不采用双向LSTM实现呢?</strong></p><p>两个单向的LSTM和一个双向LSTM之间的区别, 对于单层的LSTM, 双向和单向差别无非就是hidden state是否concat或者add到一起. 但对于多层来说, 涉及到序列前后<strong>信息泄露</strong>的问题, 深层双向LSTM会被泄露上下文词语的位置信息, 导致模型学到了不该学习的东西, 也就失去了预训练的效果.</p><h2 id="Transformer-Review"><a href="#Transformer-Review" class="headerlink" title="Transformer Review"></a>Transformer Review</h2><p>之所以把Transformer Review放在这里, 是因为剩下的两个模型和Transformer关系很大, 如果有基础的可以直接跳过.</p><h3 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h3><p>如果你还不了解Transformer, 在这个小节能帮你<strong>快速回顾</strong>关于Transformer的部分知识, 因此所有的描述都非常简陋, 更详细的内容请看我之前写的<code>&lt;Transformer精讲&gt;</code>. </p><p>Transformer采用Seq2Seq架构, 分为Encoder和Decoder两个部分:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformerreview1.png" style="zoom: 33%;" /><p>Encoder包括一个前FFN层和一个自注意力层, 结构如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformerreview2.png" style="zoom:33%;" /><p>Decoder包括FFN层, 一个对接Encoder的自注意力层, 和一个Mask自注意力层, 结构如下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformerreview3.png" style="zoom:33%;" /><h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self - Attention"></a>Self - Attention</h3><h4 id="General-Self-Attention"><a href="#General-Self-Attention" class="headerlink" title="General Self - Attention"></a>General Self - Attention</h4><p>这个注意力说的是不加Mask的自注意力, 即出现在Encoder的第一层和Decoder的第二层. 每个自注意力层通常有多个头, 类似于CV中的卷积核, 能够抽取不同角度的特征. 我们先不考虑多头, 反正它的操作也是一致的, 最后再合并即可.</p><p>但是经过自注意力的vector的大小是不发生变化的(下图可能有shape错误). 自注意力机制一共有三步.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformerreview4.png" style="zoom:33%;" /><p>首先, 每个Embedding的Token分别经过三个矩阵, 创建query vector, key vector, value vector.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformerreview5.png" style="zoom:33%;" /><p>其次, 根据Transformer中提出的缩放点积注意力, 分别计算上下文其他Token对当前的Score, 并经过Softmax, 得到注意力权重:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformerreview6.png" style="zoom:33%;" /><p>最后, 将注意力权重与对应的value vector加权求和得到结果.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformerreview7.png" style="zoom:33%;" /><h4 id="Masked-Self-Attention"><a href="#Masked-Self-Attention" class="headerlink" title="Masked Self - Attention"></a>Masked Self - Attention</h4><p>在Encoder中, 所有Embedding是并行输入的, 这并不会影响Decoder的输出. 但我们提过, Decoder具有自回归性:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformerreview13.png" style="zoom: 33%;" /><p>在解码时, 解码是单向的, 如果我们将所有数据并行输入, 那么就会造成<strong>信息泄露</strong>. </p><p>Mask正是为保证Decoder的自回归性和计算并行而存在的, 因为在解码时, Decoder应该只能看到当前时刻应该解码的内容和之前解码过的内容, 如下图:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformerreview14.png" style="zoom:33%;" /><p>灰色的向量代表Decoder不应该看见那部分信息. 我们添加一个Mask, 使得并行输入的矩阵的对角线以上的部分都不能被Decoder所解码:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformerreview9.png" style="zoom: 33%;" /><p>我们来复现这个过程, 在Softmax之前先计算出Score矩阵:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformerreview10.png" style="zoom: 50%;" /><p>具体对Score加Mask的方法就是将主对角线上的所有元素都变为负无穷:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformerreview11.png" style="zoom:50%;" /><p>这样在做Softmax时, 主对角线上的信息会自动被屏蔽为0:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformerreview12.png" style="zoom:50%;" /><p>这也就等价于Decoder对当前时刻以后的内容是不可知的:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformerreview8.png" style="zoom:33%;" /><h2 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h2><p>GPT的想法非常简单, 因为在NLP中有相当多种类的任务, 尽管有大量的未标注数据, 但用于指定任务的标注过的数据却很少, 这不能很好地评判已训练过的模型性能. GPT尝试用一种通用, <strong>任务无关</strong>的模型结构解决所有NLP问题. 对于不同的Task, 只需要在无监督的预训练后进行监督的Fine tune就行了, 这与CV界的Transfer Learning相同:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/vggpretrain.png" style="zoom:50%;" /><p>GPT的三个模型几乎都是<strong>相同架构</strong>, 只是有非常非常少量的改动. 但一代比一代更大, 也更烧钱. 所以我对GPT系列的特点就是: <strong>钞能力, 大就完事了.</strong> 其影响力和花费的成本是成正比的.</p><p>先抛出三代GPT的论文出处:</p><ul><li>GPT - 1: <a href="https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf" target="_blank" rel="noopener">Improving Language Understanding by Generative Pre-Training</a></li><li>GPT - 2: <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" target="_blank" rel="noopener">Language Models are Unsupervised Multitask Learners</a> (可能需要魔法才能看)</li><li>GPT - 3: <a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener">Language Models are Few-Shot Learners</a></li></ul><p>因为架构相同, 后两篇论文大多数内容都是对增加参数后成果的展示. 时间不充裕建议只看第一篇, 因为每代区别不大, 本节<strong>一代为主</strong>, <strong>二代为辅</strong>, 三代先挖个坑, 以后会补.</p><blockquote><p>jalammar并没有做GPT - 1的图, 并且每代间又没有明显的结构区别, 所以就直接用二代的图了.</p></blockquote><p>GPT - 1其实并没有广泛的引起人们的关注, 反倒是GPT - 2和3让它火了一把. 最出名的就是GPT - 2生成的那篇关于独角兽的文章:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/独角兽.jpg" style="zoom:50%;" /><p>对此, 我个人认为除了生成的结果十分惊艳外, 确实有些炒作的成分. 惊艳是因为第一次见到机器能够生成如此有趣而附带一些逻辑的内容. 炒作主要原因是媒体夸大事实宣传, 博人眼球, 次要原因不难理解OpenAI也是需要科研资金的嘛. GPT目前<strong>相对于</strong>其他的NLP模型来说, 强是肯定的, 只是说代价太大了. 第二代和第三代强调了GPT有<strong>Few shot Learning</strong>和<strong>Zero shot Learning</strong>的潜力, 在巨大数据集的情况下, 模型甚至都没有收敛… 三代甚至不需要Fine tune…</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/gpt13.jpg" style="zoom: 67%;" /><blockquote><p>Few - shot: 给出一个自然语言任务的一些范例, 但这些范例不能更新权重(算是一个限制, 给出更多信息).</p><p>One - shot: 仅给出一个范例, 让模型给出结果.</p><p>Zero - shot: 不给任何范例, 直接让模型给出结果.</p><p>在论文中, GPT - 3的Zero - shot和One - shot在某些任务上确实超过SOTA, 但相较人类还有很大差距. Few - shot比前二者有很大提升. 说明在没给出足够多信息的条件下, GPT - 3对<strong>问题理解能力</strong>还是比较差的.</p></blockquote><h3 id="Why-Transformer-Decoder"><a href="#Why-Transformer-Decoder" class="headerlink" title="Why Transformer Decoder?"></a>Why Transformer Decoder?</h3><p>言归正传, 来说GPT的具体结构. 受Transformer影响, GPT(Generative Pre-Training)采用<strong>Transformer</strong>作为基本的Block结构. 作者指出LSTM将预测能力限制在<strong>短距离</strong>内, 所以才采用使用Attention的Transformer作为<strong>长距离</strong>的信息抽取器. 当然, 这里只使用<strong>Decoder</strong>, 就需要去掉Decoder对Encoder的自注意力层, <strong>Mask</strong>后的自注意力保护了Decoder的<strong>自回归性</strong>:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/gpt3.jpg" style="zoom: 33%;" /><p>在GPT一代中, 采用了12个Decoder堆叠.</p><h3 id="Unsupervised-pre-training"><a href="#Unsupervised-pre-training" class="headerlink" title="Unsupervised pre-training"></a>Unsupervised pre-training</h3><p>与标准语言模型一样, 对于无监督语料库中的Token $\mathcal{U}=\left\{u_{1}, \ldots, u_{n}\right\}$, 都采用<strong>极大似然</strong>优化:<br>$$<br>L_{1}(\mathcal{U})=\sum_{i} \log P\left(u_{i} \mid u_{i-k}, \ldots, u_{i-1} ; \Theta\right)<br>$$<br>其中$k$ 为上下文窗口, $\Theta$ 是神经网络参数, 它们会用<strong>随机梯度下降</strong>得到训练.</p><p>对于GPT来说, 选用Transformer的<strong>Decoder</strong>做最基本的Block. 将输入一层一层嵌套, 经过$n$ 个Transformer Block, 其数学表达为:<br>$$<br>\begin{aligned}<br>h_{0} &amp;=U W_{e}+W_{p} \\<br>h_{l} &amp;=\text { transformer_block }\left(h_{l-1}\right) \forall i \in[1, n] \\<br>P(u) &amp;=\operatorname{softmax}\left(h_{n} W_{e}^{T}\right)<br>\end{aligned}<br>$$<br>其中, $U=\left(u_{-k}, \ldots, u_{-1}\right)$ 是上下文向量, $n$ 为Decoder的层数.</p><p>$W_e$ 是Token的Embedding矩阵(一代嵌入维度只有768一种):</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/gpt4.jpg" style="zoom:50%;" /><p>$W_p$ 是Positional Embedding矩阵. 与Transformer不同, GPT的位置编码并非是通过三角函数计算来的, 而是通过训练学习到的.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/gpt5.png" style="zoom: 50%;" /><p>$h_0$ 是嵌入后的向量和学习到的位置编码向量之和:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/gpt6.png" style="zoom:50%;" /><p>然后就经过Transformer Block的自回归得到Decoder部分的输出$h_l$ :</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/gpt7.png" style="zoom:50%;" /><p>对Decoder部分的输出做个总结吧(因为重复太多次了, 省略了再跑一遍的过程, 想看完整的去<a href="https://jalammar.github.io/illustrated-gpt2/" target="_blank" rel="noopener">这里</a>):</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/gpt11.png" style="zoom: 50%;" /><p>在得到最后输出前, 最后还需要再与<strong>Embedding相乘</strong>一次, 再通过Softmax得到结果$P(u)$: </p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/gpt9.png" style="zoom:50%;" /><p>“logits”就是在Softmax前的值, 后一般接一个Softmax输出标准化的概率. 在GPT二代中, 不再像一代直接选择概率最高的单词, 而是从<code>top_k</code> 中以某个<strong>概率</strong>选择单词, 这样来避免陷入永无止境的循环之中.</p><p>参数量实在是太大了, 每个部分占到的参数都非常非常多:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/gpt12.png" style="zoom:50%;" /><h3 id="Supervised-Fine-tuning"><a href="#Supervised-Fine-tuning" class="headerlink" title="Supervised Fine-tuning"></a>Supervised Fine-tuning</h3><p>首先要加载预训练的权重, 然后要让预训练模型适应特定的任务, 对<strong>额外参数</strong>进行微调, 所以还要继续在当前基础上再加Linear层适应输出. 假设给出数据集$\mathcal{C}=(x^{1}, \ldots, x^{m}, y)$, 在通过一堆Transformer Block后得到的最终输出为$h_{l}^{m}$, 最后加一个Linear层参数为$W_y$, 有:<br>$$<br>P\left(y \mid x^{1}, \ldots, x^{m}\right)=\operatorname{softmax}\left(h_{l}^{m} W_{y}\right)<br>$$<br>有了标注的数据, 仍然采用<strong>极大似然</strong>进行优化:<br>$$<br>L_{2}(\mathcal{C})=\sum_{(x, y)} \log P\left(y \mid x^{1}, \ldots, x^{m}\right)<br>$$<br>无监督学习反作用于监督学习可以有一些提升. 所以在Fine tune这里用了前面无监督预训练的Loss做<strong>辅助训练</strong>. $\lambda$ 是超参, 论文中设置$\lambda=0.5$.<br>$$<br>L_{3}(\mathcal{C})=L_{2}(\mathcal{C})+\lambda \ast L_{1}(\mathcal{C})<br>$$<br>在微调中, 只有最后外接的$W_y$ 和句子之间的代表分隔符的Token的Embedding(看Overview and GPT in Specific Task)是需要额外进行学习的.</p><h3 id="More-Details"><a href="#More-Details" class="headerlink" title="More Details"></a>More Details</h3><p>一些超参的设置请参考原论文, 这里只说容易被忽略的点.</p><h4 id="BPE"><a href="#BPE" class="headerlink" title="BPE"></a>BPE</h4><p>BPE(byte pair encode), 也称为<strong>字节对编码</strong>, 其主要目的是<strong>数据压缩</strong>, 现在已经被作为重要的提升NLP模型性能的算法. 这种编码拆除了语言学特性, 但通过统计学方法更好的解决语言类问题. 做法请参考论文<a href="https://arxiv.org/abs/1508.07909" target="_blank" rel="noopener">Neural Machine Translation of Rare Words with Subword Units</a>, 我认为日后有必要把几个subword技巧做个对比.</p><h4 id="GeLU"><a href="#GeLU" class="headerlink" title="GeLU"></a>GeLU</h4><p>在GPT中使用的激活函数是<strong>GeLU</strong>(<strong>G</strong>aussian <strong>E</strong>rror <strong>L</strong>inerar <strong>U</strong>nits)不是ReLU! </p><p>GeLU原文中作者给出的近似公式:<br>$$<br>\text{GeLU}(x) = 0.5x(1 + \text{tanh}[\sqrt{\frac{2}{\pi}}(x+0.044715x^3)])<br>$$<br>具体内容请见论文<a href="https://arxiv.org/abs/1606.08415" target="_blank" rel="noopener">Gaussian Error Linear Units (GELUs)</a>.</p><h4 id="WarmUp"><a href="#WarmUp" class="headerlink" title="WarmUp"></a>WarmUp</h4><p>Fine tune阶段使用了线性衰减的WarmUp, 请参考我在<code>&lt;Transformer精讲&gt;</code>中的内容.</p><h3 id="Overview-and-GPT-in-Specific-Task"><a href="#Overview-and-GPT-in-Specific-Task" class="headerlink" title="Overview and GPT in Specific Task"></a>Overview and GPT in Specific Task</h3><h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/gpt1.jpg" style="zoom: 50%;" /><p>GPT的结构非常简单, 实质就是12个Transformer Decoder的堆叠, 最后再根据任务需要<strong>接入结构</strong>以完成最终任务.</p><blockquote><p>这里能看出GPT的一个缺点, 虽然Self - Attention能够很好地利用全局的文章信息, 但是由于Decoder自回归性的限制, GPT是一个<strong>单向语言模型</strong>, 在Summary部分会继续与其他模型进行对比.</p></blockquote><h4 id="Input-Transformations"><a href="#Input-Transformations" class="headerlink" title="Input Transformations"></a>Input Transformations</h4><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/gpt2.jpg" style="zoom: 33%;" /><p>执行不同的任务时, 需要对输入进行相应的调整. 在微调的过程中, 对于每种任务的开始和结束都需要加入<strong>标记</strong>$\langle s\rangle,\langle e\rangle$, 下文不再强调, 两段不同的内容之间需要加入分隔符$ $ $ .</p><h5 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h5><p>做分类任务非常简单, 在原输入两侧加上开始结束标记即可, 可以直接使用原来的模型微调.</p><h5 id="Textual-entailment"><a href="#Textual-entailment" class="headerlink" title="Textual entailment"></a>Textual entailment</h5><p>文本蕴含任务, 将premise和hypothesis拼接起来, 在二者之间加入分隔符.</p><h5 id="Similarity"><a href="#Similarity" class="headerlink" title="Similarity"></a>Similarity</h5><p>句子相似度任务, 在句子A和句子B之间加上分隔符, 再交换句子A和句子B的位置, 分别传入Transformer, 将二者结果add起来, 传入Linear.</p><h5 id="Question-Answering-and-Commonsense-Reasoning"><a href="#Question-Answering-and-Commonsense-Reasoning" class="headerlink" title="Question Answering and Commonsense Reasoning"></a>Question Answering and Commonsense Reasoning</h5><p>QA和常识推理任务, 将内容文本和回答分别组合, 并在之间加上分隔符, 传入Transformer, 分别经过Linear, 经过Softmax, 最后得到概率分布.</p><p>GPT还能做其他的事情, 例如音乐生成之类的, 感兴趣自己了解下:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/gpt10.png" style="zoom: 50%;" /><h3 id="Difference-Between-GPT-1-and-GPT-2"><a href="#Difference-Between-GPT-1-and-GPT-2" class="headerlink" title="Difference Between GPT - 1 and GPT - 2"></a>Difference Between GPT - 1 and GPT - 2</h3><p>GPT一代到二代仅发生了几个不同:</p><ol><li><p>用了更大的数据集, 尤其是网页文本, 40G.</p></li><li><p>增加了海量参数, 并推出了几个不同的版本, 一个比一个大:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/gpt8.png" style="zoom:50%;" /><p>除了使用的Decoder个数不同, 它们的Embedding维度也是不同的.</p></li><li><p>直接去掉了Fine - tune层, 直接输入任务和所需的内容就能得到输出.</p></li><li><p>将Layer Norm放到了每个子层的输入前, 并且在最后一个自注意力层后添加了Layer Norm. 通过放缩权重更换了残差层的初始方式.</p></li></ol><h3 id="Postscript"><a href="#Postscript" class="headerlink" title="Postscript"></a>Postscript</h3><p>其实GPT一直都存在伦理上的问题. 有些人认为语言模型训练来自于语料, 语料的偏见会导致模型带有偏见. 而且GPT当初被OpenAI认为过于危险, 可能在只经过fine tune后被恶意滥用, 没有将这个庞然大物的参数开放.</p><blockquote><p>但可以通过下面几个链接去体验GPT - 2(可能需要魔法):</p><ul><li><a href="http://textsynth.org/" target="_blank" rel="noopener">Text Synth</a></li><li><a href="https://talktotransformer.com/" target="_blank" rel="noopener">Talk to Transformer</a></li></ul></blockquote><p>最后说一些自己的感想, 不想看的可以跳过这段.</p><p>GPT经历了三代, 有那么一点<strong>哲学</strong>和<strong>讽刺</strong>的意味. 第三代的1750亿参数几乎已经逼近了参数量的极限, 但实际上GPT产生的文章并没有媒体文章渲染的那么恐怖, GPT生成的内容还是经常犯一些常识性的<strong>错误</strong>, 即使是靠堆参数, 模型也并没有真的做到”Natural Language Understanding”, “大”真的意味着它<strong>智能</strong>了吗? 我们接触的世界是一个<strong>多模态</strong>的世界, 而计算机不能真正触及我们所接触的任何物体, 只能通过我们提供的<strong>数据</strong>来做到”认知”. 尽管我对AI发展持乐观态度, 但现在人们所强调的技术方法绝对不能有效的构造一个<strong>智能体</strong>, 虽是一条过渡的必经之路, 但有些<strong>矫枉过正</strong>. 至少人类距离强人工智能还有非常遥远的一段距离(如果非要形容, 距离可能是光年为单位), <strong>任重而道远</strong>.</p><h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p>也受到<strong>Transformer</strong>的影响, BERT像GPT一样把Transformer加入了自然语言理解中, 也尝试训练一个预训练模型, 只经过微调就能适应NLP领域的各种任务. 现在的BERT已经遍地开花, 很多NLP任务都是BERT或者BERT的魔改在屠榜.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bert.jpg" style="zoom:25%;" /><p>上面这个小人就是BERT, 与ELMo同样都来自芝麻街. 其实除了BERT, 大家还拼凑过Grover, ERNIE, Big Bird…这些全是芝麻街的小人, 只不过有些凑的比较强行就是了.</p><p>BERT出自论文<a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>, 我认为这篇论文的<strong>附录</strong>才是本体… 正文着重于讲BERT的训练方式, 与前人模型的区别, 以及取得的效果. 虽然正文也很重要, 但附录里才有BERT的具体实现方法, 以及与ELMO, GPT的对比. 所以在看这篇论文时, 一定记得看Appendix.</p><p>BERT(Bidirectional Encoder Representations from Transformers)延续了GPT的Pre - train + Fine tune的思路, BERT也是冲着<strong>通用语言模型</strong>的目标去的, 并适配了一套对任何任务<strong>不用变更输入模式</strong>的训练方法. </p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bert7.png" style="zoom:50%;" /><p>在预训练和微调好了之后, 只需要接上一层FFN和Softmax就能做到分类:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bert8.png" style="zoom: 50%;" /><p>另外, BERT可以<strong>抽取词向量</strong>, 近期已经被作为Word2vec的替代者了:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bert18.png" style="zoom:50%;" /><p>在论文中, 作者对BERT提取的不同特征效果做了对比, 这也证实了BERT具有特征抽取能力:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bert19.png" style="zoom:50%;" /><p>似乎将最后4层拼接起来的F1得分要高一些.</p><h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p>在BERT论文中指出了ELMo和GPT的不足: 它们不是<strong>双向语言模型</strong>, 所以BERT采用了<strong>多层双向的Transformer Encoder</strong>作为堆叠基础结构. </p><blockquote><p>所谓的双向Transformer实际上是Transformer Encoder, 所谓的”双向”是体现在MLM中(见Masked Language Model).</p><p>We note that in the literature the <strong>bidirectional Transformer</strong> is often referred to as a “<strong>Transformer encoder</strong>” while the left-context-only version is referred to as a “Transformer decoder” since it can be used for text generation.</p></blockquote><p>在论文中, BERT发表了两个不同的版本, $\mathbf{B E R T}_{\mathbf{B A S E}}$ 和 $\mathbf{B E R T}_{\mathbf{LARGE}}$.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bert9.png" style="zoom: 33%;" /><p>根据论文中的参数给出对比二者的对比:</p><table><thead><tr><th align="center">模型</th><th align="center">堆叠层数$L$</th><th align="center">隐藏层大小$H$</th><th align="center">自注意力层头数$A$</th><th align="center">共计参数</th></tr></thead><tbody><tr><td align="center">BERT (Base)</td><td align="center">12</td><td align="center">786</td><td align="center">12</td><td align="center">110M</td></tr><tr><td align="center">BERT (Large)</td><td align="center">24</td><td align="center">1024</td><td align="center">16</td><td align="center">340M</td></tr><tr><td align="center">Transformer</td><td align="center">6</td><td align="center">512</td><td align="center">8</td><td align="center">/</td></tr></tbody></table><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bert10.png" style="zoom: 33%;" /><p>此外, 在论文脚注中提到, FFN的大小为$4H$, 也采用了<strong>GeLU</strong>作为激活函数, 并使用了10000步的<strong>WarmUp</strong>.</p><h3 id="Input-Output-Representations"><a href="#Input-Output-Representations" class="headerlink" title="Input / Output Representations"></a>Input / Output Representations</h3><h4 id="CLS"><a href="#CLS" class="headerlink" title="[CLS]"></a>[CLS]</h4><p>在BERT中, 永远都将第一个位置输入<strong>分类提示符</strong><code>[CLS]</code>, 如果执行的是分类任务, 第一个位置最终会输出一个向量, 作为分类依据. </p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bert11.png" style="zoom:50%;" /><p>BERT接受一些系列单词输入, 经过Transformer Encoder的堆叠, 得到输出.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bert12.png" style="zoom:50%;" /><p>其实对于BERT来说, <code>[CLS]</code> 到底放在哪是无所谓的, 只是放在第一个习惯于我们理解. </p><p>在执行<strong>分类下游任务</strong>时(不是训练时), 其他位置无论有多少隐态输出, 我们都忽略, 只看<code>[CLS]</code> 对应位置上的输出, 也就是第一个位置:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bert14.png" style="zoom:50%;" /><p>进行分类任务所搭建的网络也是在第一个位置上继续的, 结构也可以任意调整:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bert15.png" style="zoom:50%;" /><h4 id="SEP"><a href="#SEP" class="headerlink" title="[SEP]"></a>[SEP]</h4><p>对于<strong>任何任务</strong>, BERT的输入永远都是将<strong>一对句子</strong>放在同一个序列中, 无论这对句子是真正连续的上下文还是随机拼接的. BERT句子和句子之间用分隔符<code>[SEP]</code> 隔开, 在结尾也要加上一个<code>[SEP]</code>.</p><h4 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h4><p>输入编码由三个部分组成:</p><ol><li>Token Embedding: 就是每个Token的Embedding.</li><li>Segment Embedding: 该Embedding起到了区分句子A和句子B的作用, 对A和B分别加以不同的编码.</li><li>Position Embedding: 与GPT一样, 位置信息也是学习来的, 而非公式计算.</li></ol><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bert2.jpg" style="zoom: 67%;" /><h3 id="Pre-trained-Task"><a href="#Pre-trained-Task" class="headerlink" title="Pre - trained Task"></a>Pre - trained Task</h3><p>BERT采用了两种无监督任务, 使BERT能学到双向的上下文信息, 而且这种信息不是通过Forward和Backward获取的, 而是一次性获得的, 这就促使BERT成为一个双向语言模型. 在论文中, 指出预训练BERT的损失函数为下述两个任务的损失和.</p><h4 id="Masked-Language-Model"><a href="#Masked-Language-Model" class="headerlink" title="Masked Language Model"></a>Masked Language Model</h4><p>在双向深度的结构中, 会伴随着信息泄露, 在原文中称为”<strong>See itself</strong>“, 模型能够通过深层网络来自两个方向的信息得知此处的内容. 为了训练这种网络, BERT训练时采用<strong>随机Mask</strong>的方法, 使BERT必须通过上下文预测出这个位置的Token, 然后用<strong>极大似然</strong>来调整参数. 这其实就是在模仿我们做<strong>完形填空</strong>, 该方法也就是BERT能被称之为双向语言模型的根本原因.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bert16.png" style="zoom:50%;" /><blockquote><p><strong>这种方法为什么有效?</strong> </p><p>因为BERT使用的是Transformer Encoder, 与Decoder不同, 是一种不带有自回归性的结构, 在Mask后, 无论怎么阅读句子, 对已经被Mask的位置内容都不可知, 只能强迫BERT根据上下文进行推测.</p></blockquote><p>因此, 该任务中, BERT直接根据上下文推测, 而非像ELMo一样采用两个单向的结构:<br>$$<br>P\left(w_{i} \mid w_{1}, \ldots, w_{i-1}, w_{i+1}, \ldots, w_{n}\right)<br>$$</p><blockquote><p>你看这式子, 是不是和<strong>CBOW</strong>非常像?</p></blockquote><p>但是在Fine tune的时候不可能对单词进行Mask, 这样就会导致预训练和微调的不匹配, 为缓解这种问题, 在每个句子中, 有15%的词会被选中, 在选中单词后有三种可能性:</p><ol><li><strong>80%</strong>的概率将被选中的单词替换为<code>[MASK]</code>.</li><li><strong>10%</strong>的概率将被选中的单词替换为<strong>随机词</strong>.</li><li><strong>10%</strong>的概率对被选中的单词<strong>不进行替换</strong>.</li></ol><p>这就给BERT一种非常迷惑的感觉, “<strong>即使没有Mask的单词仍然有可能是错的, 知道的也要预测, 不知道的还要预测</strong>“. 这就更加强迫BERT除了学到上下文关联外, 对每个词必须有理解能力.</p><p>另外, 随机替换在语料充足时并不会降低太多的模型性能, 因为它只有1.5%的几率发生.</p><p>当然, 因为每次只预测15%的Token, 模型的<strong>收敛速度</strong>会下降.</p><h4 id="Next-Sentence-Prediction"><a href="#Next-Sentence-Prediction" class="headerlink" title="Next Sentence Prediction"></a>Next Sentence Prediction</h4><p>另一种无监督任务比较好理解, 就是让BERT根据句子A和句子B, 在<code>[CLS]</code>处输出这两个句子是否是连贯的上下文. 无论是QA问题, 还是自然语言推理(NLI), 都是建立在理解相邻文本该关系基础之上的.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bert17.png" style="zoom:50%;" /><p>在挑选两个句子时, 句子A与B是否相关各有50%的几率.</p><p>论文中给出示例如下:</p><pre><code>Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]Label = IsNextInput = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]Label = NotNext</code></pre><blockquote><p>##less是word piece产生的, 先挖个坑以后填.</p></blockquote><h3 id="Fine-Tune"><a href="#Fine-Tune" class="headerlink" title="Fine - Tune"></a>Fine - Tune</h3><p>从预训练到微调, BERT可以很轻松的发生转换:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bert1.jpg" style="zoom:50%;" /><p>$C$ 为<code>[CLS]</code>对应位置的最终输出, $T_i$ 代表第$i$ 个Token对应位置的输出. $E_i$ 代表Embedding.</p><p>在不同任务上的微调方式可能是不同的, 但仍然不用对模型结构进行改动:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/bert3.png"/><p>图中的四类任务分别为:</p><ul><li><p>(a): 句子匹配分类任务.</p></li><li><p>(b): 单个句子分类任务.</p></li><li><p>(c): QA类任务. SQuAD中问题的答案一定在原文中会出现, 所以输出的是在原文中起始和结束的位置. </p></li><li><p>(d): 命名体识别任务.</p></li></ul><p>(a)和(b)是Sequence级的任务, (c)和(d)是Token级的任务. BERT在这些任务上都有具体的处理方法, 详情参见论文.</p><h3 id="BERT-in-One-Word"><a href="#BERT-in-One-Word" class="headerlink" title="BERT in One Word"></a>BERT in One Word</h3><p>总的来说, BERT像一个近些年人们在NLP上探索成果的<strong>融合</strong>, 但结合了自监督学习, Token Mask + 双向LM训练, Pre - train + Finetune的思想, 将Transformer, 位置编码一起使用. 现在NLP已经进入到<strong>BERT时代</strong>, 几乎由BERT魔改得到的模型都能取得显著的成果.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>这部分, 我们把ELMo, GPT, BERT三个模型放在一起来看, 可能有些零碎.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/elmogptbert.jpg" style="zoom: 67%;" /><ul><li><p>在三个模型中, 只有BERT是真正能够捕捉所有层的上下文信息的. 这受益于Transformer中的自注意力机制, 将所有Token之间的距离直接缩短到1, 加权求和. ELMo和GPT都是单向捕捉信息的.</p></li><li><p>三个模型的Basic Block: LSTM, Transformer Decoder, Transformer Encoder. GPT和BERT都是用Transformer组件作为基础Block.</p></li><li><p>GPT在预训练时并没有引入<code>[CLS]</code>和<code>[SEP]</code>, BERT全程引入.</p></li><li><p>GPT和BERT是基于<strong>微调</strong>的方法, 模型结构不用发生变化, 而ELMo是基于<strong>特征</strong>的方法, 仅用于抽取特征.</p></li><li><p>对Input来讲, ELMo在Embedding后用<strong>字符级CNN</strong>, GPT采用<strong>BPE</strong>, BERT用了<strong>Word Piece</strong>. GPT有位置编码, BERT有位置编码和段编码. 这些不同也与模型的输入方式有关.</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 词向量 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch之张量基础操作</title>
      <link href="/posts/42255.html"/>
      <url>/posts/42255.html</url>
      
        <content type="html"><![CDATA[<h1 id="Pytorch之张量基础操作"><a href="#Pytorch之张量基础操作" class="headerlink" title="Pytorch之张量基础操作"></a>Pytorch之张量基础操作</h1><p>整理内容顺序来自龙龙老师的<code>&lt;深度学习与PyTorch入门实战教程&gt;</code>, 根据个人所需情况进行删减或扩充. 如果想要自己创建新的模块, 这些操作都是基本功, 需要掌握扎实.</p><h2 id="张量数据类型"><a href="#张量数据类型" class="headerlink" title="张量数据类型"></a>张量数据类型</h2><p>下表摘自<a href="https://pytorch.org/docs/master/tensors.html#tensor-doc" target="_blank" rel="noopener">Pytorch官方文档</a>, 介绍了现在pytorch中所有涉及到的数据类型.</p><table><thead><tr><th>Data type</th><th>dtype</th><th>CPU tensor</th><th>GPU tensor</th></tr></thead><tbody><tr><td>32-bit floating point</td><td><code>torch.float32</code> or <code>torch.float</code></td><td><code>torch.FloatTensor</code></td><td><code>torch.cuda.FloatTensor</code></td></tr><tr><td>64-bit floating point</td><td><code>torch.float64</code> or <code>torch.double</code></td><td><code>torch.DoubleTensor</code></td><td><code>torch.cuda.DoubleTensor</code></td></tr><tr><td>16-bit floating point <a href="https://pytorch.org/docs/master/tensors.html#id3" target="_blank" rel="noopener">1</a></td><td><code>torch.float16</code> or <code>torch.half</code></td><td><code>torch.HalfTensor</code></td><td><code>torch.cuda.HalfTensor</code></td></tr><tr><td>16-bit floating point <a href="https://pytorch.org/docs/master/tensors.html#id4" target="_blank" rel="noopener">2</a></td><td><code>torch.bfloat16</code></td><td><code>torch.BFloat16Tensor</code></td><td><code>torch.cuda.BFloat16Tensor</code></td></tr><tr><td>32-bit complex</td><td><code>torch.complex32</code></td><td></td><td></td></tr><tr><td>64-bit complex</td><td><code>torch.complex64</code></td><td></td><td></td></tr><tr><td>128-bit complex</td><td><code>torch.complex128</code> or <code>torch.cdouble</code></td><td></td><td></td></tr><tr><td>8-bit integer (unsigned)</td><td><code>torch.uint8</code></td><td><code>torch.ByteTensor</code></td><td><code>torch.cuda.ByteTensor</code></td></tr><tr><td>8-bit integer (signed)</td><td><code>torch.int8</code></td><td><code>torch.CharTensor</code></td><td><code>torch.cuda.CharTensor</code></td></tr><tr><td>16-bit integer (signed)</td><td><code>torch.int16</code> or <code>torch.short</code></td><td><code>torch.ShortTensor</code></td><td><code>torch.cuda.ShortTensor</code></td></tr><tr><td>32-bit integer (signed)</td><td><code>torch.int32</code> or <code>torch.int</code></td><td><code>torch.IntTensor</code></td><td><code>torch.cuda.IntTensor</code></td></tr><tr><td>64-bit integer (signed)</td><td><code>torch.int64</code> or <code>torch.long</code></td><td><code>torch.LongTensor</code></td><td><code>torch.cuda.LongTensor</code></td></tr><tr><td>Boolean</td><td><code>torch.bool</code></td><td><a href="https://pytorch.org/docs/master/tensors.html#torch.BoolTensor" target="_blank" rel="noopener"><code>torch.BoolTensor</code></a></td><td><code>torch.cuda.BoolTensor</code></td></tr></tbody></table><p>一般情况下, 常用的tensor类型只有<strong>float</strong>, <strong>int</strong>, <strong>bool</strong>. 至于使用多少位精度需要结合实际情况而定, 毕竟精度高了训练时间就长了. 其他的类型基本不需要去关心, 需要时再查查文档就好.</p><p>在CPU和在GPU上的tensor是完全不同的, 它们甚至不属于同一个类. 在CPU和GPU上训练的两个tensor除非迁移到同一个位置上, 否则不能发生交互.</p><p><strong>为什么没有<code>String</code> 类型的tensor?</strong></p><p>因为在深度学习中, 文本不会直接输入到框架中. 虽然在pytorch中没有string直接的数据类型, 但是可以根据需要把string做embedding或者one-hot转换成张量输入.</p><h2 id="创建张量"><a href="#创建张量" class="headerlink" title="创建张量"></a>创建张量</h2><p>大多数张量的基本操作也会穿插在这一节里面.</p><h3 id="自动数据类型"><a href="#自动数据类型" class="headerlink" title="自动数据类型"></a>自动数据类型</h3><p>创建张量, 使用<code>torch.tensor()</code>. 它可以创建一个标量(dim=0)或者一个张量. 如果传入的对象是一个数, 那么则创建一个标量, 如果传入的对象是一个<code>list</code>, 则创建一个张量. 这种方式是不指定数据类型的, tensor会自动分配数据类型.</p><p>使用<code>Tensor.shape</code>或<code>Tensor.size()</code>可以查看张量的大小.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token comment" spellcheck="true"># 创建一个标量</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">925</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.size():'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 创建一个张量</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">925</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'b.size()'</span><span class="token punctuation">,</span> b<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># output:</span><span class="token comment" spellcheck="true"># a.size():  torch.Size([])</span><span class="token comment" spellcheck="true"># b.size(): torch.Size([1])</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>标量和张量是不同的. 对于标量来说, 它本身就是一个单独的数, <strong>没有dim和size</strong>这一说. 使用<code>Tensor.dim()</code>和<code>len(Tensor.size())</code>是等价的, 对于标量来说, 结果应该是0.</p><p>同时, 对于标量来说, 使用<code>Tensor.item()</code>能获取标量的值.</p><p>可以通过<code>Tensor.type()</code>查看tensor的类型:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.type():'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>type<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># output:</span><span class="token comment" spellcheck="true"># a.type(): torch.LongTensor</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>使用<code>torch.tensor()</code>创建张量会被自动分配数据类型, 对于浮点和整型是不一样的:</p><pre class="line-numbers language-python"><code class="language-python">c <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">925</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'c.type():'</span><span class="token punctuation">,</span> c<span class="token punctuation">.</span>type<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># output:</span><span class="token comment" spellcheck="true"># c.type(): torch.FloatTensor</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>在GPU上的tensor和CPU上的tensor完全不是一个类型:</p><pre class="line-numbers language-python"><code class="language-python">c <span class="token operator">=</span> c<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'c.type():'</span><span class="token punctuation">,</span> c<span class="token punctuation">.</span>type<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># output:</span><span class="token comment" spellcheck="true"># c.type(): torch.cuda.FloatTensor</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p><code>Tensor.cuda()</code>可以返回一个tensor在cuda上的引用, 也就是将tensor移动到GPU上去.</p><h3 id="指定类型"><a href="#指定类型" class="headerlink" title="指定类型"></a>指定类型</h3><p>与<code>torch.tensor()</code>不同的是, 如果向指定数据类型的函数中传入一个数, 不再是创建一个指定类型的标量, 而是创建一个指定数据类型和指定dim和shape的tensor. 例如, <code>torch.FloatTensor(3)</code>是创建一个维度为1, shape为[3]的tensor. tensor中的数据<strong>全是随机</strong>的.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a:'</span><span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""outputa: tensor([9.2196e-41, 0.0000e+00, 7.0295e+28])a.shape: torch.Size([3])"""</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'b:'</span><span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'b.shape:'</span><span class="token punctuation">,</span> b<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""output:b: tensor([[0., 0., 0., 0.],        [0., 0., 0., 0.],        [0., 0., 0., 0.]])b.shape: torch.Size([3, 4])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>当然, 也可以传入<code>list</code>直接对tensor初始化:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a:'</span><span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.type():'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>type<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""output:a: tensor([1., 2., 3.])a.type(): torch.FloatTensor"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="通过nump创建"><a href="#通过nump创建" class="headerlink" title="通过nump创建"></a>通过nump创建</h3><p>也可以通过numpy创建tensor.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'numpy.a:'</span><span class="token punctuation">,</span> a<span class="token punctuation">)</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'torch.a:'</span><span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""numpy.a: [[1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.]]torch.a: tensor([[1., 1., 1., 1.],        [1., 1., 1., 1.],        [1., 1., 1., 1.]], dtype=torch.float64)"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>当然创建好了以后是一个<code>float64</code>的tensor, 从numpy导入的float其实是double类型.</p><h3 id="其他创建方法"><a href="#其他创建方法" class="headerlink" title="其他创建方法"></a>其他创建方法</h3><p>大多函数与numpy相同, 有numpy基础的建议直接跳过.</p><h4 id="基本创建方法"><a href="#基本创建方法" class="headerlink" title="基本创建方法"></a>基本创建方法</h4><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 均匀分布初始化</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'rand:'</span><span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 空张量(全是随机数)</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'empty:'</span><span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 随机整数</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'ranint:'</span><span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""output:rand: tensor([[0.5267, 0.8344, 0.6042],        [0.1859, 0.3207, 0.1803]])empty: tensor([[1.0561e-38, 1.0653e-38, 1.4013e-45],        [0.0000e+00, 1.4013e-45, 0.0000e+00]])ranint: tensor([[8, 7],        [1, 8]])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>torch.randint(low, high, size)</code>是遵循切片规则的, 即生成的随机整数包含左侧不包含右侧.</p><h4 id="xx-like"><a href="#xx-like" class="headerlink" title="xx_like"></a>xx_like</h4><p>和numpy一样, 也有<code>xx_like()</code>这个函数, 能按传入的tensor的shape创建tensor:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'b:'</span><span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># output:</span><span class="token comment" spellcheck="true"># b: tensor([[1., 1., 1.],</span><span class="token comment" spellcheck="true">#        [1., 1., 1.]])</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>xx</code>可以替换成前面所说的任意初始化的函数名称.</p><h4 id="arange-range"><a href="#arange-range" class="headerlink" title="arange / range"></a>arange / range</h4><p>numpy老朋友了. <code>torch.arange()</code>生成遵循切片规则[min, max)的tensor, 支持步长. <code>torch.range()</code>与前者功能相同, 因为完全可以代替, 可能会被移除, 不建议使用.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'[0, 6):'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'[0, 6), 步长为2:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># ouput:</span><span class="token comment" spellcheck="true"># [0, 6): tensor([0, 1, 2, 3, 4, 5])</span><span class="token comment" spellcheck="true"># [0, 6), 步长为2: tensor([0, 2, 4])</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="full"><a href="#full" class="headerlink" title="full"></a>full</h4><p>使用<code>torch.full()</code>创建一个指定shape的tensor并填满某个值.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'填满6:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'创建值为6的标量:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># output:</span><span class="token comment" spellcheck="true"># 填满6: tensor([[6., 6., 6.],</span><span class="token comment" spellcheck="true">#         [6., 6., 6.]])</span><span class="token comment" spellcheck="true"># 创建值为6的标量: tensor(6.)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="randn"><a href="#randn" class="headerlink" title="randn"></a>randn</h4><p>使用<code>torch.randn()</code>按照(0, 1)初始化, 使用<code>torch.normal()</code>按照指定均值和方差进行初始化.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 0, 1初始化</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'randn:'</span><span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 指定均值和标准差</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>normal<span class="token punctuation">(</span>mean<span class="token operator">=</span>torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> std<span class="token operator">=</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'normal:'</span><span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""randn: tensor([[-1.1174,  0.8060,  0.1918],        [-0.1511,  0.3734, -0.6192]])normal: tensor([-2.1135e+00,  4.9261e-01, -9.9956e-01,  3.7895e-01, -4.1920e-01,        -1.6493e-01,  3.6504e-01, -1.1884e-01, -1.1261e-03, -4.7203e-02])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="linspace-logspace"><a href="#linspace-logspace" class="headerlink" title="linspace / logspace"></a>linspace / logspace</h4><p><code>torch.linspace()</code>和<code>torch.arange()</code>非常相似, 只不过给出的是<strong>范围和所需的元素个数</strong>.</p><p><code>torch.logspace()</code>是<code>torch.linspace()</code>的对数版本.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'[0, 10), 2:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'[0, 10), 3:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'log[0, 1), 3:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>logspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""[0, 10), 2: tensor([ 0., 10.])[0, 10), 3: tensor([ 0.,  5., 10.])log[0, 1), 3: tensor([ 1.0000,  3.1623, 10.0000])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="ones-zeros-eye"><a href="#ones-zeros-eye" class="headerlink" title="ones / zeros / eye"></a>ones / zeros / eye</h4><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'0阵:\n'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'1阵:\n'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'单位阵:\n'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>eye<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""0阵: tensor([[0., 0., 0.],        [0., 0., 0.]])1阵: tensor([[1., 1., 1.],        [1., 1., 1.]])单位阵: tensor([[1., 0., 0.],        [0., 1., 0.]])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="randperm"><a href="#randperm" class="headerlink" title="randperm"></a>randperm</h4><p>这个函数说一下功能就懂了, 是用来做<strong>shuffle</strong>的.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a:'</span><span class="token punctuation">,</span> a<span class="token punctuation">)</span>index <span class="token operator">=</span> torch<span class="token punctuation">.</span>randperm<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'index:'</span><span class="token punctuation">,</span> index<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[index]:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""a: tensor([[0.7391, 0.1663, 0.1362, 0.9353],        [0.2951, 0.9289, 0.3369, 0.8836],        [0.2730, 0.8966, 0.7737, 0.5760]])index: tensor([0, 1])a[index]: tensor([[0.7391, 0.1663, 0.1362, 0.9353],        [0.2951, 0.9289, 0.3369, 0.8836]])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>顺带一提, <code>Tensor.numel()</code>能得到tensor中的所有参数个数.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'numel:'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># numel: 6</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="默认数据类型"><a href="#默认数据类型" class="headerlink" title="默认数据类型"></a>默认数据类型</h3><p>在使用<code>torch.tensor()</code>创建的数据类型默认是<code>torch.FloatTensor</code>, 可以使用<code>torch.set_default_tensor_type(tensor_data_type)</code>来设置默认创建的tensor类型.</p><h2 id="索引和切片"><a href="#索引和切片" class="headerlink" title="索引和切片"></a>索引和切片</h2><p>索引访问与python基本一致, python的list用的比较熟的建议跳过索引访问. 多了一些其他的新东西.</p><h3 id="索引访问"><a href="#索引访问" class="headerlink" title="索引访问"></a>索引访问</h3><p>就是python中list的访问方法, 完全一致.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[0].shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[0][0].shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[0][0][23][24].shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">23</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">24</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[0][0][23][24]:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">23</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">24</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""a.shape: torch.Size([4, 3, 28, 28])a[0].shape: torch.Size([3, 28, 28])a[0][0].shape: torch.Size([28, 28])a[0][0][23][24].shape: torch.Size([])a[0][0][23][24]: tensor(0.4280)"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>通过切片访问多个元素:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[:2].shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[:2, :1, :, :].shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[:2, 1:, :, :].shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[:2, -1:, :, :].shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""a.shape: torch.Size([4, 3, 28, 28])a[:2].shape: torch.Size([2, 3, 28, 28])a[:2, :1, :, :].shape: torch.Size([2, 1, 28, 28])a[:2, 1:, :, :].shape: torch.Size([2, 2, 28, 28])a[:2, -1:, :, :].shape: torch.Size([2, 1, 28, 28])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>通过步长:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[:, :, 0:28:2, 0:28:2].shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">28</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">28</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[:, :, ::2, ::2].shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># a[:, :, 0:28:2, 0:28:2].shape: torch.Size([4, 3, 14, 14])</span><span class="token comment" spellcheck="true"># a[:, :, ::2, ::2].shape: torch.Size([4, 3, 14, 14])</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="特殊用法"><a href="#特殊用法" class="headerlink" title="特殊用法"></a>特殊用法</h3><h4 id="index-select"><a href="#index-select" class="headerlink" title="index_select"></a>index_select</h4><p>多了一个<code>Tensor.index_select(dim, index)</code>函数, 像是对切片的封装, 不知道速度有没有提升, 在python中函数好像比切片要快一些, 记不太清了. 反正这个函数用起来是比较麻烦, <code>index</code>还必须是tensor类型的.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>index_select<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># torch.Size([2, 3, 28, 28])</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h4 id="auto-filled"><a href="#auto-filled" class="headerlink" title="auto filled"></a>auto filled</h4><p><code>...</code>代表了任意多的维度, 能根据其他维度<strong>自动填充</strong>, 当维度能够根据其他值推断出来的时候特别方便.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[...].shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[0, ...].shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[:, 1, ...].shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[..., :2].shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""a.shape: torch.Size([4, 3, 28, 28])a[...].shape: torch.Size([4, 3, 28, 28])a[0, ...].shape: torch.Size([3, 28, 28])a[:, 1, ...].shape: torch.Size([4, 28, 28])a[..., :2].shape: torch.Size([4, 3, 28, 2])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="masked-select"><a href="#masked-select" class="headerlink" title="masked select"></a>masked select</h4><p>通过<code>torch.masked_select(x, mask)</code>能用Mask来筛选元素.</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'x:'</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span>mask <span class="token operator">=</span> x<span class="token punctuation">.</span>ge<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'mask:'</span><span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'masked_select:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>masked_select<span class="token punctuation">(</span>x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""x: tensor([[-0.7403, -1.3733,  0.8203],        [-0.0259,  0.4284,  0.7480]])mask: tensor([[0, 0, 1],        [0, 0, 1]], dtype=torch.uint8)masked_select: tensor([0.8203, 0.7480])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注意, 选择后的结果是<strong>Flatten</strong>的, 而非保持原来的shape.</p><h4 id="take"><a href="#take" class="headerlink" title="take"></a>take</h4><p>用的不是很多, 通过<code>torch.take(src, index_tensor)</code>能按照打平后的index进行访问.</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'x:'</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'第2和第4个元素:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>take<span class="token punctuation">(</span>x<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""x: tensor([[0, 1, 2],        [3, 4, 5]])第2和第4个元素: tensor([2, 4])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="维度变换"><a href="#维度变换" class="headerlink" title="维度变换"></a>维度变换</h2><p>很多函数也和numpy类似.</p><h3 id="view-reshape"><a href="#view-reshape" class="headerlink" title="view / reshape"></a>view / reshape</h3><p><code>Tensor.view(size)</code>能将tensor变形, 和reshape一样, 只要保证<strong>数据总数不变</strong>就能够进行shape变化.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>a <span class="token operator">=</span> a<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>a <span class="token operator">=</span> a<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">784</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""torch.Size([4, 1, 28, 28])torch.Size([4, 28, 28])torch.Size([4, 784])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>时刻注意每个dim所对应的含义, 否则在恢复时数据会失去原来的意义, 全部打乱掉. 最好是对数据的维度和意义进行<strong>追踪</strong>.</p></blockquote><p><code>Tensor.view(size)</code>和<code>Tensor.reshape(size)</code>的差别不是很大, 但仍有差别, 在后面说<code>transpose</code>的时候会提到.</p><h3 id="squeeze-unsqueeze"><a href="#squeeze-unsqueeze" class="headerlink" title="squeeze / unsqueeze"></a>squeeze / unsqueeze</h3><p>这一对函数主要是对维度进行提升或压缩.</p><p><code>Tensor.unsqueeze()</code>:</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>a <span class="token operator">=</span> x<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>a <span class="token operator">=</span> x<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>a <span class="token operator">=</span> x<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>a <span class="token operator">=</span> x<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>a <span class="token operator">=</span> x<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># wrong</span><span class="token comment" spellcheck="true"># a = x.unsqueeze(5)</span><span class="token triple-quoted-string string">"""torch.Size([4, 1, 28, 28])torch.Size([1, 4, 1, 28, 28])torch.Size([4, 1, 28, 28, 1])torch.Size([4, 1, 28, 28, 1])torch.Size([4, 1, 1, 28, 28])torch.Size([1, 4, 1, 28, 28])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>提升的维度也是遵循切片规则的, 区间为[-dim - 1, dim + 1).</p><p>与<code>Tensor.unsqueeze()</code>相反, <code>Tensor.squeeze</code>用于无用维度压缩.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""torch.Size([1, 32, 1, 1])torch.Size([32])torch.Size([32, 1, 1])torch.Size([1, 32, 1])torch.Size([1, 32, 1, 1])torch.Size([32, 1, 1])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="expand-repeat"><a href="#expand-repeat" class="headerlink" title="expand / repeat"></a>expand / repeat</h3><p>这两个函数从最终效果来说完全等价, 但过程不同.</p><p><code>Tensor.expand(size)</code>实际上是在做<strong>BroadCasting</strong>, 被动复制数据, 只有在需要时候才复制. 而<code>Tensor.repeat(copy_times)</code>是直接复制. 建议使用前者减小内存压力.</p><blockquote><p>对broadcast不理解的可以查看<a href="https://www.runoob.com/numpy/numpy-broadcast.html" target="_blank" rel="noopener">NumPy 广播(Broadcast)</a>, 这是一个很重要的机制, 广播能减少内存消耗或减少我们的操作.</p></blockquote><p><code>Tensor.expand(size)</code>:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">14</span><span class="token punctuation">,</span> <span class="token number">14</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> b<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span>expand<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""torch.Size([4, 32, 14, 14]) torch.Size([1, 32, 1, 1])torch.Size([4, 32, 14, 14])torch.Size([1, 32, 1, 1])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>此处填入-1代表维度<strong>保持不变</strong>.</p><p><code>Tensor.repeat(copy_times)</code>传入的是在每个dim上<strong>复制的次数</strong>:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""torch.Size([1, 32, 1, 1])torch.Size([4, 1024, 1, 1])torch.Size([4, 32, 1, 1])torch.Size([4, 32, 32, 32])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="transpose-t-permute"><a href="#transpose-t-permute" class="headerlink" title="transpose / t / permute"></a>transpose / t / permute</h3><p>这三个使用频率非常高.</p><p><code>Tensor.t()</code>就是转置, 只能对<strong>2d-tensor</strong>使用, 否则会报错:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""torch.Size([2, 3])torch.Size([3, 2])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>Tensor.transpose()</code>能任意交换2个维度之间的数据, 只进行一次交换时候建议使用它:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>a<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'交换1, 3:'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""a.shape: torch.Size([4, 3, 32, 32])交换1, 3: torch.Size([4, 32, 32, 3])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>但是请注意, 在使用<code>transpose</code>和<code>permute</code>后, 只是改变了访问的方式(数组的访问步长), 而不会改变底层数组的存储方式, 这也就是所谓的”<strong>不连续</strong>“. </p><p><strong>而<code>Tensor.view()</code>要求tensor必须是连续的, 所以在<code>view</code>前必须使用<code>contiguous()</code>让tensor变连续, 新版中直接使用<code>reshape</code>函数更方便, 它等价于前面的操作</strong>.</p><blockquote><p>关于连续与否更详细的解释可以看<a href="https://zhuanlan.zhihu.com/p/64551412" target="_blank" rel="noopener">PyTorch中的contiguous</a>.</p></blockquote><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 错误做法</span>a1 <span class="token operator">=</span> a<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span> <span class="token operator">*</span> <span class="token number">32</span> <span class="token operator">*</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 正确做法</span>a2 <span class="token operator">=</span> a<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span> <span class="token operator">*</span> <span class="token number">32</span> <span class="token operator">*</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 用reshape</span>a3 <span class="token operator">=</span> a<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span> <span class="token operator">*</span> <span class="token number">32</span> <span class="token operator">*</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a1.shape:'</span><span class="token punctuation">,</span> a1<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a2.shape:'</span><span class="token punctuation">,</span> a2<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a3.shape:'</span><span class="token punctuation">,</span> a3<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a == a1?:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>all<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>a<span class="token punctuation">,</span> a1<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a == a2?:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>all<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>a<span class="token punctuation">,</span> a2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a == a3?:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>all<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>a<span class="token punctuation">,</span> a3<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""a.shape: torch.Size([4, 3, 32, 32])a1.shape: torch.Size([4, 3, 32, 32])a2.shape: torch.Size([4, 3, 32, 32])a3.shape: torch.Size([4, 3, 32, 32])a == a1?: tensor(0, dtype=torch.uint8)a == a2?: tensor(1, dtype=torch.uint8)a == a3?: tensor(1, dtype=torch.uint8)"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>一定要先view交换后的shape, 再transpose回来.</p><p><code>Tensor.permute()</code>更加灵活, 能随意交换所有dim之间的位置, 如果交换多个维度最好使用这个函数:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""torch.Size([4, 3, 28, 32])torch.Size([4, 28, 32, 3])torch.Size([4, 28, 3, 32])torch.Size([4, 28, 3, 32])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>也同时要注意, 使用后也是不连续的.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>指针网络家族</title>
      <link href="/posts/36009.html"/>
      <url>/posts/36009.html</url>
      
        <content type="html"><![CDATA[<p>本文介绍了Pointer Network, CopyNet, Pointer-Generator Network以及Coverage机制在文本摘要与对话系统中的应用, 既可以作为知识点介绍, 也可以作为论文阅读笔记. 此外, 该部分内容为外部知识引入NLP任务中提供了思路.</p><blockquote><p>本文阅读所需的前置知识包括:</p><ul><li>Seq2Seq</li><li>Attention</li><li>Bi - directional RNN</li></ul></blockquote><h2 id="Pointer-Network"><a href="#Pointer-Network" class="headerlink" title="Pointer Network"></a>Pointer Network</h2><p>指针网络出自<a href="https://arxiv.org/abs/1506.03134" target="_blank" rel="noopener">Pointer Networks</a>. 在Seq2seq中, 经常有一种模型<strong>输出严重依赖输入</strong>的问题, 一旦问题规模发生变化, 那就必须重新训练网络. 作者利用Attention机制和Seq2Seq进行结合, 克服了这个问题.</p><p>“输出严重依赖输入”指的是输出往往是输入的子集. 比如在机器翻译中, 输出向量大小必须取决于字典长度, 并不能根据Encoder输入内容而发生变化. 在求凸包(Convex Hull)问题中, 输入和输出都是坐标序列, 规模是不固定的. <strong>可变大小序列的排序</strong>和<strong>各种组合优化</strong>都是这类问题. </p><blockquote><p>凸包是计算图形学中的概念, 通俗一点说凸包问题就是根据给定的二维平面点集找到能够包含点集中所有点的最外层点的连接线.</p></blockquote><p>而”指针”的命名来自于其Attention产生的权重直接决定了Decoder的输出对应着哪个Encoder的数据输入, 这样输出就从输入中进行选择, 能很好的解决该问题. 该结构非常简单, 与加权平均的注意力机制不同, Ptr - Net并非将Attention机制对信息进行筛选, 而是<strong>直接指出</strong>输出信息. 指针网络直接将Attention的权重最高者直接作为Decoder的输出, 如下图:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/ptrnet.jpg" style="zoom: 67%;" /><p>实际上这种结构不仅仅能用于解决凸包问题, 作者还在论文中给出利用Ptr - Net 解决三角形分割(Delaunay Triangulation), 旅行商问题(Travelling Salesman Problem). 作者在调整参数后, 取得了比LSTM好得多的结果:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/ptrnet1.jpg" style="zoom:50%;" /><p>指针网络因为具有解决输出严重依赖输入问题的能力, 如果应用于NLP的摘要生成任务中, 可以从原文中复现重要的细节, 在某些程度上解决预训练词典大小不足的问题(也称为<strong>OOV问题</strong>, 即Out of Vocabulary). 虽然这种结构在处理特定问题上有了优势, 但仍然受<strong>结构局限</strong>, 无法完全应用到通用任务当中.</p><h2 id="CopyNet"><a href="#CopyNet" class="headerlink" title="CopyNet"></a>CopyNet</h2><p>CopyNet出自论文<a href="http://arxiv.org/abs/1603.06393" target="_blank" rel="noopener">Incorporating Copying Mechanism in Sequence-to-Sequence Learning</a>. 该论文作者将PtrNet使用于文本<strong>摘要提取</strong>和<strong>对话任务</strong>当中.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/copynet1.jpg" style="zoom:50%;" /><p>在原文中作者提到Copy机制与Seq2Seq结合的难点:</p><blockquote><p>From a cognitive perspective, the copying mechanism is related to rote memorization, requiring less understanding but ensuring high literal fidelity. From a modeling perspective, the copying operations are more rigid and symbolic, making it more difficult than soft attention mechanism to integrate into a fully differentiable neural model. </p></blockquote><ul><li>从<strong>认知角度</strong>来看, Copy与死记硬背有关, 不需要理解, 但复制可以确保很高的字面保真度. </li><li>从<strong>建模角度</strong>来看, Copy更加僵化和符号化, 使其比软注意力机制更难集成到完全可区分的神经模型中. </li></ul><p>因此, 作者提出了另一种复制机制, 能够端到端的只通过梯度下降训练Seq2Seq模型. </p><p>前面提到的指针网络只能重复原文内容, 而非从已有的字典中提取内容, 天生就受到了极大的限制. 如果想要破除这种劣势, 就必须让之前的对话生成结构与这种Pointer(或者说Copy)机制相结合.</p><blockquote><p>注: Copy和Pointer的作用都是一致的, 都是将某个时刻的输出调整为先前某个时刻的输入, Copy也是将先前输入作为输出, 指针也是同样效果.</p></blockquote><p>在摘要生成中, 通过复现原文内容的摘要生成称为”<strong>抽取式</strong>“摘要生成, 而从外部词典中取出的内容叫”<strong>生成式</strong>“摘要生成. 而作者用复制机制将这两种方式实现了软结合.</p><p>沿用Seq2Seq结构, 仍然分为Encoder和Decoder两个部分. </p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/copynet.jpg" style="zoom: 67%;" /><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>Encoder采用双向RNN提取句子信息, 并将每个时刻与输入$x_t$ 相对应的hidden state集合$\left\{\mathbf{h}_{1}, \ldots, \mathbf{h}_{T_{S}}\right\}$ 称为短期记忆$\mathrm{M}$ .</p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><h4 id="Predition"><a href="#Predition" class="headerlink" title="Predition"></a>Predition</h4><p>作者将预测分为<strong>生成模式</strong>和<strong>复制模式</strong>两种, 当前时刻的预测结果应该是两种模式混合的结果. g代表生成模式, c代表copy模式. 而每个模式的预测结果由Decoder在当前时刻的隐藏状态输出$s_t$, 上个时刻的预测结果$y_{t-1}$, Attention生成的当前时刻的上下文向量$c_t$, 以及短时记忆$\mathrm{M}$ 共同决定.<br>$$<br>\begin{array}{r}<br>p\left(y_{t} \mid \mathbf{s}_{t}, y_{t-1}, \mathbf{c}_{t}, \mathbf{M}\right)=p\left(y_{t}, g \mid \mathbf{s}_{t}, y_{t-1}, \mathbf{c}_{t}, \mathbf{M}\right)<br>+p\left(y_{t}, c \mid \mathbf{s}_{t}, y_{t-1}, \mathbf{c}_{t}, \mathbf{M}\right)<br>\end{array}<br>$$<br>作者将词语分为在词典中的词语集合$\mathcal{V}=\left\{v_{1}, \ldots, v_{N}\right\}$, 源序列的单词集合$X=\left\{x_{1}, \ldots, x_{T_{S}}\right\}$, OOV单词记为$\mathrm{UNK}$, 每个模式生成的结果计算方式如下:<br>$$<br>\begin{array}{l}<br>p\left(y_{t}, g \mid \cdot\right)=\left\{\begin{array}{cc}<br>\frac{1}{Z} e^{\psi_{g}\left(y_{t}\right)}, &amp; y_{t} \in \mathcal{V} \\<br>0, &amp; y_{t} \in \mathcal{X} \cap \bar{V} \\<br>\frac{1}{Z} e^{\psi_{g}(\mathrm{UNK})} &amp; y_{t} \notin \mathcal{V} \cup \mathcal{X}<br>\end{array}\right. \\<br>p\left(y_{t}, \mathrm{c} \mid \cdot\right)=\left\{\begin{array}{cc}<br>\frac{1}{Z} \sum_{j: x_{j}=y_{t}} e^{\psi_{c}\left(x_{j}\right)}, &amp; y_{t} \in \mathcal{X} \\<br>0 &amp; \text { otherwise }<br>\end{array}\right.<br>\end{array}<br>$$<br>其中$Z$ 为生成模式和copy模式共享的归一化项, 作者通过设计归一化项让两种模式通过Softmax来<strong>互相竞争</strong>(将其代入原式的分母中就是Softmax的形式):<br>$$<br>Z=\sum_{v \in \mathcal{V} \cup\{\mathrm{UNK}\}} e^{\psi_{g}(v)}+\sum_{x \in X} e^{\psi_{c}(x)}<br>$$<br>$\psi_{g}(\cdot)$ 和 $\psi_{c}(\cdot)$ 是两种打分函数, 会在后面提到如何计算.</p><p>作者还给出了图加以辅助说明计算概率时所对应的不同情况:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/copynet2.jpg" style="zoom: 33%;" /><p>生成模式下采用Attention, 打分公式为:<br>$$<br>\psi_{g}\left(y_{t}=v_{i}\right)=\mathbf{v}_{i}^{\top} \mathbf{W}_{o} \mathbf{s}_{t}, \quad v_{i} \in \mathcal{V} \cup \mathrm{UNK}<br>$$<br>$W_o$ 和$\mathbf{v}_{i}$ 相乘后能获得$v_i$ 的独热编码, 它与$s_t$ 乘后得到一个分数.</p><p>copy模式下, 打分公式为:<br>$$<br>\psi_{c}\left(y_{t}=x_{j}\right)=\sigma\left(\mathbf{h}_{j}^{\top} \mathbf{W}_{c}\right) \mathbf{s}_{t}, \quad x_{j} \in \mathcal{X}<br>$$<br>$W_c$ 是训练得到的, 与$\mathbf h_j$ 相乘经过$\sigma$ (原文使用的是tanh)添加非线性, 将$h_j$ 和 $s_t$ 投射到同一个语义空间.</p><p>综上, 当$y_t$ 没出现在源序列中时, $p\left(y_{t}, c \mid \cdot\right)=0$, 只启动生成模式, 当$y_t$ 只出现在源序列时, $p\left(y_{t}, g \mid \cdot\right)=0$, 只启动copy模式.</p><h4 id="State-Update-and-Reading-M"><a href="#State-Update-and-Reading-M" class="headerlink" title="State Update and Reading M"></a>State Update and Reading M</h4><p>作者在copy机制下对Decoder的状态更新做了改良, 普通Decoder的状态更新为:<br>$$<br>\begin{array}{l}<br>\mathbf{s}_{t}=f\left(y_{t-1}, \mathbf{s}_{t-1}, \mathbf{c}\right) \\<br>p\left(y_{t} \mid y_{&lt;t}, X\right)=g\left(y_{t-1}, \mathbf{s}_{t}, \mathbf{c}\right)<br>\end{array}<br>$$<br>作者将$y_{t-1}$ 用$y_{t-1}$ 的Embedding $\mathbf{e}(y_{t-1})$ 和$\zeta\left(y_{t-1}\right)$ 来表示, 并将其concat起来, 即:<br>$$<br>\left[\mathbf{e}\left(y_{t-1}\right) ; \zeta\left(y_{t-1}\right)\right]^{\top}<br>$$<br>因为使用的是双向RNN, 所以$\mathrm M$ 中既包含了上下文信息, 也包含了位置信息, 这对状态更新很重要, $\zeta\left(y_{t-1}\right)$ 是其中的核心内容, 计算方式如下:<br>$$<br>\begin{array}{l}<br>\zeta\left(y_{t-1}\right)=\sum_{\tau=1}^{T_{S}} \rho_{t \tau} \mathbf{h}_{\tau} \\<br>\rho_{t \tau}=\left\{\begin{array}{cc}<br>\frac{1}{K} p\left(x_{\tau}, \mathrm{c} \mid \mathbf{s}_{t-1}, \mathbf{M}\right), &amp; x_{\tau}=y_{t-1} \\<br>0 &amp; \text { otherwise }<br>\end{array}\right. \\<br>K = \sum_{\tau^{\prime}: x_{\tau^{\prime}}=y_{t-1}} p\left(x_{\tau^{\prime}}, c \mid \mathbf{s}_{t-1}, \mathbf{M}\right)<br>\end{array}<br>$$<br>$K$ 是归一化项, 作者将这个过程称为Selective Read, 即用$\zeta\left(y_{t-1}\right)$ 对$\mathbf h_\tau$ 进行加权求和. 从式子直观上来理解, 仅当Encoder接受的输入在上个Decoder输出时刻相同时, 这个$\rho$ 才有意义. 通过这种方式, 对于在前文已经出现的单词, Decoder就能拿到<strong>额外的上下文信息和位置信息</strong>. 一旦$\zeta$ 有值, 当前时刻的输出就会倾向于copy模式, 因为上个时刻的输出在原文中能够找到, 那么当前时刻的内容也有相当大的概率从原文中copy.</p><blockquote><p>这个$\zeta$ 似乎和$c_t$ 差不多, 不知道有没有信息上的<strong>冗余</strong>.</p></blockquote><p>综上, 将更新过程总结如下:<br>$$<br>\zeta\left(y_{t-1}\right) \stackrel{\text { update }}{\longrightarrow} \mathbf{s}_{t} \stackrel{\text { predict }}{\longrightarrow} y_{t} \stackrel{\text { sel. read }}{\longrightarrow} \zeta\left(y_{t}\right)<br>$$</p><blockquote><p>在原文中最后的实验对比结果中. CopyNet对于Copy任务做的效果都碾压基础模型, 但唯独对于结束符的生成准确率较差, 这也为<strong>大量生成重复内容</strong>埋下了隐患.</p></blockquote><h2 id="Pointer-Generator-Network"><a href="#Pointer-Generator-Network" class="headerlink" title="Pointer - Generator Network"></a>Pointer - Generator Network</h2><p>指针生成网络受到了PtrNet和CopyNet的影响, 仍然沿用原输入的复现能力与基于已有知识的文本生成能力做结合的思路, 并针对指针生成网络出现的问题做了改进. 与CopyNet相比更加简洁.</p><p>该结构出自<a href="https://arxiv.org/abs/1704.04368" target="_blank" rel="noopener">Get To The Point: Summarization with Pointer-Generator Networks</a>, 这是一篇非常不错的论文, 如果时间不充裕我建议阅读这篇, 作者逻辑清晰, 图片也简单易懂.</p><p>该模型应用于<strong>摘要生成</strong>任务中, 作者在论文开头便提出了现存模型在摘要生成中出现的问题:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/ptrgennet1.jpg" style="zoom: 67%;" /><ol><li>对于图中的红色字, 存在内容抽取<strong>不准确</strong>的情况.</li><li>对于图中的绿色字, 存在多次与原文中生成<strong>重复且无意义内容</strong>的情况.</li></ol><p>指针生成网络沿用了CopyNet生成式和抽取式并存的思想, 分为抽取式摘要生成和基于单词表的生成式摘要生成, 并可以在生成模式和抽取模式之间更灵活的切换.</p><blockquote><p>Our pointer-generator network is a hybrid betweenour baseline and a pointer network, as it allows both copying words via pointing, and generating words from a fixed vocabulary.</p></blockquote><h3 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h3><p>在原论文中, 作者用标准的Seq2Seq + Attention作为Baseline:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/ptrgennet2.jpg" style="zoom:67%;" /><p>这是一种非常直觉性的办法. Seq2Seq架构下用双向RNN提取隐藏状态$h_i$, Decoder解码得到$s_t$, 用Bahdanau Attention提取上下文信息$h^\ast_t$:<br>$$<br>\begin{array}{l}<br>e_{i}^{t}=v^{T} \tanh \left(W_{h} h_{i}+W_{s} s_{t}+b_{\text {attn }}\right) \\<br>a^{t}=\operatorname{softmax}\left(e^{t}\right) \\<br>h_{t}^{\ast}=\sum_{i} a_{i}^{t} h_{i}<br>\end{array}<br>$$<br>然后根据得出的上下文信息和Decoder当前时刻解码信息, 经过Softmax得到当前时刻的词语概率分布$P_{\text {vocab }}$:<br>$$<br>P_{\text {vocab }}=\operatorname{softmax}\left(V^{\prime}\left(V\left[s_{t}, h_{t}^{\ast}\right]+b\right)+b^{\prime}\right)<br>$$<br>为了后面体现出指针生成网络和Baseline的差异, 直接令$P_{\text{vocab}}$就是最终结果:<br>$$<br>P(w)=P_{\text {vocab }}(w)<br>$$<br>然后用对数似然做损失, 计算总共的Loss:<br>$$<br>\begin{aligned}<br>\operatorname{loss}_{t}&amp;=-\log P\left(w_{t}^{\ast}\right) \\<br>\operatorname{loss}&amp;=\frac{1}{T} \sum_{t=0}^{T} \operatorname{loss}_{t}<br>\end{aligned}<br>$$</p><h3 id="Pointer-Generator-Network-1"><a href="#Pointer-Generator-Network-1" class="headerlink" title="Pointer - Generator Network"></a>Pointer - Generator Network</h3><p>作者基于Baseline的缺点, 给出了指针生成网络:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/ptrgennet3.jpg" style="zoom:67%;" /><p>与Baseline的图相对比, 最大的变化就是多了一个$p_{\text{gen}}$, 导致有其他很多连带的内容不同.</p><p>和之前一样, 我们仍然计算Attention和绿色对应的词表概率分布, 但同时根据当前时刻Decoder的输入$x_t$ 能额外得到一个概率$p_{\text{gen}} \in [0, 1]$:<br>$$<br>p_{\mathrm{gen}}=\sigma\left(w_{h^{\ast}}^{T} h_{t}^{\ast}+w_{s}^{T} s_{t}+w_{x}^{T} x_{t}+b_{\mathrm{ptr}}\right)<br>$$<br>引入$p_{\text{gen}}$ 就是想在Baseline的基础上将生成式与抽取式利用<strong>开关</strong>做个<strong>软结合</strong>, 从而改善最终结果:<br>$$<br>P(w)=p_{\mathrm{gen}} P_{\mathrm{vocab}}(w)+\left(1-p_{\mathrm{gen}}\right) \sum_{i: w_{i}=w} a_{i}^{t}<br>$$<br>生成模式的概率是$p_{\text{gen}}$, 那么copy模式的概率就是$1 - p_{\text{gen}}$. copy模式中用到了$a^t$, 当单词$w$ 在之前的输入中出现过时候, 我们将其Attention累加起来, 所以在原文中出现次数越多的词使用copy模式的概率就越大, 被添入摘要的概率也就越大. 如果$w$ 直接OOV了, 则$P_{\mathrm{vocab}}(w)=0$, 如果$w$ 没在原文中出现, 那么令$\sum_{i: w_{i}=w} a_{i}^{t}=0$.</p><p>模型非常简单, 图画的也特别好, 以至于让人一目了然.</p><h2 id="Coverage-Mechanism"><a href="#Coverage-Mechanism" class="headerlink" title="Coverage Mechanism"></a>Coverage Mechanism</h2><p>覆盖(汇聚?)机制与指针生成网络出自同一篇论文. 在原文中作者提到:</p><blockquote><p>Repetition is a common problem for sequence-to-sequence models, and is especially pronounced when generating multi-sentence text. We adapt the coverage model of Tu et al. (2016) to solve the problem.</p></blockquote><p>对于Seq2Seq模型来说, <strong>重复</strong>是一个非常令人头疼的问题, 作者在指针生成网络的基础上提出了Converage Mechanism. 思想非常简单, 作者通过Coverage Mechanism来记录之前时刻Attention的和, 作为当前时刻对原文单词关注位置的依据, 令其为$c^t$:</p><p>$$<br>c^{t}=\sum_{t^{\prime}=0}^{t-1} a^{t^{\prime}}<br>$$</p><blockquote><p>注: 这里的$c^t$ 不是上下文关系, 而是coverage vector, 在本论文中采用$h_t^{\ast}$ 作为上下文关系.</p></blockquote><p>$c^t$ 代表了先前模型对这些单词关注的覆盖程度(不知道是不是”覆盖”二字的来源), 模型先前越有可能copy过, 那么覆盖向量就越大. </p><p>将Coverage Mechanism加入Attention中去, 将模型对单词的关注程度也作为Attention的依据:<br>$$<br>e_{i}^{t}=v^{T} \tanh \left(W_{h} h_{i}+W_{s} s_{t}+w_{c} c_{i}^{t}+b_{\mathrm{attn}}\right)<br>$$<br>单单影响Attention不能产生太大作用, 因为模型并不知道要往哪个方向进行优化, 还需要给模型一个引导, 将相关的内容作为Loss体现:<br>$$<br>\displaylines{<br>\operatorname{covloss}_{t}=\sum_{i} \min \left(a_{i}^{t}, c_{i}^{t}\right) \\<br>\operatorname{covloss}_{t} \leq \sum_{i} a_{i}^{t}=1<br>}<br>$$<br>$\text{covloss}$ 使得模型更容易选择不同的单词做copy, 而非大量重复copy. 若重复copy, $a^t$ 和 $c^t$ 都会很高, 模型会被惩罚的更严重.</p><p>令$\lambda$ 为超参数加权, 将$\text{covloss}$ 加入总体损失中:<br>$$<br>\operatorname{loss}_{t}=-\log P\left(w_{t}^{\ast}\right)+\lambda \sum_{i} \min \left(a_{i}^{t}, c_{i}^{t}\right)<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Attention </tag>
            
            <tag> 摘要生成 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer精讲</title>
      <link href="/posts/6744.html"/>
      <url>/posts/6744.html</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>2020.10.5</strong>: 更新训练技巧.</p><p><strong>2020.9.27</strong>: 更新Masked Multi - Head Attention理解.</p><p><strong>2021.6.8</strong>: 更新Teacher Forcing.</p></blockquote><h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>Transformer (<del>擎天柱/变形金刚</del>), 是一个基于<strong>Attention和SeqSeq</strong>的模型, 完全摆脱了CNN和RNN, 整个模型单单只由自注意力和前馈神经网络组成. 该模型出自<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a>, 作者探究了Attention机制真正发挥的作用. Transformer在<strong>机器翻译</strong>等领域取得了革命性的成果, 并且由它衍生了很多在NLP方面的模型, 比如NLP现在通用的模型<a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">Bert</a>家族, 以后也会详细研究一下.</p><p>本文的图片大多数来自原论文和<a href="http://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">The Illustrated Transformer</a>这篇博客, 该文很清楚的解释了Transformer中原文的每一个细节, 图片简洁明了, 强烈推荐阅读.</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>在原文中, 作者先在介绍Transformer的细节前抛出了Transformer的大致结构:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer.jpg" style="zoom: 80%;" /><p>如果抛去所有的细节不谈, 能够清晰的看出作者使用了Seq2Seq作为模型的基本结构, 左侧输入为<strong>Encoder</strong>部分, 右侧输出部分为<strong>Decoder</strong>, 在Decoder输出后, 用一个加以Softmax的神经层来分类. </p><p>在每个Encoder和Decoder之间, 还有Attention 相连接, </p><p>这种结构天生就非常适合机器翻译, 如果我们把分类结果与词进行转换, 从高阶的视角来看, 那么结果就是一个机器翻译的Pipeline:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer1.png" style="zoom:50%;" /><h2 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h2><p>不要忘记论文中给出的大致结构, 下面一步步剖析细节如何实现.</p><h3 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h3><p>对Encoder进行输入(Inputs)和对Decoder进行输入(Outputs)时用到了Embedding, Embedding这里不再多做解释了,  在DL中普遍用Embedding做词语向量化. 论文中使用到的$d_{model}=512$, 并且在Encoder和Decoder的Embedding<strong>共享</strong>相同参数.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer6.png" style="zoom:67%;" /><blockquote><p>值得一提的是, Transformer使用的词表并非是原始单词, 而是经过<strong>BPE(byte-pair encoding)</strong> 处理后的.</p></blockquote><h3 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h3><p>其实在大致模型结构中已经提到了, Transformer可以看做是一个许多Encoder组成的编码组件和一个许多Decoder组成的解码组件构成的.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer2.png" style="zoom:50%;" /><p>在论文中, 编码组件和解码组件的数量等同, 并且假设$N=6$, 即有6个编码器和6个解码器. 作者在后文中还尝试了取2, 4, 8, 但效果上来说没有6好.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer3.png" style="zoom: 33%;" /><p>值得注意的是, 这里的Encoder和Decoder都采用了堆叠的方式, 并且对于堆叠的结构, 每次只接受一个单词的输入, 所以并不是每个Encoder的隐藏状态都会提供给每个Decoder, 而是<strong>Encoder堆叠后的输出统一提供给每个Decoder</strong>.</p><blockquote><p>在RNN + Seq2Seq执行机器翻译任务时, 每个RNN - Encoder接受的是<strong>不同单词</strong>的输入, 在Transformer中Encoder以堆叠的形式存在, 对应的是<strong>同一单词</strong>, 而Encoder - Decoder之间的Attention是为了调整对不同单词的信息权重, 所以并非每个Encoder和每个Decoder之间都有Attention, 不要搞混.</p></blockquote><h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p>每个Encoder由一个自注意力层(Self - Attention)和一个前馈神经网络层(FFN)组成, 后面会提到它们是如何实现的.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer4.png" style="zoom:67%;" /><p>我并不认为这张图做的很好, 因为在右侧的箭头是具有<strong>歧义性</strong>的, 容易让人认为每层的Encoder都与Decoder有连接, 实际上只有最后一个Encoder和所有的Decoder有连接.</p><p>如果将Embedding并添加位置编码后的输入向量设为$x_i$, 经过Self - Attention层的输出设为$z_i$, Encoder目前的向量流如下所示: </p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer7.png" style="zoom: 50%;" /><p>当然, 输出$r_i$ 会流入下个Encoder, 当做输入:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer8.png" style="zoom:50%;" /><h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><p>每个Decoder除了有和Encoder相同的自注意力和前馈神经网络层, 还多了一个对Encoder的Attention.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer5.png" style="zoom:67%;" /><p>同样, 本图也具有歧义性. </p><h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self - Attention"></a>Self - Attention</h3><p>自注意力是Transformer中提出的一种新结构, 也是核心组件. 这个注意力并非Decoder对所有Encoder的隐藏状态的不同注意力, 而是<strong>自身对自身</strong>的注意力, 在做语义编码时使得<strong>单词在编码时能够根据上下文找到自己的真正含义</strong>. 自注意力将注意力采用<strong>Q-K-V模式</strong>, 即Query, Key, Value. 用不同的矩阵与Embedding输入$x_i$ 做矩阵乘, 就能分别得到对应的$q_i$, $k_i$, $v_i$, 而矩阵是随机初始化得来的, 之后通过学习调整参数. 至于QKV是定义的, 请参考<code>&lt;Seq2Seq和Attention&gt;</code>. </p><blockquote><p>为什么叫Self - Attention呢? 假设我们在执行机器翻译任务, 这个Attention不再是作用于我们给出的一种语言的输入Source和目标语言的输出Target, 而是作用于Source和Source内部, 即<strong>源语言的语义编码与原始输入Source之间</strong>的Attention, 这样能够获得单词在句子中更好的表示.</p></blockquote><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer9.png" style="zoom:67%;" /><blockquote><p>这里注意一下维度, 在论文中的Embedding维度$d_{model}=512$, 给出的Key维度和Value维度均为64, 即$d_k=d_v=d_{model}/h=64$, 那么对应QKV的矩阵$W_Q$, $W_K$, $W_V$ 大小应该都是$(512, 64)$.</p></blockquote><p>这样就能根据输入得到一个查询向量$q_i$, 一组键值对$&lt;k_i, v_i&gt;$.</p><p>有了QKV, 接下来需要按照Attention的流程计算$q_i$ 和$k_i$ 的Score, 根据论文中提到的<strong>缩放点积注意力</strong>(Scaled Dot-Product Attention):</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/scaledotattention.jpg" style="zoom: 67%;" /><p>先进行点积, 再进行缩放, 计算完$q_i$ 与句中所有单词的$k$ 的得分(这里采用点积得到)后, 再对Score除以$\sqrt{d_k}$, 完成缩放, 最后再通过Softmax得到Attention权重, 加权求和结果称为$z_i$.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer10.png" style="zoom: 67%;" /><p>注意, 我们上面的讨论全部都是针对一个单词的, 但是在实际的运算中, 由于Encoder是线性Stack起来的, 所以其实Encoder的训练是可以并行的, 即<strong>多个单词做完Embedding后作为一个矩阵并行计算</strong>, 假设输入矩阵$X$, 通过$W_Q$, $W_K$, $W_V$ 计算后可以得到$Q$, $K$, $V$:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer11.png" style="zoom:50%;" /><p>综上, 将自注意力总结为:<br>$$<br>\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V<br>$$<br><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer12.png" style="zoom:50%;" /></p><p>这个公式其实在<code>&lt;Seq2Seq和Attention&gt;</code>一文中提到过, 但这里作者对Score使用了归一化, 即除以$\sqrt{d_k}$, $\sqrt{d_k}$ 为Key的维度. 这属于训练的一个Trick, 作者对此的解释如下:</p><blockquote><p>We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by $\frac{1}{\sqrt{d_k}}$.</p></blockquote><p>当$d_k$ 非常大时, 求得的内积可能会非常大, 如果不进行缩放, 不同的内积大小可能差异会非常大, Softmax在指数运算可能将梯度推到特别小, 导致梯度消失.</p><h3 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi - head Attention"></a>Multi - head Attention</h3><p>Multi - head Attention的思路和CNN中的多个<strong>卷积核</strong>起到的作用明显是一致的. 所谓”多头”, 放在<strong>卷积神经网络</strong>里就是卷积层多个卷积核的特征提取过程, 在这里就是进行多次注意力的提取, 就像多个卷积核一样, 多次<strong>不同的初始化矩阵</strong>经过训练可能会有多种<strong>不同的特征,</strong> 更有利于<strong>不同角度</strong>的特征抽取和信息提取. </p><p>论文中用多个头求出多组的数据堆叠, 也就是图中的$h$ 维:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/multiheadattention.jpg" style="zoom: 50%;" /><p>这样就能得到多个不同的Attention结果:</p><p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer13.png" alt=""></p><p>论文中采用了8个头的注意力, 即$h=8$, 得到多个提取出来的特征:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer14.png" style="zoom:50%;" /><p>我们将所有Self - Attention提取的特征全部concat起来, 因为维度比较大, 所以要经过一个输出矩阵$W_O$, 对特征进行进一步提取, 直到大小和Encoder接收的输入相同:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer15.png" style="zoom:50%;" /><p>也就是说经过左侧的6个Encoder, 向量大小仍然不改变.</p><p>到现在总结一下流程:</p><ol><li>做Embedding和位置编码, 获得输入$X$.</li><li>通过多头获得多组对应的$Q$, $K$, $V$.</li><li>通过缩放点积注意力, 多个头分别加权求和求得$Z_i$.</li><li>将所有$Z_i$ 全部concat起来, 然后经过$W_O$ 的特征提取, 得到最终输出$Z$, 其大小与输入$X$ 是完全相同的.</li></ol><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer16.png" style="zoom:50%;" /><blockquote><p>图中的$R$ 代表除了最开始的Encoder, 其他Encoder都不需要Embedding, 用上一个Encoder的输出作为输入.</p></blockquote><p>对每个头对句子不同部分的注意力进行可视化, 能发现每个头的注意力都在不同的位置上, 作用确实类似于CNN的卷积核, 做到了不同的注意力表示:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer17.png" style="zoom:67%;" /><h3 id="Encoder-Side"><a href="#Encoder-Side" class="headerlink" title="Encoder Side"></a>Encoder Side</h3><h4 id="Position-wise-Feed-Forward-neural-network"><a href="#Position-wise-Feed-Forward-neural-network" class="headerlink" title="Position-wise Feed Forward neural network"></a>Position-wise Feed Forward neural network</h4><p>前馈神经网络就是结构中提到的Feed Forward neural network. 当然不单单是一个全连接层, 这里还用到了<strong>ReLu</strong>作为激活函数, 并且加上了<strong>Layer Normalization</strong>. 只有一个隐藏层的神经网络也可以表示为2个<strong>一维</strong>的$1\times1$的卷积, 这二者是等价的:<br>$$<br>\operatorname{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}<br>$$</p><p>但这里值得说明的是, 在论文中有这样一段:</p><blockquote><p>In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position <strong>separately</strong> and <strong>identically</strong>. This consists of two linear transformations with a ReLU activation in between.</p></blockquote><p>对于并行计算的不同单词, 通过的FFN参数是<strong>共享</strong>的, 也可以看做不同单词先后通过同一个FFN, 如下图所示:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer20.png" style="zoom:50%;" /><p>这也就是为什么我前面要说FFN能够看做是<strong>一维卷积</strong>.</p><p>并且在经过每个Encoder后, 都<strong>不改变数据的大小</strong>.</p><h4 id="Layer-Norm"><a href="#Layer-Norm" class="headerlink" title="Layer Norm"></a>Layer Norm</h4><p>你应该接触过Batch Norm, Layer Norm也是一种类似于Batch Norm的归一化方式, 同样能起到加快收敛的作用, 在<strong>NLP任务</strong>中比较常用. Batch Norm中, 记录下多个Batch中每维Feature的均值和方差, 并进行放缩和平移, 即对<strong>不同样本的同一个通道特征</strong>进行归一化. 在Layer Norm中, 只是换了一个维度, 我们对<strong>同一个样本的不同特征</strong>进行归一化.<br>$$<br>\begin{aligned}<br>\mu_{j}&amp;=\frac{1}{m} \sum_{i=1}^{m} x_{i j} \\<br>\sigma_{j}^{2}&amp;=\frac{1}{m} \sum_{i=1}^{m}\left(x_{i j}-\mu_{j}\right)^{2}<br>\end{aligned}<br>$$<br>引入$\epsilon$ 做平滑, 使分母不为0:<br>$$<br>\text {LayerNorm}(x)=\frac{x_{i j}-\mu_{j}}{\sqrt{\sigma_{j}^{2}+\epsilon}}<br>$$<br>跟Batch Norm一样, 之后也进行平移和放缩:<br>$$<br>y = \text{LayerNorm}(x) \cdot \gamma + \beta<br>$$<br>下面这张图非常清晰:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/layernorm.png" style="zoom:50%;" /><blockquote><p>如果想了解它为什么和NLP领域比较契合, 详见下文:</p><ul><li><a href="https://zhuanlan.zhihu.com/p/54530247" target="_blank" rel="noopener">模型优化之Layer Normalization</a></li><li><a href="https://zhuanlan.zhihu.com/p/74516930" target="_blank" rel="noopener">NLP中 batch normalization与 layer normalization</a></li><li><a href="https://www.zhihu.com/question/395811291" target="_blank" rel="noopener">transformer 为什么使用 layer normalization, 而不是其他的归一化方法?</a></li><li>原论文<a href="https://arxiv.org/abs/1607.06450" target="_blank" rel="noopener">Layer Normalization</a></li></ul><p>大致原因是Batch Norm 对于Embedding后的数据进行归一化, 假设每个Batch是由多个Embedding组成的, 按照Batch方向对每个归一化, 就是对每个词的Embedding整体归一化. 这样做非常没有道理, 不符合NLP的规律, 它反而加强了不同词之间的相关性.</p><p>但如果按照Layer Norm, 按照Layer方向, 实际上是分别对每个Embedding后的词向量进行归一化, 这样每个词向量相对独立. </p><p>这主要还是CV和NLP的<strong>数据属性</strong>决定的. 在CV中, 不同样本之间的Channel信息是具有共性的(因为图像还是要用2D来表示), 这部分信息非常重要, 如果归一化会损失很多信息. 而NLP中, 数据是Embedding来的, 本来也没有包含位置信息, 反而不同词向量之间毫无相关性, 关注单词本身的归一化效果会更好.</p></blockquote><h4 id="Residual"><a href="#Residual" class="headerlink" title="Residual"></a>Residual</h4><p>残差连接从ResNet中提出也有一定年头了, 作为近些年使用频次比较高效果比较好的结构, 原理不再多赘述了, 在<code>&lt;卷积神经网络小结&gt;</code>和<code>&lt;卷积神经网络发展史&gt;</code>中都有对残差连接的详解. </p><p>在Encoder中残差连接伴随着Layer Norm, 每次经过一个子层都要做一次残差连接.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer21.png" style="zoom:50%;" /><p> 在Decoder中也是同样的, 每次经过子层也都要做残差连接.</p><h3 id="Decoder-Side"><a href="#Decoder-Side" class="headerlink" title="Decoder Side"></a>Decoder Side</h3><p>当了解了Encoder的结构后, 结合起Decoder来看一下信息流. 假设只有两个Encoder和两个Decoder的堆叠, 那么信息的流动方向是这样的:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer22.png" style="zoom:50%;" /><p>确实只有最后的Encoder将输出传递给了Decoder的Encoder - Decoder Attention. Encoder的输出也是和Decoder唯一的交互数据, 其最终输出就是经过多个堆叠的Encoder计算得来的与Encoder输入大小相同的向量, 在交互的Encoder - Decoder Attention中会体现出来.</p><h4 id="Encoder-Decoder-Attention"><a href="#Encoder-Decoder-Attention" class="headerlink" title="Encoder - Decoder Attention"></a>Encoder - Decoder Attention</h4><p>Decoder中的Encoder - Decoder Attention和Encoder中的多头Self - Attention运算机制<strong>一致</strong>, 唯一不同的就是<strong>输入数据的来源</strong>. 其中, $Q$ 对应的输入来源于Decoder的<strong>Masked Self - Attention</strong>(即每个Decoder的第一层输出), 而$K$ 和$V$ 对应的输入来源于整个<strong>Encoder</strong>的最终输出(图中Encoder上方的蓝色向量). 根据这三个输入再结合Encoder - Decoder Attention层中的$W_Q$, $W_K$, $W_V$ 分别得到$Q$, $K$, $V$. 然后再根据缩放点积得到Attention Value.</p><p>对于多个经过Encoder的单词形成含有多个单词的矩阵, Decoder是在这一环节实现翻译时对不同单词的Attention的.</p><blockquote><p>注: 下面两张动图比较大, 挂在github上了, 如果没挂梯子可能无法正常显示,</p></blockquote><img src="https://raw.githubusercontent.com/ADAning/Image/master/transformerdecode1.gif" style="zoom:50%;" /><h4 id="Outputs"><a href="#Outputs" class="headerlink" title="Outputs"></a>Outputs</h4><p>因为采用了Seq2Seq的架构, Decoder每过一个时间步不光接受Encoder的输出, 还接受了上一个Timestep的Decoder输入, 即论文中提到的”<strong>shifted right</strong>“.</p><img src="https://raw.githubusercontent.com/ADAning/Image/master/transformerdecode2.gif" style="zoom:50%;" /><h4 id="Masked-Multi-Head-Attention"><a href="#Masked-Multi-Head-Attention" class="headerlink" title="Masked Multi - Head Attention"></a>Masked Multi - Head Attention</h4><p>Mask是Transformer中一个关键点. Masked Multi - Head Attention 只出现在Decoder中. 到了Decoder, 可就不再像Encoder那样直接把数据拿过来并行训练了, 如果也像Encoder那样把所有输入的词向量全一股脑堆进去, Decoder做Self - Attention可以无视解码的时间跨度, <strong>获知全部的信息</strong>, 因此需要用Mask将当前预测的单词和之后的单词全都<strong>遮盖</strong>, 否则就没法训练了. </p><p>若仍然沿用传统Seq2Seq+RNN的思路, Decoder是一个<strong>顺序操作</strong>的结构, 我们代入一个场景来看看. 假设我们要执行<strong>机器翻译</strong>任务, 要将<code>我 是 大宁</code>翻译为<code>I am DaNing</code>, 假设所有参数与论文中提到的参数一样, batch size视为1. 根据前面已知的知识, Encoder堆叠后的输入和Embedding的大小是相同的, 在这里有三个词语, Embedding且通过Encoder后的编码大小为$(3, 512)$. 下面对Decoder进行训练:</p><ol><li>将起始符<code>&lt;start&gt;</code> 作为初始Decoder输入, 经过Decoder处理和分类得到输出<code>I</code>.</li><li>将<code>&lt;start&gt; I</code>作为Decoder输入, 经过Decoder处理和分类得到输出<code>am</code>.</li><li>将<code>&lt;start&gt; I am</code>作为Decoder输入, 经过Decoder处理和分类得到输出<code>DaNing</code>.</li><li>将<code>&lt;start&gt; I am DaNing</code>作为Decoder输入, 经过Decoder处理和分类得到结束符<code>&lt;end&gt;</code>.</li></ol><p>这种预测的方式也称为<strong>自回归</strong>.</p><p>参考将RNN更改为Self - Attention的Encoder思路, 对于这种依赖于前一个时间步预测结果的结构Decoder, 如果想做到<strong>并行</strong>训练, 需要将上面的过程转化为一个这样的矩阵直接作为Decoder的输入:<br>$$<br>\begin{bmatrix}<br>\text{&lt;start&gt;}&amp;  &amp;   &amp;   \\<br>\text{&lt;start&gt;}&amp; \text{I} &amp;   &amp;   \\<br>\text{&lt;start&gt;}&amp; \text{I} &amp; \text{am} &amp;   \\<br>\text{&lt;start&gt;}&amp; \text{I} &amp; \text{am} &amp; \text{DaNing}<br>\end{bmatrix}<br>$$<br>因为在<strong>训练时已知任务标签</strong>, 所以可以产生类似的效果. 这种方法被称为Teacher Forcing, 仅在训练阶段使用, 而不能使用在推断过程, 我会在下一节训练技巧中讲讲.</p><blockquote><p>图片取自<a href="https://wmathor.com/index.php/archives/1438/" target="_blank" rel="noopener">Transformer 详解</a>.</p></blockquote><p>在论文的图中, Mask操作顺序被放在$Q$ 和$K$ 计算并缩放后, Softmax计算前. 如果继续计算下去, 不做Mask, 与$V$ 相乘后得到Attention, 所有时间步信息全部都被泄露给Decoder, 必须用Mask将当前预测的单词信息和之后的单词信息全部遮住.</p><p>遮住的方法非常简单, 首先不能使用0进行遮盖, 因为Softmax中用零填充会产生错误, $e^0=1$. 所以必须要用$-\infty$来填充那些不能被看见的部分. 我们直接生成一个下三角全为0, 上三角全部为<strong>负无穷</strong>的矩阵, 与原数据相加就能完成遮盖的效果:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer24.png" style="zoom: 50%;" /><p>做Softmax时, 所有的负无穷全变成了0, 不再干扰计算:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer25.png" style="zoom: 67%;" /><p>其实Mask在对句子的<strong>无效部分填充</strong>时, 也是用同样方法将所有句子补齐, 无效部分用负无穷填充的.</p><blockquote><p>强调: Decoder仍然依赖与先前输出结果作为输入, 所以在正式使用时不能实现并行预测, 但在训练的时结果是已知的, 可以实现并行训练.</p></blockquote><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>最后来谈谈位置编码. 因为Transformer采用了纯粹的Attention结构, 不像RNN一样能够通过时间步来反映句子中单词的前后关系, 即不能得知<strong>位置信息.</strong> 要知道, 在NLP任务中, <strong>语序</strong>是一个相当重要的属性, 所以必须要通过某种方式让Transformer得知单词的位置, 作者通过<strong>位置编码</strong>在每次进入Encoder和Decoder前将位置信息写入. 这样来看, 与其叫位置编码, 不如叫<strong>位置嵌入</strong>. 位置编码可以直接与Embedding的向量相加:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer18.png" style="zoom:50%;" /><p>作者的做法非常有意思, 对不同的单词位置, 不同的Embedding维度, 它的编码都是<strong>唯一</strong>的, 应用正弦和余弦函数也方便Transformer学到位置的特征. 如果将当前单词位置记为$pos$, 而词向量的某个维度记为$i$, 那么位置编码的方法为:<br>$$<br>\begin{aligned}<br>P E_{(p o s, 2 i)} &amp;=\sin \left(p o s / 10000^{2 i / d_{\text {model }}}\right) \\<br>P E_{(p o s, 2 i+1)} &amp;=\cos \left(p o s / 10000^{2 i / d_{\text {model }}}\right)<br>\end{aligned}<br>$$<br>计算出来的结果应该是这样的:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer19.png" style="zoom:50%;" /><p>如果上面那组式子看起来有些乱, 写成这样或许好些:<br>$$<br>\begin{aligned}<br>P E_{(p o s, 2 i)} &amp;=\sin \left(\frac{p o s}{10000^\frac{2 i}{d_{\text {model }}}}\right) \\<br>P E_{(p o s, 2 i+1)} &amp;=\cos \left(\frac{p o s}{10000^\frac{2 i}{d_{\text {model }}}}\right)<br>\end{aligned}<br>$$<br>如果按照论文中的设定$d_{model}=512$, 由于奇偶数的计算方式是不同的, 所以$i \in[0, 255]$.</p><p>在这个式子中, 编码周期不受单词位置$pos$ 影响, 仅仅与模型开始设计的$d_{model}$ 和Embedding的不同维度$i$ 相关. 对于不同的$i$, $PE$ 的周期是$[2\pi, 10000\cdot2\pi]$.</p><p>这样看, 同一位置上的词语, 对于不同的Embedding维度, 都得到不同的编码, 并且随着$i$ 的增大, 位置编码的值的变化就越来越慢. 这种编码对于不同维度的Embedding来说是<strong>唯一</strong>的, 因此模型能够学习到关于Embedding的位置信息. </p><p>下面这个热图非常直观, 分开来看. 对于相同的Position的词语, 它不同维度的Embedding往往具有不同周期而交错的$\sin$ 和$\cos$ 组合, 而对于Embedding的同一个维度和不同Position的单词, 在Position上呈现出周期性.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/位置编码1.png" style="zoom:50%;" /><p>同样, 用折线图表示不同位置和编码后值的关系:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/位置编码2.png" style="zoom: 67%;" /><p>起初, 我并不知道为什么这种方法Work. 在看过<a href="https://zhuanlan.zhihu.com/p/92017824" target="_blank" rel="noopener">浅谈 Transformer-based 模型中的位置表示</a>后, 感觉似乎有些道理. 作者意在利用正弦余弦的数学性质(周期性, 和角公式), 使得偏移了的一定position, 记为$k$ , 能够得到正弦余弦的<strong>不同线性组合</strong>(总感觉这种编码在通信的某个地方应该很常用, 只是DL第一次用而已). </p><p>三角函数性质:<br>$$<br>\left\{\begin{array}{l}<br>\sin (\alpha+\beta)=\sin \alpha \cos \beta+\cos \alpha \sin \beta \\<br>\cos (\alpha+\beta)=\cos \alpha \cos \beta-\sin \alpha \sin \beta<br>\end{array}\right.<br>$$<br>得到偏移后, 即$pos+k$ 的$PE$:<br>$$<br>\left\{\begin{array}{l}<br>PE(pos+k, 2i)=PE(pos, 2i) \times PE(k, 2i+1)+PE(pos, 2i+1) \times PE(k, 2i) \\<br>PE(pos+k, 2i+1)=PE(pos, 2i+1) \times PE(k, 2i+1)-PE(pos, 2i) \times PE(k, 2i)<br>\end{array}<br>\right.<br>$$</p><p>除了公式计算, 作者也实验了其他对于位置编码的方式, 比如通过训练得到, 但由于实际效果与计算得来相仿, 那么还不如通过公式计算直接得到位置编码.</p><h3 id="Final-Output"><a href="#Final-Output" class="headerlink" title="Final Output"></a>Final Output</h3><p>最终输出很简单, 根据Decoder的输出经过FC层和Softmax得到对应的单词.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer23.jpg" style="zoom:50%;" /><p>注意, Decoder的Embedding层和最后输出经过Softmax前的Linear层也是<strong>共享权重</strong>的.</p><h2 id="训练技巧"><a href="#训练技巧" class="headerlink" title="训练技巧"></a>训练技巧</h2><p>让Transformer跑的起来离不开下面说的这些训练技巧.</p><h3 id="WarmUp"><a href="#WarmUp" class="headerlink" title="WarmUp"></a>WarmUp</h3><p>在原文中, Optimizer使用Adam, 除了设置参数$\beta_1 = 0.9$, $\beta_2 = 0.98$, $\epsilon=10^{-9}$外, 还设置了Warmup:<br>$$<br>\text {lrate}=d_{\text {model }}^{-0.5} \cdot \min \left(\text {step}_{-} \text {num}^{-0.5}, \text {step_num} \cdot \text {warmup_steps}^{-1.5}\right)<br>$$<br>我按照step增长, 做出WarmUp对应的learning rate变化图:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/transformer26.jpg" style="zoom: 25%;" /><p>在开始时学习率大, 然后逐渐变小. 对于越大的$d_{model}$ 来说, 初始的斜率越小. 论文中设置$\text{warmup_step}=4000$. 许多后续的模型也用到了WarmUp. </p><h3 id="Label-Smoothing"><a href="#Label-Smoothing" class="headerlink" title="Label Smoothing"></a>Label Smoothing</h3><p>在<code>&lt;卷积神经网络发展史&gt;</code>中提到过, 这里再重复一下. 标签平滑可以看做是一种添加到损失公式中的一种正规化组件, 因为独热编码后的标签只有0和1, 可能有些过于绝对, 标签平滑提供了一种手段来使其中的0也能分配到一些数值.  假设输入为$x$, 一共需要对$c$ 个类进行划分, 则标签平滑后的结果$y\prime$ 为:<br>$$<br>y\prime = (1 - \epsilon) \cdot x + \frac{\epsilon}{c}<br>$$<br>直接贴代码, Show you my code!</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># input_tensor is a tensor in pytorch</span><span class="token keyword">def</span> <span class="token function">label_smoothing</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> epsilon<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    classes <span class="token operator">=</span> input_tensor<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># compute the number of classes</span>    <span class="token keyword">return</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> epsilon<span class="token punctuation">)</span> <span class="token operator">*</span> input_tensor <span class="token operator">+</span> <span class="token punctuation">(</span>epsilon <span class="token operator">/</span> classes<span class="token punctuation">)</span> <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'label_smoothing(a):'</span><span class="token punctuation">,</span> label_smoothing<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'label_smoothing(b):'</span><span class="token punctuation">,</span> label_smoothing<span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""label_smoothing(a): tensor([0.0333, 0.0333, 0.9333])label_smoothing(b): tensor([0.0500, 0.9500])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在论文中设置$\epsilon_{ls}=0.1$. 标签平滑可能会提高句子的困惑度, 因为添加了更多的不确定性, 但会提高准确率和BLEU.</p><h3 id="Residual-Dropout"><a href="#Residual-Dropout" class="headerlink" title="Residual Dropout"></a>Residual Dropout</h3><p>在进行残差连接时, 每个子层进行add和batch norm之前, 都添加了Dropout. 原论文中设置$P_{\text {drop}}=0.1$, 个人认为Dropout没有前两种技巧作用大.</p><h3 id="Teacher-Forcing"><a href="#Teacher-Forcing" class="headerlink" title="Teacher Forcing"></a>Teacher Forcing</h3><p>在模型的训练阶段, Decoder的所有正确输入是<strong>完全已知</strong>的, 如果自回归预测在某个时间步$t$ 解码出错, 则会导致$t$ 时刻后所有的预测结果都产生<strong>偏差</strong>.</p><p>Teacher Forcing通过将自回归模型解码过程中的所有输入<strong>强制修正</strong>为Ground Truth来避免了这个问题.</p><p>Teacher Forcing有诸多优点:</p><ul><li>解决了训练阶段自回归式模型的串行问题, 能够使得模型训练时<strong>并行</strong>.</li><li>避免了模型训练时预测<strong>一步错步步皆错</strong>的问题.</li><li>由于干涉了训练错误的情况, 加快了模型的<strong>收敛</strong>速度.</li></ul><p>但它也同时容易产生<strong>矫枉过正</strong>的问题:</p><ul><li><strong>Exposure Bias</strong>: 这是最为常见的问题. 在训练时因为受到干涉, 很容易产生<strong>训练推断不一致</strong>.</li><li><strong>Overcorrect</strong>: 有时候模型解码有自己的想法, 但因为Teacher Forcing的干涉, 导致生成的句子<strong>四不像</strong>.</li><li><strong>No diversity</strong>: Teacher Forcing对Ground Truth的约束是非常强的, 模型的<strong>多样性</strong>受到严重限制.</li></ul><blockquote><p>Teacher Forcing的衍生问题, 请阅读<a href="https://zhuanlan.zhihu.com/p/93030328" target="_blank" rel="noopener">关于Teacher Forcing 和Exposure Bias的碎碎念</a>.</p></blockquote><h2 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h2><blockquote><p>摘自<a href="https://www.nowcoder.com/discuss/258321" target="_blank" rel="noopener">牛客网: NLPer看过来, 一些关于Transformer的问题整理</a></p></blockquote><h3 id="Transformer相比于RNN-LSTM-有什么优势-为什么"><a href="#Transformer相比于RNN-LSTM-有什么优势-为什么" class="headerlink" title="Transformer相比于RNN/LSTM, 有什么优势? 为什么?"></a>Transformer相比于RNN/LSTM, 有什么优势? 为什么?</h3><ol><li>RNN系列的模型<strong>并行计算</strong>能力很差.</li><li>Transformer的<strong>特征抽取</strong>能力比RNN系列的模型要好(实验结论).</li></ol><h3 id="为什么说Transformer可以代替seq2seq"><a href="#为什么说Transformer可以代替seq2seq" class="headerlink" title="为什么说Transformer可以代替seq2seq?"></a>为什么说Transformer可以代替seq2seq?</h3><p>这里用代替这个词略显不妥当, seq2seq虽已老, 但始终还是有其用武之地, seq2seq最大的问题在于<strong>将Encoder端的所有信息压缩到一个固定长度的向量中</strong>, 并将其作为Decoder端首个隐藏状态的输入, 来预测Decoder端第一个单词(token)的隐藏状态. 在输入序列比较长的时候, 这样做显然会损失Encoder端的很多信息, 而且这样一股脑的把该固定向量送入Decoder端, Decoder端不能够关注到其想要关注的信息. 上述两点都是seq2seq模型的缺点, 后续论文对这两点有所改进, 如著名的<a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">Neural Machine Translation by Jointly Learning to Align and Translate</a>, 虽然确确实实对seq2seq模型有了实质性的改进, 但是由于主体模型仍然为RNN(LSTM)系列的模型, 因此模型的并行能力还是受限, 而transformer不但对seq2seq模型这两点缺点有了实质性的改进(多头交互式attention模块), 而且还引入了self-attention模块, 让源序列和目标序列首先”自关联”起来, 这样的话, 源序列和目标序列自身的embedding表示所蕴含的信息更加丰富, 而且后续的FFN层也增强了模型的表达能力(ACL 2018会议上有论文对Self-Attention和FFN等模块都有实验分析, 见论文: <a href="http://aclweb.org/anthology/P18-1167" target="_blank" rel="noopener">How Much Attention Do You Need?A Granular Analysis of Neural Machine Translation Architectures</a>), 并且Transformer并行计算的能力是远远超过seq2seq系列的模型, 因此我认为这是transformer优于seq2seq模型的地方. </p><h3 id="Transformer中句子的encoder表示是什么？如何加入词序信息的？"><a href="#Transformer中句子的encoder表示是什么？如何加入词序信息的？" class="headerlink" title="Transformer中句子的encoder表示是什么？如何加入词序信息的？"></a>Transformer中句子的encoder表示是什么？如何加入词序信息的？</h3><p>Transformer Encoder端得到的是整个输入序列的encoding表示, 其中最重要的是经过了self-attention模块, 让输入序列的表达更加丰富, 而加入词序信息是使用不同频率的正弦和余弦函数.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Transformer </tag>
            
            <tag> Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KMP算法</title>
      <link href="/posts/35921.html"/>
      <url>/posts/35921.html</url>
      
        <content type="html"><![CDATA[<h1 id="KMP算法"><a href="#KMP算法" class="headerlink" title="KMP算法"></a>KMP算法</h1><h2 id="串"><a href="#串" class="headerlink" title="串"></a>串</h2><p>字符串是一种<strong>特殊的线性表</strong>, 其逻辑结构与线性表相同, 只是在数据类型上进行了约束, 要求元素全是字符类型. 串可以顺序存储, 链式存储, 或者堆存储. 堆结合了顺序和链式的优点, 实际在构造串也是采用的堆结构来存储, 能够方便动态扩展.</p><img src="https://gitee.com/Daning0/Images/raw/master/DS/串1.jpg" style="zoom:50%;" /><p>方便理解可以使用顺序存储.</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">// 串的定义</span><span class="token comment" spellcheck="true">//typedef struct{</span><span class="token comment" spellcheck="true">//    char str[maxSize+1];</span><span class="token comment" spellcheck="true">//    int length; </span><span class="token comment" spellcheck="true">//}Str;</span><span class="token comment" spellcheck="true">// 或者</span><span class="token keyword">typedef</span> <span class="token keyword">struct</span><span class="token punctuation">{</span>    <span class="token keyword">char</span> <span class="token operator">*</span>ch<span class="token punctuation">;</span>    <span class="token keyword">int</span> length<span class="token punctuation">;</span><span class="token punctuation">}</span>Str<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>字符串以<code>&#39;\0&#39;</code>作为<strong>结束标记</strong>. 串的基本操作有赋值, 取串长, 串比较, 求子串, 串清空, 串连接等. 在实现起来没有多大难度, 就稍微注意一下结束标记的处理即可.</p><h2 id="字符串匹配"><a href="#字符串匹配" class="headerlink" title="字符串匹配"></a>字符串匹配</h2><p>对一个串中的某子串定位操作称为串的模式匹配, 而其中与主串进行对比的子串称为模式串. 在字符串中常用到字符串匹配. </p><h3 id="简单模式匹配算法"><a href="#简单模式匹配算法" class="headerlink" title="简单模式匹配算法"></a>简单模式匹配算法</h3><p>简单而朴素的匹配算法, 就是将主串与模式串的字符挨个进行比对, 如果相同则逐一比对主串和模式串的下一个元素, 如果不同, 则从主串的下一个元素重复逐个比对的过程. 全部相同则匹配成功, 否则匹配失败.</p><p>代码实现如下:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">// 简单模式匹配算法</span><span class="token comment" spellcheck="true">// 假设字符串储存在1 ~ length上 </span><span class="token keyword">int</span> <span class="token function">index</span><span class="token punctuation">(</span>Str str<span class="token punctuation">,</span> Str substr<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> j <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> k <span class="token operator">=</span> i<span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 其中i和j分别用来表示主串和子串的位置, k用来暂存主串被比对的位置 </span>    <span class="token keyword">while</span><span class="token punctuation">(</span>i <span class="token operator">&lt;=</span> str<span class="token punctuation">.</span>length <span class="token operator">&amp;&amp;</span> j <span class="token operator">&lt;=</span> substr<span class="token punctuation">.</span>length<span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>str<span class="token punctuation">.</span>ch<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> substr<span class="token punctuation">.</span>ch<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token operator">++</span>i<span class="token punctuation">;</span>            <span class="token operator">++</span>j<span class="token punctuation">;</span>        <span class="token punctuation">}</span>         <span class="token keyword">else</span><span class="token punctuation">{</span>            j <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>            i <span class="token operator">=</span> <span class="token operator">++</span>k<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 匹配失败 i从主串下一个位置开始 </span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// 到这里有两种可能性 一个是原串被遍历完了 还有就是子串被遍历完了 </span>    <span class="token comment" spellcheck="true">// 如果是子串遍历完了说明匹配成功 </span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>j <span class="token operator">></span> substr<span class="token punctuation">.</span>length<span class="token punctuation">)</span>        <span class="token keyword">return</span> k<span class="token punctuation">;</span>    <span class="token keyword">else</span>        <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 因为假设字符串从下标1开始, 0是没有字符的</span><span class="token punctuation">}</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个简单的算法就是单纯的暴力匹配, 没有任何的预处理, 如果字符串长为n, 模式串长为m, 那么<strong>最坏时间复杂度为O((n-m+1)*m)</strong>. 即每次主串与模式串匹配时总能一直搜索到模式串的最后一个字符, 并最后匹配没有成功.</p><img src="https://gitee.com/Daning0/Images/raw/master/DS/串2.png" style="zoom: 33%;" /><h3 id="KMP算法-1"><a href="#KMP算法-1" class="headerlink" title="KMP算法"></a>KMP算法</h3><p>KMP算法是一种经典的字符串匹配算法, 相较于前面所说的简单字符串匹配算法, 在比较速度上有了相当大的提升. </p><p>来观察一个问题, 在下述串匹配过程中, 当在箭头所指的位置发生了字符不匹配:</p><img src="https://gitee.com/Daning0/Images/raw/master/DS/kmp1.jpg" style="zoom: 50%;" /><p>如果是简单字符串匹配算法, 那么很简单, 说明以主串第一个元素<code>A</code> 为起始的字符串无法与模式串相匹配, 将回溯到主串第二个元素<code>B</code> 与模式串第一个元素<code>A</code> 进行比较, 然后继续逐一比较下去… </p><p>继续观察如何才能使得模式串指针直接移动到再次能够与箭头所指的主串字符<code>B</code> 与模式串相比较的位置:</p><img src="https://gitee.com/Daning0/Images/raw/master/DS/kmp2.jpg" style="zoom: 50%;" /><p>由于主串第二个字符是<code>B</code>, 模式串第一个字符是<code>A</code>, 发生不匹配, 主串指针下移, 主串的第三四五个字符和模式串的第一二三个字符匹配, 又回到了对模式串某字符主串字符<code>B</code> 的比对. </p><p>除去简单字符串匹配, 有没有更取巧的办法, 利用<strong>模式串自身的特点</strong>, 或者说利用在主串和模式串发生不匹配时已经匹配字符的信息, 来压缩这个比较的过程呢? </p><p>继续逐一完成主串和模式串的比对, 又发现有一处不匹配.</p><img src="https://gitee.com/Daning0/Images/raw/master/DS/kmp3.jpg" style="zoom: 50%;" /><p>如果是简单字符串匹配又要逐一后移, 直到达到下述状态, 才能再次进行主串中所指的<code>B</code> 和模式串比对.</p><img src="https://gitee.com/Daning0/Images/raw/master/DS/kmp4.jpg" style="zoom: 50%;" /><p>经过这两个例子, 隐隐约约发现点问题. 总会出现一种状态, <strong>模式串中某个字符与主串字符发生不匹配, 但在这之前的所有字符都已经匹配了</strong>. 如果使用简单字符串匹配, 效率极其低下. 如果模式串的前半部分(从起始处向后取)和后半部分(从不匹配点向前取)有<strong>完全相同的子串</strong>, 那么很明显如果前半部分与主串完全匹配, 那么后半部分也一定与主串完全匹配, 那就不用重复进行比对了!</p><img src="https://gitee.com/Daning0/Images/raw/master/DS/kmp8.jpg" style="zoom: 50%;" /><p>为方便比较, 我将两次比对放在一起, 比如:</p><img src="https://gitee.com/Daning0/Images/raw/master/DS/kmp5.jpg" style="zoom: 50%;" /><img src="https://gitee.com/Daning0/Images/raw/master/DS/kmp2.jpg" style="zoom: 50%;" /><p>这里从起始处向后取, 即黄色框内的<code>ABA</code> 和从不匹配点向前取的白色框内的<code>ABA</code> 就是完全相同的子串, 这对子串称为<strong>公共前后缀</strong>. 正是因为这对前后缀完全相同, 所以发生字符不匹配时, 才能直接使得模式串的指针停留在前缀的下一个位置上, 不再重复进行前缀的比对, 这也是简单字符串匹配和KMP算法的最大不同, 即<strong>模式串的比较指针不回溯</strong>. </p><blockquote><p>指针不回溯意味着对大规模的外存中的字符串匹配操作可以分段进行. 先读入内存一部分进行匹配, 完成后再写回外存, 确保在匹配时不需要将之前写回外存的部分再次读入, 减少了IO操作, 从而提高了效率.</p></blockquote><p>在取前后缀时, 可能会有多对, 那应该取哪一对呢? 以上述发生不匹配的图为例, 从左右分别取, 应该可以形成<code>A-A</code>, <code>ABA-ABA</code>, <code>ABABA-ABABA</code> 三对符合要求的公共前后缀. 其中<code>AB-BA</code> 和 <code>ABAB-BABA</code>不是完全相同的子串, 是倒过来的, 不是公共前后缀. 如果要尽可能的减少重复比对次数, 一定是公共前后缀越长越好, 越长说明已经比对过的字符越多. 同时需要注意, <code>ABABA-ABABA</code> 这对是长度完全和子串长度相等, 再次比对时就失去了意义, 虽然是公共前后缀, 但它应该<strong>不能被使用</strong>. <code>ABA-ABA</code> 就是<strong>最长公共前后缀</strong>, 取最长的一对公共前后缀作为指针不回溯的依据.</p><p>记住, 模式串的比较指针直接就指向了前缀的下一个位置. 再看一个例子, 加深理解:</p><img src="https://gitee.com/Daning0/Images/raw/master/DS/kmp6.jpg" style="zoom:50%;" /><img src="https://gitee.com/Daning0/Images/raw/master/DS/kmp4.jpg" style="zoom: 50%;" /><p>在上述叙述的过程中, 发生主串和模式串的不匹配时, 模式串左侧与主串的对应位置一定是匹配的, 换句话说二者是一样的. 那么研究模式串就和研究主串是等价的了, 因此<strong>与主串无关</strong>, 仅保留模式串, 将指针不回溯的位置记录用数组下来, 当发生不匹配时, 指针就恢复到之前记录的位置即可. 这个数组称为<strong>next数组</strong>.</p><img src="https://gitee.com/Daning0/Images/raw/master/DS/kmp7.jpg" style="zoom:50%;" /><p>为方便, 字符串下标从1开始. 对于模式串:</p><table><thead><tr><th align="center">发生不匹配的模式串下标</th><th align="center">最长公共前后缀长度</th><th align="center">主串当前与模式串比对的下标</th><th align="center">最长公共前后缀</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">0</td><td align="center">0(特殊)</td><td align="center">-</td></tr><tr><td align="center">2</td><td align="center">0</td><td align="center">1</td><td align="center">-</td></tr><tr><td align="center">3</td><td align="center">0</td><td align="center">1</td><td align="center">-</td></tr><tr><td align="center">4</td><td align="center">1</td><td align="center">2</td><td align="center"><code>A</code></td></tr><tr><td align="center">5</td><td align="center">2</td><td align="center">3</td><td align="center"><code>AB</code></td></tr><tr><td align="center">6</td><td align="center">3</td><td align="center">4</td><td align="center"><code>ABA</code></td></tr><tr><td align="center">7</td><td align="center">1</td><td align="center">2</td><td align="center"><code>A</code></td></tr><tr><td align="center">8</td><td align="center">1</td><td align="center">2</td><td align="center"><code>A</code></td></tr><tr><td align="center">9</td><td align="center">2</td><td align="center">3</td><td align="center"><code>AB</code></td></tr><tr><td align="center">10</td><td align="center">3</td><td align="center">4</td><td align="center"><code>ABA</code></td></tr><tr><td align="center">11</td><td align="center">4</td><td align="center">5</td><td align="center"><code>ABAB</code></td></tr><tr><td align="center">12</td><td align="center">5</td><td align="center">6</td><td align="center"><code>ABABA</code></td></tr></tbody></table><p>总结上述规律, 除去模式串第一个元素外, 主串当前元素与模式串元素比对的下标是最长前后缀长度+1. 如果下标为1发生不匹配, 主串的下一个元素与模式串下标元素为1开始比较. 最坏时间复杂度是O(n+m).</p><h4 id="简单匹配升级到KMP"><a href="#简单匹配升级到KMP" class="headerlink" title="简单匹配升级到KMP"></a>简单匹配升级到KMP</h4><p>先抛开next数组如何构造不谈, 先看看如何写KMP的代码. 如果KMP完成了简单字符串匹配算法的压缩, 那也应该能够由简单算法升级为KMP算法.</p><p>实现KMP代码如下:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token keyword">int</span> <span class="token function">KMP</span><span class="token punctuation">(</span>Str str<span class="token punctuation">,</span> Str substr<span class="token punctuation">,</span> <span class="token keyword">int</span> next<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> j <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 下标从1开始</span>    <span class="token keyword">while</span><span class="token punctuation">(</span>i <span class="token operator">&lt;</span> str<span class="token punctuation">.</span>length <span class="token operator">&amp;&amp;</span> j <span class="token operator">&lt;</span> substr<span class="token punctuation">.</span>length<span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">// 主串指针下移的情况</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>j <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">||</span> str<span class="token punctuation">.</span>ch<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> substr<span class="token punctuation">.</span>ch<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token operator">++</span>i<span class="token punctuation">;</span>            <span class="token operator">++</span>j<span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token comment" spellcheck="true">// 发生不匹配时模式串指针跳到next数组所指向的位置</span>        <span class="token keyword">else</span> j <span class="token operator">=</span> next<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// 遍历完模式串没发现不匹配 说明模式串与主串相匹配</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>j <span class="token operator">></span> substr<span class="token punctuation">.</span>length<span class="token punctuation">)</span> <span class="token keyword">return</span> i <span class="token operator">-</span> substr<span class="token punctuation">.</span>length<span class="token punctuation">;</span>    <span class="token keyword">else</span> <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="求next数组"><a href="#求next数组" class="headerlink" title="求next数组"></a>求next数组</h4><p>再来看看next数组是如何构建的. 让我们回到之前找最长公共前后缀的过程. 那时曾经说过, 公共前后缀是两段<strong>完全相同的子串</strong>, 那找最长公共前后缀的过程岂不是也是字符串匹配? 我们仍然要延续不重复做事情的思路, 利用已知信息去求next数组.</p><img src="https://gitee.com/Daning0/Images/raw/master/DS/kmp9.jpg" style="zoom: 50%;" /><p>如果这样去想, 那么图中Pj所对应的next数组的值与Pt必然是有关联的.</p><p>当Pj和Pt的大小未知, 而前面模式串均相同时, 假设在Pj出发生不匹配, 模式串指针会跳转到最长公共前后缀+1处, 即next[j] = t. 有了这个初始条件, 可以根据Pj和Pt的大小关系推出next[j+1]. </p><p>假设P(j+1)处发生不匹配:</p><ul><li><p>如果Pj = Pt, 那么next[j+1] = next[j]+1 = t+1. </p></li><li><p>如果Pj != Pt, 就得在这两个串(本质是模式串自己) 中找到最长的公共前后缀, 也就<strong>回到了字符串匹配的问题中</strong>. 将P(j-t+1) ~ Pj 视为主串, P1 ~ Pt视为模式串, 继续做字符串匹配. 必须向前反复重定位指针, 找到一个位置使得Pj = Pt或满足t = 0, 即将t循环赋值为next[t], t = 0 时, 令next[j+1] = 1. </p><p>注意, 因为第二种情况与字符串匹配完全一致, 所以建立next数组的代码一定与KMP算法<strong>极其相似</strong>:</p></li></ul><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">// 求next数组的方法 substr为模式串 </span><span class="token comment" spellcheck="true">// j和t与上述图中相同</span><span class="token keyword">void</span> <span class="token function">getNext</span><span class="token punctuation">(</span>Str substr<span class="token punctuation">,</span> <span class="token keyword">int</span> next<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> t <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>    next<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>     <span class="token keyword">while</span><span class="token punctuation">(</span>j <span class="token operator">&lt;</span> substr<span class="token punctuation">.</span>length<span class="token punctuation">)</span><span class="token punctuation">{</span> <span class="token comment" spellcheck="true">// j&lt;= substr.length 会使next数组下标越界 </span>        <span class="token comment" spellcheck="true">// 模式串自身匹配</span>        <span class="token comment" spellcheck="true">// t可能被下面的else赋值为0, 在条件并入后会将t置为1</span>        <span class="token comment" spellcheck="true">// 并且第一次执行, 有next[2] = 1, 满足之前推导的结果</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>t <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">||</span> substr<span class="token punctuation">.</span>ch<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">==</span> substr<span class="token punctuation">.</span>ch<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token operator">++</span>j<span class="token punctuation">;</span>            <span class="token operator">++</span>t<span class="token punctuation">;</span>            next<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> t<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// next[j] = length+1 length实际上就是没++前的t</span>        <span class="token punctuation">}</span>        <span class="token keyword">else</span> t <span class="token operator">=</span> next<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 模式串指针重定位到next[t] </span>    <span class="token punctuation">}</span><span class="token punctuation">}</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="KMP算法改进"><a href="#KMP算法改进" class="headerlink" title="KMP算法改进"></a>KMP算法改进</h4><p>在求next数组时, 会一直用到向”前反复重定位指针”这个操作, 还可以有优化的余地.</p><img src="https://gitee.com/Daning0/Images/raw/master/DS/kmp10.jpg" style="zoom:50%;" /><p>在这个例子中出现了<strong>连续且完全相同的字符</strong>, 在j = 5时发生不匹配, next[j] = 4, 将j重定位到next[j] 上. j又为4, next[j] 又为3… 反复重定位, 直到j为0, 才发现该位置的主串和模式串完全不匹配, 主串和模式串指针都应该后移一位. 在这个过程, 从1到4位置上的字符串是相等的, 应该直接给next[5] 赋值为0.</p><p>尝试在next数组的基础上, 构建一个重定向数组, 使得其能够根据之前比较的内容跳过多余的比较, 直接将next[j] 赋值为某个已知的next数组值, 这个数组叫<strong>nextval数组</strong>.</p><img src="https://gitee.com/Daning0/Images/raw/master/DS/kmp11.jpg" style="zoom:50%;" /><p>在上面的图中, j位置的元素反复与Pd, Pc, Pb, Pa都进行了比较, 但明显前三者都是冗余比较, 不能给解决不匹配问题带来好处. 此处的nextval[j] 应该为a.</p><p>推广到一般情况, 路径上的元素都不是相邻的, 而现在j之前的nextval数组值都是已知的, 如何求j后的元素k的nextval[k] 呢?</p><img src="https://gitee.com/Daning0/Images/raw/master/DS/kmp12.jpg" style="zoom:50%;" /><p>如果k位置上的元素和j位置上的元素相等, 那么nextval[k] = nextval[next[k]], 如果不相等则令nextval[k] = next[k].</p><p>归纳为一般步骤:</p><ol><li>j = 1时, nextval[j] 赋值为 0, 作为特殊标记</li><li>j &gt; 1时:<ul><li>若Pj != P(next[j]), 则nextval[j] = next[j].</li><li>若Pj = P(next[j]), 则nextval[j] = nextval[next[j]].</li></ul></li></ol><p>求nextval数组的代码可以由求next数组的代码修改而来, 最好对比结合起来看, 实现如下:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token keyword">void</span> <span class="token function">getNextval</span><span class="token punctuation">(</span>Str substr<span class="token punctuation">,</span> <span class="token keyword">int</span> nextval<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> t <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>    nextval<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// j=1时nextval[j] = 0</span>    <span class="token keyword">while</span><span class="token punctuation">(</span>j <span class="token operator">&lt;</span> substr<span class="token punctuation">.</span>length<span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>t <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">||</span> substr<span class="token punctuation">.</span>ch<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">==</span> substr<span class="token punctuation">.</span>ch<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token operator">++</span>j<span class="token punctuation">;</span>            <span class="token operator">++</span>t<span class="token punctuation">;</span>            <span class="token comment" spellcheck="true">// 求解next数组时, 有next[j] = t; 那么t可以代替next[j] </span>            <span class="token keyword">if</span><span class="token punctuation">(</span>substr<span class="token punctuation">.</span>ch<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">!=</span> substr<span class="token punctuation">.</span>ch<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">)</span>                nextval<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> t<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// nextval[j] = next[j]</span>            <span class="token keyword">else</span>                nextval<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> nextval<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// nextval[j] = nextval[next[j]]</span>        <span class="token punctuation">}</span>        <span class="token keyword">else</span> t <span class="token operator">=</span> nextval<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
            <tag> KMP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Seq2Seq和Attention</title>
      <link href="/posts/40071.html"/>
      <url>/posts/40071.html</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>2020.9.25</strong>: 本质部分的猜想被证实.</p><p><strong>2020.9.21</strong>: 更新Attention的本质.</p><p><strong>2020.9.19</strong>: 在接触了更多NLP内容后, 发现Attention是一个有特殊表征意义的结构, 以后会加入更深的理解.</p></blockquote><h1 id="Seq2Seq和Attention"><a href="#Seq2Seq和Attention" class="headerlink" title="Seq2Seq和Attention"></a>Seq2Seq和Attention</h1><p>Seq2Seq和Attention被广泛的应用于RNN中, 当然现在不单单只是在NLP中使用, CV领域也有很多应用. </p><h2 id="RNN回顾"><a href="#RNN回顾" class="headerlink" title="RNN回顾"></a>RNN回顾</h2><p>在之前的循环神经网络小结中对RNN进行了介绍. 在此简单回顾一下.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rnn3.jpg" style="zoom: 50%;" /><p>RNN是针对时序序列数据而诞生的神经网络, 其输入是时序数据, 每个时刻$t$ 都会有一个相应的输出和隐藏状态. 对于$t \in T$, 有输入$[x_{1}, x_{2}, \ldots, x_{t}, \ldots, x_{T}]$ 和输出$[y_{1}, y_{2}, \ldots, y_{t}, \ldots, y_{T}]$ . 在经典RNN结构中给出的输入和输出是相同大小的.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rnn4.jpg" style="zoom:50%;" /><p>当前时刻隐藏状态$h_t$ 随着下个时刻$t+1$ 的输出$x_{t+1}$一起被共同作为下个时刻的输入. </p><h2 id="Sequence-to-Sequence-Encoder-Decoder"><a href="#Sequence-to-Sequence-Encoder-Decoder" class="headerlink" title="Sequence to Sequence(Encoder-Decoder)"></a>Sequence to Sequence(Encoder-Decoder)</h2><p>Seq2Seq作为一种不限制输入和输出的序列长度的结构, 被广泛应用于机器翻译, 文本摘要, 阅读理解, 语音识别等任务中. 在Seq2Seq结构中, <strong>编码器Encoder</strong>把所有的输入序列都编码成一个统一的语义向量, 保存在hidden state中, 然后再由<strong>解码器Decoder</strong>解码. 这种结构其实在介绍RNN结构时提到过. Seq2Seq和Encoder-Decoder描述的是同一种结构.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/encoder_decoder.jpg" style="zoom: 33%;" /><p>这种结构使得输入和输出与经典的RNN结构不同, 输入和输出的数据维度可以不同. 在解码器Decoder解码的过程中, 反复将上一个时刻$t-1$ 的输出作为当前时刻$t$ 的输入, 循环解码, 直到输出停止字符才停止. </p><p>下面以机器翻译为例, 来看一种Seq2Seq结构的一种经典实现方式. 将中文的”早上好”通过seq2seq转换成英文的”Good morning”.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/seq2seq1.jpg" style="zoom:33%;" /><ol><li>将”早上好”通过Encoder编码, 从$t=1$ 时刻到$t=3$ 时刻通过RNN反复完成语义向量的编码, 将$t=3$ 时刻最终的隐藏状态$h_3$ 作为语义向量.</li><li>$h_3$ 作为Decoder的初始隐藏状态$h_0$, 并在$t=1$ 时刻输入标识开始解码的特殊标识符<code>&lt;start&gt;</code>, 开始解码. 不断的将上一时刻输出作为当前时刻输入进行解码, 最终输出停止字符<code>&lt;stop&gt;</code>时, 预测解码停止.</li></ol><p>$t$ 时刻时, Decoder中的数据流流向如下所示:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/seq2seq2.jpg" style="zoom:33%;" /><ol><li>RNN先被输入大小为$[n_i, 1]$ 的向量$x_t$, 即红点.</li><li>结合传过来的语义向量, 加上这个时刻的输入, 经过RNN后输出大小为$[n_o, 1]$ 的向量$y_t$, 即蓝点.</li><li>在获取了输出向量后, 获取输出向量所对应的字符, 这就需要结合一层全连接层和激活函数(主要是Softmax), 将输出向量变为$y_t’$, 大小为$[n_c, 1]$, 即图中的黄点. $n_c$代表字典中的总字符数. 从$y_t’$ 中找到概率最大的字符index, 即图中橘红色点.</li><li>将分类后获取的字符index做一次Embedding(之前的NLP相关那篇文章中有提到过Word2vec), 输入给下个时刻, 如此循环.</li></ol><p>当然也有人将语义向量作为Decoder的输入, 而非隐藏状态提供给Decoder. Seq2Seq只是代表一种结构, 而非某种具体的实现方法.</p><p>但是Seq2Seq有许多的缺点:</p><ul><li>最大的局限性: <strong>编码和解码之间的唯一联系是固定长度的语义向量</strong>.</li><li>编码要把整个序列的信息压缩进一个<strong>固定长度</strong>的语义向量.</li><li>语义向量<strong>无法完全</strong>表达整个序列的信息.</li><li>先输入的内容携带的信息, 会被后输入的信息<strong>稀释</strong>掉, 或者被<strong>覆盖</strong>掉. 信息的量越大, 损失就越大.</li><li>输入序列<strong>越长</strong>, 这样的现象<strong>越严重</strong>, 这样使得在Decoder解码时一开始就没有获得足够的输入序列信息, 解码效果会打折扣. </li></ul><h2 id="Attention机制"><a href="#Attention机制" class="headerlink" title="Attention机制"></a>Attention机制</h2><blockquote><p>这里说的Attention都是<strong>软注意力</strong>机制.</p></blockquote><p>正是为了弥补基础的Encoder-Decoder的局限性, 提出了Attention. 因为语义向量表达信息的缺失性, 遗忘性, 以及向量长度的不可变性, Attention想要利用Encoder的隐藏状态$h_t$ 来解决语义向量存在的弊病. 当然, Attention不单单广泛的应用在NLP领域, 在CV领域也常有应用.</p><h3 id="Attention结构"><a href="#Attention结构" class="headerlink" title="Attention结构"></a>Attention结构</h3><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/seq2seq3.jpg" style="zoom:33%;" /><p>Attention的实质就是在Decoder的输入端, 将Encoder的隐藏状态<strong>加权信息</strong>, 也作为输入的一部分, 提供给Decoder. 换句话说, Encoder不再传递最后一个时间步的隐藏状态, 而是将所有时间步的隐藏状态加权提供给Decoder.</p><p>假设Encoder经过了3个时间步, 对应的隐藏状态分别是$h_1, h_2, h_3$, 分别的输入是”早”, “上”, “好”, 那么Decoder在$t=1$ 时刻通过三个不同的权重$w_{11}, w_{12}, w_{13}$ 能加权计算出一个向量$c_1$.<br>$$<br>c_{1}=h_{1} \cdot w_{11}+h_{2} \cdot w_{12}+h_{3} \cdot w_{13}<br>$$<br>将$c_1$ 和上一个状态拼接在一起形成一个新的向量, 一起输入到Decoder中, 计算结果:<br>$$<br>\bar{h}_{0} \leftarrow \operatorname{concat}\left(\bar{h}_{0}, c_{1}\right)=\operatorname{concat}\left(h_{3}, c_{1}\right)<br>$$<br>这样, 在Decoder的$t=1$ 时刻, 就应该根据之前Encoder的隐藏状态加权得到一个新的向量$c_2$, 和Decoder的上一个隐藏状态一起输入得到计算结果:<br>$$<br>\begin{array}{c}<br>c_{2}=h_{1} \cdot w_{21}+h_{2} \cdot w_{22}+h_{3} \cdot w_{23} \\<br>\bar{h}_{1} \leftarrow \operatorname{concat}\left(\bar{h}_{1}, c_{2}\right)<br>\end{array}<br>$$</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/attention1.jpg" style="zoom: 50%;" /><p>在Decoder的$t=2$ 和$t=3$ 时刻同理, 由于权重不同, 所以Decoder在解码时对隐藏状态关注的部分就不同, 权重越大注意力越强. 比如, 在翻译”好”时, 关注点应该在$h_3$ 上, 所以对应的$w_{13}$ 就应该比$w_{11}$ 和$w_{12}$ 大得多.</p><p>如果把隐藏状态和权重视为一层神经层, 那么就可以看做Encoder和Decoder之间引入了一层<strong>跨越时间步的神经层</strong>.</p><p>Attention又有 <a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau Attention</a> 和<a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">LuongAttention </a>等多种实现. 这里说一下LuongAttention.</p><h3 id="LuongAttention"><a href="#LuongAttention" class="headerlink" title="LuongAttention"></a>LuongAttention</h3><p>重新定义符号, 用$\bar{h}_{s}$ 代表Encoder的隐藏状态, $h_t$ 代表Decoder的隐藏状态, $\tilde{h}_{t}$代表Attention Layer输出的最终Decoder状态.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/luongattention.jpg" style="zoom: 50%;" /><p>首先要计算权重. 如果Decoder在$t$ 时刻的隐藏状态为$h_t$,  Encoder每一个隐藏状态$\bar{h}_{s}$ 的权重为$a_{t}$, 则权重是由规则$\mathrm {score}$ 经过$\mathrm {Softmax}$ 函数后得出的:<br>$$<br>a_{t}(s)=\frac{\exp \left(\operatorname{score}\left(h_{t}, \bar{h}_{s}\right)\right)}{\sum_{s^{\prime}} \exp \left(\operatorname{score}\left(h_{t}, \bar{h}_{s^{\prime}}\right)\right)}<br>$$<br>在luong Attention中, $\mathrm {score}$ 可以通过多种方式来计算:<br>$$<br>\operatorname{score}\left(h_{t}, \bar{h}_{s}\right)=\left\{\begin{array}{ll}<br>h_{t}^{T} \bar{h}_{s} &amp; \text { Dot } \\<br>h_{t}^{T} W_{a} \bar{h}_{s} &amp; \text { General } \\<br>v_{a}^{T} \tanh \left(W_{a} \cdot \operatorname{concat}\left(h_{t}, \bar{h}_{s}\right)\right) &amp; \text { Concat }<br>\end{array}\right.<br>$$<br>$\mathrm {Dot}$ 指的是向量内积, $\mathrm {General}$ 是再通过乘以权重矩阵$W_a$ 进行计算. 一般情况下来说$\mathrm {General}$ 要好于$\mathrm {Dot}$.</p><p>然后将计算得来的权重与Encoder的隐藏状态进行加权求和, 生成新的向量$c_t$.<br>$$<br>c_{t}=\sum_{s} a_{t}(s) \cdot \bar{h}_{s}<br>$$<br>接着将加权后的向量$c_t$ 与原始Decoder的隐藏状态$h_t$ 拼接在一起.<br>$$<br>\tilde{h}_{t}=\tanh \left(W_{c} \cdot \operatorname{concat}\left(c_{t}, h_{t}\right)\right)=\tanh \left(W_{c} \cdot\left[c_{t} ; h_{t}\right]\right)<br>$$<br>因为是拼接, 所以向量$c_t$ 和$h_t$ 拼接后的大小一定会发生变化. 如果想恢复成原来的形状则需要再乘一个恢复矩阵$W_c$ (也是用来重置大小的全连接层), 当然也可以不恢复, 只是会导致Decoder 的每个Cell大小逐渐变大.</p><p>最后, 对引入注意力的Decoder的 $\tilde{h}_{t}$ 经过一次线性运算后得到输出.<br>$$<br>y_{t}=W_{h o} \tilde{h}_{t}+b_{h o}<br>$$</p><p>也可以根据需要将新生成的状态$\tilde{h}_{t}$ 送入RNN继续学习. 上述过程提到的矩阵$W_a$, $W_c$, $W_{ho}$ 均通过学习得来.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/luongattention2.jpg" style="zoom: 33%;" /><h3 id="Attention的真正本质"><a href="#Attention的真正本质" class="headerlink" title="Attention的真正本质"></a>Attention的真正本质</h3><blockquote><p>该部分为<strong>Transformer</strong>的<strong>前置知识</strong>.</p><p>图和部分内容出自<a href="https://zhuanlan.zhihu.com/p/35571412" target="_blank" rel="noopener">浅谈Attention机制的理解</a>.</p></blockquote><p>虽然在Seq2Seq中, Attention可以看做是跨越时间步的神经层, 但只有抛弃掉开始接触的RNN和Seq2Seq, 才能看到Attention本身, 目前我们对Attention的理解是基于RNN和Seq2Seq的, 对这两种结构做了捆绑后并不能很好的看清它. 在抛弃了Seq2Seq后, 我们仍然说Attention的本质是对<strong>某些数据分配某些权重参数</strong>, 然后再对它们进行<strong>合并</strong>.</p><p>比较广泛的一个说法是, Attention的本质是一个<strong>查询</strong>(Query)到一系列<strong>键值对</strong>(&lt;Key - Value&gt;)的<strong>映射</strong>. 有些人将它比作<strong>软寻址</strong>的过程, 也是一样的道理, 当有Query = key的查询时, 根据内容的重要性, 从每个Value中<strong>都</strong>取出所需要的内容, 再通过某种方式合成起来形成输出.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/attention4.jpg" style="zoom: 67%;" /><p>假设Key和Value是不同语言中一一对应的单词, 如果根据Key能够产生一个与Value有关的概率分布, 不难发现, 这个过程和传统机器翻译中做的<strong>短语对齐</strong>起到的功能是类似的. </p><p>如果在目标中进行某数据的查找, 对于一个Query, 通过计算Query与所有Key之间的<strong>相似度或相关性</strong>, 再通过归一化得到与Value对应的概率分布(权重), 将其与对应的Value<strong>加权求和</strong>就得到了输出.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/attention5.jpg" style="zoom:67%;" /><p>如果<code>Key = Value</code> 则被称为普通模式, <code>Key != Value</code> 被称为键值对模式. 目前在大多数NLP研究中, <strong>Key和Value是相同的</strong>. </p><blockquote><p>这个结论我看很多人提到过, 我表示不理解, 想了很久目前也只有一个猜测, 我个人认为与<strong>应用领域</strong>有关. </p><p>重点在于, 之所以有Key-Value的映射结构, 很有可能在某些领域中Key的信息和对应的Value信息不是完全相同的, 这就意味着Key只用来生成权重系数, 而Value作为与Key不相同的语义信息可能有其他含义.</p></blockquote><p>可能上面写的比较散, 做个小总结, 工作机制有三步:</p><ol><li>计算Query与Key的相似度, 得到权值, 常用的相似度函数有点积，拼接，感知机等, 当然也可以不是相似度, 是其他的打分函数.</li><li>对权值进行各种Softmax归一化.</li><li>用归一化得来的权值与Value加权求和.</li></ol><p>因此, 有Attention:<br>$$<br>\begin{aligned}<br>&amp; \alpha _i = \operatorname {softmax}(\operatorname {similarity}(QK^T)) \\<br>&amp; attention((K, V), Q) = \sum_{i=1}^N \alpha_i v_i \quad(v_i \in V)<br>\end{aligned}<br>$$</p><blockquote><p>注: 该公式在Transformer中还会再次出现.</p></blockquote><p>如果把&lt;Key - Value&gt;这个映射称为<strong>字典</strong>, 那么Attention能够根据之前形成的字典轻松捕捉<strong>局部和长期依赖</strong>. 另外, Attention也可以看做是一种<strong>基于全连接的图模型</strong>, 只是连接权重是<strong>动态生成</strong>的.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/attention6.jpg" style="zoom:50%;" /><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>下面来总结一下, 图片出自<a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" target="_blank" rel="noopener">Visualizing A Neural Machine Translation Model</a>. </p><p>在Encoder如何获取经过Attention加权后的向量:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/attention2.jpg" style="zoom:50%;" /><p>从Decoder得到输出的计算过程:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/attention3.jpg" style="zoom:50%;" /><h3 id="优缺点分析"><a href="#优缺点分析" class="headerlink" title="优缺点分析"></a>优缺点分析</h3><p>Attention<strong>优点</strong>:</p><ul><li>在机器翻译时, 让生词不只是关注全局的语义向量, 增加了”<strong>注意力范围</strong>“. 表示接下来输出的词要重点关注输入序列种的哪些部分. 根据关注的区域来产生下一个输出. </li><li>不要求Encoder将所有信息全输入在一个固定长度的向量中. </li><li>将输入编码成一个向量的序列, 解码时, 每一步选择性的从序列中挑一个<strong>子集</strong>进行处理. </li><li>在每一个输出时, 能够充分利用输入携带的信息, 每个语义向量不一样, 注意力焦点不一样. </li></ul><p>Attention<strong>缺点</strong>:</p><ul><li>需要为每个输入输出组合分别计算Attention, 50个单词的输出输出序列需要计算2500个attention.</li><li>attention在决定专注于某个方面之前, 需要遍历一遍记忆再决定下一个输出是以什么.</li><li>纯粹的Attention机制并不能提供时序数据之间的<strong>位置信息</strong>, Transformer就是一个很好的例子.</li></ul><h3 id="可视化分析"><a href="#可视化分析" class="headerlink" title="可视化分析"></a>可视化分析</h3><p>如果将上述输入$x_t$ 对输出$y_t$ 的权重$a_{t}(s)$ 做一张热力图, 就能看出当预测某个单词时, 对句子其他部分的侧重程度, 也就是注意力:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/attention可视化3.png" style="zoom:50%;" /><p>能很明显的看出, 当翻译某个词时, 不考虑语态的情况下, 翻译和它邻近的几个词有强大的联系, 这也就是Attention的最直观体现.</p><p>下图揭露了句子中前后单词之间的联系, 颜色深浅表示联系的强弱, 并且对于不同的任务, Attention能够学习到不同的注意力结构. 图片出自<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Attention Is All You Need</a>(也是Transformer的论文).</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/attention可视化1.png" style="zoom: 67%;" /><p>在CV中, Attention同样有它的作用, 图片出自<a href="https://arxiv.org/pdf/1502.03044v1.pdf" target="_blank" rel="noopener">Show, Attend and Tell</a>.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/attention可视化2.png" style="zoom:67%;" /><h2 id="向RNN加入额外信息"><a href="#向RNN加入额外信息" class="headerlink" title="向RNN加入额外信息"></a>向RNN加入额外信息</h2><p>Attention机制其实就是将的Encoder的隐藏层状态加权后获得权重向量$c_t$, 额外加入到Decoder中, 从而使得网络有更完整的信息流.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/encoder_decoder2.jpg" style="zoom:67%;" /><p>如果有额外信息$z_t$ 想要添加到Decoder中, 那么主要有以下三种方式:</p><ul><li><p><strong>Add</strong>: 直接将额外信息$z_t$ 叠加在输出$y_t$ 上.<br>$$<br>y_{t} \leftarrow y_{t}+z_{t}<br>$$</p></li><li><p><strong>Concat</strong>: 将额外信息$z_t$ 拼接在隐藏层的隐藏装填$h_t$, 然后通过全连接恢复维度, luong attention中就是用的这种方法.<br>$$<br>h_{t} \leftarrow W_{c} \cdot \operatorname{concat}\left(h_{t}, z_{t}\right)<br>$$</p></li><li><p><strong>Mlp</strong>: 直接添加一个对额外信息$z$ 的神经层.<br>$$<br>\begin{array}{l}<br>h_{t}^{z h}=W_{z h} \cdot z_{t}+b_{z h} \\<br>h_{t} \leftarrow \tanh \left(h_{t}^{i h}+h_{t}^{h h}+h_{t}^{z t}\right) \\<br>\quad=\tanh \left(\left(W_{i h} \cdot x_{t}+b_{i h}\right)+\left(W_{h h} \cdot h_{t-1}+b_{h h}\right)+\left(W_{z h} \cdot z_{t}+b_{z h}\right)\right)<br>\end{array}<br>$$</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Attention </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>STL常见用法</title>
      <link href="/posts/22779.html"/>
      <url>/posts/22779.html</url>
      
        <content type="html"><![CDATA[<h1 id="STL常见用法"><a href="#STL常见用法" class="headerlink" title="STL常见用法"></a>STL常见用法</h1><p>STL是一套非常好用的C++模板, 其中内置了很多已经封装好的数据结构和算法. 如果每次都要从头实现很麻烦, STL在刷算法题时候很好用. 本文是参照<a href="https://www.bilibili.com/video/BV1uJ411r74z" target="_blank" rel="noopener">steve-yu</a>视频做下的笔记.</p><h2 id="输入输出"><a href="#输入输出" class="headerlink" title="输入输出"></a>输入输出</h2><p>C++保留了原来C语言的输入和输出, 在基础上增加了cin和cout. </p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">// 导入cin cout头文件</span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;iostream></span></span><span class="token comment" spellcheck="true">// c程序中输入输出</span><span class="token keyword">int</span> a<span class="token punctuation">;</span><span class="token function">scanf</span><span class="token punctuation">(</span><span class="token string">"%d"</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>a<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%d"</span><span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// C++输入输出</span><span class="token keyword">int</span> a<span class="token punctuation">;</span>cin <span class="token operator">>></span> a<span class="token punctuation">;</span>cout <span class="token operator">&lt;&lt;</span> a<span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 连续输入输出变量</span><span class="token keyword">int</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> c<span class="token punctuation">;</span>cin <span class="token operator">>></span> a <span class="token operator">>></span> b <span class="token operator">>></span> c<span class="token punctuation">;</span>cout <span class="token operator">&lt;&lt;</span> a<span class="token operator">&lt;&lt;</span>b <span class="token operator">&lt;&lt;</span> c<span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 换行方便</span>cout <span class="token operator">&lt;&lt;</span> a<span class="token operator">&lt;&lt;</span> endl<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>cin和cout的效率是比scanf和printf低很多的, 在刷算法题时, 尽可能使用C语言的输入输出来避免超时.</p><h2 id="头文件"><a href="#头文件" class="headerlink" title="头文件"></a>头文件</h2><p>在使用任意的STL数据类型前, 都需要导入对应的头文件. 比如说使用<code>vector</code> 就必须导入<code>&lt;vector&gt;</code>. <code>&lt;algorithm&gt;</code> 中有许多内置好的算法, 例如<code>sort</code>, <code>reverse</code>等.</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;iostream></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;algorithm></span></span><span class="token keyword">using</span> <span class="token keyword">namespace</span> std<span class="token punctuation">;</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> a<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token punctuation">{</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token function">sort</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span>a<span class="token operator">+</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span><span class="token number">7</span><span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span>        cout<span class="token operator">&lt;&lt;</span>a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&lt;&lt;</span><span class="token string">" "</span><span class="token punctuation">;</span>    cout<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span>    <span class="token function">system</span><span class="token punctuation">(</span><span class="token string">"pause"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="String"><a href="#String" class="headerlink" title="String"></a>String</h2><p>字符串可以看做是char*的封装.</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">// c语言中的字符串定义和打印</span><span class="token keyword">char</span> ch<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token string">"asdkajbf"</span><span class="token punctuation">;</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>ch<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">!=</span><span class="token string">'\0'</span><span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%c"</span><span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">(</span>ch<span class="token operator">+</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// c++</span>string s <span class="token operator">=</span> <span class="token string">"zxcvbnm"</span><span class="token punctuation">;</span>cout<span class="token operator">&lt;&lt;</span> s <span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>获取一行字符串:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">// c</span><span class="token function">scanf</span><span class="token punctuation">(</span><span class="token string">"%s"</span><span class="token punctuation">,</span> ch<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// c++</span>string s<span class="token punctuation">;</span><span class="token function">getline</span><span class="token punctuation">(</span>cin<span class="token punctuation">,</span> s<span class="token punctuation">)</span><span class="token punctuation">;</span>cout <span class="token operator">&lt;&lt;</span> s<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>还可以使用<code>+=</code>  运算, 数字会被转换成asc码.</p><pre class="line-numbers language-cpp"><code class="language-cpp">string s<span class="token punctuation">;</span>s <span class="token operator">+</span><span class="token operator">=</span> <span class="token string">"hello"</span><span class="token punctuation">;</span>s <span class="token operator">+</span><span class="token operator">=</span> <span class="token string">" world"</span><span class="token punctuation">;</span>s <span class="token operator">+</span><span class="token operator">=</span> <span class="token string">'5'</span><span class="token punctuation">;</span>s <span class="token operator">+</span><span class="token operator">=</span> <span class="token number">10</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//10对应的asc码是换行</span><span class="token keyword">int</span> a <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//想把a加入字符串</span>s <span class="token operator">+</span><span class="token operator">=</span> <span class="token punctuation">(</span>a<span class="token operator">+</span><span class="token string">'0'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>cout <span class="token operator">&lt;&lt;</span> s<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>同样也可以进行排序:</p><pre class="line-numbers language-cpp"><code class="language-cpp">string s <span class="token operator">=</span> <span class="token string">"5418340"</span><span class="token punctuation">;</span><span class="token function">sort</span><span class="token punctuation">(</span>s<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> s<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>cout <span class="token operator">&lt;&lt;</span> s<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>使用<code>erase</code>和<code>substr</code>:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">// erase</span>string s <span class="token operator">=</span> <span class="token string">"5418340"</span><span class="token punctuation">;</span>s<span class="token punctuation">.</span><span class="token function">erase</span><span class="token punctuation">(</span>s<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//删除第一个</span>s<span class="token punctuation">.</span><span class="token function">erase</span><span class="token punctuation">(</span><span class="token operator">--</span>s<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//删除最后一个</span>cout <span class="token operator">&lt;&lt;</span> s<span class="token punctuation">;</span><span class="token comment" spellcheck="true">// substr</span>string s <span class="token operator">=</span> <span class="token string">"5418340"</span><span class="token punctuation">;</span>s <span class="token operator">=</span> s<span class="token punctuation">.</span><span class="token function">substr</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//取418,取索引为1，往后截断3个</span>s <span class="token operator">=</span> s<span class="token punctuation">.</span><span class="token function">substr</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//索引为1，截断到最后</span>cout <span class="token operator">&lt;&lt;</span> s<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Vector"><a href="#Vector" class="headerlink" title="Vector"></a>Vector</h2><p><code>vector</code>是封装好的向量, 类似数组. 使用前需导入<code>vector</code>头文件.</p><pre class="line-numbers language-cpp"><code class="language-cpp">vector<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> v<span class="token punctuation">;</span><span class="token comment" spellcheck="true">//定义一个空vector</span>vector<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> <span class="token function">v2</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//定义一个4个大小的vector，初始为0</span>vector<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> <span class="token function">v3</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//定义一个4个大小的vector，初始为6</span>vector<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> v<span class="token punctuation">{</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//定义一个vector，数字为1,2,3,4,5</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>获取元素:</p><pre class="line-numbers language-cpp"><code class="language-cpp">vector<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> v<span class="token punctuation">{</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">}</span><span class="token punctuation">;</span>cout <span class="token operator">&lt;&lt;</span> v<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//取索引为1</span>cout <span class="token operator">&lt;&lt;</span> v<span class="token punctuation">.</span><span class="token function">at</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//取索引为2的</span><span class="token comment" spellcheck="true">// 获取第一个</span>cout<span class="token operator">&lt;&lt;</span>v<span class="token punctuation">.</span><span class="token function">front</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>cout<span class="token operator">&lt;&lt;</span>v<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>cout<span class="token operator">&lt;&lt;</span><span class="token operator">*</span>v<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 获取最后一个</span>cout<span class="token operator">&lt;&lt;</span>v<span class="token punctuation">.</span><span class="token function">back</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>cout<span class="token operator">&lt;&lt;</span>v<span class="token punctuation">[</span>v<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//size是获取大小</span>cout<span class="token operator">&lt;&lt;</span><span class="token operator">*</span><span class="token operator">--</span>v<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>添加元素:</p><pre class="line-numbers language-cpp"><code class="language-cpp">vector<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> v<span class="token punctuation">;</span>v<span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">;</span>v<span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">;</span>v<span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">;</span>v<span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">;</span>v<span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>修改形状:</p><pre class="line-numbers language-cpp"><code class="language-cpp">v<span class="token punctuation">.</span><span class="token function">resize</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>删除元素:</p><pre class="line-numbers language-cpp"><code class="language-cpp">v<span class="token punctuation">.</span><span class="token function">erase</span><span class="token punctuation">(</span>v<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//删除第一个元素</span>v<span class="token punctuation">.</span><span class="token function">erase</span><span class="token punctuation">(</span><span class="token operator">--</span>v<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//删除最后一个元素</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>向量排序:</p><pre class="line-numbers language-cpp"><code class="language-cpp">vector<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> v<span class="token punctuation">{</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token function">sort</span><span class="token punctuation">(</span>v<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>v<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>less<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//从小到大</span><span class="token function">sort</span><span class="token punctuation">(</span>v<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>v<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>greater<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//从大到小排序</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="Stack"><a href="#Stack" class="headerlink" title="Stack"></a>Stack</h2><p>就是数据结构中的栈, 先进后出. 使用前需导入<code>stack</code>头文件.</p><pre class="line-numbers language-cpp"><code class="language-cpp">stack<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> s<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>栈的各种操作:</p><pre class="line-numbers language-cpp"><code class="language-cpp">s<span class="token punctuation">.</span><span class="token function">push</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 入栈</span>s<span class="token punctuation">.</span><span class="token function">push</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">;</span>cout <span class="token operator">&lt;&lt;</span> s<span class="token punctuation">.</span><span class="token function">top</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;&lt;</span> endl<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 获取顶端</span>s<span class="token punctuation">.</span><span class="token function">pop</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 出栈 无返回值</span>cout <span class="token operator">&lt;&lt;</span> s<span class="token punctuation">.</span><span class="token function">top</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;&lt;</span> endl<span class="token punctuation">;</span>cout <span class="token operator">&lt;&lt;</span> s<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;&lt;</span> endl<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 查看元素个数</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>用栈实现进制转换(decimal to binary):</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token keyword">int</span> <span class="token function">itob</span><span class="token punctuation">(</span><span class="token keyword">int</span> decimal<span class="token punctuation">)</span><span class="token punctuation">{</span>    stack<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> s<span class="token punctuation">;</span>    <span class="token keyword">int</span> res <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span>decimal <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">// 除二取余</span>        s<span class="token punctuation">.</span><span class="token function">push</span><span class="token punctuation">(</span>decimal<span class="token operator">%</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        decimal <span class="token operator">/</span><span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">while</span><span class="token punctuation">(</span><span class="token operator">!</span>s<span class="token punctuation">.</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token punctuation">{</span>        res <span class="token operator">=</span> res <span class="token operator">*</span> <span class="token number">10</span> <span class="token operator">+</span> s<span class="token punctuation">.</span><span class="token function">top</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        s<span class="token punctuation">.</span><span class="token function">pop</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">return</span> res<span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>用栈实现逆序单词打印:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;stack></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;sstream></span></span><span class="token keyword">void</span> <span class="token function">inverse</span><span class="token punctuation">(</span>string str<span class="token punctuation">)</span><span class="token punctuation">{</span>    stack<span class="token operator">&lt;</span>string<span class="token operator">></span> s<span class="token punctuation">;</span>    stringstream ss<span class="token punctuation">;</span>    ss <span class="token operator">&lt;&lt;</span> str<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 让字符串流入stringstream</span>    <span class="token keyword">while</span><span class="token punctuation">(</span>ss <span class="token operator">>></span> str<span class="token punctuation">)</span><span class="token punctuation">{</span>        s<span class="token punctuation">.</span><span class="token function">push</span><span class="token punctuation">(</span>str<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">while</span><span class="token punctuation">(</span><span class="token operator">!</span>s<span class="token punctuation">.</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        cout <span class="token operator">&lt;&lt;</span> s<span class="token punctuation">.</span><span class="token function">top</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        s<span class="token punctuation">.</span><span class="token function">pop</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>s<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">!=</span><span class="token number">0</span><span class="token punctuation">)</span> cout <span class="token operator">&lt;&lt;</span> <span class="token string">" "</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里使用了<code>stringstream</code>, 用来做数据转换, 十分安全和方便. 例如字符串转数字:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">// 方法1</span>string s <span class="token operator">=</span> <span class="token string">"1234"</span><span class="token punctuation">;</span><span class="token keyword">int</span> i<span class="token punctuation">;</span>stringstream ss<span class="token punctuation">;</span>ss <span class="token operator">&lt;&lt;</span> s<span class="token punctuation">;</span>ss <span class="token operator">>></span> i<span class="token punctuation">;</span>cout <span class="token operator">&lt;&lt;</span> i<span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 方法2</span>string s <span class="token operator">=</span> <span class="token string">"1234"</span><span class="token punctuation">;</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token function">stoi</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">;</span>cout <span class="token operator">&lt;&lt;</span> i<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>数字转字符串:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">// 方法1</span><span class="token keyword">int</span> a <span class="token operator">=</span> <span class="token number">1234</span><span class="token punctuation">;</span>string out<span class="token punctuation">;</span>stringstream ss<span class="token punctuation">;</span>ss <span class="token operator">&lt;&lt;</span> a<span class="token punctuation">;</span>ss <span class="token operator">>></span> out<span class="token punctuation">;</span>cout <span class="token operator">&lt;&lt;</span> out <span class="token operator">&lt;&lt;</span> endl<span class="token punctuation">;</span><span class="token comment" spellcheck="true">//方法2(c++ 11)</span><span class="token keyword">int</span> a <span class="token operator">=</span> <span class="token number">1234</span><span class="token punctuation">;</span>cout <span class="token operator">&lt;&lt;</span> <span class="token function">to_string</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span> <span class="token operator">&lt;&lt;</span> endl<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Queue"><a href="#Queue" class="headerlink" title="Queue"></a>Queue</h2><p>此为队列的封装实现. 使用前需要导入<code>&lt;queue&gt;</code>.</p><pre class="line-numbers language-cpp"><code class="language-cpp">queue<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> q<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>其操作为:</p><pre class="line-numbers language-cpp"><code class="language-cpp">q<span class="token punctuation">.</span><span class="token function">push</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//入队</span>q<span class="token punctuation">.</span><span class="token function">push</span><span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">;</span>cout<span class="token operator">&lt;&lt;</span>q<span class="token punctuation">.</span><span class="token function">front</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 取队首</span>q<span class="token punctuation">.</span><span class="token function">pop</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 出队</span>cout<span class="token operator">&lt;&lt;</span>q<span class="token punctuation">.</span><span class="token function">front</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span>cout<span class="token operator">&lt;&lt;</span>q<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 查看队列已有元素长度</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h2><p><code>map</code>是一个映射关系(键值对). 这里的<code>map</code>分为两种, 一种是有序的<code>map</code>, 一种是无序的<code>unordered_map</code>. 使用前都需要分别导入.</p><p>map:</p><pre class="line-numbers language-cpp"><code class="language-cpp">map<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token punctuation">,</span><span class="token keyword">int</span><span class="token operator">></span> m<span class="token punctuation">;</span><span class="token comment" spellcheck="true">//有序, 底层为树状结构</span>m<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">;</span>m<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">;</span>m<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">9</span><span class="token punctuation">;</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">auto</span> it<span class="token operator">=</span>m<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>it<span class="token operator">!=</span>m<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>it<span class="token operator">++</span><span class="token punctuation">)</span>    cout<span class="token operator">&lt;&lt;</span>it<span class="token operator">-</span><span class="token operator">></span>first<span class="token operator">&lt;&lt;</span><span class="token string">" "</span><span class="token operator">&lt;&lt;</span>it<span class="token operator">-</span><span class="token operator">></span>second<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">auto</span> tmp<span class="token operator">:</span>m<span class="token punctuation">)</span><span class="token punctuation">{</span>    cout<span class="token operator">&lt;&lt;</span>tmp<span class="token punctuation">.</span>first<span class="token operator">&lt;&lt;</span><span class="token string">" "</span><span class="token operator">&lt;&lt;</span>tmp<span class="token punctuation">.</span>second<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>unordered_map:</p><pre class="line-numbers language-cpp"><code class="language-cpp">unordered_map<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token punctuation">,</span><span class="token keyword">int</span><span class="token operator">></span> m<span class="token punctuation">;</span><span class="token comment" spellcheck="true">//无序, 底层为哈希结构</span>m<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">;</span>m<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">;</span>m<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">9</span><span class="token punctuation">;</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">auto</span> it<span class="token operator">=</span>m<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>it<span class="token operator">!=</span>m<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>it<span class="token operator">++</span><span class="token punctuation">)</span>    cout<span class="token operator">&lt;&lt;</span>it<span class="token operator">-</span><span class="token operator">></span>first<span class="token operator">&lt;&lt;</span><span class="token string">" "</span><span class="token operator">&lt;&lt;</span>it<span class="token operator">-</span><span class="token operator">></span>second<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">auto</span> tmp<span class="token operator">:</span>m<span class="token punctuation">)</span><span class="token punctuation">{</span>    cout<span class="token operator">&lt;&lt;</span>tmp<span class="token punctuation">.</span>first<span class="token operator">&lt;&lt;</span><span class="token string">" "</span><span class="token operator">&lt;&lt;</span>tmp<span class="token punctuation">.</span>second<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>pair的用法, 是一个映射结构.</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token keyword">bool</span> <span class="token function">cmp</span><span class="token punctuation">(</span>pair<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token punctuation">,</span><span class="token keyword">int</span><span class="token operator">></span> a<span class="token punctuation">,</span>pair<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token punctuation">,</span><span class="token keyword">int</span><span class="token operator">></span> b<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">return</span> a<span class="token punctuation">.</span>first<span class="token operator">></span>b<span class="token punctuation">.</span>first<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    unordered_map<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token punctuation">,</span><span class="token keyword">int</span><span class="token operator">></span> m<span class="token punctuation">;</span><span class="token comment" spellcheck="true">//无序, 底层为哈希结构</span>    m<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">;</span>    m<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">;</span>    m<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">9</span><span class="token punctuation">;</span>    vector<span class="token operator">&lt;</span>pair<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token punctuation">,</span><span class="token keyword">int</span><span class="token operator">>></span> <span class="token function">v</span><span class="token punctuation">(</span>m<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>m<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">sort</span><span class="token punctuation">(</span>v<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>v<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>cmp<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">auto</span> tmp<span class="token operator">:</span>v<span class="token punctuation">)</span><span class="token punctuation">{</span>        cout<span class="token operator">&lt;&lt;</span>tmp<span class="token punctuation">.</span>first<span class="token operator">&lt;&lt;</span>tmp<span class="token punctuation">.</span>second<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Set"><a href="#Set" class="headerlink" title="Set"></a>Set</h2><p>它是集合的封装实现, 同样存在一种有序结构<code>set</code>和一种无序结构<code>unordered_set</code>. 使用前需要分别导入.</p><pre class="line-numbers language-cpp"><code class="language-cpp">set<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> s<span class="token punctuation">;</span><span class="token comment" spellcheck="true">//树状结构 有序</span>unordered_set<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> s2<span class="token punctuation">;</span><span class="token comment" spellcheck="true">//哈希结构 无序</span>s<span class="token punctuation">.</span><span class="token function">insert</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">;</span>s<span class="token punctuation">.</span><span class="token function">insert</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>s<span class="token punctuation">.</span><span class="token function">insert</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>s<span class="token punctuation">.</span><span class="token function">insert</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>cout<span class="token operator">&lt;&lt;</span>s<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">auto</span> tmp<span class="token operator">:</span>s<span class="token punctuation">)</span>    cout<span class="token operator">&lt;&lt;</span>tmp<span class="token operator">&lt;&lt;</span><span class="token string">" "</span><span class="token punctuation">;</span>cout<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">auto</span> it<span class="token operator">=</span>s<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>it<span class="token operator">!=</span>s<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>it<span class="token operator">++</span><span class="token punctuation">)</span>    cout<span class="token operator">&lt;&lt;</span><span class="token operator">*</span>it<span class="token operator">&lt;&lt;</span><span class="token string">" "</span><span class="token punctuation">;</span>cout<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Deque"><a href="#Deque" class="headerlink" title="Deque"></a>Deque</h2><p>此为双端队列的封装实现, 使用前需要导入<code>&lt;deque&gt;</code>.</p><pre class="line-numbers language-cpp"><code class="language-cpp">deque<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> d<span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 4 9 1 2</span>d<span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 尾部添加</span>d<span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">;</span>d<span class="token punctuation">.</span><span class="token function">push_front</span><span class="token punctuation">(</span><span class="token number">9</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 首部添加</span>d<span class="token punctuation">.</span><span class="token function">push_front</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>d<span class="token punctuation">.</span><span class="token function">pop_back</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 尾部删除</span>d<span class="token punctuation">.</span><span class="token function">pop_front</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 头部删除</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">auto</span> tmp<span class="token operator">:</span>d<span class="token punctuation">)</span> cout<span class="token operator">&lt;&lt;</span>tmp<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">auto</span> it<span class="token operator">=</span>d<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>it<span class="token operator">!=</span>d<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>it<span class="token operator">++</span><span class="token punctuation">)</span> cout<span class="token operator">&lt;&lt;</span><span class="token operator">*</span>it<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span><span class="token function">sort</span><span class="token punctuation">(</span>d<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>d<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>greater<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 排序</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="List"><a href="#List" class="headerlink" title="List"></a>List</h2><p>此为双向链表的实现, 使用前需要导入<code>&lt;list&gt;</code>.</p><pre class="line-numbers language-cpp"><code class="language-cpp">list<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> li<span class="token punctuation">;</span>li<span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 末尾插入</span>li<span class="token punctuation">.</span><span class="token function">push_front</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 头部插入</span>li<span class="token punctuation">.</span><span class="token function">emplace_front</span><span class="token punctuation">(</span><span class="token number">9</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 也是头插</span>li<span class="token punctuation">.</span><span class="token function">emplace_back</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 也是尾插</span>li<span class="token punctuation">.</span><span class="token function">insert</span><span class="token punctuation">(</span><span class="token operator">++</span>li<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 第一个元素后插入</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">auto</span> tmp<span class="token operator">:</span>li<span class="token punctuation">)</span> cout<span class="token operator">&lt;&lt;</span>tmp<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">auto</span> it<span class="token operator">=</span>li<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>it<span class="token operator">!=</span>li<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>it<span class="token operator">++</span><span class="token punctuation">)</span> cout<span class="token operator">&lt;&lt;</span><span class="token operator">*</span>it<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span><span class="token comment" spellcheck="true">//排序不能使用头文件algorithm中的sort 使用list模板自定义的sort方法</span>li<span class="token punctuation">.</span><span class="token function">sort</span><span class="token punctuation">(</span>greater<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//从大到小排序</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><ul><li><a href="http://c.biancheng.net/stl/" target="_blank" rel="noopener">STL中文教程</a></li><li><a href="http://www.cplusplus.com/reference/stl/" target="_blank" rel="noopener">STL英文文档</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> STL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP相关知识</title>
      <link href="/posts/26379.html"/>
      <url>/posts/26379.html</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>2020.8.24</strong>: 更新word2vec的部分内容.</p></blockquote><h1 id="NLP相关知识"><a href="#NLP相关知识" class="headerlink" title="NLP相关知识"></a>NLP相关知识</h1><p>整个流程: 分词 Tokenize -&gt; 预处理 Preprocess -&gt; 特征工程 Feature engine -&gt;  ML.</p><h2 id="分词-Tokenize"><a href="#分词-Tokenize" class="headerlink" title="分词 Tokenize"></a>分词 Tokenize</h2><p>就是把每个句子按照词语分开, 包括标点. 只有分词后才方便后续对句子的过滤. 中文分词和英文分词是不一样的. 英文分词只需要直接分离标点和空格就行, 中文分词常会因为不同NLP库的处理模式不同而结果不唯一. 有时候分词没那么容易, 在社交语言中和常常会有拼写错误, 缩写, URL, emoji, 单位名称书写不统一… 常常用正则一块处理掉. 值得注意的是, 有时去除它们不一定能带来好的效果.</p><h2 id="文本预处理"><a href="#文本预处理" class="headerlink" title="文本预处理"></a>文本预处理</h2><p>库这用<strong>NLTK</strong>用的多. 对于词形归一和词干提取, 无论是哪种方法都不能正确地处理所有的单词, 常常会引入噪声, 需要结合实际效果而定.</p><h3 id="词形归一-Lemma"><a href="#词形归一-Lemma" class="headerlink" title="词形归一 Lemma"></a>词形归一 Lemma</h3><p>把所有词的变形, 全部都归为一个形式. 通过语言学家的wordnet进行归一化. 但是Lemma的过程中常常会因为不考虑单词词性而归一错误, 这时候就需要附加标注的词性(Pos Tag)进行映射.</p><h3 id="词干提取-Stemming"><a href="#词干提取-Stemming" class="headerlink" title="词干提取 Stemming"></a>词干提取 Stemming</h3><p>简单来说就是直接把不影响词性的词根直接砍掉. </p><h3 id="停止词-Stopwords"><a href="#停止词-Stopwords" class="headerlink" title="停止词 Stopwords"></a>停止词 Stopwords</h3><p>停止词也叫停用词, 在英语里面遇到的a, the, or 等使用频率很高的词基本都是停止词. 去掉停止词后仍然可以表达出句子的意思, 去掉停止词可以节省大量的空间. 但是停止词也不是什么任务都去掉的, 比如判断文章相似度之类或者给文章打分的任务就不应该去除停止词, 因为去除后会导致句子结构发生变化.</p><h2 id="特征工程-Feature-Engineering"><a href="#特征工程-Feature-Engineering" class="headerlink" title="特征工程 Feature Engineering"></a>特征工程 Feature Engineering</h2><h3 id="基本语义特征"><a href="#基本语义特征" class="headerlink" title="基本语义特征"></a>基本语义特征</h3><p>主要是一些句子上的差别. 将各种描述句子的特征加加减减.</p><p>问题1和问题2的符号差异, 问号差异, 问题1和问题2分别的句子长度, 长度差异, 长度差异率, 字符数量差异, 字符差异率, 情感分析差异, 起始词(疑问词)的差异, 共享词的交, 并, 数量差异, 数量差异率, fuzz_qratio, fuzz_WRatio, fuzz_partial_ratio, partal_token_sort_ratio, token_set_ratio…</p><p>距离特征有cosine_word2vec, cityblock_distance, canberra_distance, euclidean_distance, braycurits_distance, minkowski_distance, skew_q1, skew_q2, kur_q1, kur_q2, wmd.</p><h3 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h3><p>TFIDF(Term Frequency - Inverse Document Frequency) 词频 - 逆文本频率. TF即词的词频, IDF可以帮助我们理解这个词的重要程度.</p><p>TF(Term Frequency): 一个term在文档中出现的频繁程度. 但是词频并不能反映这个词语的重要性, 有时候它们出现很多次, 但却没有什么意义比如停用词, 没有意义的词语没法起到文本分类的作用.</p><p>对于词语$i$, 在文档$j$ 有:<br>$$<br>TF_{i, j} = \frac{n_{i, j}}{\sum_k n_{k, j}} = \frac{某个词在文章中出现的次数}{文章的总词数}\<br>$$<br>所以就需要将他们乘个缩放因子, 来平衡掉出现次数过多但无意义的影响.</p><p>IDF(Inverse Document Frequency) 逆文档频率, 它与一个词常见程度成反比, 这样越普遍的词语, 越没有实际意义. 对于语料库文章总数$\left| D\right|$, 包含词语$j$ 的文档数$\left| j:t_i \in d_j \right|$ 有:<br>$$<br>IDF_i= \log{\frac{\left | D \right |}{1+\left| j:t_i \in d_j \right|}} = \log{\frac{语料库的文档总数}{包含该词的文档数+1}}<br>$$<br>加1是为了规避词语不在语料库中分母为0的情况.</p><p>而TF-IDF就是TF和IDF的乘积. 能够看出来, 某个词语在某篇文章出现的次数越多越重要, 在所有文章中出现的次数越多越不重要.<br>$$<br>TFIDF = TF \times IDF<br>$$<br>但是TF-IDF也有缺陷, 它忽略了文本中词语的位置信息. 有些文章的段首明显句首的权重更高. 其次有些文章的关键词可能只出现了1-2次.</p><h3 id="词袋模型-Bag-of-words-model"><a href="#词袋模型-Bag-of-words-model" class="headerlink" title="词袋模型 Bag of words model"></a>词袋模型 Bag of words model</h3><p>词袋模型(Bag of words model) 将每段文本都由装着词的袋子表示, 对于比如对于以下文本:</p><ol><li>John likes to watch movies. Mary likes movies too.</li><li>John also likes to watch football games.</li></ol><p>能够生成一个含有10个不同词语的词表:</p><p>[John, likes, to, watch, movies, also, football, games, Mary, too]</p><p>然后结合单词出现的次数, 能够将句子表示为:</p><ol><li>[1, 2, 1, 1, 2, 1, 1, 0, 0, 0]</li><li>[1, 1, 1, 1, 0, 0, 0, 1, 1, 1]</li></ol><p>词袋模型能够将文本转化为向量, 但是却没有保留文本之间的语序, N元语法对这个问题进行了改善.</p><h3 id="N-Gram"><a href="#N-Gram" class="headerlink" title="N-Gram"></a>N-Gram</h3><p>N元语法(N-Gram)基于N-1马尔科夫假设, 即句子中的第$n$个单词被认为和前$m$个单词相关, 即:<br>$$<br>P(x_1, x_2,… , x_n) = P(x_1)P(x_2|x_1)\cdots P(x_n|x_{n-m},…,x_{n-1})<br>$$<br>如果一个词出现只依赖于它前面的一个词, 称为Bi-gram, 如果依赖于前面的两个词, 称为Tri-gram.</p><p>一般就采用$N=2$或$N=3$即Bi-gram和Tri-gram. 用极大似然估计来计算频率, 有:<br>$$<br>\begin{aligned}<br>&amp;p(w_n|w_{n-1})=\frac{C(w_{n-1}w_n)}{C(w_{n})}\\<br>&amp;p(w_n|w_{n-1}w_{n-2})=\frac{C(w_{n-2}w_{n-1}w_n)}{C(w_{n-2}w_{n-1})}\\<br>&amp;p(w_n|w_{n-1}\cdots w_2w_1)=\frac{C(w_1w_2\cdots w_n)}{C(w_1w_2\cdots w_{n-1})}<br>\end{aligned}<br>$$<br>举个二元语法的例子:</p><blockquote><ol><li>oh I am Sam oh</li><li>oh Sam I am oh</li><li>oh I do not like eggs and ham. oh</li></ol></blockquote><p><code>I</code> 出现了三次, <code>I am</code> 出现了两次, 所以求得$p(am|I) = \frac{C(I\ am)}{C(am)}=\frac{2}{3}$, 同样计算出:<br>$$<br>\begin{aligned}<br>P(I|oh) &amp;= \frac{2}{3}, P(Sam|am)=\frac{1}{2}, P(oh|Sam)=\frac{1}{2}\\<br>P(do|I)&amp;=\frac{1}{3}, P(not|do)=\frac{1}{1}, P(like|not)=\frac{1}{1}<br>\end{aligned}<br>$$</p><h3 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h3><p>Word to vector是一种浅层神经网络, <strong>构建一个保留了单词的上下文相似性的低维向量来表示语料库中的文本</strong>. 在训练完成后, Word2Vec可以映射每个词到一个向量(也就是词向量). 词向量一般是稠密的, 低维(不像One-hot维数很高). Word2Vec是<strong>词嵌入</strong>(Word Embedding)的一种. 在词嵌入空间当中, 词意相似的词语通常具有相同的方向, 比如各种水果可能在空间中的位置类似. 值得一提的是, Embedding这种技术在人脸识别当中也叫作编码, 经常将人脸图片在空间中编码为一个向量, 与其他的向量进行比对, 也是利用相似度算法判断是否为该人, 原理实际一致. </p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/word2vec.jpg" style="zoom:50%;" /><p>整体步骤如下:</p><ol><li>在Input Layer, 某个词语被转化为One-Hot向量(高维, 稀疏)</li><li>在Hidden Layer, 稀疏向量被做了一次<strong>线性变换</strong>, 即$Wx+b$. 或者视为隐藏层神经元不激活.</li><li>获得对应词语在稠密空间的映射.</li></ol><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/embedding1.png" style="zoom: 50%;" /><p>由于输入向量是一个独热稀疏向量, 那么实际上就可以直接获得Hidden Layer中的<strong>唯一神经元权重被激活</strong>, 也就对应了唯一的词向量, 完成这个映射过程, 也就获得了这个词所对应的embedding形式. 如下图左侧矩阵是某个词的独热向量, 右侧是词嵌入矩阵, 二者做矩阵乘法可以获得这个词所对应的嵌入式表示.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/embedding2.png" style="zoom:67%;" /><p>Xin Rong制作了展示<a href="https://ronxin.github.io/wevi/" target="_blank" rel="noopener">词嵌入是如何训练的</a>的网站. </p><p>现在基本上Word2Vec有两种变体用的是最多的, 一种是CBOW, 一种是Skip-gram. 两种算法在进行优化时所采用的函数不同, 这二者的结构是完全相反的. 详见<a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">原论文</a>.</p><h4 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h4><p>CBOW(Continuous Bag-of-Words Model), CBOW是<strong>通过上下文来预测中心词</strong>. 即在已知上文$w_{t-c}, w_{t-c+1}, \dots, w_{t-1}$和下文$w_{t+1}, w_{t+2}, \dots, w_{t+c}$1的情况下预测中心词$w_t$.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/cbow.jpg" style="zoom:50%;" /><p>在中间的隐藏层中, 我们对$w_{t-c}, \dots, w_{t-1}, w_{t+1}, \dots, w_{t+c}$的向量相加, 并除以$2c$. 最终输出层经过Softmax得到一个近似独热的向量.</p><p>根据极大似然, 对于单词集合$T$, 最终要最大化:<br>$$<br>L=\frac{1}{T}\sum_t{\log{P(w_t|w_{t-c}\cdots w_{t+c})}}<br>$$</p><h4 id="Skip-Gram"><a href="#Skip-Gram" class="headerlink" title="Skip-Gram"></a>Skip-Gram</h4><p>Skip-gram通过<strong>中心词预测上下文</strong>.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/skipgram.png" style="zoom: 50%;" /><p>最后也是通过Softmax得出来很多近似独热的向量, 也是最大化如下函数:<br>$$<br>J=\frac{1}{T}\sum_{t=1}^T{\sum_{-c \leq j \leq c, j \neq 0}{\log p(w_{t+j}|w_t)}}<br>$$</p><h4 id="相似度算法"><a href="#相似度算法" class="headerlink" title="相似度算法"></a>相似度算法</h4><h5 id="余弦相似度"><a href="#余弦相似度" class="headerlink" title="余弦相似度"></a>余弦相似度</h5><p>对于余弦$\cos$想必都是了解的, 其实余弦相似度就是通过<strong>两个向量之间的余弦值</strong>来衡量相似度. 两个向量越不相似, 夹角就越大, 余弦值也越大. 这个向量可以是来自于任何文本向量化后的向量(比如TF-IDF和Word2Vec). </p><p>由向量内积:<br>$$<br>a\cdot b = \vert \vert a \vert \vert \ \vert \vert b \vert \vert \cos{\theta}<br>$$<br>有:<br>$$<br>similarity = \cos{\theta} = \frac{A \cdot B}{\vert\vert A\vert\vert \ \vert\vert B\vert\vert} = \frac{\sum\limits_{i=1}^nA_i\times B_i}{\sqrt{\sum\limits_{i=1}^n(A_i)^2}\times \sqrt{\sum\limits_{i=1}^n(B_i)^2}}<br>$$<br>相似度的范围在$(-1, 1)$之间, 1表示它们完全相同, 0表示相互独立, -1表示完全相反. 如果是$TF-IDF$下的向量, 由于不能为负数, 两个向量角度不大于90度.</p><h5 id="Jaccard相似度"><a href="#Jaccard相似度" class="headerlink" title="Jaccard相似度"></a>Jaccard相似度</h5><p>Jaccard相似度是在没有将文本数据向量化之前的一种度量. 它处理的是<strong>集合</strong>, 在NLP问题上就是两个句子的词语集合. Jaccard相似度是由Jaccard系数引出来的.<br>$$<br>\displaylines{<br>J(A, B) = \frac{|A \cap B|}{|A \cup B|}\\<br>d_j(A, B) = 1 - J(A, B) = \frac{|A \cup B| - |A\cap B|}{|A \cup B|}<br>}<br>$$<br>与余弦相似度不同的是, <strong>面对重复的词, Jaccard相似度不会有影响</strong>, 因为它是集合上的运算, 自带去重的效果, 如果是余弦相似度则会受到影响.</p><h5 id="词移距离"><a href="#词移距离" class="headerlink" title="词移距离"></a>词移距离</h5><p>词移距离WMD(Word Move Distance)是基于Word2vec特性开发出来的, 当单词经过Word2Vec映射成一个词向量的时候, 语义相近的单词距离会比较近, 比如king和queen在词向量空间中的距离就比sky和apple的近. </p><p>我在项目中用到的是直接用所有单词的词向量加权求和, 然后再用欧氏距离进行比较, 就能衡量两个句子的相似度, 还有用TF-IDF作为权重, 加权求和. 但是词移距离后续的叙述很麻烦, 不再详细说了, 因为涉及到求解和词转移代价等.</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 词袋模型 </tag>
            
            <tag> Word2Vec </tag>
            
            <tag> 特征工程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之XGBoost</title>
      <link href="/posts/36969.html"/>
      <url>/posts/36969.html</url>
      
        <content type="html"><![CDATA[<h1 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h1><p><code>XGBoost</code>是<code>Extreme Gradient Boosting</code>的缩写, 作者是陈天奇大神. XGB因为其高准确率, 易于使用而在各类数据科学竞赛譬如Kaggle, 天池等十分流行. XGB与GBDT十分相似, 可以将XGB视为是GBDT的一个优化形式. 事实上, 如果不考虑工程实现和解决问题上的一些差异, XGB与GBDT比较大的不同就是目标函数的定义.</p><h2 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h2><h3 id="模型和参数"><a href="#模型和参数" class="headerlink" title="模型和参数"></a>模型和参数</h3><p>对于最为常见而简单的线性模型来说, 其预测值为:<br>$$<br>\hat{y}_i = \sum_j \theta_j x_{ij}<br>$$<br>对不同的任务和模型来说, $\theta$ 有着不同的含义. 这个$\theta$ 是模型中不确定的部分, 我们需要通过反复学习来调整它使得它趋近于最优.</p><h3 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h3><p>对于一般的任务来说, 在训练时就是要找到一个最优参数$\theta$ 使得预测的效果最好. 定义一个目标函数来衡量与训练数据的拟合程度.<br>$$<br>\text{obj}(\theta) = L(\theta) + \Omega(\theta)<br>$$<br>其中$L$ 是训练损失函数, $\Omega$ 是正则项. 一般来说损失函数有多种选择, 例如大多数梯度提升算法的MSE:<br>$$<br>L(\theta) = \sum_i (y_i-\hat{y}_i)^2<br>$$<br>或者Logistics损失:<br>$$<br>L(\theta) = \sum_i[ y_i\ln (1+e^{-\hat{y}_i}) + (1-y_i)\ln (1+e^{\hat{y}_i})]<br>$$<br>正则项的作用是通过约束模型的复杂度而避免过拟合, 一个模型过于复杂时, 它的泛化能力会被约束, 当预测新的没见过的数据时能力会大打折扣. 换言之, 一个好的模型会在方差和偏差之间达到平衡. </p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/xgb正则项.png" style="zoom: 67%;" /><p>L1正则:<br>$$<br>\Omega(w)=\sum_i^W|w_i|<br>$$<br>L2正则:<br>$$<br>\Omega(w)=\sum_i^Ww_i^2<br>$$</p><h2 id="回归树-CART"><a href="#回归树-CART" class="headerlink" title="回归树 CART"></a>回归树 CART</h2><p>CART(Classification And Regression Tree)是XGB的基学习器. 在XGB官网上给出了一个讲解CART回归树的例子, 利用CART来预测谁会玩电脑游戏.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/xgbcart.png" style="zoom:50%;" /><p>CART利用输入的特征来划分输入样本, 并在每个叶子节点上都有一个预测权重, 这与分类决策树有些不同. </p><p>树每次进行划分, 实际上是将搜索空间进行分割:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/xgbcart示例.png" style="zoom: 33%;" /><p>通常, 一棵树的预测能力不够强大, 所以需要将多棵树集成起来.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/xgbtwocart.png" style="zoom: 50%;" /><p>将这两棵树的得分都加起来, 发现小男孩的得分是最高的. 事实上这两棵树在做互补的工作. 假设我们有$K$ 棵树, 分类函数为$f$, $\mathcal {F}$ 是包含所有回归树的函数空间, 那么对于第$i$ 类最终的预测值$\hat{y_i}$为:<br>$$<br>\hat{y}_i = \sum_{k=1}^K f_k(x_i), f_k \in \mathcal{F}<br>$$<br>最终的目标函数就变成了:<br>$$<br>\text{obj}(\theta) = \sum_i^n l(y_i, \hat{y}_i) + \sum_{k=1}^K \Omega(f_k)<br>$$<br>我们必须要求出来的其实就是$f_k$, 一旦有了这个函数, 就能得知树的结构和叶子节点所对应的权值.</p><p>回归树的优势:</p><ul><li>应用广泛，比如 GBM 和随机森林等。几乎一半的数据挖掘比赛获胜者都是靠树的集成方法取胜的</li><li>你不需要将特征标准化，因为做不做输入特征缩放结果是一样的.</li><li>能够学习到更多特征之间的高阶关系.</li><li>具有可扩放行(scalable)，应用于工业界.</li></ul><h2 id="树的提升"><a href="#树的提升" class="headerlink" title="树的提升"></a>树的提升</h2><p>前面已经介绍完了基学习器, 接下来该说说如何训练它们了. 对于时间步$t$, 目标函数就变成了:<br>$$<br>\text{obj} = \sum_{i=1}^n l(y_i, \hat{y}_i^{(t)}) + \sum_{i=1}^t\Omega(f_i)<br>$$</p><h3 id="增量训练"><a href="#增量训练" class="headerlink" title="增量训练"></a>增量训练</h3><p>在XGB中, 作者指出学习树的结构比传统优化问题要困难得多, 并且一次性的学习到所有树结构很难, 所以在这里采用了<strong>一次向上加一棵树</strong>的策略:<br>$$<br>\begin{split}\hat{y}_i^{(0)} &amp;= 0\\<br>\hat{y}_i^{(1)} &amp;= f_1(x_i) = \hat{y}_i^{(0)} + f_1(x_i)\\<br>\hat{y}_i^{(2)} &amp;= f_1(x_i) + f_2(x_i)= \hat{y}_i^{(1)} + f_2(x_i)\\<br>&amp;\dots\\<br>\hat{y}_i^{(t)} &amp;= \sum_{k=1}^t f_k(x_i)= \hat{y}_i^{(t-1)} + f_t(x_i)\end{split}<br>$$<br>这实际上使得XGB每次都去拟合<strong>残差</strong>, 也就是预测值和真实值之间的差.</p><p>在每次训练都要优化目标函数:<br>$$<br>\begin{split}\text{obj}^{(t)} &amp; = \sum_{i=1}^n l(y_i, \hat{y}_i^{(t)}) + \sum_{i=1}^t\Omega(f_i) \\<br>          &amp; =\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(t-1)}+f_{t}\left(x_{i}\right)\right)+\Omega\left(f_{t}\right)+\text { constant }\end{split}<br>$$<br>假设MSE为损失函数, 代入目标函数式:<br>$$<br>\begin{split}\text{obj}^{(t)} &amp; = \sum_{i=1}^n \left(y_i - (\hat{y}_i^{(t-1)} + f_t(x_i))\right)^2 + \sum_{i=1}^t\Omega(f_i) \\<br>          &amp; = \sum_{i=1}^n [2(\hat{y}_i^{(t-1)} - y_i)f_t(x_i) + f_t(x_i)^2] + \Omega(f_t) + \mathrm{constant}\end{split}<br>$$<br>这里的$\hat{y}_i^{(t-1)} - y_i$就是上一轮的残差. 这里MSE体现出非常好的数学性质, 它既含有$f_t(x_i)$ 的一次项, 也含有二次项, 这对求出新树结构非常有帮助. 但是对于其他损失函数不一定能够获得这么好的数学性质, 因此一般都是通过泰勒展开来获得这个一次项和二次项. 不知你还记得不记得二阶泰勒展开:<br>$$<br>f(x+\Delta x) \approx f(x)+f^{\prime}(x) \Delta x+\frac{1}{2} f^{\prime \prime}(x) \Delta x^{2}<br>$$<br>因为我们每过一个时间步$t$ 拟合的都是残差, 所以我们可以把新加入的$f_t(x_i)$ 看做是$\Delta x$, $\hat{y}_{i}^{(t-1)}$ 看做是$x$, 就能将损失函数展开:<br>$$<br>l\left(y_{i}, \hat{y}_{i}^{(t-1)}+f_{t}\left(x_{i}\right)\right)=l(y_i, \hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i)<br>$$<br>其中的$g_i$ 和$h_i$ 分别是$f_t(x_i)$ 的一阶偏导和二阶偏导:<br>$$<br>\begin{split}g_i &amp;= \frac{\partial l(y_i, \hat{y}_i^{(t-1)})}{\partial{\hat{y}_i^{(t-1)}}}\\<br>h_i &amp;= \frac{\partial l(y_i, \hat{y}_i^{(t-1)})}{\partial^2{\hat{y}_i^{(t-1)}}} \end{split}<br>$$<br>注: 这步能够展的原因是$y_i$ 是一个常数, 其实$l\left(y_{i}, \hat{y}_{i}^{(t-1)}+f_{t}\left(x_{i}\right)\right)$ 只是一个与$\hat{y}_{i}^{(t-1)}$ 和$f_t(x_i)$ 相关的函数.</p><p>目标函数就变成了:<br>$$<br>\begin{split}<br>\text{obj}^{(t)} &amp; = \sum_{i=1}^n l(y_i, \hat{y}_i^{(t)}) + \sum_{i=1}^t\Omega(f_i) \\<br>          &amp; =\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(t-1)}+f_{t}\left(x_{i}\right)\right)+\Omega\left(f_{t}\right)+\text { constant } \\<br>          &amp; = \sum_{i=1}^n [l(y_i, \hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i)] + \Omega(f_t) + \mathrm{constant}<br> \end{split}<br>$$<br>对于每个新的时间步$t$, 上一个时间步$t-1$所对应的损失$l(y_i, \hat{y}_i^{(t-1)})$是一个常数, 去掉目标函数中所有与优化无关的常数部分:<br>$$<br>\sum_{i=1}^n [g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i)] + \Omega(f_t)<br>$$</p><h3 id="模型复杂度"><a href="#模型复杂度" class="headerlink" title="模型复杂度"></a>模型复杂度</h3><p>XGB在正则项这里对树模型的生长加以约束. 对于树模型, $T$ 为叶子节点的总数, 设$w$ 为叶子节点的权重序列, 是一个$T$ 维的向量, $q$ 为树的结构, 那么$q(x)$ 就能表示样本$x$ 落在叶子中的位置, $w_{q(x)}$ 就是叶子中样本$x$ 所对应的权重. $d$ 为输入实例的维数.<br>$$<br>f_t(x) = w_{q(x)}, w \in R^T, q:R^d\rightarrow \{1,2,\cdots,T\}<br>$$<br>例如:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/xgb复杂度.png" style="zoom: 33%;" /><p>在XGB中, 定义正则项如下:<br>$$<br>\Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2<br>$$<br>第一项为叶子节点的数目, 第二项为叶子节点权重的L2范数. 那么在上面那个例子中, 正则项$\Omega$ 为:<br>$$<br>\Omega = 3\gamma + \frac{1}{2}\lambda(4+0.01+1)<br>$$<br>假设用$I_j = \{i|q(x_i)=j\}$ 来表示叶子节点$j$ 的样本集, 其中包含叶子$j$ 的所有样本$x_i$. 将其从$i$ 表示转化为用$j$ 表示的形式:<br>$$<br>\begin{split}<br>\text{obj}^{(t)} &amp; \approx \sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right) \\<br>&amp;=\sum_{i=1}^{n}\left[g_{i} w_{q\left(x_{i}\right)}+\frac{1}{2} h_{i} w_{q\left(x_{i}\right)}^{2}\right]+\gamma T+\gamma \frac{1}{2} \sum_{j=1}^{T} w_{j}^{2} \\<br>&amp;=\sum_{j=1}^{T}\left[\left(\sum_{i \in I_{j}} g_{i}\right) w_{j}+\frac{1}{2}\left(\sum_{i \in I_{j}} h_{i}+\gamma\right) w_{j}^{2}\right]+\gamma T\end{split}<br>$$<br>并定义$G_j = \sum_{i\in I_j} g_i ,\quad H_j = \sum_{i\in I_j} h_i$, 则原目标函数又可以写为:<br>$$<br>\text{obj}^{(t)} = \sum^T_{j=1} [G_jw_j + \frac{1}{2} (H_j+\lambda) w_j^2] +\gamma T<br>$$<br>树的结构如果是已经固定的, $G_j$ 和 $H_j$ 就是可以计算的了. 此时目标函数就是一个关于$w_j$ 的一元二次方程, 直接利用最值公式得解:<br>$$<br>\begin{split}w_j^\ast &amp;= -\frac{G_j}{H_j+\lambda}\\<br>    \text{obj}^\ast &amp;= -\frac{1}{2} \sum_{j=1}^T \frac{G_j^2}{H_j+\lambda} + \gamma T\end{split}<br>$$<br>下面是一个计算$\text{obj}$ 的例子:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/xgbstruct_score.png" style="zoom: 67%;" /><p>分数越小, 结构就越好.</p><h3 id="学习树结构"><a href="#学习树结构" class="headerlink" title="学习树结构"></a>学习树结构</h3><p>优化工作已经基本做完了, 但是还没有提到如何确定一棵树的结构. XGB使用了和CART回归树一样的想法, 遍历所有特征的所有特征划分点, 但使用的目标函数不一样. 利用贪心算法, 不断地枚举不同树的结构, 然后利用打分函数来寻找出一个最优结构的树, 接着加入到模型中, 不断重复. 叶子分裂也是依赖于分裂前后的$\text{obj}$ 而决定的. 分裂后需要检测这次分裂是否会给损失函数带来增益:<br>$$<br>\begin{split}<br>Gain &amp;= \text{obj}_{L+R} - (\text{obj}_L + \text{obj}_R) \\<br>&amp; = [-\frac{1}{2} \frac{(G_L+G_R)^2}{H_L+H_R+\lambda}+\gamma] - [-\frac{1}{2} (\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda})+ 2\gamma ] \\<br>&amp;=\frac{1}{2} \left[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}\right] - \gamma<br>\end{split}<br>$$<br>分裂前后的信息$Gain$ 是如何理解的? $Gain$ 实际上是<strong>分裂后的信息减去了分裂包含的信息</strong>, 也就是目标函数$Obj$ 的相反数. 不分裂则视$L+R$ 为一个整体. 所以才有了$Gain$ 中的第三项.</p><p>如果分裂后$Gain&gt;0$ , 说明这次分裂能够使得目标函数下降. 如果$Gain&lt;0$, 说明分裂不能使得目标函数下降, 这次分裂失败了.</p><p>找到最优分枝点的有效方法其实就是<strong>在排序后的实例上从左到右线性地扫描</strong>, 这样就足以决定按此特征的最佳分割. 以年龄作为特征为例:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/xgb扫描.png" style="zoom: 67%;" /><p>只要求出每一边的$G$ 和$H$ 的和, 然后计算$Gain$, 就能找到分枝点.</p><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>XGB用Python就可以非常简单的使用.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> xgboost <span class="token keyword">as</span> xgb<span class="token comment" spellcheck="true"># read in data</span>dtrain <span class="token operator">=</span> xgb<span class="token punctuation">.</span>DMatrix<span class="token punctuation">(</span><span class="token string">'demo/data/agaricus.txt.train'</span><span class="token punctuation">)</span>dtest <span class="token operator">=</span> xgb<span class="token punctuation">.</span>DMatrix<span class="token punctuation">(</span><span class="token string">'demo/data/agaricus.txt.test'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># specify parameters via map</span>param <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'max_depth'</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'eta'</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'objective'</span><span class="token punctuation">:</span><span class="token string">'binary:logistic'</span> <span class="token punctuation">}</span>num_round <span class="token operator">=</span> <span class="token number">2</span>bst <span class="token operator">=</span> xgb<span class="token punctuation">.</span>train<span class="token punctuation">(</span>param<span class="token punctuation">,</span> dtrain<span class="token punctuation">,</span> num_round<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># make prediction</span>preds <span class="token operator">=</span> bst<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>dtest<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/xgb%E6%8E%A8%E5%AF%BC%E6%8C%87%E7%A4%BA%E5%9B%BE.jpeg" alt=""></p><ul><li><a href="https://xgboost.readthedocs.io/" target="_blank" rel="noopener">XGBoost官方文档</a> - 强推</li><li><a href="https://arxiv.org/pdf/1603.02754v1.pdf" target="_blank" rel="noopener">XGBoost论文</a></li><li><a href="https://www.yuque.com/agoclover/ml/axgv87" target="_blank" rel="noopener">这篇博客</a>也翻译的非常非常棒, 可以多读一下.</li><li>在知乎高赞中曾经有一篇文章写的不错, 原作者的博客已经无法使用了, 但在CSDN上有人曾<a href="https://blog.csdn.net/zhaojc1995/article/details/83094853" target="_blank" rel="noopener">转载</a>过.</li><li><a href="https://mp.weixin.qq.com/s?__biz=MzI1MzY0MzE4Mg==&mid=2247485159&idx=1&sn=d429aac8370ca5127e1e786995d4e8ec&chksm=e9d01626dea79f30043ab80652c4a859760c1ebc0d602e58e13490bf525ad7608a9610495b3d&scene=21" target="_blank" rel="noopener">20道XGB面试题</a></li><li><a href="[https://yuanxiaosc.github.io/2019/09/30/XGBoost%E7%9A%84%E5%B7%A5%E7%A8%8B%E5%B8%88%E6%89%8B%E5%86%8C/](https://yuanxiaosc.github.io/2019/09/30/XGBoost的工程师手册/)">XGBoost的工程师手册</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 决策树 </tag>
            
            <tag> XGB </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之K-means</title>
      <link href="/posts/5309.html"/>
      <url>/posts/5309.html</url>
      
        <content type="html"><![CDATA[<h1 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h1><p>K-means是一种最为常用的<strong>硬聚类</strong>算法. 硬聚类指的是分出的样本必须只隶属于某一个类, 而不是给出隶属某几个类的概率. </p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kmeans.jpg" style="zoom: 33%;" /><h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><p>对于给定的$k$类, 聚类所得的簇划分$C_k$, 以及样本$\boldsymbol{x}$, K-means 的目标是最小化平方误差:<br>$$<br>E=\sum_{i=1}^{k} \sum_{\boldsymbol{x} \in C_{i}}\mid\mid\boldsymbol{x}-\boldsymbol{\mu}_{i}\mid\mid_{2}^{2}<br>$$<br>其中$\mu_i$是簇$C_i$的中心点(或者称为<strong>质心</strong>):<br>$$<br>\mu_i=\frac{1}{\mid C_i\mid}\sum_{\boldsymbol x \in C_i}\boldsymbol x<br>$$<br>直观来看, 误差在一定程度上刻画了簇内样本围绕簇均值向量的紧密程度, 误差越小簇内的样本相似度越高. 但是因为求解很难, 所以采用贪心算法通过迭代来近似求解. 具体步骤如下:</p><ol><li>从样本集中随机初始化, 随机选择$k$个样本集作为各类(簇)的中心点, $\{\mu_1,\mu_2,\dots, \mu_k\}$.</li><li>计算所有样本点与各个簇中心之间的距离, 然后把每个样本点划入离该点最近的簇中.</li><li>根据簇中已有的样本点, 重新计算簇中心$\mu_k$.</li><li>重复2 - 3, 直到所有的点无法再更新到其他分类或达到最大迭代次数, 算法结束.</li></ol><h2 id="缺陷"><a href="#缺陷" class="headerlink" title="缺陷"></a>缺陷</h2><p>如果按照上述过程进行求解, 对于一些特殊的情况, 它完全可能会陷入<strong>局部最优</strong>, 这取决于初始化中心点的初始位置. 有时候甚至能产生反直觉的聚类效果. </p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kmeans局部最优.png" style="zoom:50%;" /><p>在上图中, 因为随机初始化是一个局部最优情况, 所以下方的两个簇永远都只隶属于绿色, 上方一个簇被强行拆分成了两个簇. 为了尽可能消除这种情况, sklearn中的初始化非常粗暴, 直接随机初始化了若干组中心点, 看哪个效果比较好就采用哪组的结果. </p><p>另外, <strong>异常值</strong>会使得中心点有很大的变动, 所以K-means对异常值还是非常敏感的. 所以在处理前最好先将异常值去掉.</p><h2 id="改进-K-means"><a href="#改进-K-means" class="headerlink" title="改进 K-means++"></a>改进 K-means++</h2><p>K-means++ 改进了原来的初始化中心点选取方法, 初始聚类中心相互之间应该分得越开越好.</p><ol><li>在数据点随机选取一个样本点为第一个簇中心$C_1$.</li><li>计算剩余样本与所有簇中心的最短距离$D(x_i)=\min [dist(x_i, C_1), dist(x_i, C_2),\dots, dist(x_i, C_k)]$, 使得这个样本点被选为下一个簇中心的概率$P(x)=\frac{D(x)^{2}}{\sum_{x \in X} D(x)^{2}}$. 按照这个概率随机选取一个中心.</li><li>重复步骤2直到选出$k$个簇中心.</li></ol><p>样本点离已有的簇中心距离越远, 概率越大, 就越有可能被选为新的簇中心.</p><h2 id="适用性"><a href="#适用性" class="headerlink" title="适用性"></a>适用性</h2><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/kmeans划分情况.png" style="zoom:67%;" /><p>K-means只考虑簇中心(质心), 而不考虑簇的边缘和其他簇的关系, 所以不善于对狭长数据或质心位置与数据实际位置不着边的数据的聚类.</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 聚类 </tag>
            
            <tag> Kmeans </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>十大排序</title>
      <link href="/posts/62992.html"/>
      <url>/posts/62992.html</url>
      
        <content type="html"><![CDATA[<h1 id="十大排序"><a href="#十大排序" class="headerlink" title="十大排序"></a>十大排序</h1><p>十大排序包括插入排序, 选择排序, 冒泡排序, 归并排序, 希尔排序, 快速排序, 堆排序, 计数排序, 基数排序, 桶排序. </p><h2 id="基本知识"><a href="#基本知识" class="headerlink" title="基本知识"></a>基本知识</h2><p>基本概念:</p><ul><li><strong>稳定性</strong>: 在排序前有两个相同的关键字a和b, 若排序后仍能保证a和b的相对顺序不变(排序前a在b前, 则排序后还是a在b前), 则称为排序算法是稳定的, 否则是不稳定的.</li><li><strong>时间复杂度</strong>: 对排序数据的总的操作次数. 反映当n变化时, 操作次数呈现什么规律.</li><li><strong>空间复杂度</strong>: 是指算法在计算机内执行时所需存储空间的度量, 它也是数据规模n的函数.</li></ul><p>常见的算法排序可以分为两大类:</p><ul><li><strong>比较类排序</strong>: 通过比较来决定元素间的相对次序, 由于其时间复杂度不能突破$O(nlogn)$, 因此也称为非线性时间比较类排序.</li><li><strong>非比较类排序</strong>: 不通过比较来决定元素间的相对次序, 它可以突破基于比较排序的<strong>时间下界</strong>, 以线性时间运行, 因此也称为线性时间非比较类排序.</li></ul><p>比较排序最低的时间复杂度一定是$O(n)$, 你不比较怎么排序嘛, 每个元素比较一次, 最低时间复杂度一定是线性级的.</p><p>比较排序可以分为<strong>交换排序</strong>, <strong>插入排序</strong>, <strong>选择排序</strong>, <strong>归并排序</strong>四大类.</p><ul><li>交换排序: 就是根据序列中两个记录键值的比较结果来对换这两个记录在序列中的位置.<ul><li>冒泡排序</li><li>快速排序</li></ul></li><li>插入排序: 将一个记录插入到已经排好序的有序表中, 逐渐扩大有序表的规模至整个表.<ul><li>简单插入排序</li><li>希尔排序</li></ul></li><li>选择排序: 第一次从待排序的数据元素中选出最小(或最大) 的一个元素, 存放在序列的起始位置, 然后再从剩余的未排序元素中寻找到最小(大) 元素, 然后放到已排序的序列的某个位置(取决于结构).<ul><li>简单选择排序</li><li>堆排序</li></ul></li><li>归并排序: 用分治法, 每次将已有序的子序列合并, 最后得到完全有序的序列.</li></ul><h2 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a>插入排序</h2><p>插入排序我们每个人都会, 在打扑克的时候, 顺牌的过程其实就是多次插入排序的过程. 对于未排序的序列, 在已排序的序列中找到这个关键字的相应位置并插入.</p><ol><li>从第一个元素开始, 该元素可以认为已经被排序; </li><li>取出下一个元素, 在已经排序的元素序列中从后向前扫描; </li><li>如果该元素(已排序) 大于新元素, 将该元素移到下一位置; </li><li>重复步骤3, 直到找到已排序的元素小于或者等于新元素的位置; </li><li>将新元素插入到该位置后; </li><li>重复步骤2 ~ 5.</li></ol><img src="https://gitee.com/Daning0/Images/raw/master/DS/插入排序.gif" style="zoom:67%;" /><p>实现代码如下:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token keyword">void</span> <span class="token function">insertSort</span><span class="token punctuation">(</span><span class="token keyword">int</span> a<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">int</span> len<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> i<span class="token punctuation">,</span> j<span class="token punctuation">,</span> temp<span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 认为0号元素已经有序, 从无序序列挨个选择</span>    <span class="token keyword">for</span><span class="token punctuation">(</span>i<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">;</span> i<span class="token operator">&lt;</span>len<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span><span class="token punctuation">{</span>        temp <span class="token operator">=</span> a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>        j <span class="token operator">=</span> i <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// i-1为有序序列 </span>        <span class="token comment" spellcheck="true">// 当选中的元素比有序序列中元素小的时候, 有序序列元素依次后移 </span>        <span class="token keyword">while</span><span class="token punctuation">(</span>j <span class="token operator">>=</span> <span class="token number">0</span> <span class="token operator">&amp;&amp;</span> temp <span class="token operator">&lt;</span> a<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            a<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>            <span class="token operator">--</span>j<span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token comment" spellcheck="true">// 不要忘记插入选中的元素 </span>        a<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span>    <span class="token punctuation">}</span> <span class="token punctuation">}</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="选择排序"><a href="#选择排序" class="headerlink" title="选择排序"></a>选择排序</h2><p>选择排序是非常符合人类思维直觉的一种排序方式, 从序列中选出一个最小的或最大的元素, 加入到已经有序的排序的最左端或最右端. 最后使得整个序列都有序.</p><ol><li>初始状态：无序区为R[1..n], 有序区为空; </li><li>第i趟排序(i=1,2,3…n-1)开始时, 当前有序区和无序区分别为R[1..i-1]和R(i..n) . 该趟排序从当前无序区中-选出关键字最小的记录 R[k], 将它与无序区的第1个记录R交换, 使R[1..i]和R[i+1..n)分别变为记录个数增加1个的新有序区和记录个数减少1个的新无序区; </li><li>n-1趟结束, 数组有序化了.</li></ol><img src="https://gitee.com/Daning0/Images/raw/master/DS/选择排序.gif" style="zoom:67%;" /><p>实现代码如下:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token keyword">void</span> <span class="token function">selectSort</span><span class="token punctuation">(</span><span class="token keyword">int</span> a<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">int</span> len<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> i<span class="token punctuation">,</span> j<span class="token punctuation">;</span>    <span class="token keyword">int</span> temp<span class="token punctuation">;</span>    <span class="token keyword">int</span> min<span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 遍历每个元素 当然不包括最后一个元素 因为最后选择的一定是最大的 在最右侧</span>    <span class="token keyword">for</span><span class="token punctuation">(</span>i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> i<span class="token operator">&lt;</span>len<span class="token number">-1</span><span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span><span class="token punctuation">{</span>        min <span class="token operator">=</span> i<span class="token punctuation">;</span>         <span class="token comment" spellcheck="true">// 找到最小元素 </span>        <span class="token keyword">for</span><span class="token punctuation">(</span>j<span class="token operator">=</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">;</span> j<span class="token operator">&lt;</span>len<span class="token punctuation">;</span> <span class="token operator">++</span>j<span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token keyword">if</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">&lt;</span> a<span class="token punctuation">[</span>min<span class="token punctuation">]</span><span class="token punctuation">)</span> min <span class="token operator">=</span> j<span class="token punctuation">;</span>         <span class="token punctuation">}</span>        <span class="token comment" spellcheck="true">// 交换最小的元素和当前选中元素 </span>        temp <span class="token operator">=</span> a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>        a<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>min<span class="token punctuation">]</span><span class="token punctuation">;</span>        a<span class="token punctuation">[</span>min<span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h2><p>冒泡排序是实现起来最最最简单的算法, 三行实现. 它执行的过程就像鱼吐泡泡一样, 一点一点浮上来. 每趟排序都有一个元素停到它的最终位置.</p><ol><li>比较相邻的元素. 如果第一个比第二个大, 就交换它们两个; </li><li>对每一对相邻元素作同样的工作, 从开始第一对到结尾的最后一对, 这样在最后的元素应该会是最大的数; </li><li>针对所有的元素重复以上的步骤, 除了最后一个; </li><li>重复步骤1~3, 直到排序完成. </li></ol><img src="https://gitee.com/Daning0/Images/raw/master/DS/冒泡排序.gif" style="zoom:67%;" /><p>实现代码如下:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">// 三行版本 </span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> temp<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> len<span class="token operator">=</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token operator">/</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span> i<span class="token operator">&lt;</span>len<span class="token number">-1</span><span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> j<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> j<span class="token operator">&lt;</span>len<span class="token operator">-</span>i<span class="token number">-1</span><span class="token punctuation">;</span> <span class="token operator">++</span>j<span class="token punctuation">)</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token punctuation">;</span> <span class="token punctuation">(</span>temp<span class="token operator">=</span>a<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">></span>a<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>a<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token operator">=</span>a<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">=</span>temp<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 注: 不能放在函数里! 因为放在函数里传参时sizeof(a)会是一个指针的长度</span><span class="token comment" spellcheck="true">// 除非直接给len数组的长度</span><span class="token comment" spellcheck="true">// 函数版本</span><span class="token keyword">void</span> <span class="token function">bubbleSort</span><span class="token punctuation">(</span><span class="token keyword">int</span> a<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">int</span> len<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> i<span class="token punctuation">,</span> j<span class="token punctuation">;</span>    <span class="token keyword">int</span> temp<span class="token punctuation">;</span>     <span class="token keyword">int</span> flag<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 标识是否发生过交换 </span>    <span class="token comment" spellcheck="true">// 每次都有一个元素到最终位置, 只需要循环n-1次 </span>    <span class="token keyword">for</span><span class="token punctuation">(</span>i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> i<span class="token operator">&lt;</span>len<span class="token number">-1</span><span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span><span class="token punctuation">{</span>        flag <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">//  从头开始冒泡 </span>        <span class="token keyword">for</span><span class="token punctuation">(</span>j<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> j<span class="token operator">&lt;</span>len<span class="token number">-1</span><span class="token operator">-</span>i<span class="token punctuation">;</span> <span class="token operator">++</span>j<span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token keyword">if</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">></span> a<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>                temp <span class="token operator">=</span> a<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>                a<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>                a<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span>                flag <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>             <span class="token punctuation">}</span>        <span class="token punctuation">}</span>        <span class="token comment" spellcheck="true">// 如果没发生交换说明已全部有序 </span>        <span class="token keyword">if</span><span class="token punctuation">(</span>flag <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token keyword">return</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span> <span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h2><p>归并排序建立在归并操作上, 是<strong>分治法</strong>的典型应用. 将已有序的子序列合并, 得到完全有序的序列. </p><ol><li>把长度为n的输入序列分成两个长度为n/2的子序列; </li><li>对这两个子序列分别采用归并排序; </li><li>将两个排序好的子序列合并成一个最终的排序序列. </li></ol><img src="https://gitee.com/Daning0/Images/raw/master/DS/归并排序.gif" style="zoom:67%;" /><p>实现代码如下:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">// 归并操作 </span><span class="token comment" spellcheck="true">// 这里写出mid是因为主函数已经计算过了 </span><span class="token keyword">void</span> <span class="token function">merge</span><span class="token punctuation">(</span><span class="token keyword">int</span> a<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">int</span> low<span class="token punctuation">,</span> <span class="token keyword">int</span> mid<span class="token punctuation">,</span> <span class="token keyword">int</span> high<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> i<span class="token punctuation">,</span> j<span class="token punctuation">,</span> k<span class="token punctuation">;</span>    <span class="token keyword">int</span> n1 <span class="token operator">=</span> mid <span class="token operator">-</span> low <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span>     <span class="token keyword">int</span> n2 <span class="token operator">=</span> high <span class="token operator">-</span> mid<span class="token punctuation">;</span>    <span class="token keyword">int</span> L<span class="token punctuation">[</span>n1<span class="token punctuation">]</span><span class="token punctuation">,</span> R<span class="token punctuation">[</span>n2<span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 将原序列分装进两个数组 </span>    <span class="token keyword">for</span><span class="token punctuation">(</span>i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> i<span class="token operator">&lt;</span>n1<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span><span class="token punctuation">{</span>        L<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>low<span class="token operator">+</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>     <span class="token keyword">for</span><span class="token punctuation">(</span>j<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> j<span class="token operator">&lt;</span>n2<span class="token punctuation">;</span> <span class="token operator">++</span>j<span class="token punctuation">)</span><span class="token punctuation">{</span>        R<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>mid<span class="token operator">+</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    i <span class="token operator">=</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>    k <span class="token operator">=</span> low<span class="token punctuation">;</span>     <span class="token comment" spellcheck="true">// 开始比较归并</span>    <span class="token keyword">while</span><span class="token punctuation">(</span>i <span class="token operator">&lt;</span> n1 <span class="token operator">&amp;&amp;</span> j <span class="token operator">&lt;</span> n2<span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>L<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">&lt;</span> R<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span> a<span class="token punctuation">[</span>k<span class="token operator">++</span><span class="token punctuation">]</span> <span class="token operator">=</span> L<span class="token punctuation">[</span>i<span class="token operator">++</span><span class="token punctuation">]</span><span class="token punctuation">;</span>        <span class="token keyword">else</span> a<span class="token punctuation">[</span>k<span class="token operator">++</span><span class="token punctuation">]</span> <span class="token operator">=</span> R<span class="token punctuation">[</span>j<span class="token operator">++</span><span class="token punctuation">]</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// 如果还有序列没有装完</span>    <span class="token keyword">while</span><span class="token punctuation">(</span>i <span class="token operator">&lt;</span> n1<span class="token punctuation">)</span> a<span class="token punctuation">[</span>k<span class="token operator">++</span><span class="token punctuation">]</span> <span class="token operator">=</span> L<span class="token punctuation">[</span>i<span class="token operator">++</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span>j <span class="token operator">&lt;</span> n2<span class="token punctuation">)</span> a<span class="token punctuation">[</span>k<span class="token operator">++</span><span class="token punctuation">]</span> <span class="token operator">=</span> R<span class="token punctuation">[</span>j<span class="token operator">++</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">// 划分操作 把序列按递归划分为多个分组 </span><span class="token keyword">void</span> <span class="token function">mergeSort</span><span class="token punctuation">(</span><span class="token keyword">int</span> a<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">int</span> low<span class="token punctuation">,</span> <span class="token keyword">int</span> high<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>low <span class="token operator">&lt;</span> high<span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">int</span> mid <span class="token operator">=</span> <span class="token punctuation">(</span>low <span class="token operator">+</span> high<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">;</span>        <span class="token function">mergeSort</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> mid<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> high<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">mergeSort</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> low<span class="token punctuation">,</span> mid<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">merge</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> low<span class="token punctuation">,</span> mid<span class="token punctuation">,</span> high<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="希尔排序"><a href="#希尔排序" class="headerlink" title="希尔排序"></a>希尔排序</h2><p>虽然希尔排序基本上不怎么考虑实现, 但不代表它不重要. 希尔排序是第一个突破$O(n^2)$的算法, 是简单插入排序的改良版. 与插入排序的不同之处在于, 它会优先比较距离较远的元素. 希尔排序又叫<strong>缩小增量排序</strong>. 先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序.</p><ol><li>选择一个增量序列t1, t2, …, tk, 其中ti&gt;tj, tk=1; </li><li>按增量序列个数k, 对序列进行k 趟排序; </li><li>每趟排序, 根据对应的增量ti, 将待排序列分割成若干长度为m 的子序列, 分别对各子表进行直接插入排序. 仅增量因子为1 时, 整个序列作为一个表来处理, 表长度即为整个序列的长度. </li></ol><img src="https://raw.githubusercontent.com/ADAning/Image/master/希尔排序.gif" style="zoom:67%;" /><p>实现代码如下:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token keyword">void</span> <span class="token function">shellSort</span><span class="token punctuation">(</span><span class="token keyword">int</span> arr<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">int</span> n<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> temp<span class="token punctuation">;</span>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> gap<span class="token operator">=</span>n<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">;</span> gap<span class="token operator">></span><span class="token number">0</span><span class="token punctuation">;</span> gap<span class="token operator">/</span><span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">{</span> <span class="token comment" spellcheck="true">// 每趟排序gap是原来的1/2倍 </span>        <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span>gap<span class="token punctuation">;</span> i<span class="token operator">&lt;</span>n<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span><span class="token punctuation">{</span> <span class="token comment" spellcheck="true">// 按照gap循环子序列 </span>            <span class="token comment" spellcheck="true">// 直接插入排序 每次都只比较j和j-gap这两个元素 与直插排序一样 </span>            temp <span class="token operator">=</span> arr<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>            <span class="token keyword">int</span> j<span class="token punctuation">;</span>             <span class="token keyword">for</span><span class="token punctuation">(</span>j<span class="token operator">=</span>i<span class="token punctuation">;</span> j <span class="token operator">>=</span> gap <span class="token operator">&amp;&amp;</span> arr<span class="token punctuation">[</span>j<span class="token operator">-</span>gap<span class="token punctuation">]</span> <span class="token operator">></span> temp<span class="token punctuation">;</span> j<span class="token operator">-</span><span class="token operator">=</span>gap<span class="token punctuation">)</span><span class="token punctuation">{</span>                  arr<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> arr<span class="token punctuation">[</span>j<span class="token operator">-</span>gap<span class="token punctuation">]</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>             <span class="token comment" spellcheck="true">// 如果不小于了 在j处将temp元素插入 </span>            arr<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span> <span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h2><p>名气第一名, 所有场景都通用. <strong>快排是必须会写的排序</strong>. 快排是冒泡排序的优化版, 它每次的交换不再像冒泡排序是交换相邻元素, 而是跳跃交换. 通过一趟排序将待排记录分隔成独立的两部分, 其中一部分记录的关键字均比另一部分的关键字小, 则可分别对这两部分记录继续进行排序, 以达到整个序列有序.</p><ol><li>从数列中挑出一个元素, 称为 “基准”(pivot) ; </li><li>重新排序数列, 所有元素比基准值小的摆放在基准前面, 所有元素比基准值大的摆在基准的后面(相同的数可以到任一边) . 在这个分区退出之后, 该基准就处于数列的中间位置. 这个称为分区(partition) 操作; </li><li>递归地(recursive) 把小于基准值元素的子数列和大于基准值元素的子数列排序. </li></ol><img src="https://gitee.com/Daning0/Images/raw/master/DS/快速排序.gif" style="zoom:67%;" /><p>实现代码如下:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token keyword">void</span> <span class="token function">quickSortLR</span><span class="token punctuation">(</span><span class="token keyword">int</span> a<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">int</span> left<span class="token punctuation">,</span> <span class="token keyword">int</span> right<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> i <span class="token operator">=</span> left<span class="token punctuation">,</span> j <span class="token operator">=</span> right<span class="token punctuation">;</span>    <span class="token keyword">int</span> temp<span class="token punctuation">;</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>left <span class="token operator">&lt;</span> right<span class="token punctuation">)</span><span class="token punctuation">{</span>        temp <span class="token operator">=</span> a<span class="token punctuation">[</span>left<span class="token punctuation">]</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">// 无时不刻都要加上i &lt; j的条件</span>        <span class="token comment" spellcheck="true">// 指针相遇时应该立马停下 </span>        <span class="token keyword">while</span><span class="token punctuation">(</span>i <span class="token operator">&lt;</span> j<span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token comment" spellcheck="true">// 从右侧找一个比基准小的元素 </span>            <span class="token keyword">while</span><span class="token punctuation">(</span>i <span class="token operator">&lt;</span> j <span class="token operator">&amp;&amp;</span> a<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">>=</span> temp<span class="token punctuation">)</span><span class="token punctuation">{</span>                <span class="token operator">--</span>j<span class="token punctuation">;</span>            <span class="token punctuation">}</span>             <span class="token comment" spellcheck="true">// 直接扣到左指针上 </span>            <span class="token keyword">if</span><span class="token punctuation">(</span>i <span class="token operator">&lt;</span> j<span class="token punctuation">)</span><span class="token punctuation">{</span>                a<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>                <span class="token operator">++</span>i<span class="token punctuation">;</span>            <span class="token punctuation">}</span>            <span class="token comment" spellcheck="true">// 从左侧找一个比基准大的元素 </span>            <span class="token keyword">while</span><span class="token punctuation">(</span>i <span class="token operator">&lt;</span> j <span class="token operator">&amp;&amp;</span> a<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">&lt;=</span> temp<span class="token punctuation">)</span><span class="token punctuation">{</span>                <span class="token operator">++</span>i<span class="token punctuation">;</span>            <span class="token punctuation">}</span>            <span class="token comment" spellcheck="true">// 扣到右指针上 </span>            <span class="token keyword">if</span><span class="token punctuation">(</span>i <span class="token operator">&lt;</span> j<span class="token punctuation">)</span><span class="token punctuation">{</span>                a<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>                <span class="token operator">--</span>j<span class="token punctuation">;</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>        <span class="token comment" spellcheck="true">// 二者相遇时就是最终基准位置</span>        a<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span>        <span class="token function">quickSortLR</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> left<span class="token punctuation">,</span> i<span class="token number">-1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">quickSortLR</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> right<span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token punctuation">}</span><span class="token punctuation">}</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>上面代码实现是演示的左右交换版本, 实际上还有一种快慢指针的版本.</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">// 快慢指针版本 参考了豆豆的快排实现</span><span class="token comment" spellcheck="true">// bilibili:BV1Ab411s7To </span><span class="token comment" spellcheck="true">// 其实就是快慢指针交换比pivot小和比pivot大的过程 </span><span class="token comment" spellcheck="true">// 假定传进来的都是下标 </span><span class="token keyword">int</span> <span class="token function">partition</span><span class="token punctuation">(</span><span class="token keyword">int</span> a<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">int</span> left<span class="token punctuation">,</span> <span class="token keyword">int</span> right<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> i <span class="token operator">=</span> left <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 慢指针 并初始化为left左侧 </span>    <span class="token keyword">int</span> j<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 快指针 </span>    <span class="token keyword">int</span> temp<span class="token punctuation">;</span>    <span class="token keyword">int</span> pivot <span class="token operator">=</span> right<span class="token punctuation">;</span>  <span class="token comment" spellcheck="true">// 指定最右边的元素为基准 </span>    <span class="token comment" spellcheck="true">/*         若快指针所指元素比基准元素大        则慢指针什么都不做         若快指针所指向的元素比基准元素小        则让慢指针往前进一位 并交换快慢指针所指元素         因为只有快指针所指元素比基准元素小才让慢指针前进         所以在交换前慢指针指向的元素一定比基准元素大    */</span>     <span class="token keyword">for</span><span class="token punctuation">(</span>j<span class="token operator">=</span>left<span class="token punctuation">;</span> j<span class="token operator">&lt;</span>right<span class="token punctuation">;</span> <span class="token operator">++</span>j<span class="token punctuation">)</span><span class="token punctuation">{</span>         <span class="token keyword">if</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">&lt;</span> a<span class="token punctuation">[</span>pivot<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token operator">++</span>i<span class="token punctuation">;</span>            temp <span class="token operator">=</span> a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>            a<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>            a<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span>          <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// 慢指针i在内的所有左侧元素都比基准元素小</span>    <span class="token comment" spellcheck="true">// 所以交换i右侧的元素和基准元素的位置 </span>    temp <span class="token operator">=</span> a<span class="token punctuation">[</span>pivot<span class="token punctuation">]</span><span class="token punctuation">;</span>    a<span class="token punctuation">[</span>pivot<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    a<span class="token punctuation">[</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 返回的基准元素就是未交换时的a[pivot], 即基准元素 </span>    <span class="token keyword">return</span> i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">void</span> <span class="token function">quickSort</span><span class="token punctuation">(</span><span class="token keyword">int</span> a<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">int</span> left<span class="token punctuation">,</span> <span class="token keyword">int</span> right<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>left <span class="token operator">&lt;</span> right<span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">int</span> pivot <span class="token operator">=</span> <span class="token function">partition</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> left<span class="token punctuation">,</span> right<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">// 获取基准点 对基准的前后各使用快排 </span>        <span class="token function">quickSort</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> left<span class="token punctuation">,</span> pivot<span class="token number">-1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">quickSort</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> pivot<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> right<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h2><p>堆排序是利用堆这种数据结构所设计的一种排序算法. 因为堆是一个近似于完全二叉树的结构, 并且满足堆积的性质：即<strong>子结点的键值或索引总是小于(或者大于) 它的父节点</strong>. 在某些特殊的业务场景, 可能需要构建一个堆, 需要进行查找, 这时用堆排序就很合理.</p><ol><li>将初始待排序关键字序列(R1,R2….Rn)构建成大顶堆, 此堆为初始的无序区; </li><li>将堆顶元素R[1]与最后一个元素R[n]交换, 此时得到新的无序区(R1,R2,……Rn-1)和新的有序区(Rn),且满足R[1,2…n-1]&lt;=R[n]; </li><li>由于交换后新的堆顶R[1]可能违反堆的性质, 因此需要对当前无序区(R1,R2,……Rn-1)调整为新堆, 然后再次将R[1]与无序区最后一个元素交换, 得到新的无序区(R1,R2….Rn-2)和新的有序区(Rn-1,Rn). 不断重复此过程直到有序区的元素个数为n-1, 则整个排序过程完成. </li></ol><img src="https://raw.githubusercontent.com/ADAning/Image/master/堆排序.gif" style="zoom:67%;" /><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">// 堆排序 选择排序 和冒泡排序非常像 </span><span class="token comment" spellcheck="true">// 建堆过程是一个交换的过程  </span><span class="token comment" spellcheck="true">// 直接采用顺序存储结构存储堆(完全二叉树)</span><span class="token keyword">int</span> heap<span class="token punctuation">[</span>maxSize<span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 用它来存堆(大根堆)</span><span class="token comment" spellcheck="true">// 堆调整 low和high选择调整的范围 可以理解为冒泡排序中的一次冒泡 </span><span class="token keyword">void</span> <span class="token function">Sift</span><span class="token punctuation">(</span><span class="token keyword">int</span> heap<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">int</span> low<span class="token punctuation">,</span> <span class="token keyword">int</span> high<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> i<span class="token operator">=</span>low<span class="token punctuation">,</span> j <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">*</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// heap[j]是i的左孩子 </span>    <span class="token keyword">int</span> temp <span class="token operator">=</span> heap<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span>j <span class="token operator">&lt;=</span> high<span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>j <span class="token operator">&lt;</span> high <span class="token operator">&amp;&amp;</span> heap<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">&lt;</span> heap<span class="token punctuation">[</span>j <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span> <span class="token comment" spellcheck="true">// 如果右孩子更大 把j指向右孩子 </span>            <span class="token operator">++</span>j<span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token comment" spellcheck="true">// 当temp比子孙节点中最大的都小, 交换ij, 并将ij顺着树下移一层        </span>        <span class="token keyword">if</span><span class="token punctuation">(</span>temp <span class="token operator">&lt;</span> heap<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            heap<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> heap<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>            i <span class="token operator">=</span> j<span class="token punctuation">;</span>            j <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">*</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span>         <span class="token punctuation">}</span>        <span class="token keyword">else</span> <span class="token keyword">break</span><span class="token punctuation">;</span>     <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// 当j>high时, 将叶子节点调整为temp </span>    heap<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">// 能够看出来整个排序过程和冒泡非常像 只是添加了对堆调整的操作 </span><span class="token keyword">void</span> <span class="token function">heapSort</span><span class="token punctuation">(</span><span class="token keyword">int</span> a<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">int</span> n<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> i<span class="token punctuation">;</span>    <span class="token keyword">int</span> temp<span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 调整为大根堆 </span>    <span class="token keyword">for</span><span class="token punctuation">(</span>i<span class="token operator">=</span>n<span class="token operator">/</span><span class="token number">2</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">;</span> i<span class="token operator">>=</span><span class="token number">0</span><span class="token punctuation">;</span> <span class="token operator">--</span>i<span class="token punctuation">)</span><span class="token punctuation">{</span> <span class="token comment" spellcheck="true">// n/2-1是最后一个非叶子节点 对所有非叶子的堆依次进行调整</span>        <span class="token function">Sift</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> i<span class="token punctuation">,</span> n<span class="token number">-1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token punctuation">}</span>     <span class="token comment" spellcheck="true">// 将根节点和最后一个叶子节点进行交换, 删去原来的根节点, 当i=1即只剩一个节点的时候排序完成 </span>    <span class="token keyword">for</span><span class="token punctuation">(</span>i<span class="token operator">=</span>n<span class="token number">-1</span><span class="token punctuation">;</span> i<span class="token operator">></span><span class="token number">0</span><span class="token punctuation">;</span> <span class="token operator">--</span>i<span class="token punctuation">)</span><span class="token punctuation">{</span> <span class="token comment" spellcheck="true">// 其实i指代的是当前堆有几个节点 </span>        temp <span class="token operator">=</span> a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>        a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> arr<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>        a<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span>         <span class="token function">Sift</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> i<span class="token number">-1</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 每次交换后都进行堆调整 但不要最后一个节点</span>                            <span class="token comment" spellcheck="true">// 因为最后一个节点在调整\之前符合大根堆的定义 </span>                            <span class="token comment" spellcheck="true">// 已经被调到了指定的位置 </span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="计数排序"><a href="#计数排序" class="headerlink" title="计数排序"></a>计数排序</h2><p>计数排序不是基于比较的排序算法, 它的复杂度能到线性级. 效率虽然高, 但使用场景十分受限. 它的原理规定了输入的数据必须是<strong>有确定范围的整数</strong>.</p><ol><li>找出待排序的数组中最大和最小的元素; </li><li>统计数组中每个值为i的元素出现的次数, 存入数组C的第i项; </li><li>对所有的计数累加(从C中的第一个元素开始, 每一项和前一项相加) ; </li><li>反向填充目标数组：将每个元素i放在新数组的第C(i)项, 每放一个元素就将C(i)减去1. </li></ol><img src="https://gitee.com/Daning0/Images/raw/master/DS/计数排序.gif" style="zoom:67%;" /><h2 id="基数排序"><a href="#基数排序" class="headerlink" title="基数排序"></a>基数排序</h2><p>“基”这个字, 指的就是十进制的”<strong>个十百千</strong>“. 基数排序先按照个位进行排序, 然后再按照十位进行排序… 先按照低位进行排序, 然后收集到一起, 再按照高位进行排序, 以此类推,  最后就能获得一个有序的序列.</p><ol><li><p>取得数组中的最大数, 并取得位数; </p></li><li><p>arr为原始数组, 从最低位开始取每个位组成radix数组; </p></li><li><p>对radix进行计数排序(利用计数排序适用于小范围数的特点) ; </p></li></ol><img src="https://gitee.com/Daning0/Images/raw/master/DS/基数排序.gif" style="zoom:67%;" /><h2 id="桶排序"><a href="#桶排序" class="headerlink" title="桶排序"></a>桶排序</h2><p>桶排序跟计数排序一样, 是一种稳定的线性时间排序算法, 这个在机器学习有相似的应用. 按照特征的取值范围对特征进行划分, 然后抽成独热向量. 这里桶排序工作的原理是将数列分到有限数量的桶里, 每个桶再个别排序. 当要被排序的数组内的数值是均匀分配的时候, 桶排序时间复杂度是线性级.</p><ol><li>设置一个定量的数组当作空桶; </li><li>遍历输入数据, 并且把数据一个一个放到对应的桶里去; </li><li>对每个不是空的桶进行排序; </li><li>从不是空的桶里把排好序的数据拼接起来. </li></ol><img src="https://gitee.com/Daning0/Images/raw/master/DS/桶排序.gif" style="zoom:67%;" /><h2 id="复杂度对比"><a href="#复杂度对比" class="headerlink" title="复杂度对比"></a>复杂度对比</h2><table><thead><tr><th><strong>排序算法</strong></th><th><strong>平均时间复杂度</strong></th><th><strong>最好情况</strong></th><th><strong>最坏情况</strong></th><th><strong>空间复杂度</strong></th><th><strong>稳定性</strong></th><th>排序方式</th></tr></thead><tbody><tr><td>冒泡排序</td><td>$O(n^2)$</td><td>$O(n)$</td><td>$O(n^2)$</td><td>$O(1)$</td><td>稳定</td><td>In-place</td></tr><tr><td>选择排序</td><td>$O(n^2)$</td><td>$O(n^2)$</td><td>$O(n^2)$</td><td>$O(1)$</td><td>不稳定</td><td>In-place</td></tr><tr><td>插入排序</td><td>$O(n^2)$</td><td>$O(n)$</td><td>$O(n^2)$</td><td>$O(1)$</td><td>稳定</td><td>In-place</td></tr><tr><td>希尔排序</td><td>$O(n\log n)$</td><td>$O(n^{1.3}$)</td><td>$O(n^2)$</td><td>$O(1)$</td><td>不稳定</td><td>In-place</td></tr><tr><td>归并排序</td><td>$O(n\log n)$</td><td>$O(n\log n)$</td><td>$O(n\log n)$</td><td>$O(n)$</td><td>稳定</td><td>Out-place</td></tr><tr><td>快速排序</td><td>$O(n\log n)$</td><td>$O(n\log n)$</td><td>$O(n^2)$</td><td>$O(\log n)$</td><td>不稳定</td><td>In-place</td></tr><tr><td>堆排序</td><td>$O(n\log n)$</td><td>$O(n\log n)$</td><td>$O(n\log n)$</td><td>$O(1)$</td><td>不稳定</td><td>In-place</td></tr><tr><td>桶排序</td><td>$O(n + k)$</td><td>$O(n + k)$</td><td>$O(n^2)$</td><td>$O(n + k)$</td><td>稳定</td><td>Out-place</td></tr><tr><td>计数排序</td><td>$O(n + k)$</td><td>$O(n + k)$</td><td>$O(n + k)$</td><td>$O(k)$</td><td>稳定</td><td>Out-place</td></tr><tr><td>基数排序</td><td>$O(n × m)$</td><td>$O(n × m)$</td><td>$O(n × m)$</td><td>$O(n + m)$</td><td>稳定</td><td>Out-place</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
            <tag> 排序 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之支持向量机</title>
      <link href="/posts/44188.html"/>
      <url>/posts/44188.html</url>
      
        <content type="html"><![CDATA[<h1 id="支持向量机SVM-Support-Vector-Machine"><a href="#支持向量机SVM-Support-Vector-Machine" class="headerlink" title="支持向量机SVM Support Vector Machine"></a>支持向量机SVM Support Vector Machine</h1><p>SVM是一个监督学习下运作的<strong>线性分类器</strong>. 但是由于核技巧的存在, 使得它本质上成为一个非线性分类器. 因为SVM涉及到很多关于凸优化的内容, 我自己本身不是很了解, 所以尽可能的避开这些内容, 以大致了解工作原理为基准.</p><p>SVM的目标就是寻找一个<strong>最优超平面</strong>, 使得数据能够被这个超平面分开. 超平面这个概念可以由三维和二维进行递推, 在二维空间中的超平面就是一维的, 即一条线, 在三维空间中的超平面是一个二维的平面. 那么以此类推, 在N维空间中的超平面是N-1 维平面.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/svm超平面.png" style="zoom: 50%;" /><p>把超平面定义为:<br>$$<br>f(x)=\operatorname{sign}(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b)<br>$$<br><code>sign</code>的意思是说, 在超平面上面取正号, 下方取负号. 那么很明显$w$ 是法向量, $b$ 是位移项. </p><p>SVM的优化理念是最大化超平面到各类样本之间的距离, 使得这个超平面能够成功分开各类, 并且离这些样本点尽可能的远. 为什么要最大化这个间距呢? 因为在真实数据中的误差可能会干扰实际的预测效果, 间距大意味着更强的<strong>鲁棒性</strong>. 即使加入了一些噪声, 也会因为含有间距而不导致或尽可能少的导致预测错误.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/svm最大化间距.png" style="zoom: 67%;" /><p>把支撑超平面的点(距离超平面最近的样本点)叫做<strong>支持向量</strong>, 也就是图中的超平面旁的实心红色方块和实心蓝色圆. 一般情况下, 最佳超平面一定是到两边支持向量距离相等的那个.</p><h2 id="硬间隔-Hard-Margin"><a href="#硬间隔-Hard-Margin" class="headerlink" title="硬间隔 Hard Margin"></a>硬间隔 Hard Margin</h2><p>硬间隔SVM被转化为一个<strong>寻找最大间隔超平面</strong>的数学问题, 所以Hard Margin SVM也被称为最大间隔分类器.</p><p>样本空间中的样本$x$到超平面的距离$r$ 可以表示为:<br>$$<br>r=\frac{\left|\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\right|}{|\boldsymbol{w}|}<br>$$</p><p>如果超平面能正确分类样本, 则标签$y_i = +1$,对应的范围设为超平面上方, 标签$y_i = -1$,对应的范围设为超平面下方, 即:<br>$$<br>\left\{\begin{array}{ll}<br>\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b &gt; 0, &amp; y_{i}=+1 \\<br>\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b &lt;0, &amp; y_{i}=-1<br>\end{array}\right.<br>$$</p><p>若令间隔为1, 则上述约束更改为:<br>$$<br>\left\{\begin{array}{ll}<br>\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b \geqslant+1, &amp; y_{i}=+1 \\<br>\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b \leqslant-1, &amp; y_{i}=-1<br>\end{array}\right.<br>$$<br>那么相应的两个不同类的支持向量到超平面的距离为:<br>$$<br>\gamma=\frac{2}{|\boldsymbol{w}|}<br>$$<br>如下图所示:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/svm支持向量.jpg" style="zoom: 33%;" /><p>若想找到最大间隔划分超平面, 其实就是为了找到相应的$w$ 和$b$, 使得间距$\gamma$ 最大.<br>$$<br>\begin{aligned}<br>\max _{\boldsymbol{w}, b} &amp; \frac{2}{|\boldsymbol{w}|} \\<br>\text { s.t. } &amp; y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1, \quad i=1,2, \ldots, m<br>\end{aligned}<br>$$<br>最小化$|\boldsymbol{w}|^{-1}$的目标等同于最大化$|\boldsymbol{w}|^2$, 重写为:<br>$$<br>\begin{aligned}<br>\min _{\boldsymbol{w}, b} &amp; \frac{1}{2}|\boldsymbol{w}|^2 \\<br>\text { s.t. } &amp; y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1, \quad i=1,2, \ldots, m<br>\end{aligned}<br>$$<br>这就是SVM的基本形式.</p><p>其实满足约束的是一个<strong>凸二次规划</strong>问题, 计算量很大. 利用<strong>拉格朗日乘子法</strong>解决其<strong>对偶问题</strong>, 向上述约束添加拉格朗日乘子:<br>$$<br>L(\boldsymbol{w}, b, \boldsymbol{\alpha})=\frac{1}{2}|\boldsymbol{w}|^{2}+\sum_{i=1}^{m} \alpha_{i}\left(1-y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)\right)<br>$$<br>那么对$\boldsymbol{w}$和$b$求偏导, 则得到:<br>$$<br>\boldsymbol{w} =\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i} \quad<br>0 =\sum_{i=1}^{m} \alpha_{i} y_{i}<br>$$<br>带回原式将$\boldsymbol{w}$和$b$消去.<br>$$<br>\begin{aligned}<br>&amp;\max _{\boldsymbol{\alpha}} \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j} \\<br>&amp;\text { s.t. } \sum_{i=1}^{m} \alpha_{i} y_{i}=0 \\<br>&amp; \alpha_{i} \geqslant 0, \quad i=1,2, \ldots, m<br>\end{aligned}<br>$$<br>如果能够解出来$\boldsymbol{\alpha}$ ,  就能解出超平面:<br>$$<br>\begin{aligned}<br>f(\boldsymbol{x}) &amp;=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b \\<br>&amp;=\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}+b<br>\end{aligned}<br>$$<br>解决非线性规划问题需要用到更一般的拉格朗日乘子法, 额外加上KKT(Karush-Kuhn-Tucker)条件:<br>$$<br>\left\{\begin{array}{l}<br>\alpha_{i} \geqslant 0 \\<br>y_{i} f\left(\boldsymbol{x}_{i}\right)-1 \geqslant 0 \\<br>\alpha_{i}\left(y_{i} f\left(\boldsymbol{x}_{i}\right)-1\right)=0<br>\end{array}\right.<br>$$<br>如果$\alpha_{i}&gt;0$, 必有$y_if(\boldsymbol{x_i})=1$, 即该样本点位于最大间隔边界上.</p><p>利用<strong>SMO(Sequential minimal optimization, 序列最小优化算法)</strong>, 求解其中的各个$\alpha_i$. SMO固定了$\alpha_i$以外的其他参数, 每次选取一对需要更新的$\alpha_i$和$\alpha_j$, 利用约束$\sum_{i=1}^{m} \alpha_{i} y_{i}=0$来得到更新后的$\alpha_i$和$\alpha_j$.</p><p>上述部分的数学推导有很多细节为了理解而省略了, 数学底子有些薄弱, 暂时无法细究. 日后会补上. 这部分也是面试时的核心, 尤其是对偶, 拉格朗日乘子法和KKT, SMO.</p><h2 id="核函数-Kernel-Function"><a href="#核函数-Kernel-Function" class="headerlink" title="核函数 Kernel Function"></a>核函数 Kernel Function</h2><p>这么搞一定会有些问题. SVM本来就是一个线性分类器, 假如样本分布真的是线性不可分的呢? 核函数和核技巧使得SVM具有一定的非线性分割能力. 通过核函数, 使得样本从低维映射到高维, 如果样本在高维空间中线性可分, 那么SVM就可以成功将样本分开.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/svm核函数.jpg" style="zoom:67%;" /><p>而核函数的定义实际上也隐式的定义了额外的特征空间, 使得这些特征在空间中线性可分. 所以和函数的选择和结果有着直接的关联, 它也是SVM里最大的变数. 文本数据一般采用线性核, 情况不明时可以先尝试高斯核.</p><h2 id="软间隔-Soft-Margin"><a href="#软间隔-Soft-Margin" class="headerlink" title="软间隔 Soft Margin"></a>软间隔 Soft Margin</h2><p>在现实任务中收集的数据很难确定核函数, 即便找到了某个核函数使得所有训练数据在特征空间中线性可分, 也很难确定这个结果是不是过拟合造成的. 缓解该问题的办法是允许SVM在样本划分时出现一些错误, 引入<strong>软间隔</strong>的概念.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/svm软间隔.jpg" style="zoom:50%;" /><p>与硬间隔相比, 软间隔的优化目标更加的松弛, 体现在对约束条件加入松弛变量.<br>$$<br>\begin{aligned}<br>&amp;y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)\geq 1-\xi_{i}, \quad \xi_{i} \geq 0 \\<br>&amp;\min \sum_{i=1}^m \xi_i<br>\end{aligned}<br>$$<br>那么相应的优化目标也变为:<br>$$<br>\begin{array}{cl}<br>\min\limits_{w, b, \xi_i} &amp; \frac{1}{2}|\boldsymbol{w}|^{2}+C \sum\limits_{i=1}^{n} \xi_{i} \\<br>\text {s.t.} &amp; y_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right) \geq 1-\xi_{i} \\<br>&amp; \xi_{i} \geqslant 0 \quad i=1,2, \cdots, n<br>\end{array}<br>$$<br>松弛变量$\xi_i$ 为可替换的损失函数, 一般使用hinge损失, 对数损失, 指数损失.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/svm损失.jpg" style="zoom:50%;" />]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Markdown公式整理</title>
      <link href="/posts/54490.html"/>
      <url>/posts/54490.html</url>
      
        <content type="html"><![CDATA[<h1 id="Markdown公式整理"><a href="#Markdown公式整理" class="headerlink" title="Markdown公式整理"></a>Markdown公式整理</h1><p>Markdown支持Latex的数学公式简直是太棒了. 但是目前的<code>Mathtype3</code>仍然没有迁移所有的Latex指令过来, 只是支持其中的一部分, 大多数不支持的指令其实都是比较冷门的, 作用不是很大. 感谢<a href="https://www.zybuluo.com/codeep/note/163962" target="_blank" rel="noopener">Cmd Markdown 公式指导手册</a>和<a href="https://www.cnblogs.com/1024th/p/11623258.html" target="_blank" rel="noopener">LaTeX公式手册</a>, 整理时参考了它们. 如果你不想手敲公式, 想直接进行用图片进行识别, 可以上<a href="https://www.latexlive.com/" target="_blank" rel="noopener">妈咪叔的网站</a>或者使用<code>Mathpix</code>直接对公式进行识别, 它甚至能够识别中文, 在写论文和报告时如果有公式依据, 妈咪叔的网站会非常好用.</p><h2 id="公式使用基础"><a href="#公式使用基础" class="headerlink" title="公式使用基础"></a>公式使用基础</h2><h3 id="插入公式"><a href="#插入公式" class="headerlink" title="插入公式"></a>插入公式</h3><p>$\LaTeX$ 的数学公式分为两种, 分别是行内公式和独立公式.</p><p>行内公式可以这样使用:</p><p><code>$ 公式内容 $</code></p><p>行内公式是嵌入在一行文字中的公式, 例如:</p><p><code>$ J_\alpha(x) = \sum_{m=0}^\infty \frac{(-1)^m}{m! \Gamma (m + \alpha + 1)} {\left({ \frac{x}{2} }\right)}^{2m + \alpha} \text {，行内公式示例} $</code></p><p>$ J_\alpha(x) = \sum_{m=0}^\infty \frac{(-1)^m}{m! \Gamma (m + \alpha + 1)} {\left({ \frac{x}{2} }\right)}^{2m + \alpha} \text {，行内公式示例} $</p><p>独立公式可以这样使用:</p><p><code>$$ 公式内容 $$</code></p><p>独立公式可以写多行连续的内容, 适合写一段公式. 例如:</p><p><code>$$ J_\alpha(x) = \sum_{m=0}^\infty \frac{(-1)^m}{m! \Gamma (m + \alpha + 1)} {\left({ \frac{x}{2} }\right)}^{2m + \alpha} \text {，独立公式示例} $$</code><br>$$<br>J_\alpha(x) = \sum_{m=0}^\infty \frac{(-1)^m}{m! \Gamma (m + \alpha + 1)} {\left({ \frac{x}{2} }\right)}^{2m + \alpha} \text {，独立公式示例}<br>$$</p><h3 id="上下标"><a href="#上下标" class="headerlink" title="上下标"></a>上下标</h3><p><code>^</code> 表示上标, <code>_</code> 表示下标. 如果上下标的内容多于一个字符, 需要用 <code>{}</code> 将这些内容括成一个整体. 上下标可以嵌套, 也可以同时使用. </p><p>例如:</p><p><code>$$ x^{y^z}=(1+{\rm e}^x)^{-2xy^w} $$</code></p><p>显示:<br>$$<br>x^{y^z}=(1+{\rm e}^x)^{-2xy^w}<br>$$<br>如果想在左右两边都加上上下标, 可以使用<code>\sideset</code>命令:</p><p><code>$$ \sideset{^1_2}{^3_4}\bigotimes $$</code></p><p>显示:<br>$$<br>\sideset{^1_2}{^3_4}\bigotimes<br>$$</p><p>或者:</p><p><code>$${}_1^2\!X_3^4$$</code><br>$$<br>{}_1^2X_3^4<br>$$</p><h3 id="括号和分隔符"><a href="#括号和分隔符" class="headerlink" title="括号和分隔符"></a>括号和分隔符</h3><p><code>()</code>, <code>[]</code> 和 <code>|</code> 表示符号本身, 使用 <code>\{\}</code> 来表示 <code>{}</code> . 当要显示大号的括号或分隔符时,要用 <code>\left</code> 和 <code>\right</code> .</p><p>例如:</p><p><code>$$ f(x,y,z) = 3y^2z \left( 3+\frac{7x+5}{1+y^2} \right) $$</code><br>$$<br>f(x,y,z) = 3y^2z \left( 3+\frac{7x+5}{1+y^2} \right)<br>$$<br> <code>\left.</code> 或 <code>\right.</code> 能够进行匹配而不显示本身, 例如:</p><p><code>$$ \left. \frac{ {\rm d}u}{ {\rm d}x} \right| _{x=0} $$</code><br>$$<br>\left. \frac{ {\rm d}u}{ {\rm d}x} \right| _{x=0}<br>$$<br><code>$$\left. \frac{a}{b} \right \}$$</code><br>$$<br>\left. \frac{a}{b} \right \}<br>$$<br>下面有些特殊的括号:</p><table><thead><tr><th align="center">显示</th><th align="left">输入</th><th align="center">显示</th><th align="left">输入</th></tr></thead><tbody><tr><td align="center">$\langle$</td><td align="left"><code>\langle</code></td><td align="center">$\rangle$</td><td align="left"><code>\rangle</code></td></tr><tr><td align="center">$\lceil$</td><td align="left"><code>\lceil</code></td><td align="center">$\rceil$</td><td align="left"><code>\rceil</code></td></tr><tr><td align="center">$\lfloor$</td><td align="left"><code>\lfloor</code></td><td align="center">$\rfloor$</td><td align="left"><code>\rfloor</code></td></tr><tr><td align="center">$\lbrace$</td><td align="left"><code>\lbrace</code></td><td align="center">$\rbrace$</td><td align="left"><code>\rbrace</code></td></tr><tr><td align="center">$\lVert$</td><td align="left"><code>\lVert</code></td><td align="center">$\rVert$</td><td align="left"><code>\rVert</code></td></tr></tbody></table><p>可以使用 <code>\big, \Big, \bigg, \Bigg</code> 控制括号的大小, 例如:</p><p><code>$$\Bigg ( \bigg [ \Big \{ \big \langle \left | \| \frac{a}{b} \| \right | \big \rangle \Big \} \bigg ] \Bigg )$$</code></p><p>显示:<br>$$<br>\Bigg ( \bigg [ \Big \{ \big \langle \left | | \frac{a}{b} | \right | \big \rangle \Big \} \bigg ] \Bigg )<br>$$</p><h3 id="分数"><a href="#分数" class="headerlink" title="分数"></a>分数</h3><p>通常使用 <code>\frac {分子} {分母}</code> 命令产生一个分数，分数可嵌套。<br>便捷情况可直接输入 <code>\frac ab</code> 来快速生成一个 。<br>如果分式很复杂，亦可使用 <code>分子 \over 分母</code> 命令，此时分数仅有一层。</p><p>例如:</p><p><code>$$\frac{a-1}{b-1} \quad and \quad {a+1\over b+1}$$</code></p><p>显示:<br>$$<br>\frac{a-1}{b-1} \quad and \quad {a+1\over b+1}<br>$$</p><p>也可以控制分数的大小, 如<code>$$\tfrac{2}{4} = 0.5$$</code>是小型分数:<br>$$<br>\tfrac{2}{4} = 0.5<br>$$<br>同理, <code>\cfrac</code>(连分数使用)和<code>\dfrac</code>都表示大型分数.</p><h3 id="开根号"><a href="#开根号" class="headerlink" title="开根号"></a>开根号</h3><p>使用 <code>\sqrt [根指数，省略时为2] {被开方数}</code> 命令输入开根号. 例如:</p><p><code>$$\surd, \sqrt{2}, \sqrt[n]{}, \sqrt[3]{\frac{x^3+y^3}{2}}$$</code><br>$$<br>\surd \quad \sqrt{2} \quad \sqrt[n]{} \quad \sqrt[3]{\frac{x^3+y^3}{2}}<br>$$</p><h3 id="向量"><a href="#向量" class="headerlink" title="向量"></a>向量</h3><p>使用 <code>\vec{向量}</code> 来自动产生一个向量. 也可以使用 <code>\overrightarrow</code> 等命令自定义字母上方的符号. 例如:</p><p><code>$$\vec{a} \cdot \vec{b}=0$$</code><br>$$<br>\vec{a} \cdot \vec{b}=0<br>$$<br><code>$$\overleftarrow{xy} \quad and \quad \overleftrightarrow{xy} \quad and \quad \overrightarrow{xy}$$</code><br>$$<br>\overleftarrow{xy} \quad and \quad \overleftrightarrow{xy} \quad and \quad \overrightarrow{xy}<br>$$</p><h3 id="导数和微分"><a href="#导数和微分" class="headerlink" title="导数和微分"></a>导数和微分</h3><p>常见的微分符号通过<code>\mathrm{d}{符号}</code>来输入一个常见的微分. 其中的<code>\mathrm</code>是为了美观才加上的, 也可以直接输入<code>d符号</code>. 使用<code>\partial{符号}</code>来获得一个偏导符号, <code>\nabla</code>生成一个梯度符号.</p><p><code>$$dt, \mathrm{d}t, \partial t, \nabla\psi$$</code><br>$$<br>dt, \mathrm{d}t, \partial t, \nabla\psi<br>$$<br><code>$$dy/dx, \mathrm{d}y/\mathrm{d}x, \frac{dy}{dx}, \frac{\mathrm{d}y}{\mathrm{d}x}, \frac{\partial^2}{\partial x_1\partial x_2}y$$</code><br>$$<br>dy/dx, \mathrm{d}y/\mathrm{d}x, \frac{dy}{dx}, \frac{\mathrm{d}y}{\mathrm{d}x}, \frac{\partial^2}{\partial x_1\partial x_2}y<br>$$<br><code>$$\prime, \backprime, f^\prime, f&#39;, f&#39;&#39;, f^{(3)}, \dot y, \ddot y$$</code><br>$$<br>\prime, \backprime, f^\prime, f’, f’’, f^{(3)}, \dot y, \ddot y<br>$$</p><h3 id="积分"><a href="#积分" class="headerlink" title="积分"></a>积分</h3><p>使用 <code>\int_积分下限^积分上限 {被积表达式}</code> 来输入一个积分. 例如:</p><p><code>$$\int_0^1 {x^2} \,{\rm d}x$$</code></p><p>显示:<br>$$<br>\int_0^1 {x^2} \,{\rm d}x<br>$$<br><code>\,</code> 和 <code>{\rm d}</code> 部分可省略, 加入能使式子更美观. <code>{\rm d}</code>可以用<code>\mathrm{d}</code>等价替换.</p><p><code>$$\iint_{D}^{W} \, \mathrm{d}x\,\mathrm{d}y$$</code><br>$$<br>\iint_{D}^{W} \, \mathrm{d}x\,\mathrm{d}y<br>$$<br><code>$$\iiint_{E}^{V} \, \mathrm{d}x\,\mathrm{d}y\,\mathrm{d}z$$</code><br>$$<br>\iiint_{E}^{V} \, \mathrm{d}x\,\mathrm{d}y\,\mathrm{d}z<br>$$<br><code>$$\oint_{C} x^3\, \mathrm{d}x + 4y^2\, \mathrm{d}y$$</code><br>$$<br>\oint_{C} x^3\, \mathrm{d}x + 4y^2\, \mathrm{d}y<br>$$</p><h3 id="极限"><a href="#极限" class="headerlink" title="极限"></a>极限</h3><p>使用 <code>\lim_{变量 \to 表达式} 表达式</code> 来输入一个极限. 如有其他需求, 可以更改 <code>\to</code> 符号至任意符号. 例如:</p><p><code>$$ \lim_{n \to +\infty} \frac{1}{n(n+1)} \quad and \quad \lim_{x\leftarrow{示例}} \frac{1}{n(n+1)} $$</code></p><p>显示:<br>$$<br>\lim_{n \to +\infty} \frac{1}{n(n+1)} \quad and \quad \lim_{x\leftarrow{示例}} \frac{1}{n(n+1)}<br>$$<br>如果在行内的话, 显示会将极限放到下标的位置:</p><p>行内公式示例:</p><p><code>$\lim_{x \to \infty} \frac{1}{n(n+1)}$</code></p><p>$\lim_{x \to \infty} \frac{1}{n(n+1)}$</p><p>可以通过<code>\limits</code>强制加到下方或上方.</p><p><code>$\lim\limits_{x \to \infty} \frac{1}{n(n+1)}$</code></p><p>$\lim\limits_{x \to \infty} \frac{1}{n(n+1)}$</p><p>或者通过<code>\displaystyle</code>强制转为独立公式模式:</p><p><code>$\displaystyle \lim_{x \to \infty} \frac{1}{n(n+1)}$</code></p><p>$\displaystyle \lim_{x \to \infty} \frac{1}{n(n+1)}$</p><h3 id="累加和累乘"><a href="#累加和累乘" class="headerlink" title="累加和累乘"></a>累加和累乘</h3><p>使用 <code>\sum_{下标表达式}^{上标表达式} {累加表达式}</code> 来输入一个累加. 与之类似, 使用 <code>\prod</code> <code>\bigcup</code> <code>\bigcap</code> 来分别输入累乘、并集和交集. 此类符号在行内显示时上下标表达式将会移至右上角和右下角. </p><p>独立公式示例:</p><p><code>$$\sum_{i=1}^n \frac{1}{i^2} \quad and \quad \prod_{i=1}^n \frac{1}{i^2} \quad and \quad \bigcup_{i=1}^{2} R$$</code></p><p>显示:<br>$$<br>\sum_{i=1}^n \frac{1}{i^2} \quad and \quad \prod_{i=1}^n \frac{1}{i^2} \quad and \quad \bigcup_{i=1}^{2} R<br>$$</p><blockquote><p>和极限一样, 行内可以通过<code>\limts</code>将内容强制加到上方或下方.</p></blockquote><h3 id="二项式"><a href="#二项式" class="headerlink" title="二项式"></a>二项式</h3><p>看例子即可.</p><p>二项式系数:</p><p><code>$$\dbinom{n}{r}=\binom{n}{n-r}=\mathrm{C}_n^r=\mathrm{C}_n^{n-r}$$</code><br>$$<br>\dbinom{n}{r}=\binom{n}{n-r}=\mathrm{C}_n^r=\mathrm{C}_n^{n-r}<br>$$<br>小二项式系数:</p><p><code>$$\tbinom{n}{r}=\tbinom{n}{n-r}=\mathrm{C}_n^r=\mathrm{C}_n^{n-r}$$</code><br>$$<br>\tbinom{n}{r}=\tbinom{n}{n-r}=\mathrm{C}_n^r=\mathrm{C}_n^{n-r}<br>$$<br>大型二项式系数:</p><p><code>$$\binom{n}{r}=\dbinom{n}{n-r}=\mathrm{C}_n^r=\mathrm{C}_n^{n-r}$$</code><br>$$<br>\binom{n}{r}=\dbinom{n}{n-r}=\mathrm{C}_n^r=\mathrm{C}_n^{n-r}<br>$$</p><h3 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h3><p>使用<code>\operatorname{函数名}</code>就能将其作为自定义的函数进行使用, 例如:</p><p><code>$$\operatorname{sh}k, \operatorname{ch}l, \operatorname{th}m, \operatorname{coth}n$$</code></p><p>显示:<br>$$<br>\operatorname{sh}k, \operatorname{ch}l, \operatorname{th}m, \operatorname{coth}n<br>$$</p><h3 id="模运算"><a href="#模运算" class="headerlink" title="模运算"></a>模运算</h3><p>直接看例子即可.</p><p><code>$$s_k \equiv 0 \pmod{m}$$</code><br>$$<br>s_k \equiv 0 \pmod{m}<br>$$<br><code>$$a \bmod b$$</code><br>$$<br>a \bmod b<br>$$</p><h3 id="绝对值和范数"><a href="#绝对值和范数" class="headerlink" title="绝对值和范数"></a>绝对值和范数</h3><p>用<code>\left\vert s \right\vert</code>即可给Z两侧加上绝对值.</p><p>$$<br>\left\vert Z \right\vert<br>$$<br>用<code>\lVert s \rVert</code>即可给Z两侧加上范数.<br>$$<br>\lVert Z \rVert<br>$$<br>符号函数<code>\sgn</code>在当前版本中暂不支持.</p><h3 id="连接"><a href="#连接" class="headerlink" title="连接"></a>连接</h3><table><thead><tr><th align="center">显示</th><th align="left">输入</th></tr></thead><tbody><tr><td align="center">$\fbox{a+b+c+d}$</td><td align="left"><code>\fbox{a+b+c+d}</code></td></tr><tr><td align="center">$\overleftarrow{a+b+c+d}$</td><td align="left"><code>\overleftarrow{a+b+c+d}</code></td></tr><tr><td align="center">$\overrightarrow{a+b+c+d}$</td><td align="left"><code>\overrightarrow{a+b+c+d}</code></td></tr><tr><td align="center">$\overleftrightarrow{a+b+c+d}$</td><td align="left"><code>\overleftrightarrow{a+b+c+d}</code></td></tr><tr><td align="center">$\underleftarrow{a+b+c+d}$</td><td align="left"><code>\underleftarrow{a+b+c+d}</code></td></tr><tr><td align="center">$\underrightarrow{a+b+c+d}$</td><td align="left"><code>\underrightarrow{a+b+c+d}</code></td></tr><tr><td align="center">$\underleftrightarrow{a+b+c+d}$</td><td align="left"><code>\underleftrightarrow{a+b+c+d}</code></td></tr><tr><td align="center">$\overline{a+b+c+d}$</td><td align="left"><code>\overline{a+b+c+d}</code></td></tr><tr><td align="center">$\underline{a+b+c+d}$</td><td align="left"><code>\underline{a+b+c+d}</code></td></tr><tr><td align="center">$\overbrace{a+b+c+d}^{Sample}$</td><td align="left"><code>\overbrace{a+b+c+d}^{Sample}</code></td></tr><tr><td align="center">$\underbrace{a+b+c+d}_{Sample}$</td><td align="left"><code>\underbrace{a+b+c+d}_{Sample}</code></td></tr><tr><td align="center">$\overbrace{a+\underbrace{b+c}_{1.0}+d}^{2.0}$</td><td align="left"><code>\overbrace{a+\underbrace{b+c}_{1.0}+d}^{2.0}</code></td></tr><tr><td align="center">$\underbrace{a\cdot a\cdots a}_{b\text{ times}}$</td><td align="left"><code>\underbrace{a\cdot a\cdots a}_{b\text{ times}}</code></td></tr><tr><td align="center">$\underrightarrow{1℃/min}$</td><td align="left"><code>\underrightarrow{1℃/min}</code></td></tr></tbody></table><h3 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h3><p>用<code>\text {文字}</code>来添加方程中的注释文本, 在注释中仍然可以使用<code>$ 公式 $</code>将内容公式化.</p><pre><code>f(n)= \begin{cases}n/2, &amp; \text {if $n$ is even} \\3n+1, &amp;\text{if $n$ is odd}\end{cases} </code></pre><p>显示:<br>$$<br>f(n)= \begin{cases}<br>n/2, &amp; \text {if $n$ is even} \\<br>3n+1, &amp;\text{if $n$ is odd}<br>\end{cases}<br>$$</p><h3 id="空格"><a href="#空格" class="headerlink" title="空格"></a>空格</h3><table><thead><tr><th>效果</th><th>写法</th><th align="center">间距</th><th>名称</th></tr></thead><tbody><tr><td>$\alpha\qquad\beta$</td><td><code>\alpha\qquad\beta</code></td><td align="center">${\displaystyle mm}$</td><td>2 个 quad 空格</td></tr><tr><td>$\alpha \quad \beta$</td><td><code>\alpha\quad\beta</code></td><td align="center">${\displaystyle m}$</td><td>quad 空格</td></tr><tr><td>$\alpha\ \beta$</td><td><code>\alpha\ \beta</code></td><td align="center">${\displaystyle {\frac{m}{3}}}$</td><td>大空格</td></tr><tr><td>$\alpha\;\beta$</td><td><code>\alpha\;\beta</code></td><td align="center">${\displaystyle {\frac {2m}{7}}}$</td><td>中等空格</td></tr><tr><td>$\alpha\,\beta$</td><td><code>\alpha\,\beta</code></td><td align="center">${\displaystyle {\frac {m}{6}}}$</td><td>小空格</td></tr><tr><td>$\alpha\beta$</td><td><code>\alpha\beta</code></td><td align="center">${\displaystyle 0}$</td><td>没有空格</td></tr><tr><td>$\alpha!\beta$</td><td><code>\alpha\!\beta</code></td><td align="center">${\displaystyle {-\frac {m}{6}}}$</td><td>紧贴</td></tr></tbody></table><h3 id="删除线"><a href="#删除线" class="headerlink" title="删除线"></a>删除线</h3><p>使用删除线功能必须声明 <code>$$</code> 符号. </p><p>在公式内使用 <code>\require{cancel}</code> 来允许 <strong>片段删除线</strong> 的显示.<br>声明片段删除线后, 使用 <code>\cancel{字符}</code>、<code>\bcancel{字符}</code>、<code>\xcancel{字符}</code> 和 <code>\cancelto{字符}</code> 来实现各种片段删除线效果. </p><pre><code>$$\require{cancel}\begin{array}{rl}\verb|y+\cancel{x}| &amp; y+\cancel{x}\\\verb|\cancel{y+x}| &amp; \cancel{y+x}\\\verb|y+\bcancel{x}| &amp; y+\bcancel{x}\\\verb|y+\xcancel{x}| &amp; y+\xcancel{x}\\\verb|y+\cancelto{0}{x}| &amp; y+\cancelto{0}{x}\\\verb+\frac{1\cancel9}{\cancel95} = \frac15+&amp; \frac{1\cancel9}{\cancel95} = \frac15 \\\end{array}$$</code></pre><p>可得:<br>$$<br>\require{cancel}<br>\begin{array}{rl}<br>\verb|y+\cancel{x}| &amp; y+\cancel{x}\\<br>\verb|\cancel{y+x}| &amp; \cancel{y+x}\\<br>\verb|y+\bcancel{x}| &amp; y+\bcancel{x}\\<br>\verb|y+\xcancel{x}| &amp; y+\xcancel{x}\\<br>\verb|y+\cancelto{0}{x}| &amp; y+\cancelto{0}{x}\\<br>\verb+\frac{1\cancel9}{\cancel95} = \frac15+&amp; \frac{1\cancel9}{\cancel95} = \frac15 \\<br>\end{array}<br>$$<br>使用 <code>\require{enclose}</code> 来允许 <strong>整段删除线</strong> 的显示.<br>声明整段删除线后, 使用 <code>\enclose{删除线效果}{字符}</code> 来实现各种整段删除线效果.<br>其中, 删除线效果有 <code>horizontalstrike</code>、<code>verticalstrike</code>、<code>updiagonalstrike</code> 和 <code>downdiagonalstrike</code>, 可叠加使用. </p><h2 id="使用参考"><a href="#使用参考" class="headerlink" title="使用参考"></a>使用参考</h2><h3 id="大括号和行标"><a href="#大括号和行标" class="headerlink" title="大括号和行标"></a>大括号和行标</h3><p>使用 <code>\left</code> 和 <code>\right</code> 来创建自动匹配高度的 (圆括号), [方括号] 和 {花括号} .  在每个公式末尾前使用 <code>\tag{行标}</code> 来实现行标. 例如:</p><pre><code>$$f\left(   \left[      \frac{       1+\left\{x,y\right\}     }{       \left(          \frac{x}{y}+\frac{y}{x}       \right)       \left(u+1\right)     }+a   \right]^{3/2}\right)\tag{行标}$$</code></pre><p>显示:<br>$$<br>f\left(<br>   \left[<br>     \frac{<br>       1+\left\{x,y\right\}<br>     }{<br>       \left(<br>          \frac{x}{y}+\frac{y}{x}<br>       \right)<br>       \left(u+1\right)<br>     }+a<br>   \right]^{3/2}<br>\right)<br>\tag{行标}<br>$$<br>如果你需要在不同的行显示对应括号, 可以在每一行对应处使用 <code>\left.</code> 或 <code>\right.</code> 来放一个”影子”括号: </p><pre><code>\begin{aligned}a=&amp;\left(1+2+3+  \cdots \right. \\&amp; \cdots+ \left. \infty-2+\infty-1+\infty\right)\end{aligned}</code></pre><p>显示:<br>$$<br>\begin{aligned}<br>a=&amp;\left(1+2+3+  \cdots \right. \\<br>&amp; \cdots+ \left. \infty-2+\infty-1+\infty\right)<br>\end{aligned}<br>$$<br>如果你需要将行内显示的分隔符也变大, 可以使用 <code>\middle</code> :</p><pre><code>\left\langle    q\middle\|  \frac{\frac{x}{y}}{\frac{u}{v}}\middle|    p \right\rangle</code></pre><p>显示:<br>$$<br>\left\langle<br>  q<br>\middle|<br>  \frac{\frac{x}{y}}{\frac{u}{v}}<br>\middle|<br>   p<br>\right\rangle<br>$$</p><h3 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h3><h4 id="无框矩阵"><a href="#无框矩阵" class="headerlink" title="无框矩阵"></a>无框矩阵</h4><p>在开头使用 <code>begin{matrix}</code>, 在结尾使用 <code>end{matrix}</code>, 在中间插入矩阵元素, 每个元素之间插入 <code>&amp;</code> , 并在每行结尾处使用 <code>\\</code> .<br>使用矩阵时必须声明 <code>$</code> 或 <code>$$</code> 符号. </p><p>例如:</p><pre><code>$$        \begin{matrix}        1 &amp; x &amp; x^2 \\        1 &amp; y &amp; y^2 \\        1 &amp; z &amp; z^2 \\        \end{matrix}$$</code></pre><p>显示:<br>$$<br>\begin{matrix}<br>        1 &amp; x &amp; x^2 \\<br>        1 &amp; y &amp; y^2 \\<br>        1 &amp; z &amp; z^2 \\<br> \end{matrix}<br>$$</p><h4 id="边框矩阵"><a href="#边框矩阵" class="headerlink" title="边框矩阵"></a>边框矩阵</h4><p>在开头将 <code>matrix</code> 替换为 <code>pmatrix</code> <code>bmatrix</code> <code>Bmatrix</code> <code>vmatrix</code> <code>Vmatrix</code>, 就能获得不同样式的带框矩阵.</p><p>例如:</p><pre><code>$ \begin{matrix} 1 &amp; 2 \\ 3 &amp; 4 \\ \end{matrix} $$ \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ \end{pmatrix} $$ \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ \end{bmatrix} $$ \begin{Bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ \end{Bmatrix} $$ \begin{vmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ \end{vmatrix} $$ \begin{Vmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ \end{Vmatrix} $</code></pre><p>显示:</p><table><thead><tr><th align="center">matrix</th><th align="center">pmatrix</th><th align="center">bmatrix</th><th align="center">Bmatrix</th><th align="center">vmatrix</th><th align="center">Vmatrix</th></tr></thead><tbody><tr><td align="center">$ \begin{matrix} 1 &amp; 2 \\\ 3 &amp; 4  \end{matrix} $</td><td align="center">$ \begin{pmatrix} 1 &amp; 2 \\\ 3 &amp; 4  \end{pmatrix} $</td><td align="center">$ \begin{bmatrix} 1 &amp; 2 \\\ 3 &amp; 4  \end{bmatrix} $</td><td align="center">$ \begin{Bmatrix} 1 &amp; 2 \\\ 3 &amp; 4  \end{Bmatrix} $</td><td align="center">$ \begin{vmatrix} 1 &amp; 2 \\\ 3 &amp; 4  \end{vmatrix} $</td><td align="center">$ \begin{Vmatrix} 1 &amp; 2 \\\ 3 &amp; 4  \end{Vmatrix} $</td></tr></tbody></table><h4 id="带省略号的矩阵"><a href="#带省略号的矩阵" class="headerlink" title="带省略号的矩阵"></a>带省略号的矩阵</h4><p>使用 <code>\cdots</code> $\cdots$ , <code>\ddots</code> $\ddots$ , <code>\vdots</code> $\vdots$ 来输入省略符号. </p><p>例如:</p><pre><code>$$        \begin{pmatrix}        1 &amp; a_1 &amp; a_1^2 &amp; \cdots &amp; a_1^n \\        1 &amp; a_2 &amp; a_2^2 &amp; \cdots &amp; a_2^n \\        \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\        1 &amp; a_m &amp; a_m^2 &amp; \cdots &amp; a_m^n \\        \end{pmatrix}$$</code></pre><p>得到:<br>$$<br>\begin{pmatrix}<br>        1 &amp; a_1 &amp; a_1^2 &amp; \cdots &amp; a_1^n \\<br>        1 &amp; a_2 &amp; a_2^2 &amp; \cdots &amp; a_2^n \\<br>        \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>        1 &amp; a_m &amp; a_m^2 &amp; \cdots &amp; a_m^n<br> \end{pmatrix}<br>$$</p><h4 id="带分割符号的矩阵"><a href="#带分割符号的矩阵" class="headerlink" title="带分割符号的矩阵"></a>带分割符号的矩阵</h4><p>利用数组或表格的排版进行分割.</p><pre><code>$$\left[    \begin{array}{cc|c}      1&amp;2&amp;3\\      4&amp;5&amp;6    \end{array}\right]$$</code></pre><p>其中 <code>cc|c</code> 代表在一个三列矩阵中的第二和第三列之间插入分割线. </p><p>显示:<br>$$<br>\left[<br>    \begin{array}{cc|c}<br>      1&amp;2&amp;3\\<br>      4&amp;5&amp;6<br>    \end{array}<br>\right]<br>$$</p><h4 id="行中矩阵"><a href="#行中矩阵" class="headerlink" title="行中矩阵"></a>行中矩阵</h4><p>例如:</p><p><code>$\bigl( \begin{smallmatrix} a &amp; b \\ c &amp; d \end{smallmatrix} \bigr)$</code></p><p>显示$\bigl( \begin{smallmatrix} a &amp; b \\\ c &amp; d \end{smallmatrix} \bigr)$.</p><h3 id="方程组"><a href="#方程组" class="headerlink" title="方程组"></a>方程组</h3><h4 id="条件表达式"><a href="#条件表达式" class="headerlink" title="条件表达式"></a>条件表达式</h4><p>使用 <code>begin{cases}</code> 来创造一组条件表达式, 在每一行条件中插入 <code>&amp;</code> 来指定需要对齐的内容, 并在每一行结尾处使用 <code>\\</code>, 以 <code>end{cases}</code> 结束. </p><pre><code>$$        f(n) =        \begin{cases}        n/2,  &amp; \text{if $n$ is even} \\        3n+1, &amp; \text{if $n$ is odd}        \end{cases}$$</code></pre><p>显示:<br>$$<br>f(n) =<br>        \begin{cases}<br>        n/2,  &amp; \text{if $n$ is even} \\<br>        3n+1, &amp; \text{if $n$ is odd}<br>        \end{cases}<br>$$<br>如果想要让条件表达式变为左对齐显示, 可以使用如下方式:</p><pre><code>$$        \left.        \begin{array}{l}        \text{if $n$ is even:}&amp;n/2\\        \text{if $n$ is odd:}&amp;3n+1        \end{array}        \right\}        =f(n)$$</code></pre><p>显示:<br>$$<br>\left.<br>        \begin{array}{l}<br>        \text{if $n$ is even:}&amp;n/2\\<br>        \text{if $n$ is odd:}&amp;3n+1<br>        \end{array}<br>        \right\}<br>        =f(n)<br>$$<br>在换行时可以用<code>\\[?ex]</code>使得其适配行高<code>?</code>. <em><em>一个 <code>[ex]</code> 指一个 “X-Height”, 即x字母高度. 可以根据情况指定多个 <code>[ex]</code>, 如 <code>[3ex]</code>、<code>[4ex]</code> 等. </em></em> 其实可以在任何地方使用 <code>\\[2ex]</code> 语句, 只要你觉得合适. </p><h4 id="输入方程组"><a href="#输入方程组" class="headerlink" title="输入方程组"></a>输入方程组</h4><p>方程组不光可以通过条件表达式的<code>\begin{cases}…\end{cases}</code>实现, 还可以使用 <code>\begin{array}…\end{array}</code> 和 <code>\left\{…\right.</code> 来创建一个方程组. 例如:</p><pre><code>\left\{ \begin{array}{c}a_1x+b_1y+c_1z=d_1 \\ a_2x+b_2y+c_2z=d_2 \\ a_3x+b_3y+c_3z=d_3\end{array}\right. </code></pre><p>$$<br>\left\{<br>\begin{array}{c}<br>a_1x+b_1y+c_1z=d_1 \\<br>a_2x+b_2y+c_2z=d_2 \\<br>a_3x+b_3y+c_3z=d_3<br>\end{array}<br>\right.<br>$$</p><h4 id="对齐方程组"><a href="#对齐方程组" class="headerlink" title="对齐方程组"></a>对齐方程组</h4><p>使用 <code>\begin{aligned}…\end{aligned}</code>获得一列整齐且居中的方程式序列. 通过<code>&amp;</code>来控制方程对齐的位置.</p><p>例如:</p><pre><code>\begin{aligned}f(x) &amp; = (m+n)^2 \\     &amp; = m^2+2mn+n^2 \\\end{aligned}</code></pre><p>显示:<br>$$<br>\begin{aligned}<br>f(x) &amp; = (m+n)^2 \\<br>     &amp; = m^2+2mn+n^2 \\<br>\end{aligned}<br>$$</p><h3 id="数组和表格"><a href="#数组和表格" class="headerlink" title="数组和表格"></a>数组和表格</h3><p>通常, 一个格式化后的表格比单纯的文字或排版后的文字更具有可读性. 数组和表格均以 <code>begin{array}</code> 开头, 并在其后定义列数及每一列的文本对齐属性, <code>c</code> <code>l</code> <code>r</code> 分别代表居中、左对齐及右对齐. 若需要插入垂直分割线, 在定义式中插入 <code>|</code> , 若要插入水平分割线, 在下一行输入前插入 <code>\hline</code> . 与矩阵相似, 每行元素间均须要插入 <code>&amp;</code> , 每行元素以 <code>\\</code> 结尾, 最后以 <code>end{array}</code> 结束数组. </p><p>例如:</p><pre><code>$$\begin{array}{c|lcr}n &amp; \text{左对齐} &amp; \text{居中对齐} &amp; \text{右对齐} \\\hline1 &amp; 0.24 &amp; 1 &amp; 125 \\2 &amp; -1 &amp; 189 &amp; -8 \\3 &amp; -20 &amp; 2000 &amp; 1+10i\end{array}$$</code></pre><p>显示:<br>$$<br>\begin{array}{c|lcr}<br>n &amp; \text{左对齐} &amp; \text{居中对齐} &amp; \text{右对齐} \\<br>\hline<br>1 &amp; 0.24 &amp; 1 &amp; 125 \\<br>2 &amp; -1 &amp; 189 &amp; -8 \\<br>3 &amp; -20 &amp; 2000 &amp; 1+10i<br>\end{array}<br>$$</p><h4 id="嵌套"><a href="#嵌套" class="headerlink" title="嵌套"></a>嵌套</h4><p>多个数组/表格可 <strong>互相嵌套</strong> 并组成一组数组/一组表格. 使用嵌套前必须声明 <code>$$</code> 符号.  例如:</p><pre><code>$$% outer vertical array of arrays 外层垂直表格\begin{array}{c}    % inner horizontal array of arrays 内层水平表格    \begin{array}{cc}        % inner array of minimum values 内层&quot;最小值&quot;数组        \begin{array}{c|cccc}        \text{min} &amp; 0 &amp; 1 &amp; 2 &amp; 3\\        \hline        0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\        1 &amp; 0 &amp; 1 &amp; 1 &amp; 1\\        2 &amp; 0 &amp; 1 &amp; 2 &amp; 2\\        3 &amp; 0 &amp; 1 &amp; 2 &amp; 3        \end{array}    &amp;        % inner array of maximum values 内层&quot;最大值&quot;数组        \begin{array}{c|cccc}        \text{max}&amp;0&amp;1&amp;2&amp;3\\        \hline        0 &amp; 0 &amp; 1 &amp; 2 &amp; 3\\        1 &amp; 1 &amp; 1 &amp; 2 &amp; 3\\        2 &amp; 2 &amp; 2 &amp; 2 &amp; 3\\        3 &amp; 3 &amp; 3 &amp; 3 &amp; 3        \end{array}    \end{array}    % 内层第一行表格组结束    \\    % inner array of delta values 内层第二行Delta值数组        \begin{array}{c|cccc}        \Delta&amp;0&amp;1&amp;2&amp;3\\        \hline        0 &amp; 0 &amp; 1 &amp; 2 &amp; 3\\        1 &amp; 1 &amp; 0 &amp; 1 &amp; 2\\        2 &amp; 2 &amp; 1 &amp; 0 &amp; 1\\        3 &amp; 3 &amp; 2 &amp; 1 &amp; 0        \end{array}        % 内层第二行表格组结束\end{array}$$</code></pre><p>显示:<br>$$<br>\begin{array}{c}<br>    \begin{array}{cc}<br>        \begin{array}{c|cccc}<br>        \text{min} &amp; 0 &amp; 1 &amp; 2 &amp; 3\\<br>        \hline<br>        0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\<br>        1 &amp; 0 &amp; 1 &amp; 1 &amp; 1\\<br>        2 &amp; 0 &amp; 1 &amp; 2 &amp; 2\\<br>        3 &amp; 0 &amp; 1 &amp; 2 &amp; 3<br>        \end{array}<br>    &amp;<br>        \begin{array}{c|cccc}<br>        \text{max}&amp;0&amp;1&amp;2&amp;3\\<br>        \hline<br>        0 &amp; 0 &amp; 1 &amp; 2 &amp; 3\\<br>        1 &amp; 1 &amp; 1 &amp; 2 &amp; 3\\<br>        2 &amp; 2 &amp; 2 &amp; 2 &amp; 3\\<br>        3 &amp; 3 &amp; 3 &amp; 3 &amp; 3<br>        \end{array}<br>    \end{array}<br>    \\<br>        \begin{array}{c|cccc}<br>        \Delta&amp;0&amp;1&amp;2&amp;3\\<br>        \hline<br>        0 &amp; 0 &amp; 1 &amp; 2 &amp; 3\\<br>        1 &amp; 1 &amp; 0 &amp; 1 &amp; 2\\<br>        2 &amp; 2 &amp; 1 &amp; 0 &amp; 1\\<br>        3 &amp; 3 &amp; 2 &amp; 1 &amp; 0<br>        \end{array}<br>        % 内层第二行表格组结束<br>\end{array}<br>$$</p><h3 id="连分数"><a href="#连分数" class="headerlink" title="连分数"></a>连分数</h3><p>就像输入分式时使用 <code>\frac</code> 一样, 使用 <code>\cfrac</code> 来创建一个连分数. 不要使用普通的 <code>\frac</code> 或 <code>\over</code> 来创建, 否则会看起来很丑. 例如:</p><pre><code>$$x = a_0 + \cfrac{1^2}{a_1          + \cfrac{2^2}{a_2          + \cfrac{3^2}{a_3 + \cfrac{4^4}{a_4 + \cdots}}}}$$</code></pre><p>显示:<br>$$<br>x = a_0 + \cfrac{1^2}{a_1<br>          + \cfrac{2^2}{a_2<br>          + \cfrac{3^2}{a_3 + \cfrac{4^4}{a_4 + \cdots}}}}<br>$$<br>反例, 使用<code>\frac</code>和<code>\over</code>:</p><pre><code>$$x = a_0 + \frac{1^2}{a_1          + \frac{2^2}{a_2          + \frac{3^2}{a_3 + \frac{4^4}{a_4 + \cdots}}}}$$</code></pre><p>显示:<br>$$<br>x = a_0 + \frac{1^2}{a_1<br>          + \frac{2^2}{a_2<br>          + \frac{3^2}{a_3 + \frac{4^4}{a_4 + \cdots}}}}<br>$$<br>当然, 可以使用 <code>\frac</code> 来表达连分数的 <strong>紧缩记法</strong> .  例如:</p><pre><code>x = a_0 + \frac{1^2}{a_1+}          \frac{2^2}{a_2+}          \frac{3^2}{a_3 +} \frac{4^4}{a_4 +} \cdots</code></pre><p>显示:<br>$$<br>x = a_0 + \frac{1^2}{a_1+}<br>          \frac{2^2}{a_2+}<br>          \frac{3^2}{a_3 +} \frac{4^4}{a_4 +} \cdots<br>$$<br>连分数通常都太大以至于不易排版, 建议使用 <code>[a0;a1,a2,a3,…]</code> 一样的紧缩记法. </p><h2 id="符号汇总"><a href="#符号汇总" class="headerlink" title="符号汇总"></a>符号汇总</h2><p>这里的符号正常使用绝对够了, 如果还有更奇葩的符号需要使用可以参考之前我引用的文章. 因为同一个符号可能有不同的使用领域, 所以不同的表可能有重叠的地方.</p><p><code>|</code> 符号在被当作分隔符时会产生错误的间隔, 因此在需要分隔时最好使用 <code>\mid</code> 来代替它. </p><h3 id="三角"><a href="#三角" class="headerlink" title="三角"></a>三角</h3><table><thead><tr><th align="center">显示</th><th align="left">输入</th><th align="center">显示</th><th align="left">输入</th><th align="center">显示</th><th align="left">输入</th></tr></thead><tbody><tr><td align="center">$30^\circ$</td><td align="left"><code>30^\circ</code></td><td align="center">$\bot$</td><td align="left"><code>\bot</code></td><td align="center">$\angle A$</td><td align="left"><code>\angle A</code></td></tr><tr><td align="center">$\sin$</td><td align="left"><code>\sin</code></td><td align="center">$\cos$</td><td align="left"><code>\cos</code></td><td align="center">$\tan$</td><td align="left"><code>\tan</code></td></tr><tr><td align="center">$\csc$</td><td align="left"><code>\csc</code></td><td align="center">$\sec$</td><td align="left"><code>\sec</code></td><td align="center">$\cot$</td><td align="left"><code>\cot</code></td></tr><tr><td align="center">$\sinh$</td><td align="left"><code>\sinh</code></td><td align="center">$\cosh$</td><td align="left"><code>\cosh</code></td><td align="center">$\tanh$</td><td align="left"><code>\tanh</code></td></tr><tr><td align="center">$\arcsin$</td><td align="left"><code>\arcsin</code></td><td align="center">$\arccos$</td><td align="left"><code>\arccos</code></td><td align="center">$\arctan$</td><td align="left"><code>\arctan</code></td></tr><tr><td align="center">$\textrm{arccsc}$</td><td align="left"><code>\textrm{arccsc}</code></td><td align="center">$\textrm{arcsec}$</td><td align="left"><code>\textrm{arcsec}</code></td><td align="center">$\textrm{arccot}$</td><td align="left"><code>\textrm{arccot}</code></td></tr><tr><td align="center">$\sin^{-1}$</td><td align="left"><code>\sin^{-1}</code></td><td align="center">$\cos^{-1}$</td><td align="left"><code>\cos^{-1}</code></td><td align="center">$\tan^{-1}$</td><td align="left"><code>\tan^{-1}</code></td></tr><tr><td align="center">$\sinh^{-1}$</td><td align="left"><code>\sinh^{-1}</code></td><td align="center">$\cosh^{-1}$</td><td align="left"><code>\cosh^{-1}</code></td><td align="center">$\tanh^{-1}$</td><td align="left"><code>\tanh^{-1}</code></td></tr><tr><td align="center">$\sphericalangle$</td><td align="left"><code>sphericalangle</code></td><td align="center">$\measuredangle$</td><td align="left"><code>\measuredangle</code></td><td align="center"></td><td align="left"></td></tr></tbody></table><h3 id="对数"><a href="#对数" class="headerlink" title="对数"></a>对数</h3><table><thead><tr><th align="center">显示</th><th align="center">输入</th><th align="center">显示</th><th align="center">输入</th><th align="center">显示</th><th align="center">输入</th></tr></thead><tbody><tr><td align="center">$\log$</td><td align="center"><code>\log​</code></td><td align="center">$\lg$</td><td align="center"><code>\lg​</code></td><td align="center">$\ln$</td><td align="center"><code>\ln</code></td></tr><tr><td align="center">$\exp$</td><td align="center"><code>\exp​</code></td><td align="center">$\log_{e}$</td><td align="center"><code>\log_{e}​</code></td><td align="center">$\log_{10}$</td><td align="center"><code>\log_{10}</code></td></tr></tbody></table><h3 id="微积分和导数"><a href="#微积分和导数" class="headerlink" title="微积分和导数"></a>微积分和导数</h3><table><thead><tr><th align="center">显示</th><th align="left">输入</th><th align="center">显示</th><th align="left">输入</th><th align="center">显示</th><th align="left">输入</th></tr></thead><tbody><tr><td align="center">$\int$</td><td align="left"><code>\int</code></td><td align="center">$\iint$</td><td align="left"><code>\iint</code></td><td align="center">$\iiint$</td><td align="left"><code>\iiint</code></td></tr><tr><td align="center">$\iiiint$</td><td align="left"><code>\iiiint</code></td><td align="center">$\oint$</td><td align="left"><code>\oint</code></td><td align="center">$\prime$</td><td align="left"><code>\prime</code></td></tr><tr><td align="center">$\lim$</td><td align="left"><code>\lim</code></td><td align="center">$\infty$</td><td align="left"><code>\infty</code></td><td align="center">$\nabla$</td><td align="left"><code>\nabla</code></td></tr></tbody></table><h3 id="其他公式"><a href="#其他公式" class="headerlink" title="其他公式"></a>其他公式</h3><table><thead><tr><th align="center">显示</th><th align="left">输入</th><th align="center">显示</th><th align="left">输入</th><th align="center">显示</th><th align="left">输入</th></tr></thead><tbody><tr><td align="center">$\inf$</td><td align="left"><code>\inf</code></td><td align="center">$\arg$</td><td align="left"><code>\arg</code></td><td align="center">$\det$</td><td align="left"><code>\det</code></td></tr><tr><td align="center">$\dim$</td><td align="left"><code>\dim</code></td><td align="center">$\gcd$</td><td align="left"><code>\gcd</code></td><td align="center">$\ker$</td><td align="left"><code>\ker</code></td></tr><tr><td align="center">$\Pr$</td><td align="left"><code>\Pr</code></td><td align="center">$\deg$</td><td align="left"><code>\deg</code></td><td align="center">$\sup$</td><td align="left"><code>\sup</code></td></tr><tr><td align="center">$\hom$</td><td align="left"><code>\hom</code></td><td align="center">$\max$</td><td align="left"><code>\max</code></td><td align="center">$\min$</td><td align="left"><code>\min</code></td></tr></tbody></table><h3 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h3><table><thead><tr><th align="center">符号</th><th align="left">公式</th><th align="center">符号</th><th align="left">公式</th><th align="center">符号</th><th align="left">公式</th></tr></thead><tbody><tr><td align="center">$+$</td><td align="left"><code>+</code></td><td align="center">$-$</td><td align="left"><code>-</code></td><td align="center">$\pm$</td><td align="left"><code>\pm</code></td></tr><tr><td align="center">$\mp$</td><td align="left"><code>\mp</code></td><td align="center">$\dotplus$</td><td align="left"><code>\dotplus</code></td><td align="center">$\times$</td><td align="left"><code>\times</code></td></tr><tr><td align="center">$\div$</td><td align="left"><code>\div</code></td><td align="center">$\divideontimes$</td><td align="left"><code>\divideontimes</code></td><td align="center">$/$</td><td align="left"><code>/</code></td></tr><tr><td align="center">$\backslash$</td><td align="left"><code>\backslash</code></td><td align="center">$\cdot$</td><td align="left"><code>\cdot</code></td><td align="center">$*$</td><td align="left"><code>*</code>或<code>\ast</code></td></tr><tr><td align="center">$\star$</td><td align="left"><code>\star</code></td><td align="center">$\circ$</td><td align="left"><code>\circ</code></td><td align="center">$\bullet$</td><td align="left"><code>\bullet</code></td></tr><tr><td align="center">$\boxplus$</td><td align="left"><code>\boxplus</code></td><td align="center">$\boxminus$</td><td align="left"><code>\boxminus</code></td><td align="center">$\boxtimes$</td><td align="left"><code>\boxtimes</code></td></tr><tr><td align="center">$\boxdot$</td><td align="left"><code>\boxdot</code></td><td align="center">$\oplus$</td><td align="left"><code>\oplus</code></td><td align="center">$\ominus$</td><td align="left"><code>\ominus</code></td></tr><tr><td align="center">$\otimes$</td><td align="left"><code>\otimes</code></td><td align="center">$\oslash$</td><td align="left"><code>\oslash</code></td><td align="center">$\odot$</td><td align="left"><code>\odot</code></td></tr><tr><td align="center">$\circleddash$</td><td align="left"><code>\circleddash</code></td><td align="center">$\circledcirc$</td><td align="left"><code>\circledcirc</code></td><td align="center">$\circledast$</td><td align="left"><code>\circledast</code></td></tr><tr><td align="center">$\bigoplus$</td><td align="left"><code>\bigoplus</code></td><td align="center">$\bigotimes$</td><td align="left"><code>\bigotimes</code></td><td align="center">$\bigodot$</td><td align="left"><code>\bigodot</code></td></tr></tbody></table><h3 id="集合相关"><a href="#集合相关" class="headerlink" title="集合相关"></a>集合相关</h3><table><thead><tr><th align="center">符号</th><th align="left">公式</th><th align="center">符号</th><th align="left">公式</th><th align="center">符号</th><th align="left">公式</th></tr></thead><tbody><tr><td align="center">$\{ \}$</td><td align="left"><code>\{ \}</code></td><td align="center">$\O$</td><td align="left"><code>\O</code>, <code>\empty</code>, <code>emptyset</code></td><td align="center">$\varnothing$</td><td align="left"><code>\varnothing</code></td></tr><tr><td align="center">$\in$</td><td align="left"><code>\in</code></td><td align="center">$\notin$</td><td align="left"><code>\notin</code>或<code>\not\in</code></td><td align="center">$\ni$</td><td align="left"><code>\ni</code></td></tr><tr><td align="center">$\not\ni$</td><td align="left"><code>\not\ni</code></td><td align="center">$\cap$</td><td align="left"><code>\cap</code></td><td align="center">$\Cap$</td><td align="left"><code>\Cap</code></td></tr><tr><td align="center">$\sqcap$</td><td align="left"><code>\sqcap</code></td><td align="center">$\bigcap$</td><td align="left"><code>\bigcap</code></td><td align="center">$\cup$</td><td align="left"><code>\cup</code></td></tr><tr><td align="center">$\Cup$</td><td align="left"><code>\Cup</code></td><td align="center">$\sqcup$</td><td align="left"><code>\sqcup</code></td><td align="center">$\bigcup$</td><td align="left"><code>\bigcup</code></td></tr><tr><td align="center">$\bigsqcup$</td><td align="left"><code>\bigsqcup</code></td><td align="center">$\uplus$</td><td align="left"><code>\uplus</code></td><td align="center">$\biguplus$</td><td align="left"><code>\biguplus</code></td></tr><tr><td align="center">$\bigvee$</td><td align="left"><code>\bigvee</code></td><td align="center">$\bigwedge$</td><td align="left"><code>\bigwedge</code></td><td align="center"></td><td align="left"></td></tr><tr><td align="center">$\setminus$</td><td align="left"><code>\setminus</code></td><td align="center">$\smallsetminus$</td><td align="left"><code>\smallsetminus</code></td><td align="center">$\times$</td><td align="left"><code>\times</code></td></tr><tr><td align="center">$\supset$</td><td align="left"><code>\subset</code></td><td align="center">$\Subset$</td><td align="left"><code>\Subset</code></td><td align="center">$\sqsubset$</td><td align="left"><code>\sqsubset</code></td></tr><tr><td align="center">$\circleddash$</td><td align="left"><code>\supset</code></td><td align="center">$\Supset$</td><td align="left"><code>\Supset</code></td><td align="center">$\sqsupset$</td><td align="left"><code>\sqsupset</code></td></tr><tr><td align="center">$\subseteq$</td><td align="left"><code>\subseteq</code></td><td align="center">$\nsubseteq$</td><td align="left"><code>\nsubseteq</code></td><td align="center">$\subsetneq$</td><td align="left"><code>\subsetneq</code></td></tr><tr><td align="center">$\varsubsetneq$</td><td align="left"><code>\varsubsetneq</code></td><td align="center">$\sqsubseteq$</td><td align="left"><code>\sqsubseteq</code></td><td align="center"></td><td align="left"></td></tr><tr><td align="center">$\supseteq$</td><td align="left"><code>\supseteq</code></td><td align="center">$\nsupseteq$</td><td align="left"><code>\nsupseteq</code></td><td align="center">$\supsetneq$</td><td align="left"><code>\supsetneq</code></td></tr><tr><td align="center">$\varsupsetneq$</td><td align="left"><code>\varsupsetneq</code></td><td align="center">$\sqsupseteq$</td><td align="left"><code>\sqsupseteq</code></td><td align="center"></td><td align="left"></td></tr><tr><td align="center">$\subseteqq$</td><td align="left"><code>\subseteqq</code></td><td align="center">$\nsubseteqq$</td><td align="left"><code>\nsubseteqq</code></td><td align="center">$\subsetneqq$</td><td align="left"><code>\subsetneqq</code></td></tr><tr><td align="center">$\varsubsetneqq$</td><td align="left"><code>\varsubsetneqq</code></td><td align="center"></td><td align="left"></td><td align="center"></td><td align="left"></td></tr><tr><td align="center">$\supseteqq$</td><td align="left"><code>\supseteqq</code></td><td align="center">$\nsupseteqq$</td><td align="left"><code>\nsupseteqq</code></td><td align="center">$\supsetneqq$</td><td align="left"><code>\supsetneqq</code></td></tr><tr><td align="center">$\varsupsetneqq$</td><td align="left"><code>\varsupsetneqq</code></td><td align="center"></td><td align="left"></td><td align="center"></td><td align="left"></td></tr></tbody></table><h3 id="关系符号"><a href="#关系符号" class="headerlink" title="关系符号"></a>关系符号</h3><table><thead><tr><th align="center">符号</th><th align="left">公式</th><th align="center">符号</th><th align="left">公式</th><th align="center">符号</th><th align="left">公式</th></tr></thead><tbody><tr><td align="center">$=$</td><td align="left"><code>=</code></td><td align="center">$\ne$</td><td align="left"><code>\ne</code>或<code>\neq</code></td><td align="center">$\equiv$</td><td align="left"><code>\equiv</code></td></tr><tr><td align="center">$\not\equiv$</td><td align="left"><code>\not\equiv</code></td><td align="center">$\doteq$</td><td align="left"><code>\doteq</code></td><td align="center">$\doteqdot$</td><td align="left"><code>\doteqdot</code></td></tr><tr><td align="center">$\sim$</td><td align="left"><code>\sim</code></td><td align="center">$\nsim$</td><td align="left"><code>\nsim</code></td><td align="center">$\backsim$</td><td align="left"><code>\backsim</code></td></tr><tr><td align="center">$\thicksim$</td><td align="left"><code>\thicksim</code></td><td align="center">$\simeq$</td><td align="left"><code>\simeq</code></td><td align="center">$\backsimeq$</td><td align="left"><code>\backsimeq</code></td></tr><tr><td align="center">$\eqsim$</td><td align="left"><code>\eqsim</code></td><td align="center">$\cong$</td><td align="left"><code>\cong</code></td><td align="center">$\ncong$</td><td align="left"><code>\ncong</code></td></tr><tr><td align="center">$\approx$</td><td align="left"><code>\approx</code></td><td align="center">$\thickapprox$</td><td align="left"><code>\thickapprox</code></td><td align="center">$\approxeq$</td><td align="left"><code>\approxeq</code></td></tr><tr><td align="center">$\asymp$</td><td align="left"><code>\asymp</code></td><td align="center">$\propto$</td><td align="left"><code>\propto</code></td><td align="center">$\varpropto$</td><td align="left"><code>\varpropto</code></td></tr><tr><td align="center">$&lt;$</td><td align="left"><code>&lt;</code></td><td align="center">$\nless$</td><td align="left"><code>\nless</code></td><td align="center">$\ll$</td><td align="left"><code>\ll</code></td></tr><tr><td align="center">$\not\ll$</td><td align="left"><code>\not\ll</code></td><td align="center">$\lll$</td><td align="left"><code>\lll</code></td><td align="center">$\not\lll$</td><td align="left"><code>\not\lll</code></td></tr><tr><td align="center">$\lessdot$</td><td align="left"><code>\lessdot</code></td><td align="center">$\le$</td><td align="left"><code>\le</code>或<code>\leq</code></td><td align="center">$\lneq$</td><td align="left"><code>\lneq</code></td></tr><tr><td align="center">$\leqq$</td><td align="left"><code>\leqq</code></td><td align="center">$\nleq$</td><td align="left"><code>\nleq</code></td><td align="center">$\nleqq$</td><td align="left"><code>\nleqq</code></td></tr><tr><td align="center">$\lneqq$</td><td align="left"><code>\lneqq</code></td><td align="center">$\lvertneqq$</td><td align="left"><code>\lvertneqq</code></td><td align="center">$\leqslant$</td><td align="left"><code>\leqslant</code></td></tr><tr><td align="center">$\nleqslant$</td><td align="left"><code>\nleqslant</code></td><td align="center">$\eqslantless$</td><td align="left"><code>\eqslantless</code></td><td align="center">$\lesssim$</td><td align="left"><code>\lesssim</code></td></tr><tr><td align="center">$\lnsim$</td><td align="left"><code>\lnsim</code></td><td align="center">$\lessapprox$</td><td align="left"><code>\lessapprox</code></td><td align="center">$\lnapprox$</td><td align="left"><code>\lnapprox</code></td></tr><tr><td align="center">$&gt;$</td><td align="left"><code>&gt;</code></td><td align="center">$\ngtr$</td><td align="left"><code>\ngtr</code></td><td align="center">$\gg$</td><td align="left"><code>\gg</code></td></tr><tr><td align="center">$\not\gg$</td><td align="left"><code>\not\gg</code></td><td align="center">$\ggg$</td><td align="left"><code>\ggg</code></td><td align="center">$\not\ggg$</td><td align="left"><code>\not\ggg</code></td></tr><tr><td align="center">$\gtrdot$</td><td align="left"><code>\gtrdot</code></td><td align="center">$\ge$</td><td align="left"><code>\ge</code>或<code>\geq</code></td><td align="center">$\gneq$</td><td align="left"><code>\gneq</code></td></tr><tr><td align="center">$\geqq$</td><td align="left"><code>\geqq</code></td><td align="center">$\ngeq$</td><td align="left"><code>\ngeq</code></td><td align="center">$\ngeqq$</td><td align="left"><code>\ngeqq</code></td></tr><tr><td align="center">$\gneqq$</td><td align="left"><code>\gneqq</code></td><td align="center">$\gvertneqq$</td><td align="left"><code>\gvertneqq</code></td><td align="center">$\geqslant$</td><td align="left"><code>\geqslant</code></td></tr><tr><td align="center">$\ngeqslant$</td><td align="left"><code>\ngeqslant</code></td><td align="center">$\eqslantgtr$</td><td align="left"><code>\eqslantgtr</code></td><td align="center">$\gtrsim$</td><td align="left"><code>\gtrsim</code></td></tr><tr><td align="center">$\gnsim$</td><td align="left"><code>\gnsim</code></td><td align="center">$\gtrapprox$</td><td align="left"><code>\gtrapprox</code></td><td align="center">$\gnapprox$</td><td align="left"><code>\gnapprox</code></td></tr><tr><td align="center">$\lessgtr$</td><td align="left"><code>\lessgtr</code></td><td align="center">$\lesseqgtr$</td><td align="left"><code>\lesseqgtr</code></td><td align="center">$\lesseqqgtr$</td><td align="left"><code>\lesseqqgtr</code></td></tr><tr><td align="center">$\gtrless$</td><td align="left"><code>\gtrless</code></td><td align="center">$\gtreqless$</td><td align="left"><code>\gtreqless</code></td><td align="center">$\gtreqqless$</td><td align="left"><code>\gtreqqless</code></td></tr></tbody></table><h3 id="逻辑符号"><a href="#逻辑符号" class="headerlink" title="逻辑符号"></a>逻辑符号</h3><table><thead><tr><th align="center">符号</th><th align="left">公式</th><th align="center">符号</th><th align="left">公式</th><th align="center">符号</th><th align="left">公式</th></tr></thead><tbody><tr><td align="center">$\forall$</td><td align="left"><code>\forall</code></td><td align="center">$\exists$</td><td align="left"><code>\exists</code></td><td align="center">$\nexists$</td><td align="left"><code>\nexists</code></td></tr><tr><td align="center">$\therefore$</td><td align="left"><code>\therefore</code></td><td align="center">$\because$</td><td align="left"><code>\because</code></td><td align="center">$\And$</td><td align="left"><code>\And</code></td></tr><tr><td align="center">$\or$</td><td align="left"><code>\or</code>, <code>\lor</code>, <code>\vee</code></td><td align="center">$\and$</td><td align="left"><code>\and</code>, <code>\land</code>, <code>\wedge</code></td><td align="center">$\overline{abc}$</td><td align="left"><code>\overline{abc}</code></td></tr><tr><td align="center">$\neg$</td><td align="left"><code>\lnot</code>, <code>\neg</code></td><td align="center">$\not\operatorname{R}$</td><td align="left"><code>\not\operatorname{R}</code></td><td align="center">$\bar{abc}$</td><td align="left"><code>\bar{abc}</code></td></tr></tbody></table><h3 id="各种箭头"><a href="#各种箭头" class="headerlink" title="各种箭头"></a>各种箭头</h3><table><thead><tr><th align="center">箭头</th><th align="left">公式</th><th align="center">箭头</th><th align="left">公式</th><th align="center">箭头</th><th align="left">公式</th></tr></thead><tbody><tr><td align="center">$\Rrightarrow$</td><td align="left"><code>\Rrightarrow</code></td><td align="center">$\Lleftarrow$</td><td align="left"><code>\Lleftarrow</code></td><td align="center">$\Rightarrow$</td><td align="left"><code>\Rightarrow</code></td></tr><tr><td align="center">$\nRightarrow$</td><td align="left"><code>\nRightarrow</code></td><td align="center">$\Longrightarrow$</td><td align="left"><code>\Longrightarrow</code></td><td align="center">$\implies$</td><td align="left"><code>\implies</code></td></tr><tr><td align="center">$\Leftarrow$</td><td align="left"><code>\Leftarrow</code></td><td align="center">$\nLeftarrow$</td><td align="left"><code>\nLeftarrow</code></td><td align="center">$\Longleftarrow$</td><td align="left"><code>\Longleftarrow</code></td></tr><tr><td align="center">$\Leftrightarrow$</td><td align="left"><code>\Leftrightarrow</code></td><td align="center">$\nLeftrightarrow$</td><td align="left"><code>\nLeftrightarrow</code></td><td align="center">$\Longleftrightarrow$</td><td align="left"><code>\Longleftrightarrow</code></td></tr><tr><td align="center">$\iff$</td><td align="left"><code>\iff</code></td><td align="center">$\Uparrow$</td><td align="left"><code>\Uparrow</code></td><td align="center">$\Downarrow$</td><td align="left"><code>\Downarrow</code></td></tr><tr><td align="center">$\Updownarrow$</td><td align="left"><code>\Updownarrow</code></td><td align="center">$\rightarrow$</td><td align="left"><code>\rightarrow</code>, <code>\to</code></td><td align="center">$\nrightarrow$</td><td align="left"><code>\nrightarrow</code></td></tr><tr><td align="center">$\longrightarrow$</td><td align="left"><code>\longrightarrow</code></td><td align="center">$\leftarrow$</td><td align="left"><code>\leftarrow</code>,<code>\gets</code></td><td align="center">$\nleftarrow$</td><td align="left"><code>\nleftarrow</code></td></tr><tr><td align="center">$\longleftarrow$</td><td align="left"><code>\longleftarrow</code></td><td align="center">$\leftrightarrow$</td><td align="left"><code>\leftrightarrow</code></td><td align="center">$\nleftrightarrow$</td><td align="left"><code>\nleftrightarrow</code></td></tr><tr><td align="center">$\longleftrightarrow$</td><td align="left"><code>\longleftrightarrow</code></td><td align="center">$\uparrow$</td><td align="left"><code>\uparrow</code></td><td align="center">$\downarrow$</td><td align="left"><code>\downarrow</code></td></tr><tr><td align="center">$\updownarrow$</td><td align="left"><code>\updownarrow</code></td><td align="center">$\nearrow$</td><td align="left"><code>\nearrow</code></td><td align="center">$\swarrow$</td><td align="left"><code>\swarrow</code></td></tr><tr><td align="center">$\nwarrow$</td><td align="left"><code>\nwarrow</code></td><td align="center">$\searrow$</td><td align="left"><code>\searrow</code></td><td align="center">$\mapsto$</td><td align="left"><code>\mapsto</code></td></tr><tr><td align="center">$\longmapsto$</td><td align="left"><code>\longmapsto</code></td><td align="center">$\rightharpoonup$</td><td align="left"><code>\rightharpoonup</code></td><td align="center">$\rightharpoondown$</td><td align="left"><code>\rightharpoondown</code></td></tr><tr><td align="center">$\leftharpoonup$</td><td align="left"><code>\leftharpoonup</code></td><td align="center">$\leftharpoondown$</td><td align="left"><code>\leftharpoondown</code></td><td align="center">$\upharpoonleft$</td><td align="left"><code>\upharpoonleft</code></td></tr><tr><td align="center">$\upharpoonright$</td><td align="left"><code>\upharpoonright</code></td><td align="center">$\downharpoonleft$</td><td align="left"><code>\downharpoonleft</code></td><td align="center">$\downharpoonright$</td><td align="left"><code>\downharpoonright</code></td></tr><tr><td align="center">$\rightleftharpoons$</td><td align="left"><code>\rightleftharpoons</code></td><td align="center">$\leftrightharpoons$</td><td align="left"><code>\leftrightharpoons</code></td><td align="center">$\curvearrowleft$</td><td align="left"><code>\curvearrowleft</code></td></tr><tr><td align="center">$\circlearrowleft$</td><td align="left"><code>\circlearrowleft</code></td><td align="center">$\Lsh$</td><td align="left"><code>\Lsh</code></td><td align="center">$\upuparrows$</td><td align="left"><code>\upuparrows</code></td></tr><tr><td align="center">$\rightrightarrows$</td><td align="left"><code>\rightrightarrows</code></td><td align="center">$\rightleftarrows$</td><td align="left"><code>\rightleftarrows</code></td><td align="center">$\rightarrowtail$</td><td align="left"><code>\rightarrowtail</code></td></tr><tr><td align="center">$\looparrowright$</td><td align="left"><code>\looparrowright</code></td><td align="center">$\curvearrowright$</td><td align="left"><code>\curvearrowright</code></td><td align="center">$\circlearrowright$</td><td align="left"><code>\circlearrowright</code></td></tr><tr><td align="center">$\Rsh$</td><td align="left"><code>\Rsh</code></td><td align="center">$\downdownarrows$</td><td align="left"><code>\downdownarrows</code></td><td align="center">$\leftleftarrows$</td><td align="left"><code>\leftleftarrows</code></td></tr><tr><td align="center">$\leftrightarrows$</td><td align="left"><code>\leftrightarrows</code></td><td align="center">$\leftarrowtail$</td><td align="left"><code>\leftarrowtail</code></td><td align="center">$\looparrowleft$</td><td align="left"><code>\looparrowleft</code></td></tr><tr><td align="center">$\hookrightarrow$</td><td align="left"><code>\hookrightarrow</code></td><td align="center">$\hookleftarrow$</td><td align="left"><code>\hookleftarrow</code></td><td align="center">$\multimap$</td><td align="left"><code>\multimap</code></td></tr><tr><td align="center">$\leftrightsquigarrow$</td><td align="left"><code>\leftrightsquigarrow</code></td><td align="center">$\rightsquigarrow$</td><td align="left"><code>\rightsquigarrow</code></td><td align="center">$\twoheadrightarrow$</td><td align="left"><code>\twoheadrightarrow</code></td></tr><tr><td align="center">$\twoheadleftarrow$</td><td align="left"><code>\twoheadleftarrow</code></td><td align="center"></td><td align="left"></td><td align="center"></td><td align="left"></td></tr></tbody></table><h3 id="带帽符号"><a href="#带帽符号" class="headerlink" title="带帽符号"></a>带帽符号</h3><table><thead><tr><th align="center">效果</th><th>公式</th><th align="center">效果</th><th>公式</th><th align="center">效果</th><th>公式</th></tr></thead><tbody><tr><td align="center">$\dot{a}$</td><td><code>\dot{a}</code></td><td align="center">$\ddot{a}$</td><td><code>\ddot{a}</code></td><td align="center">$\acute{a}$</td><td><code>\acute{a}</code></td></tr><tr><td align="center">$\grave{a}$</td><td><code>\grave{a}</code></td><td align="center">$\check{a}$</td><td><code>\check{a}</code></td><td align="center">$\tilde{a}$</td><td><code>\tilde{a}</code></td></tr><tr><td align="center">$\bar{a}$</td><td><code>\bar{a}</code></td><td align="center">$\hat{a}$</td><td><code>\hat{a}</code></td><td align="center">$\widehat{abc}$</td><td><code>\widehat{abc}</code></td></tr><tr><td align="center">$\vec{a}$</td><td><code>\vec{a}</code></td><td align="center">$\breve{a}$</td><td><code>\breve{a}</code></td><td align="center">$\widetilde{abc}$</td><td><code>\widetilde{abc}</code></td></tr></tbody></table><h3 id="希腊字母"><a href="#希腊字母" class="headerlink" title="希腊字母"></a>希腊字母</h3><h4 id="大写希腊字母"><a href="#大写希腊字母" class="headerlink" title="大写希腊字母"></a>大写希腊字母</h4><table><thead><tr><th align="center">字母</th><th align="left">公式</th><th align="center">字母</th><th align="left">公式</th><th align="center">字母</th><th align="left">公式</th></tr></thead><tbody><tr><td align="center">$\Alpha$</td><td align="left"><code>\Alpha</code></td><td align="center">$\Beta$</td><td align="left"><code>\Beta</code></td><td align="center">$\Gamma$</td><td align="left"><code>\Gamma</code></td></tr><tr><td align="center">$\Delta$</td><td align="left"><code>\Delta</code></td><td align="center">$\Epsilon$</td><td align="left"><code>\Epsilon</code></td><td align="center">$\Zeta$</td><td align="left"><code>\Zeta</code></td></tr><tr><td align="center">$\Eta$</td><td align="left"><code>\Eta</code></td><td align="center">$\Theta$</td><td align="left"><code>\Theta</code></td><td align="center">$\Iota$</td><td align="left"><code>\Iota</code></td></tr><tr><td align="center">$\Kappa$</td><td align="left"><code>\Kappa</code></td><td align="center">$\Lambda$</td><td align="left"><code>\Lambda</code></td><td align="center">$\Mu$</td><td align="left"><code>\Mu</code></td></tr><tr><td align="center">$\Nu$</td><td align="left"><code>\Nu</code></td><td align="center">$\Xi$</td><td align="left"><code>\Xi</code></td><td align="center">$\Omicron$</td><td align="left"><code>\Omicron</code></td></tr><tr><td align="center">$\Pi$</td><td align="left"><code>\Pi</code></td><td align="center">$\Rho$</td><td align="left"><code>\Rho</code></td><td align="center">$\Sigma$</td><td align="left"><code>\Sigma</code></td></tr><tr><td align="center">$\Tau$</td><td align="left"><code>\Tau</code></td><td align="center">$\Upsilon$</td><td align="left"><code>\Upsilon</code></td><td align="center">$\Phi$</td><td align="left"><code>\Phi</code></td></tr><tr><td align="center">$\Chi$</td><td align="left"><code>\Chi​</code></td><td align="center">$\Psi$</td><td align="left"><code>\Psi</code></td><td align="center">$\Omega$</td><td align="left"><code>\Omega</code></td></tr></tbody></table><p>PS: 如果公式出现标红, 只是因为新版本的MathType不支持在当前页面的显示.</p><h4 id="小写希腊字母"><a href="#小写希腊字母" class="headerlink" title="小写希腊字母"></a>小写希腊字母</h4><table><thead><tr><th align="center">字母</th><th align="left">公式</th><th align="center">字母</th><th align="left">公式</th><th align="center">字母</th><th align="left">公式</th></tr></thead><tbody><tr><td align="center">$\alpha$</td><td align="left"><code>\alpha</code></td><td align="center">$\beta$</td><td align="left"><code>\beta</code></td><td align="center">$\gamma$</td><td align="left"><code>\gamma</code></td></tr><tr><td align="center">$\delta$</td><td align="left"><code>\delta</code></td><td align="center">$\epsilon$</td><td align="left"><code>\epsilon</code></td><td align="center">$\zeta$</td><td align="left"><code>\zeta</code></td></tr><tr><td align="center">$\eta$</td><td align="left"><code>\eta</code></td><td align="center">$\theta$</td><td align="left"><code>\theta</code></td><td align="center">$\iota$</td><td align="left"><code>\iota</code></td></tr><tr><td align="center">$\kappa$</td><td align="left"><code>\kappa</code></td><td align="center">$\lambda$</td><td align="left"><code>\lambda</code></td><td align="center">$\mu$</td><td align="left"><code>\mu</code></td></tr><tr><td align="center">$\nu$</td><td align="left"><code>\nu</code></td><td align="center">$\omicron$</td><td align="left"><code>\omicron</code></td><td align="center">$\xi$</td><td align="left"><code>\xi</code></td></tr><tr><td align="center">$\pi$</td><td align="left"><code>\pi</code></td><td align="center">$\rho$</td><td align="left"><code>\rho</code></td><td align="center">$\sigma$</td><td align="left"><code>\sigma</code></td></tr><tr><td align="center">$\tau$</td><td align="left"><code>\tau</code></td><td align="center">$\upsilon$</td><td align="left"><code>\upsilon</code></td><td align="center">$\phi$</td><td align="left"><code>\phi</code></td></tr><tr><td align="center">$\chi$</td><td align="left"><code>\chi</code></td><td align="center">$\psi$</td><td align="left"><code>psi</code></td><td align="center">$\omega$</td><td align="left"><code>\omega</code></td></tr></tbody></table><h4 id="部分字母变量专用形式"><a href="#部分字母变量专用形式" class="headerlink" title="部分字母变量专用形式"></a>部分字母变量专用形式</h4><p>以<code>\var-</code>开头.</p><table><thead><tr><th align="center">字母</th><th>公式</th><th align="center">字母</th><th>公式</th></tr></thead><tbody><tr><td align="center">$\varepsilon$</td><td><code>\varepsilon</code></td><td align="center">$\varrho$</td><td><code>\varrho</code></td></tr><tr><td align="center">$\varphi$</td><td><code>\varphi</code></td><td align="center">$\varsigma$</td><td><code>\varsigma</code></td></tr><tr><td align="center">$\varkappa$</td><td><code>\varkappa</code></td><td align="center">$\vartheta$</td><td><code>\vartheta</code></td></tr><tr><td align="center">$\varpi$</td><td><code>\varpi</code></td><td align="center">$\digamma$</td><td><code>\digamm</code></td></tr></tbody></table><h4 id="类字母和常数"><a href="#类字母和常数" class="headerlink" title="类字母和常数"></a>类字母和常数</h4><table><thead><tr><th align="center">字母</th><th align="left">公式</th><th align="center">字母</th><th align="left">公式</th><th align="center">字母</th><th align="left">公式</th></tr></thead><tbody><tr><td align="center">$\infty$</td><td align="left"><code>\infty</code></td><td align="center">$\aleph$</td><td align="left"><code>\aleph</code></td><td align="center">$\complement$</td><td align="left"><code>\complement</code></td></tr><tr><td align="center">$\backepsilon$</td><td align="left"><code>\backepsilon</code></td><td align="center">$\eth$</td><td align="left"><code>\eth</code></td><td align="center">$\Finv$</td><td align="left"><code>\Finv</code></td></tr><tr><td align="center">$\hbar$</td><td align="left"><code>\hbar</code></td><td align="center">$\theta$</td><td align="left"><code>\Im</code></td><td align="center">$\imath$</td><td align="left"><code>\imath</code></td></tr><tr><td align="center">$\jmath$</td><td align="left"><code>\jmath</code></td><td align="center">$\Bbbk$</td><td align="left"><code>\Bbbk</code></td><td align="center">$\ell$</td><td align="left"><code>\ell</code></td></tr><tr><td align="center">$\mho$</td><td align="left"><code>\mho</code></td><td align="center">$\wp$</td><td align="left"><code>\wp</code></td><td align="center">$\Re$</td><td align="left"><code>\Re</code></td></tr><tr><td align="center">$\circledS$</td><td align="left"><code>\circledS</code></td><td align="center"></td><td align="left"></td><td align="center"></td><td align="left"></td></tr></tbody></table><h2 id="字体转换"><a href="#字体转换" class="headerlink" title="字体转换"></a>字体转换</h2><p>用<code>{\字体 {需转换的部分字符}}</code>来变更字体, 公式默认为意大利体.</p><table><thead><tr><th align="center">输入</th><th align="center">说明</th><th align="center">显示</th><th align="center">输入</th><th align="center">说明</th><th align="center">显示</th></tr></thead><tbody><tr><td align="center"><code>\rm</code></td><td align="center">罗马体</td><td align="center">${\rm Sample}$</td><td align="center"><code>\cal</code></td><td align="center">花体</td><td align="center">${\cal Sample}$</td></tr><tr><td align="center"><code>\it</code></td><td align="center">意大利体</td><td align="center">${\it Sample}$</td><td align="center"><code>\Bbb</code></td><td align="center">黑板粗体</td><td align="center">${\Bbb Sample}$</td></tr><tr><td align="center"><code>\bf</code></td><td align="center">粗体</td><td align="center">${\bf Sample}$</td><td align="center"><code>\mit</code></td><td align="center">数学斜体</td><td align="center">${\mit Sample}$</td></tr><tr><td align="center"><code>\sf</code></td><td align="center">等线体</td><td align="center">${\sf Sample}$</td><td align="center"><code>\scr</code></td><td align="center">手写体</td><td align="center">${\scr Sample}$</td></tr><tr><td align="center"><code>\tt</code></td><td align="center">打字机体</td><td align="center">${\tt Sample}$</td><td align="center"><code>\frak</code></td><td align="center">旧德式字体</td><td align="center">${\frak Sample}$</td></tr></tbody></table><p>如果想直接从斜体变为非斜体, 可以使用<code>\text{内容}</code>.<br>$$<br>\begin{array}{cc}<br>\mathrm{Bad} &amp; \mathrm{Better} \\<br>\hline \\<br>\int_0^1 x^2 dx &amp; \int_0^1 x^2 \,{\rm d}x<br>\end{array}<br>$$</p><h2 id="更改字体颜色"><a href="#更改字体颜色" class="headerlink" title="更改字体颜色"></a>更改字体颜色</h2><p><code>MathJax3</code>还不支持渲染字体的颜色, 大多数情况下这个颜色也是一个鸡肋功能. 如果想知道如何添加可以看文章最开头说的两篇博客, 里面有写到更改颜色的详细方法.</p>]]></content>
      
      
      <categories>
          
          <category> Markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Latex </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之K邻近</title>
      <link href="/posts/40015.html"/>
      <url>/posts/40015.html</url>
      
        <content type="html"><![CDATA[<h1 id="K邻近KNN-K-Nearest-Neighbor"><a href="#K邻近KNN-K-Nearest-Neighbor" class="headerlink" title="K邻近KNN K-Nearest Neighbor"></a>K邻近KNN K-Nearest Neighbor</h1><p>K邻近是一种非常简单的监督学习分类方法. KNN指的是每个样本都可以通过它最近的K个样本来代表. 比方说在下述图片中, 若K=3, 找到距离未知样本即绿色圆圈最近的3个样本, 在该范围内红色三角占$\frac 2 3$, 则绿色圆圈被认为是红色三角的类别. 若K=5, 则蓝色方块所占的比例为$\frac 3 5$, 绿色圆圈被认为是蓝色方块. 如果K的取指不同, 则未知样本的类别也会产生改变, 所以结果很大程度取决于<strong>K的选择</strong>.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/knn.jpg" style="zoom:50%;" /><p>当然, 在这个过程中距离不一定是欧氏距离, 还可以选择曼哈顿距离.<br>$$<br>d(x, y) = \sqrt{\sum_{k=1}^n |x_k - y_k|}<br>$$<br>在实际应用过程中, 还可以基于距离的远近进行加权平均或投票, 距离越近的样本权重越大.</p><p>KNN是一种Lazy learner, 也就是懒惰学习算法. 它不需要训练, 只是单纯的记住所有的训练样本, 在进行预测时根据已经记住的训练集去寻找临近, 从而获得结果.</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之决策树</title>
      <link href="/posts/11302.html"/>
      <url>/posts/11302.html</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>2020.9.8</strong>: 更新了剪枝.</p></blockquote><h1 id="决策树DT-Desicion-Tree"><a href="#决策树DT-Desicion-Tree" class="headerlink" title="决策树DT Desicion Tree"></a>决策树DT Desicion Tree</h1><p>决策树(Decision Tree) 是在已知各种情况发生概率的基础上, 通过构成决策树来求取净现值的期望值大于等于零的概率, 评价项目风险, 判断其可行性的决策分析方法, 是直观运用概率分析的一种图解法. 由于这种决策分支画成图形很像一棵树的枝干, 故称决策树. 在机器学习中, 决策树是一个预测模型, 他代表的是对象属性与对象值之间的一种映射关系. Entropy = 系统的凌乱程度, 使用算法ID3, C4.5和C5.0生成树算法使用熵. 这一度量是基于信息学理论中熵的概念.  决策树是一种<strong>树形结构</strong>, 其中每个内部节点表示一个属性上的测试, 每个分支代表一个测试输出, 每个叶节点代表一种类别. </p><h2 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h2><p>决策树分为两种, 也就是按照它们功能进行区分的, 回归树和分类树. 决策树作为树类模型, 不依赖于特征的离散或连续, 树的分裂仅取决于分裂时的判断条件, 与特征的离散或连续性无关. </p><blockquote><ol><li><strong>根结点</strong>(Root Node): 它表示整个样本集合, 并且该节点可以进一步划分成两个或多个子集. </li><li><strong>拆分</strong>(Splitting): 表示将一个结点拆分成多个子集的过程. </li><li><strong>决策结点</strong>(Decision Node): 当一个子结点进一步被拆分成多个子节点时, 这个子节点就叫做决策结点. </li><li><strong>叶子结点</strong>(Leaf/Terminal Node): 无法再拆分的结点被称为叶子结点. </li><li><strong>剪枝</strong>(Pruning): 移除决策树中子结点的过程就叫做剪枝, 跟拆分过程相反. </li><li><strong>分支/子树</strong>(Branch/Sub-Tree): 一棵决策树的一部分就叫做分支或子树. </li><li><strong>父结点和子结点</strong>(Paren and Child Node): 一个结点被拆分成多个子节点, 这个结点就叫做父节点；其拆分后的子结点也叫做子结点. </li></ol></blockquote><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/决策树.png" style="zoom:67%;" /><h2 id="构造过程"><a href="#构造过程" class="headerlink" title="构造过程"></a>构造过程</h2><p>决策树的构造过程一般分为3个部分, 分别是特征选择, 决策树生产和决策树裁剪. </p><ol><li><p><strong>特征选择</strong></p><p>特征选择表示从众多的特征中选择一个特征作为当前节点分裂的标准, 如何选择特征有不同的量化评估方法, 从而衍生出不同的决策树, 如ID3(通过信息增益选择特征) , C4.5(通过信息增益比选择特征) , CART(通过Gini指数选择特征) 等. </p><p>目的(准则) : 使用某特征对数据集划分之后, 各数据子集的纯度要比划分钱的数据集D的纯度高(也就是不确定性要比划分前数据集D的不确定性低.</p></li><li><p><strong>决策树的生成</strong></p><p>根据选择的特征评估标准, 从上至下递归地生成子节点, 直到数据集不可分则停止决策树停止生长. 这个过程实际上就是使用满足划分准则的特征不断的将数据集划分成纯度更高, 不确定行更小的子集的过程. 对于当前数据集的每一次划分, 都希望根据某个特征划分之后的各个子集的纯度更高, 不确定性更小. </p></li><li><p><strong>决策树的裁剪</strong></p><p>决策树容易过拟合, 一般需要剪枝来缩小树结构规模, 缓解过拟合. </p></li></ol><h2 id="划分选择"><a href="#划分选择" class="headerlink" title="划分选择"></a>划分选择</h2><p>划分选择是决策树进行学习的关键. 我们希望决策树的分支节点包含的样本尽可能属于同一类别, 即节点的纯度越来越高, 选择方法都是基于<strong>最大熵原理</strong>的. 最大熵原理是一种选择随机变量统计特性最符合客观情况的准则, 也称为最大信息原理, 在已知一些知识的情况下, 将其他所有未知的事件全部当做等概率事件来处理. <strong>万物趋近于无序, 当事件越不确定(等事件概率发生)时, 熵就最大</strong>.</p><h3 id="信息增益-Information-Gain"><a href="#信息增益-Information-Gain" class="headerlink" title="信息增益 Information Gain"></a>信息增益 Information Gain</h3><p>信息熵是度量样本集合纯度的一种常用指标. 对于当前样本集合$D$中第$k$类样本所占的比例$p_k$, 其信息熵定义为:<br>$$<br>{\rm Ent}(D) = - \sum_k^{K}p_k \log_2{p_k}<br>$$<br><strong>熵越小, 未知的信息就越少</strong>, 即纯度越高. 这里约定如果$p=0$, 则$p\log_2p=0$ .</p><p>而在节点划分时, 可能特征$a$有多个可以取到的可能性$V$, 对数据集进行划分, 这时候可以用条件熵是用来表示在这个特征下某值的不确定性. 这里的${\rm Ent}(D^v)$ 其实就是${\rm Ent}(D|a=a_v)$. 为了保证后续和西瓜书上的公式形式一致采取了前者.<br>$$<br>{\rm Ent}(D|a)=\sum_{v=1}^V\frac{|D^v|}{|D|}{\rm Ent}(D^v)<br>$$<br>信息增益就是在按照特征的某值进行划分后的熵的变化, 条件熵越大, 证明划分效果越差, 对应的信息增益就越少:<br>$$<br>{\rm Gain}(D, a) ={\rm Ent}(D)-{\rm Ent}(D|a)<br>$$<br>使用信息增益作为节点划分依据的算法称为<strong>ID3算法</strong>.</p><p>以周志华老师的西瓜书中的西瓜数据集为例:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/西瓜数据集.png" style="zoom: 33%;" /><p>明显西瓜只有好瓜和坏瓜两种, 好瓜$p_1=\frac{8}{17}$. 坏瓜$p_2=\frac{9}{17}$首先计算出根节点的信息熵:<br>$$<br>{\rm Ent}(D) = - \sum_{k=1}^{2}p_k \log_2{p_k}=-(\frac{8}{17}\log_2\frac{8}{17}+\frac{9}{17}\log_2\frac{9}{17})=0.998<br>$$<br>然后要遍历当前的属性集合$\{色泽, 根蒂, 敲声, 纹理, 脐部, 触感\}$. 以色泽为例, 有三个可能取值$\{青绿, 乌黑, 浅白\}$. 将这个属性进行划分, $D^1(色泽=青绿)$, $D^2(色泽=乌黑)$, $D^3(色泽=浅白)$. 对于子集$D^1, p_1=\frac{3}{6}, p_2=\frac{3}{6}$, 对于子集$D^2, p_1=\frac{4}{6}, p_2=\frac{2}{6}$, 对于子集$D^3, p_1=\frac{1}{5}, p_2=\frac{4}{5}$, 以色泽为划分依据之后得到三个节点的信息熵为:<br>$$<br>\begin{aligned}<br>{\rm Ent}(D^1)&amp;=-(\frac{3}{6}\log_2\frac{3}{6}+\frac{3}{6}\log_2\frac{3}{6})=1 \\<br>{\rm Ent}(D^2)&amp;=-(\frac{4}{6}\log_2\frac{4}{6}+\frac{2}{6}\log_2\frac{2}{6})=0.918 \\<br>{\rm Ent}(D^3)&amp;=-(\frac{1}{5}\log_2\frac{1}{5}+\frac{4}{5}\log_2\frac{4}{5})=0.722 \\<br>\end{aligned}<br>$$<br>因此能够根据信息熵计算出信息增益为:<br>$$<br>\begin{aligned}<br>{\rm Gain}(D, 色泽)&amp;={\rm Ent}(D)-\sum_{v=1}^3\frac{|D^v|}{|D|}{\rm Ent}(D^v)\\<br>&amp;=0.998-(\frac{6}{17}\times 1 + \frac{6}{17} \times 0.918 + \frac{5}{17}\times 0.722) \\<br>&amp;=0.109<br>\end{aligned}<br>$$</p><h3 id="信息增益比-Information-Gain-Ratio"><a href="#信息增益比-Information-Gain-Ratio" class="headerlink" title="信息增益比 Information Gain Ratio"></a>信息增益比 Information Gain Ratio</h3><p>不要忘记西瓜数据集中, 仍然含有”编号”这一特征, 如果将其作为特征纳入决策树的选择中, 那岂不是决策树可以完美拟合编号这一特征, 而完全丧失了泛化能力? 其实对于其他取值较多的特征亦是如此, 当特征的可能性较多时, 每次做一次节点划分, <strong>信息增益会偏爱那些取值多的特征</strong>, 因为原本特征的不确定性就比较大, 所以当选择一个值作为划分时, 消除的熵也很多, 信息增益就会变得大.</p><p>这时就需要用某种熵的定义来平衡掉这个偏好, 也就是”信息增益率”.<br>$$<br>{\rm Gain\_ratio}(D, a) = \frac{ {\rm Gain}(D, a)} { {\rm IV}(a)}<br>$$<br>其中有:<br>$$<br>{\rm IV}(a) = -\sum_{v=1}^V\frac{|D^v|}{|D|}\log_2 \frac{|D^v|}{|D|}<br>$$<br>$a$ 的可能取值数目越多, 则${\rm IV}(a)$就越大(其实它完全就是按照熵来定义的, 不确定性越多熵越大). 但是信息增益率可能对取值数目少的特征有所偏好, 因此是先从候选划分属性中找出信息增益高于平均水平的属性, 然后再从中选择增益率最高的.</p><p>使用信息增益比作为节点划分依据的算法称为<strong>C4.5算法</strong>.</p><h3 id="基尼指数-Gini-Index"><a href="#基尼指数-Gini-Index" class="headerlink" title="基尼指数 Gini Index"></a>基尼指数 Gini Index</h3><p>基尼值表示了<strong>数据集的纯度</strong>, 因此划分准则可以用基尼指数:<br>$$<br>\begin{aligned}<br>{\rm Gini}(D)&amp;=\sum_{k=1}^K\sum_{k’\neq k}p_kp_{k’} \\<br>&amp;=1-\sum_{k=1}^Kp_k^2<br>\end{aligned}<br>$$<br>某个属性的基尼指数定义为:<br>$$<br>{\rm Gini\_index}(D,a)=\sum_{v=1}^{V}\frac{|D^v|}{|D|}{\rm Gini}(D^v)<br>$$<br>其实和之前的条件熵的形式如出一辙. 因此我们在选择候选属性时, 选择划分后基尼指数最小的属性为最优划分属性.</p><p>使用信息增益比作为节点划分依据的算法称为<strong>CART算法</strong>.</p><h2 id="连续值处理"><a href="#连续值处理" class="headerlink" title="连续值处理"></a>连续值处理</h2><p>对于连续属性, 可取值书目不再有限, 因此采用最简单的<strong>二分法</strong>. 对于给定样本中连续属性的所有不同取值, 从小到大进行排序, 然后对每个值进行一次二分, 每次都产生比划分点$t$ 的值不大的集合$D_t^-$和比它大的集合$D_t^+$, 然后再进行计算信息增益, 以信息增益最大的划分点作为划分依据即可.</p><h2 id="决策树剪枝"><a href="#决策树剪枝" class="headerlink" title="决策树剪枝"></a>决策树剪枝</h2><p>剪枝是决策树缓解过拟合的重要手段, 当决策树对节点划分过于细致时候就容易发生过拟合. 决策树有预剪枝和后剪枝两种剪枝策略, 即分别在决策树构建划分节点时和决策树生长完成后剪枝.</p><h3 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h3><p>预剪枝基于分支划分的准则, 在局部子树生成完成后, 依据<strong>验证集</strong>计算节点划分后能否带来收益, 如果没有性能上的提升或者导致性能下降, 则禁止该节点的划分.</p><p>除了依据划分准则进行预剪枝外, 还可以设定树的生长高度或导致叶子节点分裂的最小样本数, 禁止节点划分的最小阈值等参数, 达到防止过拟合的效果.</p><p>因为预剪枝是与决策树构建并行的, 所以可能会因为局部的性能提升而剪掉更有潜力的节点, 可能过早的停止决策树构造. 常常效果也没有后剪枝好.</p><h3 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h3><p>等决策树完整生成后, 后剪枝才开始工作. 后剪枝将从决策树的底部往上进行剪枝, 如果剪枝能够提高验证集的精度, 那么就将其裁去. 后剪枝还有许多其他的剪枝方法, 通过采用不同的性能标准来决定节点的保留与否. </p><p>后剪枝因为不具有视野上的问题, 保留的分支常比预剪枝要多, 泛化能力也更好一些.</p><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p>优点: 可解释性好, 分类速度快.</p><p>缺点: 如果不采用剪枝或随机森林很容易发生过拟合, 体现在决策树结构中就是树划分的过细, 或者深度过深.</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 决策树 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之朴素贝叶斯</title>
      <link href="/posts/63092.html"/>
      <url>/posts/63092.html</url>
      
        <content type="html"><![CDATA[<h1 id="朴素贝叶斯NB-Naive-Bayes"><a href="#朴素贝叶斯NB-Naive-Bayes" class="headerlink" title="朴素贝叶斯NB Naive Bayes"></a>朴素贝叶斯NB Naive Bayes</h1><p>朴素贝叶斯有一个非常Naive的假设: 所有特征都是相互独立的, 因此所有特征总的条件概率总是每个特征条件概率的乘积. 这个算法的核心就在于贝叶斯公式.</p><h2 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h2><p>条件概率是贝叶斯定理的铺垫. 指的是事件A在另外一个事件B已经发生条件下的发生概率, 记为$P(A|B)$. </p><p>比方说有某个布袋, 其中的有2个蓝球3个红球. 问每次不放回的随机从布袋中取出一个球, 连续两次拿到蓝球概率是多少? 第一次拿到蓝球记为$P(A)$, 概率是$\frac{2}{5}$, 假设第一次拿到了蓝球, 那么第二次布袋里一定只有1个蓝球和3个红球. 第二次拿到蓝球记为$P(B|A)$, 概率为$\frac{1}{4}$, 记为, 所以连续两次拿到蓝球的概率为$\frac{2}{5}*\frac{1}{4}=\frac{1}{10}$. 如果把连续两次拿到蓝球记为$P(A, B)$, 那么就得到了条件概率公式:<br>$$<br>P(A, B) = P(B|A)\cdot P(A)<br>$$<br>反推一下B在A的条件下的概率$P(B|A)$:<br>$$<br>P(B|A) = \frac{P(A, B)}{P(A)}<br>$$</p><h2 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h2><p>根据条件概率, 可以类似的推出A在B的条件下的概率$P(A|B)$:<br>$$<br>P(A|B) = \frac{P(A, B)}{P(B)}<br>$$<br>将两侧相同的$P(A,B)$联立消去, 就得到了贝叶斯定理:<br>$$<br>P(B|A) = \frac{P(A|B)P(B)}{P(A)}<br>$$<br>在这个式子中, 称$P(B)$为<strong>先验概率</strong>, 即在不知道A事件条件下, 对B事件发生做出的判断. $P(B|A)$称为<strong>后验概率</strong>, 即在A事件发生后对B事件的重新评估. $\frac{P(A|B)}{P(A)}$称为调整因子或者<strong>似然概率</strong>, 它对先验概率进行调整, 使其变为后验概率. </p><h2 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h2><p>朴素贝叶斯正是建立在贝叶斯定理的基础上, 对条件概率分布做条件独立性假设, 即特征之间相互独立, 并且他们同等重要. 虽然影响精度, 但仍然在某些问题上取得良好的效果.</p><p>在贝叶斯定理中, 对于多特征的情况, 将特征写成一个向量$\textbf x$, 对于该样本属于第$k$的个类别的事件记为$C_k$, 将其转化为向量形式有:<br>$$<br>P(C_k|\textbf x) = \frac{P(\textbf x|C_k)P(C_k)}{P(\textbf x)}<br>$$<br>写成文字也就是:<br>$$<br>P(类别|特征) = \frac{P(特征|类别)P(类别)}{P(特征)}<br>$$<br>基于各个假设独立的特征, 分母$P(\textbf x)$是一个常数, 将分子$P(\textbf x|C_k)P(C_k)$基于链式法则, 在特征独立的情况下, 将分子重新写为:<br>$$<br>P(C_k| x_1, x_2, \dots, x_n)=P(C_k)\prod_{i=1}^nP(x_i|C_k)<br>$$<br>这样根据样本中的特征出现频率, 就能得到各个先验概率, 从而推断出后验概率了.</p><h2 id="分类准则"><a href="#分类准则" class="headerlink" title="分类准则"></a>分类准则</h2><p>分母$P(\textbf x)$是一个常数, 从优化目标中略去. 只要满足分类器基于特征独立下, 预测目标的概率最大即可.<br>$$<br>\hat{y}=\arg\max\limits_{c \in y} P(c)\prod_{i=1}^dP(x_i|c)<br>$$</p><h2 id="拉普拉斯平滑"><a href="#拉普拉斯平滑" class="headerlink" title="拉普拉斯平滑"></a>拉普拉斯平滑</h2><p>如何处理概率为0的情况? 由于概率在分子上和分母上都是连乘形式, 如果其中一个为0则导致整个分类错误, 这显然不合理. 根据拉普拉斯平滑(也称为拉普拉斯修正), 对类别概率$P(c)$和预测中使用的先验概率$P(x_i|c)$进行修正, 如下:<br>$$<br>\begin{aligned}<br>\hat{P}(c) &amp;= \frac{|D_c|+1}{|D|+N} \\<br>\hat{P}(x_i|c) &amp;= \frac{|D_{c, x_i}|+1}{|D_c|+N_i}<br>\end{aligned}<br>$$<br>其中$D$代表数据集, $N$为可能的类别总数, $N_i$为第$i$个属性可能取值数, </p><h2 id="离散和连续"><a href="#离散和连续" class="headerlink" title="离散和连续"></a>离散和连续</h2><p>如果是离散特征, 直接用特征的频率除以样本总数作为概率即可. </p><p>如果是连续特征, 必须要结合概率密度函数, 假设特征服从正态:<br>$$<br>G(x, \mu, \sigma) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}<br>$$<br>因此$P(x_i|C_k)=G(x_i,\mu_{c,i}, \sigma_{c,i})$. $\mu_{c,i}$和$\sigma^2_{c,i}$分别是第$c$类样本在第$i$个特征上的均值和方差.</p><h2 id="优缺点分析"><a href="#优缺点分析" class="headerlink" title="优缺点分析"></a>优缺点分析</h2><p>优点:</p><ul><li>既简单又快速，预测表现良好；</li><li>如果变量独立这个条件成立，相比Logistic回归等其他分类方法，朴素贝叶斯分类器性能更优，且只需少量训练数据；</li><li>相较于数值变量，朴素贝叶斯分类器在多个分类变量的情况下表现更好。若是数值变量，需要正态分布假设。</li></ul><p>缺点:</p><ul><li>如果分类变量的类别（测试数据集）没有在训练数据集总被观察到，那这个模型会分配一个0（零）概率给它，同时也会无法进行预测。这通常被称为“零频率”。为了解决这个问题，我们可以使用平滑技术，拉普拉斯估计是其中最基础的技术。</li><li>朴素贝叶斯也被称为<strong>bad estimator</strong>，所以它的概率输出predict_proba不应被太认真对待。</li><li>朴素贝叶斯的另一个限制是独立预测的假设。在现实生活中，这几乎是不可能的，各变量间或多或少都会存在相互影响。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之模型选择</title>
      <link href="/posts/63889.html"/>
      <url>/posts/63889.html</url>
      
        <content type="html"><![CDATA[<h1 id="模型选择-Model-Selection"><a href="#模型选择-Model-Selection" class="headerlink" title="模型选择 Model Selection"></a>模型选择 Model Selection</h1><h2 id="过拟合和欠拟合-Overfitting-and-Underfitting"><a href="#过拟合和欠拟合-Overfitting-and-Underfitting" class="headerlink" title="过拟合和欠拟合 Overfitting and Underfitting"></a>过拟合和欠拟合 Overfitting and Underfitting</h2><p>这个其实非常好解释, 就放在一起说了.</p><p>过拟合就像是平时做很多作业题但是却不会考试的学生, 一到考试就拉胯, 但是平时作业写得很完美. 过拟合导致了过度的学习了作业中的内容, 甚至是单纯错误的把作业题背下来了, 导致<strong>失去了泛化能力</strong>.</p><p>欠拟合就像是平时不怎么学习的学渣. 考试题也不会, 作业也不会写. 欠拟合根本没有对作业中的内容进行学习, 对考试题型的拟合程度根本不够.</p><p>欠拟合对应着高偏差, 过拟合对应着高方差.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/偏差和方差.png" style="zoom:67%;" /><ol><li>左上的模型偏差最大, 右下的模型偏差最小.</li><li>左上的模型方差最小, 右下的模型方差最大. 这样去理解, 如果两个训练集的分布有一丝丝差异, 由于采用了复杂的模型拟合, 会导致分类结果或回归结果截然不同. 就导致新样本点散落在原样本的两侧, 方差极高.</li></ol><p>用CV代表在验证集上的训练误差, 而Training error代表在训练集上的训练误差.</p><p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/learning-curves.png" alt=""></p><p>欠拟合的训练误差和验证误差都很大, 过拟合的训练误差很小, 但是测试误差很大. 只有在中间的模型刚刚好, 具有泛化能力, 也同时具有判断问题的能力.</p><h2 id="交叉验证-Cross-Validation"><a href="#交叉验证-Cross-Validation" class="headerlink" title="交叉验证 Cross Validation"></a>交叉验证 Cross Validation</h2><p>先解释一下数据的分割问题. 我们常常将数据分割为三类, 分别是<strong>训练集, 验证集, 和测试集</strong>. 一般都将一份数据集按照比例划分成训练集和验证集, 测试集一般取样于实际问题中, 这样才能够检验模型真正的泛化能力. 但是在后面说的”全部数据集” 都是在不考虑测试集情况下说的, 也就是只划分为训练集和测试集的情况.</p><table><thead><tr><th align="center">划分</th><th align="left">意义</th></tr></thead><tbody><tr><td align="center">训练集</td><td align="left">专门用来训练模型的, 在训练过程中, 模型参数会随着训练而改变, 就像是上课一样.</td></tr><tr><td align="center">验证集</td><td align="left">用来检测当前模型对数据集的拟合程度, 此时模型参数不会更新, 它像作业题.</td></tr><tr><td align="center">测试集</td><td align="left">模型从来没有见过的, 真正的考试题, 能够判决模型能力的终极手段.</td></tr></tbody></table><p><strong>测试集只能用一次, 就是最后对模型参数调好后进行终极测试的时候</strong>.</p><p>对于一份数据, 如果我们既将它全部用来训练, 也用于验证, 那么就相当于用作业题去上课, 造成了<strong>标签泄露</strong>. 如果只将训练集拿去训练呢, 又会损失一部分训练的数据, 并且模型最终的结果严重取决于训练集和验证集的划分. 基于这个矛盾, 提出了交叉验证.</p><h3 id="K折交叉验证-K-Fold-Cross-Validation"><a href="#K折交叉验证-K-Fold-Cross-Validation" class="headerlink" title="K折交叉验证 K-Fold Cross Validation"></a>K折交叉验证 K-Fold Cross Validation</h3><p>将含有$m$个样本的数据集划分为$k$个不相交的子集, 每个子集有$\frac{m}{k}$ 个样本. 从每次分好的样本中, 选一个子集作为验证集, 其他$k-1$个子集全部作为训练集, 每次训练都能得到一个评估值(每次都用同样的参数训练一个新的分类器), 重复进行$k$ 次最后取平均, 就是k折交叉验证所得到的验证结果. K-Fold CV 能反映出当前模型真正的拟合能力, 经验值常取k为5, 10… </p><h3 id="留一法-Leave-one-out-cross-validation"><a href="#留一法-Leave-one-out-cross-validation" class="headerlink" title="留一法 Leave-one-out cross-validation"></a>留一法 Leave-one-out cross-validation</h3><p>当$k=m$的时候, 每个子集只有一个样本, 每次只取一个样本作为验证集, 用其余所有样本进行训练, K折交叉验证就变成了留一法. 留一法能够最大程度的利用样本, 但是也同时牺牲了时间. 如果有k个样本, 则需要训练k次, 测试k次. <strong>小样本适用</strong>.</p><h2 id="网格搜索-Grid-Search"><a href="#网格搜索-Grid-Search" class="headerlink" title="网格搜索 Grid Search"></a>网格搜索 Grid Search</h2><p>当我们不知道模型的参数如何选择时, 可以采用网格搜索. 网格搜索其实就是<strong>暴力穷举</strong>, 将模型所有可能的参数取值全都试一遍, 这能保证不遗漏模型组合参数造成的影响, 但是同时这样做的时间成本也非常高. 所以一般情况下都是先进行参数范围的缩小, 最后再使用网格搜索. 网格搜索也常和交叉验证一起使用, 用来更精确的评价模型.</p><p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/gridSearch.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 交叉验证 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>循环神经网络小结</title>
      <link href="/posts/60202.html"/>
      <url>/posts/60202.html</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>2020.9.7</strong>: 重写了LSTM和GRU的描述.</p><p><strong>2020.8.22</strong>: 对部分内容进行了更新.</p></blockquote><h1 id="循环神经网络-Recurrent-Neural-Network"><a href="#循环神经网络-Recurrent-Neural-Network" class="headerlink" title="循环神经网络 Recurrent Neural Network"></a>循环神经网络 Recurrent Neural Network</h1><p>面对时序型数据, 如自然语言, 乐谱, 金融数据等包含隐含的时间信息在内的数据, 不能采用原始的稠密神经网络, 会产生很多问题. 比如, 很容易就产生海量的参数, 或者输入输出的神经元很有可能是不确定的, 最主要的是无法将时序的信息传递给稠密神经网络. 采用循环神经网络可以很好的处理时序问题.</p><h2 id="RNN的基本结构"><a href="#RNN的基本结构" class="headerlink" title="RNN的基本结构"></a>RNN的基本结构</h2><p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rnn.jpg" alt=""></p><p>RNN因为满足<strong>循环递归</strong>的结构, 很多时候也画成左边折叠的样子. 能够看到, 下方的$X_i$ 对应着$t$时刻的输入. 在图中没有画出来的是, 对于图中的$A$ 实际上是一个含有若干个神经元的与输入输出相连的神经网络(其实对应数学结构就是一个矩阵), 只不过<strong>隐藏层之间相互有了连接</strong>, 下面这个图可能更好解释一些:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rnn6.png" style="zoom:50%;" /><p>在新的时刻$t$ ,输入视为是$t-1$时刻的神经元结果和$t$时刻的时序序列输入$X_t$的合成结果. 假设激活函数$f$, 当前时刻为$t$有:<br>$$<br>\displaylines{<br>A_t = f(UX_t + WA_{t-1} + b_a) \\<br>h_t = f(VA_t + b_y)}<br>$$<br><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rnn2.jpg" alt=""></p><p>在进行计算时, 既然是循环使用的神经元, 那么<strong>在时刻尺度上</strong>它的参数一定是<strong>共享</strong>的, 即对于同一组RNN, 它们的$U, W, V, b$ 采用的都是<strong>相同的值</strong>, 这也符合我们对<strong>递归</strong>的理解, RNN每个时间步都在做相同的事情, 只是输入不同. 并且在实际计算的过程中, 经常将$U$和$W$合并为同一个矩阵, 将$X_t, A_{t-1}$也合并成一个矩阵做矩阵乘.</p><p>其实上述过程就是RNN的前向传播, 非常的符合逻辑, 也同时隐含了它<strong>只能进行串行计算</strong>的弊病. 其实反向传播只要把前向传播过程反过来就行了, 损失函数为所有时刻的损失平均值.</p><h2 id="RNN的几种结构"><a href="#RNN的几种结构" class="headerlink" title="RNN的几种结构"></a>RNN的几种结构</h2><p>RNN也可以分为很多种, 有一对一, 一对多, 多对一, 多对多, 具体采用哪种需要结合具体的任务目标而定.</p><p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84rnn.jpg" alt=""></p><p>经典RNN由于结构输入和输出必须是一一对应的,应用范围受限很大. 图中所示的第一种多对多经常用于机器翻译, 前一段没有输出的神经网络对应的称为<code>encoder</code>即编码器, 后一段有输出的神经网络称为<code>decoder</code>即解码器, 机器在经过编码器读完整个句子后从解码器获取输出. 第二种多对多经常用于多对多经常用于序列生成. </p><h3 id="RNN的采样"><a href="#RNN的采样" class="headerlink" title="RNN的采样"></a>RNN的采样</h3><p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rnn%E9%87%87%E6%A0%B7.jpg" alt=""></p><p>假设有个已经训练好的模型, 那么我们对输入$x^{&lt;1&gt;}$和$a^{&lt;0&gt;}$都置为零向量, 然后让RNN自己预测第一个单词的概率向量$\hat y^{&lt;1&gt;}$, 根据这个概率利用例如<code>np.random.choice</code>获取一个单词的index, 将这个单词的one-hot形式作为下时刻的输入. 当然不一定是单词级别的RNN, 字符级别也可以用, 但是计算成本非常大, 一般只有很多专业词汇才用.</p><h2 id="RNN的梯度爆炸和梯度消失"><a href="#RNN的梯度爆炸和梯度消失" class="headerlink" title="RNN的梯度爆炸和梯度消失"></a>RNN的梯度爆炸和梯度消失</h2><p>RNN因为是循环的结构, 循环多次很容易导致网络层数加深, 这样前面的网络参数很难被反向传播影响. 比如说RNN可能很难记住一个长句子里的时态语态信息, 其每个时刻的输出主要由临近的几个时刻所影响, 导致<strong>不善于处理长期依赖</strong>问题. 必须引入一些结构来传递需要被长期记忆的信息.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rnn长期依赖.png" style="zoom: 33%;" /><h3 id="长短期记忆神经网络LSTM-Long-Short-Term-Memory"><a href="#长短期记忆神经网络LSTM-Long-Short-Term-Memory" class="headerlink" title="长短期记忆神经网络LSTM Long Short-Term Memory"></a>长短期记忆神经网络LSTM Long Short-Term Memory</h3><p>LSTM于1997年在<a href="(https://www.bioinf.jku.at/publications/older/2604.pdf)">Long Short-Term Memory</a>中出现, 是一种尝试保存长期记忆避免梯度消失的RNN. LSTM主要有四个部分, <strong>遗忘门</strong>, <strong>输入门</strong>, <strong>输出门</strong>, <strong>细胞状态</strong>. 因为引入了循环神经网络的记忆机制, 常形象地将结构单位称为<strong>记忆细胞</strong>(memory cell).</p><p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lstm.png" alt=""></p><p>LSTM中所有的向量操作如下:</p><p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lstm%E6%93%8D%E4%BD%9C.png" alt=""></p><p>黄框框代表<strong>神经层</strong>(注意是一层神经层, 不单单是图中标注的激活函数), 粉圈圈代表某种向量的操作, 单箭头代表向量的流向, 汇聚箭头代表向量的concat, 分离箭头代表向量拷贝成了两份.</p><p>LSTM最上面的那条水平线代表<strong>细胞状态</strong>, 细胞存储的状态实际上就是需要<strong>长期记忆</strong>的信息. 这条路上只有遗忘门和输入门能够对细胞状态进行更改, 只有一些少量的线性交互, 实际上这条线是非常<strong>容易不发生任何转变</strong>而传递到下一个时刻的. 如下所示:</p><p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lstm%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3.png" alt=""></p><p>下面来看数据在LSTM中从输入到输出的完整过程.</p><p>数据输入后, 数据首先经过LSTM的<strong>遗忘门</strong>. 遗忘门接收了当前时刻$t$的输入$x_t$和上个时刻的隐藏状态 $h_{t-1}$, 经过$\sigma$函数的处理, 能够决定到底<strong>留下</strong>多少在细胞状态中. $f_t=1$表示<strong>完全记住</strong>这个信息, $f_t=0$代表<strong>完全忘记</strong>这个值. 基于上文预测下文词的语言模型中, 可能细胞状态会包含前文的主题, 那么最好记住$h_{t-1}$, 如果得到一个新的语言主题, 则希望遗忘掉过去的信息.</p><p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lstm%E9%81%97%E5%BF%98%E9%97%A8.png" alt=""></p><p>第二步决定在细胞状态中<strong>储存</strong>什么样的信息, 与之对应的结构称为<strong>输入门</strong>. 输入门有两条并行的向量流向. 左侧线路用$\sigma$函数决定有哪些位置上的信息是需要更新的, 右侧线路利用$tanh$为等待细胞状态更新时的使用的候选值创建一个新的向量.</p><p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lstm%E8%BE%93%E5%85%A5%E9%97%A8.png" alt=""></p><p>第三步便是对<strong>细胞状态更新</strong>的过程. 细胞状态先通过遗忘门所给出的遗忘程度对先前状态遗忘, 然后再将输入门的左右两条并行路线的结果计算出来, 与细胞状态相加, 就完成了细胞状态的更新. 从下述式子中可以看到, 对于旧信息的遗忘和新信息的输入, 是让它们自己决定到底留下哪些, 加入哪些. 这点与GRU的风格是不同的, 后面会提到.</p><p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lstm%E7%BB%86%E8%83%9E%E7%8A%B6%E6%80%81.png" alt=""></p><p>最后决定要输出的东西, 所对应的结构就是<strong>输出门</strong>. 输出基于细胞当前的状态, 但会经过一次过滤才输出. 与输入门对应, 先利用$\sigma$函数得到一个输出的系数, 再将细胞状态过一次$tanh$将信息压到$[-1, 1]$之间, 与系数相乘就得到了结果. 也正是因为输出门的设置, 导致LSTM的输出$h_t$ 其实只是经过输出门过滤后的$C_t$ 的一部分信息.</p><p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lstm%E8%BE%93%E5%87%BA%E9%97%A8.png" alt=""></p><p>LSTM的遗忘门和输入门都对细胞状态进行了更改, 先遗忘再存储. 输出门发生作用的时候是不会对细胞状态再次更改的.</p><p>在这三种门控结构中, 不难观察门控作为一种关键的结构, 能起到让<strong>信息选择性通过</strong>的作用, 根据$\sigma$函数的特性, 向量的每个维度都能够得到一个介于$[0, 1]$ 之间的值, 在与其他向量相乘时, 可以作为保留或遗忘程度的依据.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/lstm门控.png" style="zoom:50%;" /><h3 id="门控循环单元-GRU-Gate-Recurrent-Unit"><a href="#门控循环单元-GRU-Gate-Recurrent-Unit" class="headerlink" title="门控循环单元 GRU Gate Recurrent Unit"></a>门控循环单元 GRU Gate Recurrent Unit</h3><p>GRU是一个LSTM的一个变种, 于<a href="https://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="noopener">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</a>中提出. 它比LSTM参数更少, 结构更简单, 具有更高的<strong>效率</strong>, 达到的精度和LSTM相近. GRU和LSTM同样借鉴了<strong>门控</strong>的思想, 在RNN中添加不同的门控, 从而决定是否要更新信息. GRU结构比较简单, 所以用一张图就完全可以说得清.</p><p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/gru.jpg" alt=""></p><p>GRU分别有两个门控结构, 分别是<strong>重置门</strong>$r_t$ (reset gate)和<strong>更新门</strong>$z_t$ (update gate). </p><blockquote><p>重置门: 重置门用于决定丢弃<strong>先前信息</strong>的程度.</p><p>更新门: 更新门能决定在当前时刻要丢弃哪些信息和要添加哪些<strong>新信息</strong>. 作用类似于LSTM中的遗忘门和输入门.</p></blockquote><p>$\tilde h_t$表示的是相较于普通RNN的隐藏状态输出$h_t$的<strong>候选</strong>, 最终不会使用它, 仅作为最终输出$h_t$的<strong>依据</strong>. </p><p>下面描述一下数据在GRU的传播过程:</p><ol><li><p>对于重置门, 上一个时刻$t-1$的隐藏状态$h_{t-1}$和当前时刻的输入$x_t$传入GRU, 通过重置门后将得到一个将$h_{t-1}$和$x_t$ 丢弃的程度系数向量$r_t$. </p></li><li><p>而对于更新门, 同样将是上一个时刻$t-1$的隐藏状态$h_{t-1}$和当前时刻的输入$x_t$传入, 获得表示一个更新程度的系数向量$z_t$. </p></li><li><p>重置门和更新门(或者说重置系数和更新系数)的值已经求出来了, 接着利用重置门的丢弃系数和上一时刻的隐藏状态$h_{t-1}$ 相乘, 并结合当前时刻输入, 经过激活函数为$tanh$ 的神经层, 求出当前时刻的候选隐藏状态$\tilde {h_t}$. </p></li><li><p>根据重置和更新的互斥关系, 求出当前时刻隐藏状态$h_t$ . 当更新门$z_t$的值趋近于1时, 就更倾向于更新记忆细胞的信息, $h_t$的值会和$\tilde{h_{t}}$相仿, 反之不更新, 即仍然沿用上个时刻的信息$h_{t-1}$. </p></li></ol><p>两个门的作用在进行运算中十分明显, <strong>重置门决定了要丢弃多少先前信息</strong>, 直接影响了$\tilde{h_t}$, 间接影响了结果$h_t$, <strong>更新门决定了细胞要替换多少新信息</strong>, 直接影响最终输出的隐藏状态 $h_t$.</p><h3 id="LSTM与GRU对比及个人理解"><a href="#LSTM与GRU对比及个人理解" class="headerlink" title="LSTM与GRU对比及个人理解"></a>LSTM与GRU对比及个人理解</h3><p>首先要明确细胞状态和隐藏状态的区别, 细胞状态$C_t$ 代表的是初始时刻一直到$t$ 时刻的全局信息, 而$h_t$ 代表的是初始时刻到$t-1$ 时刻的全局信息影响下, 当前$t$ 时刻的上下文表示.</p><p>LSTM中, 有细胞状态这个概念, GRU只有隐藏状态. LSTM对信息的保留更加<strong>细腻</strong>, 但对下一个时刻只暴露<strong>部分</strong>信息, 因为在LSTM中$h_t$ 才是真正的输出, $C_t$ 只作为一个信息载体继续传递下去. 相比于LSTM, GRU则更加<strong>简单粗暴</strong>, 对下个时刻暴露<strong>全部</strong>信息.</p><p>LSTM对细胞状态的更新过程中, 经过遗忘门和输入门后求出细胞状态$C_t$ 是<strong>相互独立</strong>的, 即$f_t$ 和 $i_t$ 之间没有关联, 由遗忘门和输入门分别控制遗忘和存储. 而对于GRU来说, 既然$h_t$ 被包含在$C_t$ 中了, 干脆将细胞状态与隐藏状态合并, 在求出最终隐藏状态$h_t$ 时, 而去除了细胞状态后, 当前信息与全局信息是此消彼长的, 即对于写入新信息和保留旧信息是<strong>互相制约</strong>的, 故令$f_t$ 和 $i_t$ 的总和1, 直接用一个更新门$z_t$ 来代替原有的遗忘门和输入门. 同样因为输出的调整, 重置门本质上是输出门的一种变化.</p><h2 id="双向RNN和RNN的堆叠"><a href="#双向RNN和RNN的堆叠" class="headerlink" title="双向RNN和RNN的堆叠"></a>双向RNN和RNN的堆叠</h2><p>双向RNN解决了网络不知道下文信息的问题, 使网络不光会结合前文进行判断, 还会结合后文信息进行预测. 如下图所示, 对于给定的$x^{&lt;1&gt;}, x^{&lt;2&gt;}, x^{&lt;3&gt;}, x^{&lt;4&gt;}$, 每个时刻都增加一个反向链接的神经元. 这样RNN就构成了一个无环图. 当进行前向传播时, 信息会从左到右, 再从右逆着传回来, 最后再做出预测, 即对于$t$时刻的预测值$y_t$, 是由$a^{&lt;t\rightarrow&gt;}$和$a^{&lt;t\leftarrow&gt;}$, $x^{&lt;t&gt;}$共同决定的.</p><p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/%E5%8F%8C%E5%90%91rnn.jpg" alt=""></p><p>和下面这张图是一样的:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/双向rnn2.jpg" style="zoom: 67%;" /><p>RNN的堆叠方式其实也非常简单, 就是按照层数往上传递隐藏状态, 最终得到预测值.</p><p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/rnn%E5%A0%86%E5%8F%A0.png" alt=""></p><p>如果是若干双向RNN, 堆叠起来也和普通RNN大同小异:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/双向堆叠rnn.png" style="zoom: 67%;" />]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RNN </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>卷积神经网络小结</title>
      <link href="/posts/28799.html"/>
      <url>/posts/28799.html</url>
      
        <content type="html"><![CDATA[<h1 id="卷积神经网络-Convolutional-Neural-Network"><a href="#卷积神经网络-Convolutional-Neural-Network" class="headerlink" title="卷积神经网络 Convolutional Neural Network"></a>卷积神经网络 Convolutional Neural Network</h1><p>卷积神经网络是一种含有空间信息的数据表示方法. 它与普通的DNN不同, 它包含了数据的位置信息, 以保证每次看到的是数据矩阵的一个区域, 而不是单纯的矩阵某一维. 卷积神经网络里所说的”卷积”并非真正意义上的卷积运算, 而是互相关运算. 下面这个是VGG16, 算早期CNN的先驱之一了. 用了很多小的卷积核, 网络也比较深(相对之前的来说).</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/vgg16.png" style="zoom: 33%;" /><p>从图中能看到, 卷积神经网络就是将数据从短而粗, 变为长而细. 也就是从原始数据表征, 通过复杂的神经元连接转换为人们难以理解, 高度抽象的特征. 这类特征往往是通过多次学习得来的, 但效果也非常好.</p><h2 id="卷积层-Convolutional-Layer"><a href="#卷积层-Convolutional-Layer" class="headerlink" title="卷积层 Convolutional Layer"></a>卷积层 Convolutional Layer</h2><p>每个卷积层由若干个卷积核和一个偏置$b$组成, 每一个卷积核和输入数据相互作用可以得到一张特征图. 卷积层的反向传播方式与DNN反向传播完全一致.</p><h3 id="卷积核-Filter"><a href="#卷积核-Filter" class="headerlink" title="卷积核 Filter"></a>卷积核 Filter</h3><p>以二维为例子, 卷积核(也叫滤波器)就是一个含有过滤信息的滑动窗口, 在二维平面上不断滑动, 卷积核内的权重是学习得来的, 它与原数据进行”卷积”运算(其实叫<strong>点积求和</strong>运算更合适), 就是对应位置相乘最后加到一起, 形成这个卷积核在这个位置上得到的数据, 将数据仍然以矩阵的形式拼接, 这个矩阵就称为特征图. 在卷积过程之中, <strong>卷积核的参数不会发生改变</strong>, 这叫做<strong>权值共享</strong>. 也就是一个卷积核提取了原图不同位置的相同特征. 所以引入多个卷积核就能提取原图中的不同特征.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/卷积运算.png" style="zoom: 50%;" /><p>在这里左上角的元素是$[10\times 1 + 10\times 0 + 10\times(-1)]\times3=0$, 中间下方的元素是$[10\times 1+0\times0+0\times-1]\times3=10$. 如果写成一个抽象的公式, 在二维情况下:<br>$$<br>C(x, y)=\sum_{t=-\infty}^\infty\sum_{s=-\infty}^\infty F(s, t)\times G(x-s, y-t)\Delta s\Delta t<br>$$<br>如果是三维及以上情况呢? 有一点很重要, 每个卷积核进行运算时, 是<strong>贯穿所有维的</strong>.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/filter贯穿.png" style="zoom:50%;" /><p>比如图中的运算, 上方的输入的RGB图像数据大小为$4\times4\times3$, 卷积核所对应的最后一维也就是深度一定会和输入数据一致, 也就是$2\times2\times3$. 所以但是<strong>深度上卷积核并不共享权重</strong>, 每层深度用的是自己的权重. 对于$R, G, B$三个通道第$i$个位置上的输入数据分别为$x_{ri}, x_{gi}, x_{bi}$, 卷积核中的对应权重有$w_{ri}, w_{gi}, w_{bi}$, 有如下形式:<br>$$<br>\begin{bmatrix}<br>w_{r1} &amp; w_{r2} \\<br>w_{r3} &amp; w_{r4}<br>\end{bmatrix},<br>\begin{bmatrix}<br>w_{g1} &amp; w_{g2} \\<br>w_{g3} &amp; w_{g4}<br>\end{bmatrix},<br>\begin{bmatrix}<br>w_{b1} &amp; w_{b2} \\<br>w_{b3} &amp; w_{b4}<br>\end{bmatrix}<br>$$<br>和深度神经网络一样, 每个卷积核都有一个权重. 那么当计算特征图的时候, 第$k$ 个卷积核对应的特征图每个位置的输出$y_k$ 其实是将深度维上的对应位置的所有结果加起来(别忘了偏置$b_k$, 因为前面卷积运算计算完以后是一个数, 而非一个矩阵, 所以$b_k$ 也是一个数).<br>$$<br>y_k = \begin{bmatrix}w_{r1} &amp; w_{r2} &amp;w_{r3} &amp; w_{r4}\end{bmatrix}\cdot\begin{bmatrix}x_{r1} \\ x_{r2} \\x_{r3} \\ x_{r4}\end{bmatrix}+\begin{bmatrix}w_{g1} &amp; w_{g2} &amp;w_{g3} &amp; g_{r4}\end{bmatrix}\cdot\begin{bmatrix}x_{g1} \\ x_{g2} \\x_{g3} \\ x_{g4}\end{bmatrix}+\begin{bmatrix}w_{b1} &amp; w_{b2} &amp;w_{b3} &amp; w_{b4}\end{bmatrix}\cdot\begin{bmatrix}x_{b1} \\ x_{b2} \\x_{b3} \\ x_{b4}\end{bmatrix}+b_k<br>$$<br>在计算完后, 将filters的所有结果一层层的<strong>叠加</strong>起来, 就使得新的数据又拥有了深度.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/filter深度.jpg" style="zoom: 33%;" /><p>图中经过四个卷积核扫描输入空间后, 得到了四个$3\times3\times1$的输出. 也就是$3\times3\times4$的输出. 由此可见<strong>卷积层有多少卷积核, 就有多深</strong>. 我们当然不能忘记激活函数, 加上激活函数:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/filter个数.png" style="zoom: 33%;" /><p>这里就有个特例, <strong>$1\times1$卷积</strong>. 它能够通过任意的卷积核数量在不改变特征图长和宽的情况下, 改变深度, 在Inception系列模型中, 大量的使用了这个技巧来控制信息.</p><h3 id="步长-Stride"><a href="#步长-Stride" class="headerlink" title="步长 Stride"></a>步长 Stride</h3><p>步长是每滑动一次所前进的距离, 也就是每次向前拖动多少个单位. 即$x’=x+p$.</p><h3 id="填充-Padding"><a href="#填充-Padding" class="headerlink" title="填充 Padding"></a>填充 Padding</h3><p>卷积一步步计算后, 会让图片越来越小. 但是我们可以在图片边缘<strong>补上一圈数据</strong>, 使得卷积能够继续运算. Tensorflow中有两种padding模式, 在<strong>Stride=1</strong>的情况下, SAME指的是在输入图片的边缘补0, 输出图像大小与原来是<strong>相同</strong>的, 并且$p=\frac{f-1}{2}$, 大小为VALID就是不补. VALID可能导致在某些情况下卷积运算的数据丢失. 输出图像为$(n-f+1)\times(n-f+1)$.</p><blockquote><p>关于空间中的位置信息泄露是否来源于Padding, 这个一直都有争议.</p></blockquote><h3 id="参数计算"><a href="#参数计算" class="headerlink" title="参数计算"></a>参数计算</h3><p>对于输入数据的大小$W_{in}\times H_{in}\times D_{in}$, 卷积核个数$K$, 卷积核大小$F$, 步长$S$, 填充$P$, 有输出:<br>$$<br>\displaylines{<br>W_{out}=H_{out}= \lfloor\frac{W_{in}-F+2P}{S}\rfloor+1 \\<br>D_{out}=K}<br>$$<br>该层卷积核的参数个数是(本层的卷积核体积+偏置)*本层卷积核个数:<br>$$<br>N=(F\times F\times D_{in} + 1)\times K<br>$$</p><h2 id="池化层-Pooling-Layer"><a href="#池化层-Pooling-Layer" class="headerlink" title="池化层 Pooling Layer"></a>池化层 Pooling Layer</h2><p>池化层也叫下采样层, 其实也是一种滑动的操作, 池化层里没有需要训练的参数. 池化也分为最大池化和平均池化两种. 池化的<strong>步长和窗口大小相同</strong>, 这保证了每次池化时都能取到<strong>不相交</strong>的区域. 池化增大了每个元素单元对应的感受野, 更利于抽取更抽象而有效的特征, 减少了过拟合. 池化时, 池化操作发生在<strong>每个通道上</strong>, 而不是像CNN一样将各通道输入加在一起. 也就是说, 池化后的输出深度和输入深度相等.</p><h3 id="平均池化层-AveragePooling"><a href="#平均池化层-AveragePooling" class="headerlink" title="平均池化层 AveragePooling"></a>平均池化层 AveragePooling</h3><p>平均池化取的是窗口内所有元素的<strong>平均值</strong>. 平均池化在反向传播时, 将梯度平均分为$n$份, 平均分配到原来对应的位置上, 这样保持池化前后梯度之和不变.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/平均池化.png" style="zoom: 80%;" /><h3 id="最大池化层-MaxPooling"><a href="#最大池化层-MaxPooling" class="headerlink" title="最大池化层 MaxPooling"></a>最大池化层 MaxPooling</h3><p>最大池化层取的是窗口内所有元素的<strong>最大值</strong>. 有一更为极端的例子是全局最大池化层, 它将每一张特征图取最大值, 最终只得到一个值. 最大池化层在前向传播时会记录最大值的位置, 反向传播时只对对应位置的参数进行调整.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/全局池化.png" style="zoom: 50%;" /><h2 id="批量标准化层-BatchNormalization-Layer"><a href="#批量标准化层-BatchNormalization-Layer" class="headerlink" title="批量标准化层 BatchNormalization Layer"></a>批量标准化层 BatchNormalization Layer</h2><p>批量标准化能把输入直接强行拉$(0, 1)$分布上, 使得激活函数输入值落在对输入相对来说敏感的区域, 由小的输入变化导致更大的梯度变化, 从而加快收敛速度. 如果不加BN, 输入分布会经常发生变化, 后续网络在学习时总会因为前面层的分布变化而变化, BN减少了层和层之间的耦合度.</p><h2 id="全连接层-Fully-Connected-Layer"><a href="#全连接层-Fully-Connected-Layer" class="headerlink" title="全连接层 Fully Connected Layer"></a>全连接层 Fully Connected Layer</h2><p>FC层其实就是一层普通的神经网络, 它与上一层是全连接的. 用于分类所以激活函数为Softmax, 一般只用在最后一层或两层, 或预训练网络后添加的几层. 如果是在预训练网络后添加了几层FC进行调参, 那么这个预训练网络在做的事情也是<strong>特征抽取或特征提取</strong>, 它通过预训练的知识抽取了对事物的一般看法, 加上FC层后能够较好地完成我们指定的分类. 这种学习方式也叫作<strong>迁移学习</strong>(Transfer Learning). 但是过多的FC层会导致参数过多, 并且提高过拟合的可能性. 现在一般直接用<strong>全局最大池化层代替全连接层</strong>能够降低模型的参数, 并且表现稳定.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/fc.png" style="zoom:33%;" /><h2 id="残差块-Residual-Block"><a href="#残差块-Residual-Block" class="headerlink" title="残差块 Residual Block"></a>残差块 Residual Block</h2><p>VGG在研究时候发现, 有时深度神经网络的层数越多, 起到的效果却不是很好. 这是因为在网络加深的过程中, 因为特征不断地被高度抽象, 导致最低阶时的原始特征已经被逐渐的消磨掉了. 假设已有一个最优化的18层网络结构, 当我们设计网络结构时, 假设设计了34层, 多出来的16层实际上是完全冗余的, 那么经过冗余层时的输入和输出实际上是和最优层的输入输出完全一致的. 那么实际模型训练的效果不一定能比最优化模型的效果好, 这称为<strong>退化问题</strong>. 理论上来说, 网络越深, 模型的表达能力越强. 如何在维持网络深度的情况下提升性能?</p><p>基于这个思想, 如果能通过某种方式实现低阶信息到高阶信息的直接传递, 应该可以在很深的神经网络的情况下, 达到很好的效果. 这种跳跃式的做法也可以形象的叫Shortcut connection, 一条捷径. 可以看到下述两个图中的信息传递都是跨层的. 我们把下述结构称为残差块:</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/残差块.png" style="zoom:67%;" /><p>残差网络就是由一系列的残差块组成的. 残差块分为<strong>直接映射部分$h(x_l)$和残差部分</strong>$\mathcal{F}(x_l, W_l)$. 一般情况下在卷积神经网络中, 很有可能输入$x_l$和输出$x_{l+1}$的特征图尺寸不同, 这时候就必须要用$1\times 1$卷积核进行升维和降维, $h(x)$描述的就是这个升维和降维的过程. 映射中有多重方式, 但根据数学证明和事实证明直接映射是最好的选择, 效果最好.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/简单残差块.png" style="zoom:50%;" /><p>它的数学表达:<br>$$<br>x_{l+1} = h(x_l) + \mathcal{F}(x_l,W_l)<br>$$<br>在上图中, 两层权重都是冗余的, 那么神经网络最终拟合的结果应该是函数$H(x)=x$这个恒等映射, 因为非线性映射导致<strong>直接学习恒等映射函数十分困难</strong>, 网络学习的肯定是不恒等映射. 如果将网络设计为$H(x)=F(x)+x$, 从而转化为学习一个残差函数$F(x)=H(x)-x$, 当$F(x)=0$时, 就能得到恒等映射函数, 对残差的拟合肯定更容易. 也就是说, <strong>对于冗余层, 模型最差程度也能学习到和原来一样的结果</strong>.</p><p>除了解决了网络的退化问题, 还解决了梯度消失和梯度爆炸. 即使上图中的$\mathcal{F}(x)$的部分为0, 也仍然可以由直接加过来的$\mathcal x$. 进行良性的梯度传播, 而不至于因为连乘而导致梯度消失或爆炸.</p><p>点这里看<a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">ResNet论文</a>和<a href="https://arxiv.org/pdf/1603.05027.pdf" target="_blank" rel="noopener">恒等映射起到作用的分析</a>.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度神经网络小结</title>
      <link href="/posts/50630.html"/>
      <url>/posts/50630.html</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>2020.9.9</strong>: 叙述调整.</p></blockquote><h1 id="深度神经网络-Deep-Neural-Network"><a href="#深度神经网络-Deep-Neural-Network" class="headerlink" title="深度神经网络 Deep Neural Network"></a>深度神经网络 Deep Neural Network</h1><p>在本文中的神经网络是指的<strong>深度神经网络 Deep Neural Network</strong>. 进入到深度学习领域后, 除去DL计算量大, 参数多的特点外, 还会发现深度学习中有一个很大的特点: 不需要特征工程. 特征提取直接由模型自动端到端的完成. DL大多是用仿生学的结构, 来获得信息的新的表示, 所以我们也常说, <strong>深度学习的本质是表示学习</strong>. 其强大的功能我认为是由神经网络本身对事物的表征方式和神经网络的万能逼近定理决定的. </p><p>在错杂的神经网络结构中, 对数据进行非线性拟合. 由于其结构的复杂性, DL也<strong>没有很好的解释性</strong>, 但是研究人员常常将其逐步拆开, 发现其中的规律, 看看神经网络究竟学习到了什么.</p><h2 id="神经元-Neuron"><a href="#神经元-Neuron" class="headerlink" title="神经元 Neuron"></a>神经元 Neuron</h2><p>神经元的概念这个比较简单, 神经网络是由神经元构成的. 一定都看过一张类似的图:</p><p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/DNN.png" alt=""></p><p>这就是神经网络的结构, 两端的分别称为<strong>输入层</strong>和<strong>输出层</strong>, 中间的称为<strong>隐藏层</strong>. 最左侧的输入层大小与输入的特征维数是保持一致的. 最右侧的输出层大小与输出的维数保持一致, 如果任务是分类任务, 输出层的神经元个数就是要分类的类别数, 预测的是归属于每类的类别概率. 如果是回归任务, 最后一层的神经元只有一个, 预测的是回归值. </p><p>在不对神经网络注入灵魂的情况下, 每个神经层之间是做最简单的<strong>线性回归</strong>, 只不过我们形象的将这个过程拆解为神经元的描述(或者说二者互相表示).</p><p>从神经元和神经层的角度来看, 在第$l$ 层每个神经元$j$, 都有一个与上一层神经元$i$ 连接的权重$w_{ij}^{(l)}$, 以及每个神经元的偏置项$b^{(l)}_j$, 把权重矩阵记作$W^{(l)}$, 上一层$l-1$ 的输出$Y^{(l-1)}$ 给进当前层$l$ 记作本层输入$X^{(l-1)}$, 偏置向量记为$b^{(l)}$, 那么该层的输出$Y^{(l)}$ 为:<br>$$<br>Y^{(l)} = W^{(l)}X^{(l-1)} +b^{(l)}<br>$$<br>它的形式上就是单纯的线性回归. 如果只是这样, 神经网络不论有多少层, 输入输出都是线性的, 加不加隐藏层都一样, 是无法拟合非线性关系的. 这时候必须加入一些非线性元素使得神经网络具有非线性拟合的能力, 必须用某种方式给神经网络注入灵魂.</p><h2 id="激活函数-Activation-Function"><a href="#激活函数-Activation-Function" class="headerlink" title="激活函数 Activation Function"></a>激活函数 Activation Function</h2><p>激活函数是作用在神经元输出上的<strong>非线性函数</strong>, 最开始是被单独作为一个神经层而存在的, 后来被集成到神经元身上. 它使得神经网络具有了逼近任意函数的潜力, 只要神经元够多, 数据足够健壮, 那么它可以拟合任意的情况. 激活函数常用的只有几种, 现在最常见的是<code>Relu</code>, 解决了梯度爆炸和梯度消失的难题(后面提到). 在有了激活函数$f(x)$后, 每个神经元的输出变为了:<br>$$<br>Y^{(l)} = f(W^{(l)}X^{(l-1)} + b^{(l)})<br>$$</p><h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><p>它是早期很常见的激活函数, 但是因为存在一些缺陷, 近些年使用的人数越来越少. 公式如下:<br>$$<br>\sigma(x) = \frac{1}{1+e^{-x}}<br>$$<br>这个函数的导数很好求. 并且它能够将任意的输入都放缩到$[0, 1]$的区间内.<br>$$<br>\sigma’(x) = \sigma(x)(1-f(x))<br>$$<br><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/sigmoid.png" style="zoom: 33%;" /></p><p>它的缺点也很明显, 当输入过大或过小时, 所对应的的导数近乎为0, 这种现象称为梯度消失. 这对于之后梯度下降的链式求导是极为不利的. 因为我们对神经网络的权值初始化范围在$[0, 1]$之间, 当发生反向传播时, 如果隐藏层特别多, 就很容易发生梯度消失, 使链式求导趋于0. 还有一个问题就是, <code>Sigmoid</code>每次输出的数据都不是Zero-centered, 其输出值全是正数, 会在收敛的路上越走越远, 导致收敛慢.</p><h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3><p>Tanh不是很了解, 但是现在用在NLP里很多(RNN经常用).<br>$$<br>tanh(x) =\frac{e^x-e^{-x}}{e^x+e^{-x}} \<br>tanh’(x) = 1-tanh^2(x)<br>$$<br><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/tanh.png" style="zoom:33%;" /></p><h3 id="Relu"><a href="#Relu" class="headerlink" title="Relu"></a>Relu</h3><p>为了改善梯度消失和梯度爆炸的问题, 计算也非常简单. 当时的<code>Relu</code>是非常具有统治力的(现在也是).<br>$$<br>ReLu(x) = max(0, x)<br>$$<br>其实就是一个取最大值的函数, 如果输入是负数直接取0. 不是全区间可导, 但是可以取次梯度.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/relu.jpg" style="zoom: 33%;" /><p>它的优点就是收敛快, 在输入大于0的情况下, 导数恒为1, 改善了梯度消失问题. 但是<code>Relu​</code>的输出同样不是Zero-centered. 而且还有可能会造成<code>Dead neuron</code>. 因为有些输入是小于0的, 或者有些节点因为不幸的初始化或者Learning rate设的太大, 导致整个权重矩阵的分布发生变化, 若矩阵分布中心在负区域, 则负输入的梯度为0, 导致神经元可能永远不被更新. </p><p>基于这个问题, 人们提出了许多别的解决方案, 都是在负输入上做一些手脚. 比如说LeakyRelu, 在负输入区域上的值就不全为零, 而是用一个可以调整的参数$\alpha$(通常取0.01)乘上输入.即:<br>$$<br>LeakyReLu(x)=max(\alpha x, x)<br>$$<br>参数$\alpha$可以通过反向传播学习到. 理论上来说<code>LeakyRelu</code>会继承<code>Relu</code>的所有优点, 并改善它的缺点. 但是实际上并没有相关实验证明它会在所有情况下比Relu好.</p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/leakyrelu.png" style="zoom:33%;" /><h3 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h3><p>在处理多分类问题时, 最后一层上会使用<code>Softmax</code>函数作为激活函数. <code>Softmax</code>保证了最后神经元输出是以概率形式输出. 对于分$C$类的问题, 最后一层第$i$个神经元在未激活的情况下的输出$y_i$, 有:<br>$$<br>Softmax(y_i) = \frac{e^i}{\sum\limits_i^Ce^i}<br>$$</p><h2 id="反向传播BP-Back-Propagation"><a href="#反向传播BP-Back-Propagation" class="headerlink" title="反向传播BP Back Propagation"></a>反向传播BP Back Propagation</h2><p>深度神经网络也被称为BP神经网络, 就是指的反向传播. 反向传播可以说是神经网络中最重要的部分之一了. 这种方式告诉我们如何调整神经元的相关权重. 首先, BP是基于梯度下降来调整权重的. 对于第$l$层, 第$i$个神经元对下一层第$j$个神经的权重$w_{ij}$, 损失函数为$E$, 人为设定学习率$\eta$ , 更新$w_{ij}$有:</p><p>$$<br>w^{(l)}_{ij} = w^{(l)}_{ij} - \eta\frac{\partial E(W, b)}{\partial w^{(l)}_{ij}}<br>$$<br>对于每层的偏置$b^{(l)}$ 同理. 这其中涉及到链式求导, 因为在计算时, 是通过输出层的最终复合函数逐渐向输入层求导, 所以就叫反向传播.</p><h2 id="随机失活-Dropout"><a href="#随机失活-Dropout" class="headerlink" title="随机失活 Dropout"></a>随机失活 Dropout</h2><p>当某些神经元过于强势时, 导致其他某些神经元会不被得到训练, 从而增大过拟合的几率, 当强大神经元对应输入的部分数据出现问题时, 就会出现单点故障. 所以需要一个方法使得其他神经元也得到训练, 并避免某些神经元过于强大. 此时, 采用神经元的随机失活策略, 使得每个神经元在训练时都有一定的概率权重不被更新, 能够保证绝大多数的神经元都处于活跃状态.</p><p><img src="https://gitee.com/Daning0/Images/raw/master/MLDL/dropout.png" alt=""></p><h2 id="参数计算"><a href="#参数计算" class="headerlink" title="参数计算"></a>参数计算</h2><p>DNN的参数计算比较简单, 假设第$l$层有$m$个神经元, 第$l+1$层有$n$个神经元, 那么$l$层的每个神经元都对应$l+1$层的$n$个权重, 外加一个偏置$b$, 第$l$层需训练的参数个数是$m\times n + 1$.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>操作系统常见问题整理</title>
      <link href="/posts/57255.html"/>
      <url>/posts/57255.html</url>
      
        <content type="html"><![CDATA[<h1 id="操作系统"><a href="#操作系统" class="headerlink" title="操作系统"></a>操作系统</h1><p>操作系统问的比较多的就是进程线程的区别, 作业调度算法, 分页分段, 死锁很关键, 假脱机这部分也需要重视起来. 文中有部分算法的细节没有提到, 只是列出来名字, 需要自己补充. 当时整理的排版可能比较乱, 以后有空再整理.</p><h2 id="计算机系统概述"><a href="#计算机系统概述" class="headerlink" title="计算机系统概述"></a>计算机系统概述</h2><h3 id="操作系统的基本概念"><a href="#操作系统的基本概念" class="headerlink" title="操作系统的基本概念"></a>操作系统的基本概念</h3><p><strong>操作系统</strong>: 是指控制和管理整个计算机系统的硬件与软件, 合理地组织, 调度计算机的工作与资源的分配, 进而为用户和其他软件提供方便接口与环境的程序集合. 操作系统是计算机系统中最基本的系统软件.</p><p><strong>操作系统的特征</strong>: </p><ul><li>并发</li><li>共享</li><li>虚拟: 物理资源的虚拟化.</li><li>异步</li></ul><p><strong>操作系统的目标和功能</strong>:</p><ol><li>计算机系统资源的管理者</li><li>用户与计算机硬件系统之间的接口</li><li>用作扩充机器</li></ol><h3 id="操作系统的发展与分类"><a href="#操作系统的发展与分类" class="headerlink" title="操作系统的发展与分类"></a>操作系统的发展与分类</h3><ol><li><p>手工操作阶段: 无操作系统</p></li><li><p>批处理阶段: 用户脱机使用计算机. 单道批处理 - 顺序执行, 多道批处理 - 多道程序并行, 共享资源, 不利于人机交互 但效率高</p></li><li><p>分时操作系统: 时间片轮转, 多用户通过终端共享一台主机, 具有可交互性, 响应时间短. 且用户之间彼此独立互不干扰.</p></li><li><p>实时操作系统: 嵌入式和工业界, 紧急状态常用. 注重响应时间.</p></li><li><p>网络操作系统和分布式计算机操作系统</p></li><li><p>个人计算机操作系统: 现在用, Widnows等.</p><p><img src="https://gitee.com/Daning0/Images/raw/master/OS/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B.jpg" alt=""></p></li></ol><h3 id="操作系统运行的环境"><a href="#操作系统运行的环境" class="headerlink" title="操作系统运行的环境"></a>操作系统运行的环境</h3><p>CPU执行两种程序: 操作系统的内核程序,用户APP.</p><p>因此CPU执行时常分为用户态和核心态, 一些与硬件关联比较紧密或者权限比较大的都是处于核心态执行.</p><p><strong>内核主要包括四方面</strong>:</p><ul><li><p>时钟管理</p></li><li><p>中断机制</p></li><li><p>原语: 底层可以被调用的一些小程序, 距离硬件很近, 运行具有原子性, 执行时间短, 调用频繁.</p></li><li><p>系统控制的数据结构及处理: 进程管理, 存储器管理, 设备管理. 分别对应不同的数据结构, 需要进行有效管理和操作.</p></li></ul><p><strong>中断和异常</strong>:</p><ul><li><p><strong>异常</strong>: 内中断(指令中断或强迫中断). 异常通常出现立即处理, 依赖于当前程序的运行现场. 一般是出现意想不到的错误, 如程序非法操作码, 地址越界等.</p></li><li><p><strong>中断</strong>: 外中断(强迫中断). 来自CPU指令以外的事件发生, 主要是设备的I/O处理完成,希望能向下一个设备发出I/O请求.</p></li></ul><p><strong>中断处理流程</strong>:</p><img src="https://gitee.com/Daning0/Images/raw/master/OS/中断.jpg" style="zoom:50%;" /><p><strong>系统调用</strong>: 当用户在程序中调用操作系统的一些特殊子功能(包括异常), 必须切换到内核态, 比如设备管理, 内存管理, 文件管理, 进程控制, 进程通信. 用户执行”陷入指令trap”将CPU使用权交给操作系统内核. 等到执行完成后, 内核会将使用权交还给用户, 即返回.</p><h3 id="操作系统体系结构"><a href="#操作系统体系结构" class="headerlink" title="操作系统体系结构"></a>操作系统体系结构</h3><p><strong>大内核和微内核</strong>: 大内核将操作系统功能作为紧密的整体放倒内核, 性能高, 各种信息共享, 但耦合度高, 管理复杂. 微内核架构下, 操作系统的一部分功能被移出内核降低内核复杂度, 移出去的分层分成若干相互独立的服务. 操作系统被划分为若干个小的定义良好的木块. 只有一个模块在内核态, 其余在用户态. 操作系统因需要频繁在用户态和核心态进行切换有性能损失.</p><p><strong>并行性和并发性</strong>: 并行性是多个事件在同一时刻发生, 并发性指多个事件在同一时间间隔内发生. 多道程序环境下, 宏观上程序同时运行, 微观上是交替进行, 即并发性. 若想满足并行性, 可以分配到多个处理器上并行执行.</p><h2 id="进程管理"><a href="#进程管理" class="headerlink" title="进程管理"></a>进程管理</h2><h3 id="进程与线程"><a href="#进程与线程" class="headerlink" title="进程与线程"></a>进程与线程</h3><p>在多道程序环境下, 允许多个进程并发执行, 此时他们将失去封闭性, 并具有间断性及不可再现 性的特征. 为此引入了进程的概念, 以便更好地描述和控制程序的并发执行, 实现操作系统的并发性和共享性.  进程是程序的运行过程, 是系统进行资源分配和调度的一个独立单位. </p><p>PCB是进程存在的<strong>唯一标志</strong>.</p><p><strong>进程的五个状态</strong>: 运行态, 就绪态, 阻塞态, 创建态, 结束态.</p><p><img src="https://gitee.com/Daning0/Images/raw/master/OS/%E8%BF%9B%E7%A8%8B%E5%88%9B%E5%BB%BA.jpg" alt=""></p><p>阻塞也就是挂起, 即使处理器空闲, 没有触发挂起结束的信号, 就不能运行. 就绪态是缺资源.</p><p><strong>进程的通信</strong>: </p><ol><li><strong>共享存储</strong>: 有一块直接访问的共享空间, 直接读写, 互斥访问.</li><li><strong>消息传递</strong>: 通过消息来通知另一个进程要做什么. 有直接和间接, 间接就是邮箱通信, 消息队列.</li><li><strong>管道通信</strong>: 读写一个共享的pip文件(也可以视为缓冲区), 以字符流形式写入和取出.</li></ol><p><strong>线程和进程的比较</strong>:</p><ol><li>进程是系统进行资源分配和调度的基本单位, 线程是CPU调度和分配资源的基本单位.</li><li>线程依赖于进程存在. 每个进程至少有一个线程.</li><li>进程有自己的独立地址空间, 线程共享进程的地址空间.</li><li>进程是用于系统资源的独立单位, 线程基本自己不拥有系统资源, 只有一些运行必不可少的资源, 其他的资源由线程共享进程的资源.</li><li>进程切换开销大, 涉及CPU环境的保存和设置. 线程保存少量寄存器内容.</li><li>线程通信直接共享数据即可, 进程通信需要通过进程间通信.</li><li>多线程程序若有一个线程崩溃则整个程序崩溃. 多进程程序进程崩溃不影响其他进程.</li></ol><h3 id="处理机调度"><a href="#处理机调度" class="headerlink" title="处理机调度"></a>处理机调度</h3><p>调度往往分为三个层次:</p><ul><li>作业调度(高级调度): 按照某个原则从后备状态的作业中选一个, 分配资源建立进程.</li><li>内存调度(中级调度): 将暂时不运行的进程调到外存等待, 挂起. 提高内存使用率和吞吐量.</li><li>进程调度(低级调度): 操作系统中最基本的调度, 选取进程分配.</li></ul><p>进程调度方式: 抢占和非抢占.</p><p><strong>经典调度算法</strong>:</p><ol><li><p>先来先服务 FCFS: 效率低, 非抢占 长作业有利, CPU密集型有利</p></li><li><p>短作业优先 SJF: 非抢占, 长作业不利, 容易触发死锁. 作业运行时间是估计的, 不一定真正最短. 而且未考虑作业的紧迫程度.</p></li><li><p>优先级调度: 设立一个优先级, 每当当前进程让出处理机时(可以是主动或被动的, 也就是抢占和非抢占), 把处理机分配给更紧迫的进程. 优先级也可以是动态的和静态的, 静态的在创建进程时就已经被确定, 动态的可以根据情况调整.</p></li><li><p>高响应比优先级调度: 计算响应比, 将响应比最高的作业投入运行.</p><p>响应比为(1+等待时间/要求服务时间).</p><ul><li>等待时间相同, 有利于短作业.</li><li>要求服务时间相同, 等待时间越长响应比越高, 是先来先服务.</li><li>长作业可以通过等待时间增加而提高.</li></ul></li><li><p>时间片轮转调度: 将系统所有就绪进程按到达时间分为先后次序排成一个队列, 每次都选队列中第一个进程执行, 仅能运行一个时间片, 即使未运行完成也要强制释放处理机给下一个就绪进程, 自身回到就绪队列尾端. 性能严重依赖于时间片大小的选取.</p></li><li><p>多级队列: 设置多个就绪队列1、2、3…, 优先级递减, 时间片递 增. 只有等到优先级更高的队列为空时才会调度当前队列中的进程. 如果进程用完了当前队列的时间片还未执行, 则会被移到下一队列. 抢占式(时间片用完时), 开销可能较大, 对IO型进程有利, 可能会出现饥饿问题. </p></li></ol><h3 id="进程同步"><a href="#进程同步" class="headerlink" title="进程同步"></a>进程同步</h3><ul><li>同步: 串行执行的先后顺序不能乱.</li><li>互斥: 共享的临界资源只能同时由一个进程访问.</li></ul><p>信号量 P为申请, V为释放, 互斥即信号量为1</p><p><strong>同步问题</strong>:</p><ol><li>生产者-消费者</li><li>读者-写者</li><li>哲学家进餐</li><li>吸烟者</li></ol><h3 id="死锁"><a href="#死锁" class="headerlink" title="死锁"></a>死锁</h3><p><strong>死锁</strong>: 多进程的并发执行, 进行资源竞争导致的僵局, 无外力作用则每个进程都无法向前执行.</p><p><strong>原因</strong>:</p><ol><li>系统资源竞争</li><li>进程推进顺序非法(释放资源顺序不当)</li><li>死锁产生的必要条件: 互斥, 非抢占, 请求并保持(要了还要), 循环等待(资源分配图有环, )</li></ol><p><strong>死锁避免</strong></p><p><strong>安全状态</strong>: 按照某个序列分配资源能够使得当前所有进程全部执行完.</p><p>只要系统处于安全状态, 就不会进入死锁. 处于不安全不一定死锁.</p><p>算法: 银行家算法</p><p><strong>解决死锁三个方法</strong>: 死锁避免, 死锁检测, 死锁解除</p><p><strong>饥饿和死锁的区别</strong>:</p><p>等待时间给进程推进和响应带来明显影响时成为进程饥饿.  饥饿并不代表系统已经死锁, 但至少有一个程序的执行被无限期地推迟.  差别:  ① 进入饥饿的进程可以只有一个, 但是死锁必须大于等于两个;  ② 出于饥饿状态的进程可以是一个就绪进程, 但是死锁状态的进程必定是阻塞进程. </p><h2 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h2><h3 id="内存管理概念"><a href="#内存管理概念" class="headerlink" title="内存管理概念"></a>内存管理概念</h3><p>由源程序变为内存中可以执行的程序通常需要: </p><ol><li>编译: 由编译程序将用户源代码编译成若干目标模块</li><li>链接: 由链接程序将编译后形成的一组目标模块及所需的库函数链接在一起, 形成一个完整的装入模块.</li><li>装入: 由装入程序将装入模块装入内存中运行</li></ol><p><img src="https://gitee.com/Daning0/Images/raw/master/OS/%E7%A8%8B%E5%BA%8F%E6%89%A7%E8%A1%8C%E4%B8%89%E6%AD%A5%E9%AA%A4.jpg" alt=""></p><p>链接可以分为静态链接, 装入时动态链接, 运行时动态链接.</p><ol><li>静态链接: 在程序运行之前, 先把各个目标模块及所需库链接为一个完整的可执行程序, 以后不再拆开.  </li><li>装入时动态链接: 将应用程序编译后所得到的一组目标模块在装入内存时采用边装入边链接的 链接方式.  </li><li>运行时动态链接: 知道程序运行过程中需要一些模块时, 才对这些模块进行链接. </li></ol><p>内存装入分为绝对装入, 可重定位装入,动态运行时装入.</p><ol><li>绝对装入: 在编译时就知道程序将要驻留在内存的物理地址, 编译程序产生含有物理地址的目 标代码,  不适合多道程序设计. </li><li>可重定位装入: 根据内存当前情况, 将装入模块装入到内存的适当位置, 地址变换通常在装入时一次完成, 之后不再改变, 也称静态重定位. 当操作系统为程序分配一个以某地址为起始地址的连续主存区域后, 重定位时将程序中指令或操作数的逻辑地址加上这个起始地址就得到了物理地址.  在作业装入内存时, 必须分配全部的内存空间, 如果没有足够的内存, 则不能装入该作业. 相应的, 因为采用了相对地址, 一旦进入内存, 作业在运行期间也不能移动和申请新的内存空间.</li><li>动态运行装入: 允许程序运行时在内存中移动位置, 把装入模块装入到内存后的所有地址都是 相对地 址, 在程序执行过程中每当访问到相应指令或数据时, 才将要访问的程序或数据的相对地址转换为物理地址. 动态重定位的实现要依靠硬件地址变换机构. </li></ol><p>物理地址转换成逻辑地址的过程称为<strong>地址重定位</strong>.</p><p><strong>内存保护</strong>: 主要目标还是为了保护程序地址不越界, 需要用额外寄存器维护.(不常)</p><p><strong>覆盖与交换</strong></p><ul><li><p>覆盖: 把一个大的程序划分为一系列覆盖, 每个覆盖是一个相对独立的程序单位, 把程序执行时并不要求同时装入内存的覆盖组成一组, 成为覆盖段, 这个覆盖段分配到同一个存储区域, 这个存储区域成为覆盖区, 它与覆盖段一一对应. 覆盖段的大小由覆盖段中最大的覆盖来确定. (为了解决内存容量太小的问题, 打破了必须将一个程序全部信息装入内存后才能运行的限制) </p></li><li><p>交换: 把暂时不用的某个程序及数据部分从内存移到外存中去, 以便腾出必要的内存空间; 或者把指定 的程序或数据从外存读到相应的内存中, 并将控制权交给他, 让其在系统上运行的一种内存扩充技术. 处理器的中级调度就是采用交换技术</p></li></ul><p>区别: </p><ol><li>与覆盖技术相比, 交换技术不要求程序员给出的 程序段之间的覆盖结构;  </li><li>交换技术主要在进程和作业之间进行, 覆盖技术主要在同一个进程或作业中进行; </li><li>覆盖技术只能覆盖于覆盖程序段无关的程序段, 交换进程由换出和换入两个过程组成. </li></ol><p><strong>连续分配管理</strong></p><ol><li><p><strong>单一连续分配</strong>: 内存在此方式下分为系统区和用户区, 系统区仅提供给操作系统使用, 通常在低地址部分; 用户区是为用户提供的、除系统区之外的内存空间. 这种方式无需进行内存保护.  这种方式的优点是简单、无外部碎片, 可以釆用覆盖技术, 不需要额外的技术支持. 缺点是只能用于单用户、单任务的操作系统中, 有内部碎片, 存储器的利用率极低. </p></li><li><p><strong>固定连续分配</strong>: 固定分区分配是最简单的一种多道程序存储管理方式, 它将用户内存空间划分为若干个固定大小 的区域, 每个分区只装入一道作业. 当有空闲分区时, 便可以再从外存的后备作业队列中,选择适 当大小的作业装入该分区, 如此循环.  固定分区分配在划分分区时, 有两种不同的方法.  (1) 分区大小相等: 用于利用一台计算机去控制多个相同对象的场合, 缺乏灵活性.  (2) 分区大小不等: 划分为含有多个较小的分区、适量的中等分区及少量的大分区. </p></li><li><p><strong>动态分区分配</strong>: 动态分区分配又称为可变分区分配, 是一种动态划分内存的分区方法. 这种分区方法不预先将内存划分, 而是在进程装入内存时, 根据进程的大小动态地建立分区, 并使分区的大小正好适合进 程的需要. 因此系统中分区的大小和数目是可变的. 由于大内存的进程被换出, 若小内存进程被换入, 则容易产生更小内存的碎片.</p><p><img src="https://gitee.com/Daning0/Images/raw/master/OS/%E5%8A%A8%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86.jpg" alt=""></p><p>所以必须考虑下面<strong>四种内存管理算法</strong>:</p><ol><li><strong>首次适应算法</strong>(First fit): 空闲分区地址递增查找, 找第一个大小能满足要求的空闲分区. UNIX采用了这种, 不花里胡哨, 性能也不错.</li><li><strong>最佳适应算法</strong>(Best fit): 空闲分区按容量递增排列, 找第一个大小满足要求的. 内存空间不一定每次都完美契合, 每次都产生很小的外部碎片, 根本无法利用.</li><li><strong>最坏适应算法</strong>(Worst fit): 与最佳相反, 按照容量递减排列. 把大的连续内存划分开了, 导致没有大的内存块.</li><li><strong>邻近适应</strong>(Next fit): 是首次适应的升级版, 在上次查找结束的位置继续查找.</li></ol></li></ol><p><strong>非连续分配管理</strong></p><p>将作业要求的内存空间分散地分配开</p><p><strong>基本分页存储管理</strong></p><p>分页思想: 主存空间划分为大小相等且固定的块, 块相对小, 作为主存的基本单位. 进程也按照块为单位进行划分, 进程在执行的时候, 也以块为单位逐个申请主存空间.</p><p>分页不会产生外部碎片, 但容易产生内部碎片(很小, 也称页内碎片).</p><p>进程中的块称为页, 内存中的块称为页框或页帧. 外存也按照块划分, 直接称为块. 为了方便地址转换, 页面大小设置为$2^k$. 页面大小也应该适中, 过大会导致页内碎片过多, 过小会导致交换频繁从而降低页面换入换出的效率.</p><p>地址结构分为两部分: 页号和页内偏移量. 地址长度为32位, 则0<del>11为页内地址, 即每页大小4KB, 12</del>31位页号, 即地址空间最多允许$2^{20}$页.</p><p>地址结构决定了虚拟内存的寻址空间.</p><p>页表是便于在内存中找到指定进程所对应的每个页面的物理块, 系统为每个进程建立一个页表, 记录页面在内存中的物理块号(页帧号).</p><p><img src="https://gitee.com/Daning0/Images/raw/master/OS/%E9%A1%B5%E8%A1%A8.jpg" alt=""></p><p>页表由页表项组成. 页表项和物理地址都由两部分组成, 第一部分都是页号, 页表项第二部分是物理内存的块号, 地址的第二部分是页内偏移量.</p><p>一页对应多个地址单元, 所以地址结构是由页内偏移量和页号组成的. 页表的作用是实现页号到物理块号的地址映射.</p><p><strong>基本地址变换机构</strong></p><p>系统中的页表寄存器PTR, 有页表存放在内存的起始位置F, 页表长度M. 进程执行前F和M放在PCB中, 执行时将其调入页表寄存器. 设页面大小L., 逻辑地址A到物理地址E的变化如下:</p><ol><li>计算页号P, P = A / L, 页内偏移量W, W = A % L.</li><li>检查P和M是否满足P &lt; M, 否则产生越界中断.</li><li>然后找P对应的页表项地址 = F + P * 页地址长度. 取出该地址对应的物理块号, 在内存中找到内容</li><li>E = b * L + W, 就得到了物理地址E. 然后取E中的数据或指令.</li></ol><p>这个过程中只需要给出逻辑地址就能确定物理地址, 地址结构是页号和页内偏移量组成的, 不同的页号会映射到不同的位置, 所以说页面管理的地址是一维的.</p><p><strong>快表</strong>: 上述过程访问了两次内存, 第一次是访问页表, 第二次是根据地址取数据或指令. 加一个高速缓存, 也叫相联存储器TLB, 用来存放当前访问的若干页表项, 只访问一次主存.</p><p><img src="https://gitee.com/Daning0/Images/raw/master/OS/%E5%BF%AB%E8%A1%A8.jpg" alt=""></p><p><strong>两级页表</strong>: 继续延伸页表映射的思想, 页表项过多时, 存在内存中十分浪费空间. 于是引入二级页表的结构来完成空间压缩. 地址结构变为一级页号, 二级页号, 页内偏移量. 每次进程执行时, 只需要调入一级页号其中的一页就能完成对应的地址转换. 大大降低了内存使用量(实际上就是构造页表的页表). 多级页表同理.</p><p><img src="https://gitee.com/Daning0/Images/raw/master/OS/%E4%BA%8C%E7%BA%A7%E9%A1%B5%E8%A1%A8.jpg" alt=""></p><p><strong>基本分段存储管理</strong></p><p>分段是为了满足用户和程序员方便编程, 信息保护等需要. 不是通过硬件实现的.</p><p>分段将进程分为多个段, 段间可不连续, 段内连续. 每段从0开始编址, 分配一段    连续的地址空间. 分段的逻辑地质结构是段号和段内偏移量. 段表的地址结构为段号, 段长, 本段在主存的起始地址. 在访问某地址时, 需要给出段号和段内偏移量, 结合段表的本段起始地址, 就能找到对应的物理地址单元.</p><p><img src="https://gitee.com/Daning0/Images/raw/master/OS/%E6%AE%B5%E8%A1%A8%E6%98%A0%E5%B0%84.jpg" alt=""></p><p>分段管理不能根据给出的一个整数确定物理地址, 因为每段的段长是不固定的, 所以无法求出段内偏移, 因此说分段管理的地址空间是二维的.</p><p><strong>段页式管理</strong></p><p>分页能提高内存利用率, 分段能反映程序的罗结构, 有利于段的共享.</p><p>在分段管理的基础上给每个段添加一个页表.</p><p><img src="https://gitee.com/Daning0/Images/raw/master/OS/%E6%AE%B5%E9%A1%B5%E7%AE%A1%E7%90%86.jpg" alt=""></p><p>段页式的逻辑地质结构为段号, 页号, 页内偏移量. 段表只有一个, 页表可能有多个.</p><p>进行地址转换时, 先从段表查询到页表的起始地址, 再从页表中找到页帧号, 最后结合页内偏移量能找到对应的物理地址. 访问三次主存. 段页式的地址空间是二维的, 主要通过段号和页内偏移量就能访问对应的物理单元.</p><h3 id="虚拟内存"><a href="#虚拟内存" class="headerlink" title="虚拟内存"></a>虚拟内存</h3><p>传统内存有一次性和驻留性, 必须一次装入内存, 且装入后就一直留在内存直到作业运行结束.</p><p><strong>局部性原理</strong></p><ul><li>时间局部性: 某条指令一旦被执行, 不久后可能被再次执行, 数据同理.</li><li>空间局部性: 程序访问某存储单元, 不久后 附近的存储单元也会被访问, 集中在一定范围内.</li></ul><p>虚拟存储器是基于局部性原理设计的, 程序装入时只将一部分装入内存, 就可以执行程序, 访问内容不在内存时将那部分调入内存然后继续执行. 还有一个就是把暂时不用的调出内存.</p><p><strong>请求分页管理方式</strong></p><p><strong>页表机制</strong></p><p>请求分页的页表和普通也表不同, 因为不需要一次性将程序调入内存. 所以就一定会有缺页的情况, 因此必须给页表项加上标志位.</p><p>因此页表项变为: 页号, 物理块号, 状态位P, 访问字段A, 修改位M, 外存地址.</p><p>状态位P: 是否已调入内存.</p><p>访问字段A: 记录本页在一段时间内被访问的次数, 供页面置换算法参考.</p><p>修改位M: 调入内存后是否被修改过.</p><p>外存地址: 用于指出该页在外存上的地址.</p><p>缺页中断: 当要访问的页面不在内存时产生缺页中断, 操作系统将缺的页调入内存, 如果有空闲块则分配块, 如果没有则淘汰某页. 并阻塞进程, 在完成调页后唤醒.</p><p>地址变换: 在分页地址变换基础上加了一些功能.</p><p>地址变换先找快表, 找到后修改访问位, 然后利用物理块号和页内地址形成物理地址. 如果没找到就去内存中找页表, 对比状态位P, 看是否已调入内存, 未调入则调入.</p><p><img src="https://gitee.com/Daning0/Images/raw/master/OS/%E8%AF%B7%E6%B1%82%E5%88%86%E9%A1%B5.jpg" alt=""></p><p><strong>页面置换算法</strong></p><ol><li><p><strong>最佳置换算法</strong>: 从主存中移出永远不再需要的页面; 如无这样的页面存在, 则选择最长时间不需要访问的页面.  于所选择的被淘汰页面将是以后永不使用的, 或者是在最长时间内不再被访问的页面, 这样可以 保证获得最低的缺页率.  即被淘汰页面是以后永不使用或最长时间内不再访问的页面. (往后看) </p></li><li><p><strong>先进先出</strong>(FIFO) 置换算法: 是最简单的页面置换算法. 这种算法的基本思想是: 当需要淘汰一个页面时, 总是选择驻留主存 时间最长的页面进行淘汰, 即先进入主存的页面先淘汰. 其理由是: 最早调入主存的页面不再被 使用的可能性最大.  即优先淘汰最早进入内存的页面. (往前看)  但容易产生Belady异常, 即当物理块数增大时, 故障数不减反增.</p></li><li><p><strong>最近最久未使用</strong>(LRU) 算法: 这种算法的基本思想是: 利用局部性原理, 根据一个作业在执行过程中过去的页面访问历史来推 测未来的行为. 它认为过去一段时间里不曾被访问过的页面, 在最近的将来可能也不会再被访 问. 所以, 这种算法的实质是: 当需要淘汰一个页面时, 总是选择在最近一段时间内最久不用的 页面予以淘汰.  即淘汰最近最长时间未访问过的页面. (往前看) </p></li><li><p><strong>时钟(CLOCK)置换</strong>算法: LRU算法的性能接近于OPT,但是实现起来比较困难, 且开销大; FIFO算法实现简单, 但性能差.  所以操作系统的设计者尝试了很多算法, 试图用比较小的开销接近LRU的性能, 这类算法都是 CLOCK算法的变体.  简单的CLOCK算法是给每一帧关联一个附加位, 称为使用位. 当某一页首次装入主存时, 该帧的 使用位设置为1;当该页随后再被访问到时, 它的使用位也被置为1. 对于页替换算法, 用于替换的 候选帧集合看做一个循环缓冲区, 并且有一个指针与之相关联. 当某一页被替换时, 该指针被设 置成指向缓冲区中的下一帧. 当需要替换一页时, 操作系统扫描缓冲区, 以查找使用位被置为0的 一帧. 每当遇到一个使用位为1的帧时, 操作系统就将该位重新置为0; 如果在这个过程开始时,  缓冲区中所有帧的使用位均为0, 则选择遇到的第一个帧替换; 如果所有帧的使用位均为1,则指针 在缓冲区中完整地循环一周, 把所有使用位都置为0, 并且停留在最初的位置上, 替换该帧中的 页. 由于该算法循环地检查各页面的情况, 故称为CLOCK算法, 又称为最近未用(Not Recently Used, NRU)算法. </p></li></ol><p><strong>地址翻译</strong>: TLB-&gt;页表(TLB不命中) -&gt;Cache-&gt;主存(Cache不命中) -&gt;外存</p><h2 id="文件管理"><a href="#文件管理" class="headerlink" title="文件管理"></a>文件管理</h2><p><strong>文件基本操作</strong></p><p>文件属于抽象数据类型. 为了恰当地定义文件, 就需要考虑有关文件的操作. 操作系统提供系统 调用, 它对文件进行创建、写、读、定位和截断. </p><ol><li>创建文件: 创建文件有两个必要步骤, 一是在文件系统中为文件找到空间; 二是在目录中为新 文件创建条目, 该条目记录文件名称、在文件系统中的位置及其他可能信息. </li><li>写文件: 为了写文件, 执行一个系统调用, 指明文件名称和要写入文件的内容. 对于给定文件 名称, 系统搜索目录以查找文件位置. 系统必须为该文件维护一个写位置的指针. 每当发生写操 作, 便更新写指针.  </li><li>读文件: 为了读文件, 执行一个系统调用, 指明文件名称和要读入文件块的内存位置. 同样,  需要搜索目录以找到相关目录项, 系统维护一个读位置的指针. 每当发生读操作时, 更新读指 针. 一个进程通常只对一个文件读或写, 所以当前操作位置可作为每个进程当前文件位置指针.  由于读和写操作都使用同一指针, 节省了空间也降低了系统复杂度. </li><li>文件重定位(文件寻址) : 按某条件搜索目录, 将当前文件位置设为给定值, 并且不会读、写 文件. </li><li>删除文件: 先从目录中找到要删除文件的目录项, 使之成为空项, 然后回收该文件所占用的存 储空间.  </li><li>截断文件: 允许文件所有属性不变, 并删除文件内容, 即将其长度设为0并释放其空间.  这6个基本操作可以组合执行其他文件操作. 例如, 一个文件的复制, 可以创建新文件、 从旧文 件读出并写入到新文件. </li></ol><p><strong>磁盘调度算法</strong></p><p><strong>寻道时间</strong>: 跨越磁道数为n, 启动磁臂时间s, m为磁盘驱动器相关的常数, 约为0.2ms<br>$$<br>T_s = m \times n + s<br>$$<br><strong>延迟时间</strong>: 磁头定位到某一磁道的扇区所需的时间. 磁盘旋转速度为r.<br>$$<br>T_r = \frac{1}{2r}<br>$$<br><strong>传输时间</strong>: b为每次读写的字节数和磁盘旋转速度, r为磁盘每秒转数, N为一个磁道上的字节数.<br>$$<br>T_t = \frac{b}{rN}<br>$$<br><strong>平均存取时间</strong>:<br>$$<br>T_a = T_s+T_r+T_t<br>$$<br><strong>磁盘调度算法</strong>:</p><ol><li><strong>先来先服务</strong>算法(FCFS) First Come First Service 这是一种比较简单的磁盘调度算法. 它根据进程请求访问磁盘的先后次序进行调度. 此算法的优 点是公平、简单, 且每个进程的请求都能依次得到处理, 不会出现某一进程的请求长期得不到满 足的情况. 此算法由于未对寻道进行优化, 在对磁盘的访问请求比较多的情况下, 此算法将降低 设备服务的吞吐量, 致使平均寻道时间可能较长, 但各进程得到服务的响应时间的变化幅度较 小. </li><li><strong>最短寻道时间优先</strong>算法(SSTF)  Shortest Seek Time First 该算法选择这样的进程, 其要求访问的磁道与当前磁头所在的磁道距离最近, 以使每次的寻道时 间最短, 该算法可以得到比较好的吞吐量, 但却不能保证平均寻道时间最短. 其缺点是对用户的 服务请求的响应机会不是均等的, 因而导致响应时间的变化幅度很大. 在服务请求很多的情况 下, 对内外边缘磁道的请求将会无限期的被延迟, 有些请求的响应时间将不可预期. </li><li><strong>扫描算法(SCAN) 电梯调度扫描</strong>算法不仅考虑到欲访问的磁道与当前磁道的距离, 更优先考虑的是磁头的当前移动方向. 例 如, 当磁头正在自里向外移动时, 扫描算法所选择的下一个访问对象应是其欲访问的磁道既在当 前磁道之外, 又是距离最近的. 这样自里向外地访问, 直到再无更外的磁道需要访问才将磁臂换 向, 自外向里移动. 这时, 同样也是每次选择这样的进程来调度, 即其要访问的磁道, 在当前磁 道之内, 从而避免了饥饿现象的出现. 由于这种算法中磁头移动的规律颇似电梯的运行, 故又称 为电梯调度算法. 此算法基本上克服了最短寻道时间优先算法的服务集中于中间磁道和响应时间 变化比较大的缺点, 而具有最短寻道时间优先算法的优点即吞吐量较大, 平均响应时间较小, 但 由于是摆动式的扫描方法, 两侧磁道被访问的频率仍低于中间磁道. </li><li><strong>循环扫描算法(CSCAN)  循环扫描</strong>算法是对扫描算法的改进. 如果对磁道的访问请求是均匀分布的, 当磁头到达磁盘的一 端, 并反向运动时落在磁头之后的访问请求相对较少. 这是由于这些磁道刚被处理, 而磁盘另一 端的请求密度相当高, 且这些访问请求等待的时间较长, 为了解决这种情况, 循环扫描算法规定 磁头单向移动. 例如, 只自里向外移动, 当磁头移到最外的被访问磁道时, 磁头立即返回到最里 的欲访磁道, 即将最小磁道号紧接着最大磁道号构成循环, 进行扫描. </li></ol><h2 id="I-O管理"><a href="#I-O管理" class="headerlink" title="I/O管理"></a>I/O管理</h2><p>IO的控制方式:</p><ol><li><p>程序直接控制方式</p><p>计算机从外设读取数据到存储器, 每次读一个字, 每读入一个字都对外设进行循环检查, 直到确定该字已在IO控制器的数据寄存器中. CPU利用率很低, 大多数时间都在检查.</p><img src="https://gitee.com/Daning0/Images/raw/master/OS/程序式IO.jpg" style="zoom: 50%;" /></li><li><p>中断驱动方式</p><p>当某进程要启动某个 I/O 设备工作时, 便由 CPU 向相应的设备控制器发出一条 I/O 命令, 然后立 即返回继续执行原来的任务. 仅当输完一个数据时, 才需 CPU 花费极短的时间去做些中断处理. </p><img src="https://gitee.com/Daning0/Images/raw/master/OS/中断驱动式io.jpg" style="zoom:50%;" /></li><li><p>DMA方式</p><p>通过在I/O设备和内存之间开启一个可以直接传输数据的通路, 采用DMA控制器来控制一个数据块 的传输, CPU只需在一个数据块传输开始阶段设置好传输所需的控制信息, 并在传输结束阶段做 进一步处理. </p><img src="https://gitee.com/Daning0/Images/raw/master/OS/DMAIO.jpg" style="zoom:50%;" /></li></ol><p><strong>IO子系统层次结构</strong></p><ol><li>用户层IO软件</li><li>设备独立性软件</li><li>设备驱动程序</li><li>中断处理程序</li><li>硬件</li></ol><p><strong>假脱机技术</strong></p><p>虚拟性是OS的四大特性之一. 如果说可以通过多道程序技术将一台物理CPU虚拟为多台逻辑 CPU, 从而允许多个用户共享一台主机, 那么, 通过SPOOling技术便可将一台物理I/O设备虚拟 为多台逻辑I/O设备, 同样允许多个用户共享一台物理I/O设备.  SPOOLing技术是对脱机输入、输出系统的模拟. 相应地, SPOOLing系统必须建立在具有多道程 序功能的操作系统上, 而且还应有高速随机外存的支持, 这通常是采用磁盘存储技术.  SPOOLing系统主要有以下三部分:  </p><ol><li>输入井和输出井. 这是在磁盘上开辟的两个大存储空间. 输入井是模拟脱机输入时的磁盘设 备, 用于暂存I/Q设备输入的数据; 输出井是模拟脱机输出时的磁盘, 用于暂存用户程序的输出数据.  </li><li>输入缓冲区和输出缓冲区. 为了缓和和CPU和磁盘之间速度不匹配的矛盾, 在内存中要开辟 两个缓冲区; 输入缓冲区和输出缓冲区. 输入缓冲区用于暂存由输入设备送来的数据, 以后再传送到输入井. 输出缓冲区用与暂存从输出井送来的数据, 以后在传送给输出设备.  </li><li>输入进程SPi 和输入进程SP0. 这里利用两个进程来模拟脱机I/O时的外围控制机. 其中, 进 程SPi模拟脱机输入时的外围控制机, 将用户要求的数据从输入机通过输入缓冲区再送到输入井,  当CPU需要输入数据时, 直接从输入井读入内存; 进程SP0模拟脱机输出时的外围控制机, 把用户 要求输出的数据从先内存送到输出井, 待输出设备空闲时, 在将输出井中的数据经过输出缓冲区 送到输出设备上.  </li></ol><p>SPOOLing技术的特点:  </p><ol><li>提高了I/O速度. 从对低速I/O设备进行的I/O操作变为对输入井或输出井的操作, 如同脱机操作 一样, 提高了I/O速度, 缓和了CPU与低速I/O设备速度不匹配的矛盾. </li><li>将独占设备改造为共享设备. 因为在SPOOLing系统的系统中, 实际上并没为任何进程分配设 备, 而知识在输入井或输出井中为进程分配一个存储区和建立一张I/O请求表. 这样, 便把独占设 备改造为共享设备. </li><li>实现了虚拟设备功能. 多个进程同时使用一独享设备, 而对每一进程而言, 都认为自己独占这 一设备, 从而实现了设备的虚拟分配. 不过, 该设备是逻辑上的设备. </li></ol>]]></content>
      
      
      <categories>
          
          <category> 计算机基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 操作系统 </tag>
            
            <tag> 面试 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo博客优化</title>
      <link href="/posts/42790.html"/>
      <url>/posts/42790.html</url>
      
        <content type="html"><![CDATA[<h1 id="博客优化"><a href="#博客优化" class="headerlink" title="博客优化"></a>博客优化</h1><p>当东西多了以后, 博客的运行速度就会被拖慢. 所以优化访问速度是非常有必要的. 本文参考了<strong>Yafine</strong>的<a href="https://yafine-blog.cn/posts/4ab2.html" target="_blank" rel="noopener">这篇博客</a>和<strong>Sky03</strong>的<a href="https://blog.sky03.cn/posts/42790.html#toc-heading-5" target="_blank" rel="noopener">博客</a>对访问进行了优化.</p><h2 id="图片懒加载"><a href="#图片懒加载" class="headerlink" title="图片懒加载"></a>图片懒加载</h2><blockquote><p>经过多次尝试, 懒加载虽然带来了访问速度的提升, 但即使加大了懒加载的范围, 还是能够看到Loading图, 总是会给人产生一种网速慢的感觉. 对于现在大家的网速来说, <strong>似乎体验的重要性要大于性能的重要性</strong>, 建议不开启懒加载.</p></blockquote><h3 id="预加载和懒加载"><a href="#预加载和懒加载" class="headerlink" title="预加载和懒加载"></a>预加载和懒加载</h3><blockquote><p>图片预加载：顾名思义，图片预加载就是在网页全部加载之前，提前加载图片。当用户需要查看时可直接从本地缓存中渲染，以提供给用户更好的体验，减少等待的时间。否则，如果一个页面的内容过于庞大，没有使用预加载技术的页面就会长时间的展现为一片空白，这样浏览者可能以为图片预览慢而没兴趣浏览，把网页关掉，这时，就需要图片预加载。当然这种做法实际上牺牲了服务器的性能换取了更好的用户体验。<br>图片懒加载（缓载）：延迟加载图片或符合某些条件时才加载某些图片。这样做的好处是减少不必要的访问数据库或延迟访问数据库的次数，因为每次访问数据库都是比较耗时的即只有真正使用该对象的数据时才会创建。懒加载的主要目的是作为服务器前端的优化，减少请求数或延迟请求数。</p></blockquote><p>预加载会在用户没看到图片之前, 就将图片显示加载好. 而懒加载恰恰相反, 当用户快要看到图片之前(或满足某个触发条件时), 才会进行加载. 图片是一种非常吃流量的内容, 所以当点入一篇新文章时, 预加载会导致瞬间流量过大, 用户等待的时间会增加. 其实完全没必要让图片在用户看不见的时候就去加载, 最好是用户即将看到图片之前, 再对图片进行加载. 这样可以减缓访问的压力, 提高用户体验.</p><h3 id="加入懒加载"><a href="#加入懒加载" class="headerlink" title="加入懒加载"></a>加入懒加载</h3><p>懒加载需要安装<a href="https://github.com/Troy-Yang/hexo-lazyload-image" target="_blank" rel="noopener">hexo-lazyload-image</a>插件. 在blog根目录命令行输入:</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> hexo-lazyload-image --save<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>然后在Hexo配置文件末尾加入:</p><pre><code>lazyload:  enable: true  # 是否开启图片懒加载  onlypost: false  # 是否只对文章的图片做懒加载  loadingImg: # eg ./images/loading.gif</code></pre><p>这里的<code>loadingImg</code>路径起始就从主题的<code>source</code>下开始算的.</p><p>最后执行<code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo s</code>就能看到懒加载效果了.</p><h3 id="Matery的bug"><a href="#Matery的bug" class="headerlink" title="Matery的bug"></a>Matery的bug</h3><p>针对Matery主题这个插件有两个小bug, 感谢原博主的解决.</p><ol><li><p>查看大图, 发现全是loading加载图:</p><p>原因是因为懒加载插件与 lightgallery 插件冲突. 在 <code>blog\themes\hexo-theme-matery\source\jsmatery.js</code>中，在 108 行左右添加以下代码</p><pre class="line-numbers language-js"><code class="language-js"><span class="token function">$</span><span class="token punctuation">(</span>document<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">find</span><span class="token punctuation">(</span><span class="token string">'img[data-original]'</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">each</span><span class="token punctuation">(</span><span class="token keyword">function</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token function">$</span><span class="token punctuation">(</span><span class="token keyword">this</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">parent</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">attr</span><span class="token punctuation">(</span><span class="token string">"href"</span><span class="token punctuation">,</span> <span class="token function">$</span><span class="token punctuation">(</span><span class="token keyword">this</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">attr</span><span class="token punctuation">(</span><span class="token string">"data-original"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p>点击首页 logo 不是跳转到首页, 而是查看 logo 图片:</p><p>修改<code>blog\themes\hexo-theme-matery\layout\ header.ejs</code>. </p><pre class="line-numbers language-js"><code class="language-js"><span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"brand-logo"</span><span class="token operator">></span>    <span class="token operator">&lt;</span>a href<span class="token operator">=</span><span class="token string">"&lt;%- url_for() %>"</span> <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"waves-effect waves-light"</span><span class="token operator">></span>         <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>theme<span class="token punctuation">.</span>logo <span class="token operator">!==</span> undefined <span class="token operator">&amp;&amp;</span> theme<span class="token punctuation">.</span>logo<span class="token punctuation">.</span>length <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span> <span class="token operator">%</span><span class="token operator">></span>         <span class="token operator">&lt;</span>img src<span class="token operator">=</span><span class="token string">"&lt;%= theme.logo %>"</span> <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"logo-img"</span> alt<span class="token operator">=</span><span class="token string">"LOGO"</span><span class="token operator">></span>         <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token punctuation">}</span> <span class="token operator">%</span><span class="token operator">></span>         <span class="token operator">&lt;</span>span <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"logo-span"</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> config<span class="token punctuation">.</span>title <span class="token operator">%</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>span<span class="token operator">></span>    <span class="token operator">&lt;</span><span class="token operator">/</span>a<span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>修改为:</p><pre class="line-numbers language-js"><code class="language-js"><span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"brand-logo"</span><span class="token operator">></span>    <span class="token operator">&lt;</span>a href<span class="token operator">=</span><span class="token string">"&lt;%- url_for() %>"</span> <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"waves-effect waves-light"</span><span class="token operator">></span>        <span class="token operator">&lt;</span>div<span class="token operator">></span>            <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>theme<span class="token punctuation">.</span>logo <span class="token operator">!==</span> undefined <span class="token operator">&amp;&amp;</span> theme<span class="token punctuation">.</span>logo<span class="token punctuation">.</span>length <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span> <span class="token operator">%</span><span class="token operator">></span>            <span class="token operator">&lt;</span>img src<span class="token operator">=</span><span class="token string">"&lt;%= theme.logo %>"</span> <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"logo-img"</span> alt<span class="token operator">=</span><span class="token string">"LOGO"</span><span class="token operator">></span>            <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token punctuation">}</span> <span class="token operator">%</span><span class="token operator">></span>            <span class="token operator">&lt;</span>span <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"logo-span"</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> config<span class="token punctuation">.</span>title <span class="token operator">%</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>span<span class="token operator">></span>        <span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span>    <span class="token operator">&lt;</span><span class="token operator">/</span>a<span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><h3 id="懒加载优化"><a href="#懒加载优化" class="headerlink" title="懒加载优化"></a>懒加载优化</h3><p>每次加载完后本地应该都会有图片缓存, 但是还是会显示loading的logo. 所以需要对插件进行修改, 提前显示出图片. 打开<code>blogtest\node_modules\hexo-lazyload-image\lib\simple-lazyload.js</code>, 第九行修改为:</p><pre class="line-numbers language-js"><code class="language-js"><span class="token operator">&amp;&amp;</span> rect<span class="token punctuation">.</span>top <span class="token operator">&lt;=</span> <span class="token punctuation">(</span>window<span class="token punctuation">.</span>innerHeight <span class="token operator">+</span> <span class="token number">360</span> <span class="token operator">||</span> document<span class="token punctuation">.</span>documentElement<span class="token punctuation">.</span>clientHeight <span class="token operator">+</span> <span class="token number">360</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>能够在图片前360个pix加载图片, 我这里试了试360效果比较好, 可以自行修改.</p><h2 id="代码压缩"><a href="#代码压缩" class="headerlink" title="代码压缩"></a>代码压缩</h2><p>其实Hexo生成的所有html, css, js全是有空行和空格的, 但是空格和空行是需要占用空间的. 这就是为什么js总是发布一个完整版和一个min版, <strong>min版是压缩后的</strong>, 不利于代码的阅读, 但是使用起来会比原版快. 我这里网络问题很大, cnpm安装这个库<strong>有各种权限问题</strong>, 所以就没有采用<code>gulp</code>的方式. 如果没有条件建议尝试<code>hexo-neat</code>, 比较简单快捷. 我没有对比过二者的压缩效率, 但后者对某些比较大的文件能够压缩<strong>38.13%</strong>左右的空间, 效果已经很不错了.</p><h3 id="gulp"><a href="#gulp" class="headerlink" title="gulp"></a>gulp</h3><p>利用gulp进行代码的压缩. 先在博客根目录下安装好<code>gulp</code>:</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># 全局安装gulp模块</span><span class="token function">npm</span> <span class="token function">install</span> gulp -g<span class="token comment" spellcheck="true"># 安装各种小功能模块  执行这步的时候，可能会提示权限的问题，最好以管理员模式执行</span><span class="token function">npm</span> <span class="token function">install</span> gulp gulp-htmlclean gulp-htmlmin gulp-minify-css gulp-uglify gulp-imagemin --save<span class="token comment" spellcheck="true"># 额外的功能模块</span><span class="token function">npm</span> <span class="token function">install</span> gulp-debug gulp-clean-css gulp-changed gulp-if gulp-plumber gulp-babel babel-preset-es2015 del @babel/core --save<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在博客根目录下创建<code>gulpfile.js</code>(必须是这个文件名), 并复制以下内容:</p><pre class="line-numbers language-js"><code class="language-js"><span class="token keyword">var</span> gulp <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"gulp"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">var</span> debug <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"gulp-debug"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">var</span> cleancss <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"gulp-clean-css"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//css压缩组件</span><span class="token keyword">var</span> uglify <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"gulp-uglify"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//js压缩组件</span><span class="token keyword">var</span> htmlmin <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"gulp-htmlmin"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//html压缩组件</span><span class="token keyword">var</span> htmlclean <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"gulp-htmlclean"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//html清理组件</span><span class="token keyword">var</span> imagemin <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"gulp-imagemin"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//图片压缩组件</span><span class="token keyword">var</span> changed <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"gulp-changed"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//文件更改校验组件</span><span class="token keyword">var</span> gulpif <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"gulp-if"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//任务 帮助调用组件</span><span class="token keyword">var</span> plumber <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"gulp-plumber"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//容错组件（发生错误不跳出任务，并报出错误内容）</span><span class="token keyword">var</span> isScriptAll <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//是否处理所有文件，(true|处理所有文件)(false|只处理有更改的文件)</span><span class="token keyword">var</span> isDebug <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//是否调试显示 编译通过的文件</span><span class="token keyword">var</span> gulpBabel <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"gulp-babel"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">var</span> es2015Preset <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"babel-preset-es2015"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">var</span> del <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"del"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">var</span> Hexo <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"hexo"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">var</span> hexo <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Hexo</span><span class="token punctuation">(</span>process<span class="token punctuation">.</span><span class="token function">cwd</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 初始化一个hexo对象</span><span class="token comment" spellcheck="true">// 清除public文件夹</span>gulp<span class="token punctuation">.</span><span class="token function">task</span><span class="token punctuation">(</span><span class="token string">"clean"</span><span class="token punctuation">,</span> <span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">return</span> <span class="token function">del</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"public/**/*"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 下面几个跟hexo有关的操作，主要通过hexo.call()去执行，注意return</span><span class="token comment" spellcheck="true">// 创建静态页面 （等同 hexo generate）</span>gulp<span class="token punctuation">.</span><span class="token function">task</span><span class="token punctuation">(</span><span class="token string">"generate"</span><span class="token punctuation">,</span> <span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">return</span> hexo<span class="token punctuation">.</span><span class="token function">init</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">then</span><span class="token punctuation">(</span><span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">return</span> hexo            <span class="token punctuation">.</span><span class="token function">call</span><span class="token punctuation">(</span><span class="token string">"generate"</span><span class="token punctuation">,</span> <span class="token punctuation">{</span>                watch<span class="token punctuation">:</span> <span class="token boolean">false</span>            <span class="token punctuation">}</span><span class="token punctuation">)</span>            <span class="token punctuation">.</span><span class="token function">then</span><span class="token punctuation">(</span><span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">return</span> hexo<span class="token punctuation">.</span><span class="token function">exit</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span><span class="token punctuation">)</span>            <span class="token punctuation">.</span><span class="token keyword">catch</span><span class="token punctuation">(</span><span class="token keyword">function</span> <span class="token punctuation">(</span>err<span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">return</span> hexo<span class="token punctuation">.</span><span class="token function">exit</span><span class="token punctuation">(</span>err<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 启动Hexo服务器</span>gulp<span class="token punctuation">.</span><span class="token function">task</span><span class="token punctuation">(</span><span class="token string">"server"</span><span class="token punctuation">,</span> <span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">return</span> hexo        <span class="token punctuation">.</span><span class="token function">init</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">then</span><span class="token punctuation">(</span><span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">return</span> hexo<span class="token punctuation">.</span><span class="token function">call</span><span class="token punctuation">(</span><span class="token string">"server"</span><span class="token punctuation">,</span> <span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token keyword">catch</span><span class="token punctuation">(</span><span class="token keyword">function</span> <span class="token punctuation">(</span>err<span class="token punctuation">)</span> <span class="token punctuation">{</span>            console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span>err<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 部署到服务器</span>gulp<span class="token punctuation">.</span><span class="token function">task</span><span class="token punctuation">(</span><span class="token string">"deploy"</span><span class="token punctuation">,</span> <span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">return</span> hexo<span class="token punctuation">.</span><span class="token function">init</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">then</span><span class="token punctuation">(</span><span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">return</span> hexo            <span class="token punctuation">.</span><span class="token function">call</span><span class="token punctuation">(</span><span class="token string">"deploy"</span><span class="token punctuation">,</span> <span class="token punctuation">{</span>                watch<span class="token punctuation">:</span> <span class="token boolean">false</span>            <span class="token punctuation">}</span><span class="token punctuation">)</span>            <span class="token punctuation">.</span><span class="token function">then</span><span class="token punctuation">(</span><span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">return</span> hexo<span class="token punctuation">.</span><span class="token function">exit</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span><span class="token punctuation">)</span>            <span class="token punctuation">.</span><span class="token keyword">catch</span><span class="token punctuation">(</span><span class="token keyword">function</span> <span class="token punctuation">(</span>err<span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">return</span> hexo<span class="token punctuation">.</span><span class="token function">exit</span><span class="token punctuation">(</span>err<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 压缩public目录下的js文件</span>gulp<span class="token punctuation">.</span><span class="token function">task</span><span class="token punctuation">(</span><span class="token string">"compressJs"</span><span class="token punctuation">,</span> <span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">return</span> gulp        <span class="token punctuation">.</span><span class="token function">src</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"./public/**/*.js"</span><span class="token punctuation">,</span> <span class="token string">"!./public/libs/**"</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span><span class="token operator">/</span>排除的js        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">gulpif</span><span class="token punctuation">(</span><span class="token operator">!</span>isScriptAll<span class="token punctuation">,</span> <span class="token function">changed</span><span class="token punctuation">(</span><span class="token string">"./public"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">gulpif</span><span class="token punctuation">(</span>isDebug<span class="token punctuation">,</span> <span class="token function">debug</span><span class="token punctuation">(</span><span class="token punctuation">{</span> title<span class="token punctuation">:</span> <span class="token string">"Compress JS:"</span> <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">plumber</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span>            <span class="token function">gulpBabel</span><span class="token punctuation">(</span><span class="token punctuation">{</span>                presets<span class="token punctuation">:</span> <span class="token punctuation">[</span>es2015Preset<span class="token punctuation">]</span> <span class="token operator">/</span><span class="token operator">/</span> es5检查机制            <span class="token punctuation">}</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">uglify</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span><span class="token operator">/</span><span class="token function">调用压缩组件方法uglify</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>对合并的文件进行压缩        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span>gulp<span class="token punctuation">.</span><span class="token function">dest</span><span class="token punctuation">(</span><span class="token string">"./public"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token operator">/</span><span class="token operator">/</span>输出到目标目录<span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token operator">/</span><span class="token operator">/</span> 压缩<span class="token keyword">public</span>目录下的css文件gulp<span class="token punctuation">.</span><span class="token function">task</span><span class="token punctuation">(</span><span class="token string">"compressCss"</span><span class="token punctuation">,</span> <span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">var</span> option <span class="token operator">=</span> <span class="token punctuation">{</span>        rebase<span class="token punctuation">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span>        <span class="token operator">/</span><span class="token operator">/</span>advanced<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token operator">/</span><span class="token operator">/</span>类型：Boolean 默认：<span class="token boolean">true</span> <span class="token punctuation">[</span>是否开启高级优化（合并选择器等）<span class="token punctuation">]</span>        compatibility<span class="token punctuation">:</span> <span class="token string">"ie7"</span> <span class="token operator">/</span><span class="token operator">/</span>保留ie7及以下兼容写法 类型：String 默认：<span class="token string">''</span>or<span class="token string">'*'</span> <span class="token punctuation">[</span>启用兼容模式； <span class="token string">'ie7'</span>：IE7兼容模式，<span class="token string">'ie8'</span>：IE8兼容模式，<span class="token string">'*'</span>：IE9<span class="token operator">+</span>兼容模式<span class="token punctuation">]</span>        <span class="token operator">/</span><span class="token operator">/</span>keepBreaks<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token operator">/</span><span class="token operator">/</span>类型：Boolean 默认：<span class="token boolean">false</span> <span class="token punctuation">[</span>是否保留换行<span class="token punctuation">]</span>        <span class="token operator">/</span><span class="token operator">/</span>keepSpecialComments<span class="token punctuation">:</span> <span class="token string">'*'</span> <span class="token operator">/</span><span class="token operator">/</span>保留所有特殊前缀 当你用autoprefixer生成的浏览器前缀，如果不加这个参数，有可能将会删除你的部分前缀    <span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> gulp        <span class="token punctuation">.</span><span class="token function">src</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"./public/**/*.css"</span><span class="token punctuation">,</span> <span class="token string">"!./public/**/*.min.css"</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//排除的css</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">gulpif</span><span class="token punctuation">(</span><span class="token operator">!</span>isScriptAll<span class="token punctuation">,</span> <span class="token function">changed</span><span class="token punctuation">(</span><span class="token string">"./public"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">gulpif</span><span class="token punctuation">(</span>isDebug<span class="token punctuation">,</span> <span class="token function">debug</span><span class="token punctuation">(</span><span class="token punctuation">{</span> title<span class="token punctuation">:</span> <span class="token string">"Compress CSS:"</span> <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">plumber</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">cleancss</span><span class="token punctuation">(</span>option<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span>gulp<span class="token punctuation">.</span><span class="token function">dest</span><span class="token punctuation">(</span><span class="token string">"./public"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 压缩public目录下的html文件</span>gulp<span class="token punctuation">.</span><span class="token function">task</span><span class="token punctuation">(</span><span class="token string">"compressHtml"</span><span class="token punctuation">,</span> <span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">var</span> cleanOptions <span class="token operator">=</span> <span class="token punctuation">{</span>        protect<span class="token punctuation">:</span> <span class="token regex">/&lt;\!--%fooTemplate\b.*?%-->/g</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">//忽略处理</span>        unprotect<span class="token punctuation">:</span> <span class="token operator">/</span><span class="token operator">&lt;</span>script <span class="token punctuation">[</span><span class="token operator">^</span><span class="token operator">></span><span class="token punctuation">]</span><span class="token operator">*</span>\btype<span class="token operator">=</span><span class="token string">"text\/x-handlebars-template"</span><span class="token punctuation">[</span>\s\S<span class="token punctuation">]</span><span class="token operator">+</span><span class="token operator">?</span><span class="token operator">&lt;</span>\<span class="token operator">/</span>script<span class="token operator">></span><span class="token operator">/</span>gi <span class="token comment" spellcheck="true">//特殊处理</span>    <span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token keyword">var</span> minOption <span class="token operator">=</span> <span class="token punctuation">{</span>        collapseWhitespace<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">//压缩HTML</span>        collapseBooleanAttributes<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">//省略布尔属性的值 &lt;input checked="true"/> ==> &lt;input /></span>        removeEmptyAttributes<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">//删除所有空格作属性值 &lt;input id="" /> ==> &lt;input /></span>        removeScriptTypeAttributes<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">//删除&lt;script>的type="text/javascript"</span>        removeStyleLinkTypeAttributes<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">//删除&lt;style>和&lt;link>的type="text/css"</span>        removeComments<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">//清除HTML注释</span>        minifyJS<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">//压缩页面JS</span>        minifyCSS<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">//压缩页面CSS</span>        minifyURLs<span class="token punctuation">:</span> <span class="token boolean">true</span> <span class="token comment" spellcheck="true">//替换页面URL</span>    <span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> gulp        <span class="token punctuation">.</span><span class="token function">src</span><span class="token punctuation">(</span><span class="token string">"./public/**/*.html"</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">gulpif</span><span class="token punctuation">(</span>isDebug<span class="token punctuation">,</span> <span class="token function">debug</span><span class="token punctuation">(</span><span class="token punctuation">{</span> title<span class="token punctuation">:</span> <span class="token string">"Compress HTML:"</span> <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">plumber</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">htmlclean</span><span class="token punctuation">(</span>cleanOptions<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">htmlmin</span><span class="token punctuation">(</span>minOption<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span>gulp<span class="token punctuation">.</span><span class="token function">dest</span><span class="token punctuation">(</span><span class="token string">"./public"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 压缩 public/medias 目录内图片</span>gulp<span class="token punctuation">.</span><span class="token function">task</span><span class="token punctuation">(</span><span class="token string">"compressImage"</span><span class="token punctuation">,</span> <span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">var</span> option <span class="token operator">=</span> <span class="token punctuation">{</span>        optimizationLevel<span class="token punctuation">:</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">//类型：Number 默认：3 取值范围：0-7（优化等级）</span>        progressive<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">//类型：Boolean 默认：false 无损压缩jpg图片</span>        interlaced<span class="token punctuation">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">//类型：Boolean 默认：false 隔行扫描gif进行渲染</span>        multipass<span class="token punctuation">:</span> <span class="token boolean">false</span> <span class="token comment" spellcheck="true">//类型：Boolean 默认：false 多次优化svg直到完全优化</span>    <span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> gulp        <span class="token punctuation">.</span><span class="token function">src</span><span class="token punctuation">(</span><span class="token string">"./public/medias/**/*.*"</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">gulpif</span><span class="token punctuation">(</span><span class="token operator">!</span>isScriptAll<span class="token punctuation">,</span> <span class="token function">changed</span><span class="token punctuation">(</span><span class="token string">"./public/medias"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">gulpif</span><span class="token punctuation">(</span>isDebug<span class="token punctuation">,</span> <span class="token function">debug</span><span class="token punctuation">(</span><span class="token punctuation">{</span> title<span class="token punctuation">:</span> <span class="token string">"Compress Images:"</span> <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">plumber</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">imagemin</span><span class="token punctuation">(</span>option<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span>gulp<span class="token punctuation">.</span><span class="token function">dest</span><span class="token punctuation">(</span><span class="token string">"./public"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 执行顺序： 清除public目录 -> 产生原始博客内容 -> 执行压缩混淆 -> 部署到服务器</span>gulp<span class="token punctuation">.</span><span class="token function">task</span><span class="token punctuation">(</span>    <span class="token string">"build"</span><span class="token punctuation">,</span>    gulp<span class="token punctuation">.</span><span class="token function">series</span><span class="token punctuation">(</span>        <span class="token string">"clean"</span><span class="token punctuation">,</span>        <span class="token string">"generate"</span><span class="token punctuation">,</span>        <span class="token string">"compressHtml"</span><span class="token punctuation">,</span>        <span class="token string">"compressCss"</span><span class="token punctuation">,</span>        <span class="token string">"compressJs"</span><span class="token punctuation">,</span>        <span class="token string">"compressImage"</span><span class="token punctuation">,</span>        gulp<span class="token punctuation">.</span><span class="token function">parallel</span><span class="token punctuation">(</span><span class="token string">"deploy"</span><span class="token punctuation">)</span>    <span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 默认任务</span>gulp<span class="token punctuation">.</span><span class="token function">task</span><span class="token punctuation">(</span>    <span class="token string">"default"</span><span class="token punctuation">,</span>    gulp<span class="token punctuation">.</span><span class="token function">series</span><span class="token punctuation">(</span>        <span class="token string">"clean"</span><span class="token punctuation">,</span>        <span class="token string">"generate"</span><span class="token punctuation">,</span>        gulp<span class="token punctuation">.</span><span class="token function">parallel</span><span class="token punctuation">(</span><span class="token string">"compressHtml"</span><span class="token punctuation">,</span> <span class="token string">"compressCss"</span><span class="token punctuation">,</span> <span class="token string">"compressJs"</span><span class="token punctuation">,</span><span class="token string">"compressImage"</span><span class="token punctuation">)</span>    <span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//Gulp4最大的一个改变就是gulp.task函数现在只支持两个参数，分别是任务名和运行任务的函数</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>最后在根目录下命令行输入<code>gulp</code>或者<code>gulp default</code>, 相当于输入了<code>hexo cl &amp;&amp; hexo g</code>, 然后再压缩图片和代码.</p><h3 id="hexo-neat"><a href="#hexo-neat" class="headerlink" title="hexo-neat"></a>hexo-neat</h3><p>使用<a href="https://github.com/rozbo/hexo-neat" target="_blank" rel="noopener">hexo-neat</a>更为简单, 美中不足的是这个插件有俩小bug:</p><ul><li>压缩<code>.md</code>文件会使 markdown 语法的代码块消失.</li><li>会删除全角空格.</li></ul><p>但是它避免了国内<code>npm</code>的使用问题. 所以我推荐这种方式.</p><p>在博客根目录命令行输入:</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> hexo-neat --save<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>然后在Hexo配置文件末尾加入(已针对matery的bug优化):</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token comment" spellcheck="true">#hexo-neat 优化提速插件（去掉HTML、css、js的blank字符）</span><span class="token key atrule">neat_enable</span><span class="token punctuation">:</span> <span class="token boolean important">true</span><span class="token key atrule">neat_html</span><span class="token punctuation">:</span>  <span class="token key atrule">enable</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">exclude</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token string">'**/*.md'</span><span class="token key atrule">neat_css</span><span class="token punctuation">:</span>  <span class="token key atrule">enable</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">exclude</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token string">'**/*.min.css'</span><span class="token key atrule">neat_js</span><span class="token punctuation">:</span>  <span class="token key atrule">enable</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">mangle</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">output</span><span class="token punctuation">:</span>  <span class="token key atrule">compress</span><span class="token punctuation">:</span>  <span class="token key atrule">exclude</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token string">'**/*.min.js'</span>    <span class="token punctuation">-</span> <span class="token string">'**/**/instantpage.js'</span>    <span class="token punctuation">-</span> <span class="token string">'**/matery.js'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="CDN加速"><a href="#CDN加速" class="headerlink" title="CDN加速"></a>CDN加速</h2><p>因为Github在国内访问速度比较慢, 所以用CDN加速来优化网站访问速度. jsDelivr + Github就能免费实现博客网站的访问加速.</p><blockquote><p>CDN 的全称是 Content Delivery Network，即内容分发网络。CDN 是构建在网络之上的内容分发网络，依靠部署在各地的边缘服务器，通过中心平台的负载均衡、内容分发、调度等功能模块，使用户就近获取所需内容，降低网络拥塞，提高用户访问响应速度和命中率。</p></blockquote><p>在Matery主题的配置文件末尾已经有相应的配置:</p><pre class="line-numbers language-ejs"><code class="language-ejs">jsDelivr:  url: #https://cdn.jsdelivr.net/gh/<github用户名>/<github仓库名><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>配置文件中有关于<code>jsDelivr</code>相应的注释, 即在内容没有被部署到github仓库之前, 是没法使用cdn在本地查看效果的, 只有部署之后才能看到效果.</p><blockquote><p>CDN访问加速<br>第一次使用本功能，一定要先配置url，再<code>hexo cl &amp;&amp; hexo g &amp;&amp; hexo d</code>部署到GitHub的仓库，注意！必须是GitHub的仓库！<br>如果必须要使用国内的coding或者gitee，可以采用双部署，同时将网站部署到两个仓库（其中一个必须是GitHub的仓库）<br>URL配置规则（例子如下）： <a href="https://cdn.jsdelivr.net/gh/你的GitHub用户名/你的仓库名" target="_blank" rel="noopener">https://cdn.jsdelivr.net/gh/你的GitHub用户名/你的仓库名</a><br>如果想关闭此功能，将 url地址 注释或删除即可！</p><p>注：配置了此项，就代表着本地调试的时候，网站依然会去GitHub请求资源（原来的资源），本地调试的时候记得将 此项配置 注释或者删除掉.</p></blockquote><p><strong>jsDelivr 不支持加载超过 20M 的资源.</strong></p>]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> gulp </tag>
            
            <tag> 图片懒加载 </tag>
            
            <tag> CDN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CSS样式覆盖</title>
      <link href="/posts/42309.html"/>
      <url>/posts/42309.html</url>
      
        <content type="html"><![CDATA[<h1 id="CSS样式覆盖"><a href="#CSS样式覆盖" class="headerlink" title="CSS样式覆盖"></a>CSS样式覆盖</h1><h2 id="覆盖"><a href="#覆盖" class="headerlink" title="覆盖"></a>覆盖</h2><p>今天下午想给博客配个在线Markdown的小工具, 按照教程搭配完了, 结果发现这行高简直欺骗感情. </p><img src="https://gitee.com/Daning0/Images/raw/master/Other/20200725212325.png" style="zoom:50%;" /><p>明明<a href="https://hehung.top/MyMarkdown/" target="_blank" rel="noopener">这位博主</a>的在线Markdown和editormd官方的行距就是正常的, 怎么到我这就开始变宽了呢?</p><p>在一轮乱搞<code>editormd.css</code>之后, 发现仍然不能改变前端显示的效果. 用Chrome按下F12看, 发现右侧很多css的样式都被线划掉了.</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/20200725213113.png" style="zoom:67%;" /><p>按过F12查看元素实际大小, 发现就是padding和margin出现了问题, 导致行距很宽. 我上网查了很久, 不知道到底是哪个css覆盖了它. </p><h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>我把那位博主的css文件都下载下来, 发现<code>matery.css</code>的大小和我本地的大小居然不一样! 比对后果然发现了端倪. 他将直接将<code>matery.css</code>中同名的<code>pre</code>中<code>margin</code>和<code>padding</code>后的<code>!important</code>注释掉了.</p><p>其实只要<strong>按照关键词进行查找</strong>, 找到同名的所有样式, 没被线划掉的就是没被覆盖的样式, 根据后面的文件位置就能快速定位. 样式被覆盖的原因是<strong>优先级不够</strong>.</p><h3 id="优先级规则"><a href="#优先级规则" class="headerlink" title="优先级规则"></a>优先级规则</h3><p>内容来自<a href="https://blog.csdn.net/cuishizun/article/details/81979809" target="_blank" rel="noopener">CSDN</a>, 我这里只遇到过前三个.</p><ul><li>优先级就近原则, 同权重的样式谁离标签内容近谁就优先级高.</li><li>载入样式以最后载入的定位为准(覆盖)</li><li>!important优先级最高.</li><li>按照类别进行区分:<ul><li>内联, 如style=””——1000</li><li>id, 如#content——100</li><li>类、伪类和属性选择器, 如.content——10，</li><li>标签选择器和伪元素选择器, 如div, p——1</li><li>通配符、子选择器和相邻选择器, 如*, &gt;, +——0</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 前端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CSS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo主题修改</title>
      <link href="/posts/1370.html"/>
      <url>/posts/1370.html</url>
      
        <content type="html"><![CDATA[<h1 id="Hexo主题修改"><a href="#Hexo主题修改" class="headerlink" title="Hexo主题修改"></a>Hexo主题修改</h1><p>Hexo有很多漂亮的主题模板. 在它的<a href="https://hexo.io/themes/" target="_blank" rel="noopener">官网主题页</a>有很多好看的模板. 但是有一部分主题是没有显示在官网上的, 如果想看更多可以直接在Github上搜.</p><h2 id="推荐的主题"><a href="#推荐的主题" class="headerlink" title="推荐的主题"></a>推荐的主题</h2><p>我挑选了几个比较赏心悦目的模板, 在这列出来.</p><h3 id="Ayer"><a href="#Ayer" class="headerlink" title="Ayer"></a>Ayer</h3><p><img src="https://gitee.com/Daning0/Images/raw/master/Other/ayer.png" alt=""></p><p>极简风格, 刚进博客有整个屏, 很棒. 博客预览的背景也很干净.</p><h3 id="Yilia"><a href="#Yilia" class="headerlink" title="Yilia"></a>Yilia</h3><p><img src="https://gitee.com/Daning0/Images/raw/master/Other/20200724005316.png" alt=""></p><p>看起来很精简的博客, 该有的都有. 麻雀虽小五脏俱全.</p><h3 id="Yilia-plus"><a href="#Yilia-plus" class="headerlink" title="Yilia-plus"></a>Yilia-plus</h3><p><img src="https://gitee.com/Daning0/Images/raw/master/Other/20200724010601.png" alt=""></p><p>在原来的Yilia上做的升级, 扩展了很多功能, 好像代码羊用的就是这个.</p><h3 id="Yelee"><a href="#Yelee" class="headerlink" title="Yelee"></a>Yelee</h3><p><img src="https://gitee.com/Daning0/Images/raw/master/Other/20200724012420.png" alt=""></p><p>也是一个Yilia的变种, 看来大家都很喜欢Yilia, 这个版本换上了更多彩的背景.</p><h3 id="Tranquilpeak"><a href="#Tranquilpeak" class="headerlink" title="Tranquilpeak"></a>Tranquilpeak</h3><p><img src="https://gitee.com/Daning0/Images/raw/master/Other/20200724010150.png" alt=""></p><p>双栏的, 看着贼舒服.</p><h3 id="Icarus"><a href="#Icarus" class="headerlink" title="Icarus"></a>Icarus</h3><p><img src="https://gitee.com/Daning0/Images/raw/master/Other/20200726012900.png" alt=""></p><p>简洁大气.</p><h3 id="NexT"><a href="#NexT" class="headerlink" title="NexT"></a>NexT</h3><p><img src="https://gitee.com/Daning0/Images/raw/master/Other/20200724011238.png" alt=""></p><p>简到极致. Github上最火爆的主题.</p><h3 id="Butterfly"><a href="#Butterfly" class="headerlink" title="Butterfly"></a>Butterfly</h3><p><img src="https://gitee.com/Daning0/Images/raw/master/Other/TIM%E6%88%AA%E5%9B%BE20200724011924.jpg" alt=""></p><p>这个主题也很酷, 有一整面的图. 博客浏览也很是赏心悦目.</p><h3 id="Matery"><a href="#Matery" class="headerlink" title="Matery"></a>Matery</h3><p><img src="https://gitee.com/Daning0/Images/raw/master/Other/20200724012502.png" alt=""></p><p>这款主题具有预加载, 访问速度比较快, 更像是年轻人使用的. 我主要看上了它的文章浏览风格和模板功能的强大. 虽然说有多余的视觉效果, 但是根据个人情况进行调整后肯定会符合自己的需求. 所以我采用Matery作为我现在的博客模板.</p><h3 id="主题汇总"><a href="#主题汇总" class="headerlink" title="主题汇总"></a>主题汇总</h3><table><thead><tr><th>Preview</th><th>Github</th></tr></thead><tbody><tr><td><a href="https://shen-yu.gitee.io/" target="_blank" rel="noopener">Ayer</a></td><td><a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank" rel="noopener">https://github.com/Shen-Yu/hexo-theme-ayer</a></td></tr><tr><td><a href="http://litten.me/" target="_blank" rel="noopener">Yilia</a></td><td><a href="https://github.com/litten/hexo-theme-yilia" target="_blank" rel="noopener">https://github.com/litten/hexo-theme-yilia</a></td></tr><tr><td><a href="https://zhousiwei.gitee.io/yilia-plus-demo/" target="_blank" rel="noopener">Yilia-Plus</a></td><td><a href="https://github.com/JoeyBling/hexo-theme-yilia-plus" target="_blank" rel="noopener">https://github.com/JoeyBling/hexo-theme-yilia-plus</a></td></tr><tr><td><a href="http://moxfive.xyz/" target="_blank" rel="noopener">Yelee</a></td><td><a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" rel="noopener">https://github.com/MOxFIVE/hexo-theme-yelee</a></td></tr><tr><td><a href="https://louisbarranqueiro.github.io/hexo-theme-tranquilpeak/" target="_blank" rel="noopener">Tranquilpeak</a></td><td><a href="https://github.com/LouisBarranqueiro/hexo-theme-tranquilpeak" target="_blank" rel="noopener">https://github.com/LouisBarranqueiro/hexo-theme-tranquilpeak</a></td></tr><tr><td><a href="https://blog.zhangruipeng.me/hexo-theme-icarus/" target="_blank" rel="noopener">Icarus</a></td><td><a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">https://github.com/ppoffice/hexo-theme-icarus</a></td></tr><tr><td><a href="https://www.jiaxi.io/" target="_blank" rel="noopener">NexT</a></td><td><a href="https://github.com/theme-next/hexo-theme-next" target="_blank" rel="noopener">https://github.com/theme-next/hexo-theme-next</a></td></tr><tr><td><a href="https://demo.jerryc.me/" target="_blank" rel="noopener">Butterfly</a></td><td><a href="https://github.com/jerryc127/hexo-theme-butterfly/tree/dev" target="_blank" rel="noopener">https://github.com/jerryc127/hexo-theme-butterfly/tree/dev</a></td></tr><tr><td><a href="http://blinkfox.com/" target="_blank" rel="noopener">Matery</a></td><td><a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank" rel="noopener">https://github.com/blinkfox/hexo-theme-matery</a></td></tr></tbody></table><h2 id="变更模板"><a href="#变更模板" class="headerlink" title="变更模板"></a>变更模板</h2><p>安装任何Hexo的主题路子都是一样的. 在按照之前的操作建立好Hexo基本框架后, 需要更改的内容不多.</p><ol><li>下载好github中的模板, 并放到博客文件夹下的<code>themes</code>中.</li><li>调整博客文件夹根目录下的<code>_config.yml</code>中的<code>theme</code>字段, 在使用默认主题时字段应该是<code>landscape</code>. <code>_config.yml</code>这个文件是全局的Hexo设置, 无论你的主题是什么, 这里都记载一些基本的博客配置, 更详细的博客配置在对应主题文件夹的<code>_config.yml</code>下进行修改.</li><li>调整全局的其他设置, 如<code>per_page</code>等, 需要根据主题的要求而更改.</li></ol><h2 id="Matery定制化"><a href="#Matery定制化" class="headerlink" title="Matery定制化"></a>Matery定制化</h2><p>这部分基本上根据Github上提供的<a href="https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md" target="_blank" rel="noopener">官方文档</a>进行个性化修改. 其实这个主题已经足够满足美观了, 不要再往上加机器人, 即时对话之类的功能了. 背景彩带, 点击爱心之类的视觉效果也很鸡肋, 我也都关闭了.</p><h3 id="视觉效果"><a href="#视觉效果" class="headerlink" title="视觉效果"></a>视觉效果</h3><p>很多内容能在配置文件中找到, 这里只说一些出现的bug或者经常要调整的东西.</p><h4 id="代码显示BUG"><a href="#代码显示BUG" class="headerlink" title="代码显示BUG"></a>代码显示BUG</h4><p>起初, 如果使用官方自带的代码高亮会非常的拉胯, 代码行号和代码段直接裂开.</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/20200724013845.png" style="zoom:50%;" /><p>所以我们必须安装<code>prism_plugin</code>插件.</p><p>在命令行中输入:</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> i -S hexo-prism-plugin<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>安装这个插件, 并在Hexo根目录的<code>_config.yml</code>中修改<code>highlight</code>的值为<code>false</code>, 再添加关于<code>prism</code> 插件相关的配置.</p><pre class="line-numbers language-yml"><code class="language-yml">highlight:  enable: false  # ....prism_plugin:  mode: 'preprocess' # realtime/preprocess  theme: 'tomorrow'  line_number: true # default false<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>prism</code>的<code>mode</code>指的是处理代码显示是实时模式, 还是预处理模式. 一般选预处理.</p><h4 id="头疼的Banner"><a href="#头疼的Banner" class="headerlink" title="头疼的Banner"></a>头疼的Banner</h4><p>博客刚进来的那个背景总是有个滤镜, 其实很多时间是起反作用的. 在<code>博客文件夹\themes\hexo-theme-matery\source\css\matery.css</code>中将下面这段代码直接注释掉, 就不再添加有色滤镜了.</p><pre class="line-numbers language-yaml"><code class="language-yaml">.bg<span class="token punctuation">-</span>cover<span class="token punctuation">:</span>after <span class="token punctuation">{</span>    <span class="token punctuation">-</span><span class="token key atrule">webkit-animation</span><span class="token punctuation">:</span> rainbow 60s infinite;    <span class="token key atrule">animation</span><span class="token punctuation">:</span> rainbow 60s infinite;<span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h4 id="主题配色和文章配色"><a href="#主题配色和文章配色" class="headerlink" title="主题配色和文章配色"></a>主题配色和文章配色</h4><p>主题配色主要是顶部导航栏和右侧浮动按钮的颜色, 文章配色则是文章的字体颜色. 同样在<code>matery.css</code>文件中进行更改:</p><pre class="line-numbers language-yaml"><code class="language-yaml">.bg<span class="token punctuation">-</span>color <span class="token punctuation">{</span>    <span class="token key atrule">background-image</span><span class="token punctuation">:</span> linear<span class="token punctuation">-</span>gradient(to right<span class="token punctuation">,</span> <span class="token comment" spellcheck="true">#16b182 0%, #058044 100%);</span>    <span class="token key atrule">opacity</span><span class="token punctuation">:</span> 0.8; //透明效果 值范围 0~1，看情况自己修改 <span class="token punctuation">}</span><span class="token punctuation">}</span>.text<span class="token punctuation">-</span>color <span class="token punctuation">{</span>    <span class="token key atrule">color</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true">#0f9d58 !important;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>bg-color</code>中对应的两个颜色是渐变, 从0%到100%是什么颜色到什么颜色的变化, <code>text-color</code>则是文字字体颜色. 下方的进度条颜色和宽度需要找<code>.progress-bar</code>修改.</p><h4 id="网站icon和标题icon"><a href="#网站icon和标题icon" class="headerlink" title="网站icon和标题icon"></a>网站icon和标题icon</h4><p>在<code>博客文件夹\themes\hexo-theme-matery\_config.yml</code>中找到<code>favicon</code>和<code>logo</code>, 分别对应的是网站在浏览器标签上的图标和标题中的logo. 进行修改即可. 由于某些未知原因我这里无法访问到<code>source</code>目录下的图像, 所以我一并放到了<code>source\medias</code>中. </p><h4 id="Banner图片和文章图片"><a href="#Banner图片和文章图片" class="headerlink" title="Banner图片和文章图片"></a>Banner图片和文章图片</h4><p>Banner的图片是可以进行轮换的, 默认一天换一次, 可以在配置中关掉. 如果不关掉的话会自动读取<code>themes\hexo-theme-matery\source\medias\banner</code>下的图片根据日期进行更换. 关掉后默认读取<code>0.jpg</code>. 文章图片路径在<code>themes\hexo-theme-matery\source\medias\featureimages</code>下, 这些图片是当文章没有指定图片时进行随机挑选的. 如果加入了新的图片, 需要在配置中加入新图片的路径.</p><h3 id="功能增强"><a href="#功能增强" class="headerlink" title="功能增强"></a>功能增强</h3><h4 id="新建文章头修改"><a href="#新建文章头修改" class="headerlink" title="新建文章头修改"></a>新建文章头修改</h4><p>每次新建文章都需要输入很多额外的文件头, 其实只要更改<code>博客目录\scaffolds\post.md</code>的内容, 之后再新建文章时就不需要重新敲很多东西了. 当然, 每次<code>new page</code>创建的md也可以修改, 方式是一样的, 文件在<code>scaffolds\page.md</code>.</p><pre><code>---title: {{ title }}date: {{ date }}mathjax: falsesummary: keywords: password: top: falseimg: categories: tags:---</code></pre><h4 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h4><p>对应着右上角的搜索栏, 如果不装这个插件就是个摆设.</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> hexo-generator-search --save<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>并在根目录的<code>_config.yml</code>添加:</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">search</span><span class="token punctuation">:</span>  <span class="token key atrule">path</span><span class="token punctuation">:</span> search.xml  <span class="token key atrule">field</span><span class="token punctuation">:</span> post<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h4 id="中文链接转拼音"><a href="#中文链接转拼音" class="headerlink" title="中文链接转拼音"></a>中文链接转拼音</h4><p>如果文章名称是中文的, Hexo 默认生成的永久链接也会有中文, 这样不利于 SEO. 可以用一个插件使在生成文章时生成中文拼音的永久链接.</p><pre class="line-numbers language-yaml"><code class="language-yaml">npm i hexo<span class="token punctuation">-</span>permalink<span class="token punctuation">-</span>pinyin <span class="token punctuation">-</span><span class="token punctuation">-</span>save<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>根目录<code>_config.yml</code>添加:</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">permalink_pinyin</span><span class="token punctuation">:</span>  <span class="token key atrule">enable</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">separator</span><span class="token punctuation">:</span> <span class="token string">'-'</span> <span class="token comment" spellcheck="true"># default: '-'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h4 id="文章字数统计插件"><a href="#文章字数统计插件" class="headerlink" title="文章字数统计插件"></a>文章字数统计插件</h4><p>在文章中显示文章字数, 阅读时长信息.</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> i --save hexo-wordcount<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在<strong>主题</strong>的<code>_config.yml</code>下添加:</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">postInfo</span><span class="token punctuation">:</span>  <span class="token key atrule">date</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">update</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>  <span class="token key atrule">wordCount</span><span class="token punctuation">:</span> <span class="token boolean important">false </span><span class="token comment" spellcheck="true"># 设置文章字数统计为 true.</span>  <span class="token key atrule">totalCount</span><span class="token punctuation">:</span> <span class="token boolean important">false </span><span class="token comment" spellcheck="true"># 设置站点文章总字数统计为 true.</span>  <span class="token key atrule">min2read</span><span class="token punctuation">:</span> <span class="token boolean important">false </span><span class="token comment" spellcheck="true"># 阅读时长.</span>  <span class="token key atrule">readCount</span><span class="token punctuation">:</span> <span class="token boolean important">false </span><span class="token comment" spellcheck="true"># 阅读次数.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="评论"><a href="#评论" class="headerlink" title="评论"></a>评论</h4><p>评论所用的插件有很多, 比如Gitalk, Gitment, Valine 和 Disqus. 最后一种被墙了, 前两种必须有Github账号才能登陆评论, 门槛比较高. 我选择了回复门槛低的<code>Valine</code>. Matery加入的<code>Valine</code>版本是不支持<code>enableqq</code>这个功能的(其实也可以直接更新, 自行替换配置, 但是minivaline比valine好看一些), 所以采用<a href="https://github.com/MiniValine/MiniValine" target="_blank" rel="noopener">minivaline</a>作为代替, 它比<code>valine</code>界面更好看一些, 并且能识别qq邮箱, 自动显示用户qq的头像.</p><p>在国内使用<code>valine</code>主要借助<a href="https://www.leancloud.cn/" target="_blank" rel="noopener">LeanCloud</a>, 要注册一个账号和应用, 并选择免费的<strong>开发版</strong>. 注册过程和创建APP过程就不做演示了. 在控制台中的应用找到对应的key, 并填入主题下的<code>_config.yml</code>的<code>minivaline</code>字段即可, 其余内容都可以自己更改.</p><img src="https://gitee.com/Daning0/Images/raw/master/Other/20200724021143.png" style="zoom: 50%;" /><h4 id="添加额外社交链接"><a href="#添加额外社交链接" class="headerlink" title="添加额外社交链接"></a>添加额外社交链接</h4><p>主页上的社交链接可能不够用, 直接修改插件即可. 比如说我要添加Bilibili的社交链接, 但是主题配置文件中没有, 在<code>themes\hexo-theme-matery\layout\_partial\social-link.ejs</code>中添加入以下代码:</p><pre class="line-numbers language-html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>%</span> <span class="token attr-name">if</span> <span class="token attr-name">(theme.socialLink.bilibili)</span> <span class="token attr-name">{</span> <span class="token attr-name">%</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>&lt;%<span class="token punctuation">=</span> theme.socialLink.bilibili %<span class="token punctuation">></span><span class="token punctuation">"</span></span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>tooltipped<span class="token punctuation">"</span></span> <span class="token attr-name">target</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>_blank<span class="token punctuation">"</span></span> <span class="token attr-name">data-tooltip</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>关注我的Bilibili: &lt;%<span class="token punctuation">=</span> theme.socialLink.bilibili %<span class="token punctuation">></span><span class="token punctuation">"</span></span> <span class="token attr-name">data-position</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>top<span class="token punctuation">"</span></span> <span class="token attr-name">data-delay</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>50<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>i</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>fas fa-bold<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>i</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>%</span> <span class="token attr-name">}</span> <span class="token attr-name">%</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>然后再在主题配置文件中对应的<code>socialink</code>部分加入bilibli网址即可.</p><p>其余的内容可以看<strong>小师弟</strong>的<a href="https://www.liuyao-blog.cn/posts/7410.html#toc-heading-59" target="_blank" rel="noopener">这篇博客</a>, 和<strong>Yafine</strong>的<a href="https://yafine-blog.cn/posts/4ab2.html" target="_blank" rel="noopener">这篇博客</a>写的很详细, 后半部分涉及到加速之类的内容, 强推.</p>]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Matery </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MathJax常见问题</title>
      <link href="/posts/33457.html"/>
      <url>/posts/33457.html</url>
      
        <content type="html"><![CDATA[<h1 id="MathJax常见问题"><a href="#MathJax常见问题" class="headerlink" title="MathJax常见问题"></a>MathJax常见问题</h1><p>本文记录了在配置hexo博客和MathJax一起使用的多个问题及解决方法. hexo本身和数学公式的兼容性就不是很好, 所以发生了很多兼容性问题.</p><h2 id="MathJax和英文小括号的冲突"><a href="#MathJax和英文小括号的冲突" class="headerlink" title="MathJax和英文小括号的冲突"></a>MathJax和英文小括号的冲突</h2><p>如果你即使用了hexo也使用了MathJax, 那么有很大几率你会使用小括号来正常书写文章内容. </p><p>譬如:</p><p>This text is (in parenthesis) but it doesn’t work.</p><p>它会显示成:</p><p>This text is $in parenthesis$ but it doesn’t work.</p><p>起初, 我一度认为是MathJax的大括号冲突导致了某个地方的小括号也出了同样的渲染问题, 但后来发现问题完全跑偏了. 这个真的卡了我挺长时间的… 很多地方都没有记载过这个问题. 直到我看到了关于MathJax3.0版本的配置问题, 才找到一段额外管理内联公式的script, 那么2.7是不是也应该有相应的一段呢?</p><p>在<code>blog\themes\hexo-theme-matery\layout\post.ejs</code>中, 有这样一段代码, 是用来调整内联公式的识别方式的. 如果不添加这段js, Mathjax只能识别大的公式, 而不能识别内联公式. 很明显, 这里直接就将转义后的<code>()</code>小括号对也当做公式匹配范围了, 也就不难解释之前错误识别的情况.</p><pre class="line-numbers language-html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>%</span> <span class="token attr-name">if</span> <span class="token attr-name">(theme.mathjax.enable</span> <span class="token attr-name">&amp;&amp;</span> <span class="token attr-name">page.mathjax)</span> <span class="token attr-name">{</span> <span class="token attr-name">%</span><span class="token punctuation">></span></span>&lt;script src="<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>%-</span> <span class="token attr-name">theme.mathjax.cdn</span> <span class="token attr-name">%</span><span class="token punctuation">></span></span><span class="token script language-javascript">"<span class="token operator">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>script</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>script</span><span class="token punctuation">></span></span><span class="token script language-javascript">    MathJax<span class="token punctuation">.</span>Hub<span class="token punctuation">.</span><span class="token function">Config</span><span class="token punctuation">(</span><span class="token punctuation">{</span>        tex2jax<span class="token punctuation">:</span> <span class="token punctuation">{</span>inlineMath<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">'$'</span><span class="token punctuation">,</span> <span class="token string">'$'</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token string">'\('</span><span class="token punctuation">,</span> <span class="token string">'\)'</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>script</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>%</span> <span class="token attr-name">}</span> <span class="token attr-name">%</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我们只要把<code>[&#39;\(&#39;, &#39;\)&#39;]</code>删去, 或者替换成更严格匹配的括号对<code>[&#39;\\(&#39;, &#39;\\)&#39;]</code>, 就能解决在博客markdown不能正确显示小括号内容的问题.</p><h2 id="MathJax和Markdown冲突"><a href="#MathJax和Markdown冲突" class="headerlink" title="MathJax和Markdown冲突"></a>MathJax和Markdown冲突</h2><p>Mathjax和Markdown的下划线语法冲了. 比如:<br>$$<br>w^{(l)}_{ij} = w^{(l)}_{ij} - \eta\frac{\partial E(W, b)}{\partial w^{(l)}_{ij}}<br>$$<br>它大概率会渲染不出来, 变成:</p><blockquote><p>$$</p><p>w^{(l)}<em>{ij} = w^{(l)}</em>{ij} - \eta\frac{\partial E(W, b)}{\partial w^{(l)}_{ij}}</p><p>$$</p></blockquote><p>这是因为Markdown和Mathjax的语法冲了, 这时候必须要修改<code>blogtest\node_modules\marked\lib\marked.js</code>, 找到<code>var inline</code>, 修改<code>escape</code>和<code>em</code>的正则匹配式:</p><pre class="line-numbers language-js"><code class="language-js"><span class="token keyword">var</span> inline <span class="token operator">=</span> <span class="token punctuation">{</span> escape<span class="token punctuation">:</span> <span class="token regex">/^\\([`*\[\]()#$+\-.!_>])/</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">// ...</span> em<span class="token punctuation">:</span> <span class="token regex">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true">// 去除了下划线_的斜体匹配</span> <span class="token comment" spellcheck="true">// ...</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>千万不要去装<code>hexo-renderer-kramed</code>!</strong> 否则如果你安装了<code>hexo-prism-plugin</code>, 也就是代码显示插件, 那这个代码高亮插件就崩了.</p><p>在更换<code>escape</code>的正则后, 一定要避免<code></code>的出现, 最起码要在其中加上一个空格变成<code>{ {</code>和<code>} }</code>, 否则会编译报错. </p><p>我尝试过保留 <code>escape</code>中对<code>\\</code>的转义,  这样4个反斜杠才能换行, <code>Typora</code>里写的时候会额外多空出来一行. 我这里在去掉这个<code>\\</code>后, 在公式换行时仍然会莫名其妙有1个反斜杠被转义, 这样需要3个反斜杠才能换行, 在<code>Typora</code>里会导致公式第二行多一个空格(实际渲染以后没有问题), 但比上面多换一行要强. </p><p><strong>2020.8.11</strong>: 发现3个反斜杠在<code>Typora</code>写表格时使表格无法正确显示.</p><p>还有一个很蛋疼的地方, 有些地方不能在公式中使用带有<code>*</code>的字符, 会导致之后的内容变成斜体. 因为去掉了<code>_</code>在公式中的正则, 那就只能通过<code>*</code>来表示斜体. 虽然斜体我几乎不用, 但是星号是表示斜体唯一的方法了, 考虑到这个就没有把斜体的正则匹配也去掉.</p><p><strong>2020.8.11</strong>: 今天在整理Markdown公式写法的时候, 意外发现了<strong>星号可以用<code>\ast</code>代替</strong>! 于是这个问题也被解决了.</p><h2 id="MiniValine冲突"><a href="#MiniValine冲突" class="headerlink" title="MiniValine冲突"></a>MiniValine冲突</h2><p><strong>2020.8.18</strong>: 当博客的<code>mathjax</code>和<code>minivaline</code>同时使用时, 会变得很卡. 这是因为<code>minivaline</code>里的<code>math</code>和博客公式显示的<code>mathjax</code>有冲突. 把它关掉就行了.</p><pre class="line-numbers language-yml"><code class="language-yml">minivaline:  enable: true  # ...  math: false # Support MathJax.  # ...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="MathJax2迁移到MathJax3"><a href="#MathJax2迁移到MathJax3" class="headerlink" title="MathJax2迁移到MathJax3"></a>MathJax2迁移到MathJax3</h2><p>其实<code>MathJax</code>除了上面那些不太兼容的问题(确实有点恶心)来说, 没什么太大的毛病. 但是一旦公式多了就体现出来一个问题, 慢! 现在<code>MathJax</code>已经更新到3.0及以上版本, 官方团队直接把2的代码重写了, 效率提高了<strong>60% ~ 80%</strong>, 这个速度还是非常诱人的.</p><blockquote><p>There were a number of design goals to the version 3 rewrite. A primary one was to improve the rendering speed of MathJax, and we feel we have accomplished that. Because the two versions operate so differently, it is difficult to make precise comparisons, but in tests that render a complete page with several hundred expressions, we see a reduction in rendering time of between 60 and 80 percent, depending on the browser and type of computer.</p></blockquote><h3 id="更新版本"><a href="#更新版本" class="headerlink" title="更新版本"></a>更新版本</h3><p>因为我用的是<code>Matery</code>, 所以自己稍微改了改. 其他主题的更改方法大致相同.</p><p>先在主题下的<code>_config.yml</code>文件中, 将之前的<code>mathjax</code>连接的网址更换为最新的. 当然我这里直接采用了保留原来2.7的版本. 除了因为需要旧版本可以随时更换回来, 还有一个原因就是3.0的配置方法与2.7版本的配置方式是不同的, 这就意味着在渲染过程中的代码需要重新写一份, 没有必要把旧版本的内容完全删除.</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">mathjax</span><span class="token punctuation">:</span>  <span class="token key atrule">enable2</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>  <span class="token key atrule">enable3</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">cdn2</span><span class="token punctuation">:</span> https<span class="token punctuation">:</span>//cdn.bootcss.com/mathjax/2.7.7/MathJax.js<span class="token punctuation">?</span>config=TeX<span class="token punctuation">-</span>AMS<span class="token punctuation">-</span>MML_HTMLorMML  <span class="token key atrule">cdn3</span><span class="token punctuation">:</span> https<span class="token punctuation">:</span>//cdn.jsdelivr.net/npm/mathjax@3/es5/tex<span class="token punctuation">-</span>mml<span class="token punctuation">-</span>chtml.js<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>然后再去改渲染的脚本<code>blogtest\themes\hexo-theme-matery\layout\post.ejs</code>,  并参考<a href="http://docs.mathjax.org/en/latest/web/configuration.html#configuration" target="_blank" rel="noopener">官方文档配置教程</a>, 在末尾更改:</p><pre class="line-numbers language-javascript"><code class="language-javascript"><span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>theme<span class="token punctuation">.</span>mathjax<span class="token punctuation">.</span>enable2 <span class="token operator">&amp;&amp;</span> page<span class="token punctuation">.</span>mathjax<span class="token punctuation">)</span> <span class="token punctuation">{</span> <span class="token operator">%</span><span class="token operator">></span><span class="token operator">&lt;</span>script src<span class="token operator">=</span><span class="token string">"&lt;%- theme.mathjax.cdn2 %>"</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>script<span class="token operator">></span><span class="token operator">&lt;</span>script<span class="token operator">></span>    MathJax<span class="token punctuation">.</span>Hub<span class="token punctuation">.</span><span class="token function">Config</span><span class="token punctuation">(</span><span class="token punctuation">{</span>        tex2jax<span class="token punctuation">:</span> <span class="token punctuation">{</span> inlineMath<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">'$'</span><span class="token punctuation">,</span> <span class="token string">'$'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">'\\('</span><span class="token punctuation">,</span> <span class="token string">'\\)'</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token punctuation">}</span> <span class="token comment" spellcheck="true">// 行内公式标识</span>    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token operator">&lt;</span><span class="token operator">/</span>script<span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token punctuation">}</span> <span class="token operator">%</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>theme<span class="token punctuation">.</span>mathjax<span class="token punctuation">.</span>enable3 <span class="token operator">&amp;&amp;</span> page<span class="token punctuation">.</span>mathjax<span class="token punctuation">)</span> <span class="token punctuation">{</span> <span class="token operator">%</span><span class="token operator">></span><span class="token operator">&lt;</span>script id<span class="token operator">=</span><span class="token string">"MathJax-script"</span> <span class="token keyword">async</span> src<span class="token operator">=</span><span class="token string">"&lt;%- theme.mathjax.cdn3 %>"</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>script<span class="token operator">></span><span class="token operator">&lt;</span>script<span class="token operator">></span>    MathJax <span class="token operator">=</span> <span class="token punctuation">{</span>        tex<span class="token punctuation">:</span> <span class="token punctuation">{</span>            inlineMath<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">'$'</span><span class="token punctuation">,</span> <span class="token string">'$'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">'\\('</span><span class="token punctuation">,</span> <span class="token string">'\\)'</span><span class="token punctuation">]</span><span class="token punctuation">]</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token operator">&lt;</span><span class="token operator">/</span>script<span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token punctuation">}</span> <span class="token operator">%</span><span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这样在需要旧版本时候直接调整<code>enable</code>就行了. 当然你还可以更改其他的配置, 参考教程即可.</p><h3 id="无法换行"><a href="#无法换行" class="headerlink" title="无法换行"></a>无法换行</h3><p>我换了以后发现一个问题, 多行公式开始变得没法换行了. 我已经排除掉了转义的问题, 确确实实已经有两个反斜杠, 可是公式没法换行.</p><p>测试了一会后, 发现无论多少反斜杠都没有办法换行, 我在<a href="https://github.com/mathjax/MathJax/issues/2312" target="_blank" rel="noopener">官方issue</a>找到了答案(有超多人有这个问题). 官方从旧版本迁移过来还没有实现换行符, 官方提到了两种办法来解决这个问题.</p><ol><li>使用<code>\displaylines</code>.</li></ol><p>$$<br>\displaylines{\lim_{N\to\infty}E(p_{max})=N\frac{1}{N}(1-\frac{1}{N})^{N-1} = \frac{(1-\frac{1}{N})^N}{1-\frac{1}{N}} \\<br>\lim_{N\to \infty}{1-\frac{1}{N}}=1  \qquad\lim_{N\to \infty}({1-\frac{1}{N}})^N=\frac{1}{e} \\<br>\lim_{N\to \infty}{E(p_{max})}=\frac{1}{e}=37\%}<br>$$</p><p>​            我当然比较倾向于用这种, 因为我不习惯下面那种编写方式, 在<code>Typora</code>里实在是太麻烦了.</p><ol start="2"><li>使用<code>{align}</code>或者<code>{aligned}</code>.</li></ol><p>$$<br>\begin{align}<br>x’ = \frac{x - x_{median}}{IQR}  \\<br>IQR = x_{q3} - x_{q1}<br>\end{align}<br>$$<br>​            这种方式当然也有优点, 就是方便公式的<strong>对齐</strong>, 因为对齐时候必须要在多行公式的环境里面.<br>$$<br>\begin{aligned}<br>E(p)&amp;=Np(1-p)^{N-1} \\<br>E’(p)&amp;=N(1-p)^{N-1} -Np(1-p)^{N-2} \\<br>&amp;=N(1-p)^{N-2}((1-p)-p(N-1))\\<br>E’(p)&amp;=0 \Rightarrow p_{max}=\frac{1}{N}<br>\end{aligned}<br>$$<br>还发现了一些意外小惊喜:</p><blockquote><p>WordPress uses <code>\</code> as a special character, and it is often stripped out of WordPress posts (depending on what editor you are using). As an escape character, it can prevent the normal action of the character that follows it, so to get an explicit backslash into your post, you need to double it. That is why you need an extra backslash (because the first two equal a single backslash in the output). You probably should double the second one as well, but apparently that is not required.</p></blockquote><p>官方人员提到了WordPress中的类似问题, 我猜hexo第一个反斜杠被转义也是类似的原因.</p>]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Mathjax </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title>404</title>
      <link href="/404.html"/>
      <url>/404.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>分类</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>about</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>留言板</title>
      <link href="/contact/index.html"/>
      <url>/contact/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>markdown</title>
      <link href="/markdown/index.html"/>
      <url>/markdown/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>tags</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
  
</search>
