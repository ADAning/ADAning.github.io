<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Introduction: Vector Quantization</title>
      <link href="/posts/18380.html"/>
      <url>/posts/18380.html</url>
      
        <content type="html"><![CDATA[<h1 id="Introduction-Vector-Quantization"><a href="#Introduction-Vector-Quantization" class="headerlink" title="Introduction: Vector Quantization"></a>Introduction: Vector Quantization</h1><h2 id="Vector-Quantization"><a href="#Vector-Quantization" class="headerlink" title="Vector Quantization"></a>Vector Quantization</h2><p>AutoEncoder(AE)由Encoder和Decoder组成, Encoder将图像压缩为一个低维的隐向量(Latent), 再由Decoder使用Latent将图像恢复出来. 在此基础上有向原始图像中加噪的Denoising AutoEncoder(DAE)和从将Latent规约为标准正态分布的生成式模型Variational AutoEncoder(VAE)两种. 我们在<a href="https://adaning.github.io/posts/53598.html">之前的博客</a>中已经讲述过了:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/introduction vq1.png" style="zoom: 25%;" /><p>它们的Latent Vector都是连续的, <strong>能不能将中间的Latent从连续变成一个离散的状态呢</strong>?</p><p>因为有时, 物体的特征可能是一种离散状态, 此时我们不希望样本在Latent Space连续的分布.</p><p>例如人是否在坐着, 只能是坐与不坐两种状态, 不能介于二者之间变成一个即坐又不坐的状态. 如果是连续的Latent Space设计, 模型可能会学习到从坐到不坐的连续变化过程, 但实际上它并不应该存在.</p><p>我们可以通过<strong>Vector Quantization</strong>(VQ)来实现这一想法:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/introduction vq2.png" style="zoom: 33%;" /><h2 id="VQ-VAE"><a href="#VQ-VAE" class="headerlink" title="VQ - VAE"></a>VQ - VAE</h2><ul><li>论文:  <a href="https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html" target="_blank" rel="noopener">Neural Discrete Representation Learning</a>.</li></ul><h3 id="Discrete-Latent-Variables"><a href="#Discrete-Latent-Variables" class="headerlink" title="Discrete Latent Variables"></a>Discrete Latent Variables</h3><p>VQ - VAE仍然遵循AE的框架, 它也有Encoder $z_e$, Decoder $z_q$. 对图像用Encoder抽得一个表示$z_e(x)$, 然后用Decoder对输入$z_q(x)$ 解码, 得到原图像$x$.</p><p>与一般AE不同的是, VQ - VAE额外有一个<strong>Codebook</strong>, 存放了$K$ 个$D$ 维的Embedding $e_i \in \mathbb{R}^D$, 称为Latent Embedding Space $e \in \mathbb{R}^{K \times D}$. 这个Codebook也可以看做是VQ - VAE的先验分布, 只不过它是离散的.</p><p>在Latent处, 后验类别分布$q(z|x)$ 也不再是连续空间, 而是离散空间, 这与做一个$K$ 分类任务类似. 其分布可以用One - Hot来表示:</p><p>$$<br>q(z=k \mid x)= \begin{cases}1 &amp; \text { for } \mathrm{k}=\operatorname{argmin}_j\left|z_e(x)-e_j\right|_2 \\\ 0 &amp; \text { otherwise }\end{cases}<br>$$</p><p>接下来, 将Encoder抽取到的图像表示$z_e(x)$ 直接替换为Codebook中离$z_q(x)$ 距离最近的$e_k$, 并作为Decoder的输入$z_q(x)$:</p><p>$$<br>z_q(x)=e_k, \quad \text {where} \quad k=\operatorname{argmin}_j\left|z_e(x)-e_j\right|_2<br>$$</p><p>由于其中有$\text{argmin}$, 所以这里存在<strong>梯度断裂</strong>, Decoder侧的梯度是没法通过链式法则传到Encoder做更新的.</p><blockquote><p>注意, 这里Encoder抽取出的表示$z_e(x)$ 一般是一个用CNN获得的$m \times m \times D$ 的特征, 所以这张图像$x$ 对应的离散表示就是一个$m \times m$ 的二维矩阵. 如果$z_e(x) \in \mathbb{R}^D$, 在重构的时候就比较困难了, 这代表着Codebook里的某个Embedding对应了若干张训练集图片, 而不是图像中的某个部分.</p></blockquote><h3 id="Learning"><a href="#Learning" class="headerlink" title="Learning"></a>Learning</h3><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/introduction vq3.png" style="zoom: 67%;" /><p>接上文, 因为Encoder的输出$z_e(x)$ 维度和Decoder输入$z_q(x)$ 维度是一致的, 作者使用<strong>Straight - Through Estimator</strong>(STE), 直接将Decoder input处$z_q(x)$ 的梯度Copy到Encoder output处$z_e(x)$即可, 这样就链接了Encoder和Decoder的梯度更新, 官方给出的代码如下:</p><pre class="line-numbers language-python"><code class="language-python">decoder_input <span class="token operator">=</span> z_e <span class="token operator">+</span> <span class="token punctuation">(</span>z_q <span class="token operator">-</span> z_e<span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>虽然作者引入的STE解决了Encoder和Decoder的梯度更新, 但如果按照上述代码来走, Codebook里面的Embedding的梯度没法通过Decoder拿到了, 因为<code>z_q</code> 的梯度被detach掉了. 这样就需要通过额外约束来优化Codebook.</p><p>Codebook中的Embedding $e$ 肯定是希望离Encoder的输出$z_e(x)$ 越近越好, 这样才说明Codebook的对应Embedding $e_i$ 跟Encoder输出匹配的更好, 也更有利于Decoder做生成, 因为$z_q(x) = e_k$ 嘛.</p><p>最终, Training Loss共包含三个部分:</p><p>$$<br>L=\underbrace{\log p\left(x \mid z_q(x)\right)}_{\text{Reconstruction}}+\underbrace{\left|\operatorname{sg}\left[z_e(x)\right]-e\right|_2^2}_{\text{VQ}}+\beta\underbrace{\left|z_e(x)-\operatorname{sg}[e]\right|_2^2}_{\text{Commitment}}<br>$$</p><p>其中, $\text{sg}$ 为Stop Gradient Operator.</p><ul><li>第一项为Reconstruction Loss, 就是由Decoder重建图像的Loss, 与AE损失相同. 并由STE链接Encoder和Decoder的梯度更新.</li><li>第二项为VQ Loss, 负责让Codebook中的Embedding $e$ 离Encoder output $z_e(x)$ 更近.</li><li>第三项为Commitment Loss, 这是与第二项对称的Loss, 前面多乘上了一个超参$\beta$ 用于调节比例, 负责让Encoder output$z_e(x)$ 也朝着$e$ 移动. 作者认为Codebook Embedding无量纲, 且Codebook和Encoder的参数更新速度不同添加的.</li></ul><p>另外, $\beta$ 文中作者取0.25, 并且作者发现从0.1取到2.0模型表现变化都不大.</p><blockquote><p>看到这里, 能够发现VQ - VAE实际上并不是一个VAE, 它和VAE没有任何关系, 只是一个用VQ实现的AE. 因为<strong>只靠VQ - VAE自己是不能直接生成若干张不同随机图像的</strong>, 因此它只能看做是一种<strong>压缩方式</strong>, 图像从原始输入大小$H \times W \times 3$变成了更小的二维矩阵$m \times m$. 如果想要做生成, 需要结合PixelCNN完成.</p></blockquote><h2 id="VQ-GAN"><a href="#VQ-GAN" class="headerlink" title="VQ - GAN"></a>VQ - GAN</h2><ul><li>论文: <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf" target="_blank" rel="noopener">Taming Transformers for High-Resolution Image Synthesis</a>.</li></ul><p>VQ - GAN实际是<strong>VQ-VAE</strong>的一种变体, 都是利用VQ, 但VQ - GAN用了Transformer做自回归生成. VQ - VAE的自回归生成采用的是PixelCNN, 这种级别的模型对于自回归生成来说还是比较脆弱的. 恰好, Transformer擅长自回归离散Token序列的生成, 所以Transformer理应成为Autoregressive Manner的首选.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/introduction vq4.png" style="zoom: 25%;" /><h3 id="Learning-an-Effective-Codebook-of-Image-Constituents-for-Use-in-Transformers"><a href="#Learning-an-Effective-Codebook-of-Image-Constituents-for-Use-in-Transformers" class="headerlink" title="Learning an Effective Codebook of Image Constituents for Use in Transformers"></a>Learning an Effective Codebook of Image Constituents for Use in Transformers</h3><p>与VQ - VAE类似的, VQ - GAN中生成用的主体有Encoder $E$, Decoder $G$(因为这里是GAN, 所以Decoder是生成器$G$), 和Codebook $\mathcal{Z} = \set{z_k}^{K}_{k=1} \subset \mathbb{R}^{n_z}$.</p><p>对于给定的图片$x \in \mathbb{R}^{H \times W \times 3}$, 首先用CNN的Encoder $E$ 抽取出图像$x$ 的编码表示$\hat{z} = E(x) \in \mathbb{R}^{h \times w \times n_z}$. </p><p>然后, VQ - GAN会用一个由$h \times w$ 个$n_z$ 维的Embedding组成的二维特征矩阵$z_\mathbf{q} \in \mathcal{R}^{h \times w \times n_z}$ 作为CNN Decoder $G$ 的输入.</p><p>这里的VQ的过程与VQ - VAE是一样的, 都是用<strong>最近邻</strong>替换每个Spatial Code$\hat{z}_{ij} \in \mathbb{R}^{n_z}$:</p><p>$$<br>z_{\mathbf{q}}=\mathbf{q}(\hat{z}):=\left(\underset{z_k \in \mathcal{Z}}{\arg \min }\left|\hat{z}_{i j}-z_k\right|\right) \in \mathbb{R}^{h \times w \times n_z}<br>$$</p><p>将$z_{\mathbf{q}}$ 作为Decoder $G$ 的输入, 重建图像$\hat{x}$:</p><p>$$<br>\hat{x}=G\left(z_{\mathbf{q}}\right)=G(\mathbf{q}(E(x)))<br>$$</p><p>与VQ - VAE同样的, 由于Decoder $G$ 和Encoder $E$ 中间的quantization存在$\text{argmin}$, 所以梯度不能从$G$ 传递到$E$, VQ - GAN也使用了STE, 与VQ - VAE形式一样的Loss $\mathcal{L}_{\text{VQ}}$如下:</p><p>$$<br>\mathcal{L}_{\mathrm{VQ}}(E, G, \mathcal{Z})=|x-\hat{x}|^2  +\left|\operatorname{sg}[E(x)]-z_{\mathbf{q}}\right|_2^2+\beta\left|\operatorname{sg}\left[z_{\mathbf{q}}\right]-E(x)\right|_2^2<br>$$</p><p>虽然MSE可以从像素角度来描述图像之间的相似性, 但并不能从抽象的特征角度描述图像重构的好不好, 所以作者将L2 Loss $|x-\hat{x}|^2$ 替换为<a href="https://arxiv.org/abs/1603.08155" target="_blank" rel="noopener">Perceptual Loss</a> $\mathcal{L}_{\text{rec}}$:</p><p>$$<br>\mathcal{L}_{\mathrm{VQ}}(E, G, \mathcal{Z})=\mathcal{L}_{\text{rec}} +\left|\operatorname{sg}[E(x)]-z_{\mathbf{q}}\right|_2^2+\beta\left|\operatorname{sg}\left[z_{\mathbf{q}}\right]-E(x)\right|_2^2<br>$$</p><blockquote><p>据说作者给的代码MSE和Perceptual Loss都用了.</p></blockquote><h4 id="Learning-a-Perceptually-Rich-Codebook"><a href="#Learning-a-Perceptually-Rich-Codebook" class="headerlink" title="Learning a Perceptually Rich Codebook"></a>Learning a Perceptually Rich Codebook</h4><p>因为是GAN嘛, 训练GAN除了有Generator, 还需要有一个<strong>Discriminator</strong>. 作者用一个Patch - based Discriminator $D$ (其实就是<a href="https://arxiv.org/abs/1803.07422" target="_blank" rel="noopener">PatchGAN</a>)来评估Decoder $G$ 将<strong>每个Code生成为Patch的真假</strong>, 即对$h \times w$ 个Patch的真假都做判断:</p><p>$$<br>\mathcal{L}_{\mathrm{GAN}}(\{E, G, \mathcal{Z}\}, D)=[\log D(x)+\log (1-D(\hat{x}))]<br>$$</p><p>最终目标函数为找到最优模型参数$\mathcal{Q}^\ast = \set{E^\ast, G^\ast, \mathcal{Z}^\ast}$:</p><p>$$<br>\mathcal{Q}^\ast=\underset{E, G, \mathcal{Z}}{\arg \min } \max _D \mathbb{E}_{x \sim p(x)}{\left[\mathcal{L}_{\mathrm{VQ}}(E, G, \mathcal{Z})\right.} \left.+\lambda \mathcal{L}_{\mathrm{GAN}}(\{E, G, \mathcal{Z}\}, D)\right]<br>$$</p><p>对应的Loss就是前面说过的VQ Loss和GAN Loss两项之和.</p><blockquote><p>GAN的Loss最外层是Minimize $E, G, \mathcal{Z}$, 最内层是Maximize $D$, 所以它的目标其实是在有一个具有真正判别能力的判别器$D$ 的情况下优化生成器$G$ 的参数.</p></blockquote><p>其中, $\lambda$ 为自适应权重:</p><p>$$<br>\lambda=\frac{\nabla_{G_L}\left[\mathcal{L}_{\mathrm{rec}}\right]}{\nabla_{G_L}\left[\mathcal{L}_{\mathrm{GAN}}\right]+\delta}<br>$$</p><p>这个式子里的$\mathcal{L}_{\text{rec}}$ 是Perceptual Loss, $\delta$ 为1e-6, 防止分母零除.</p><p>作者在论文中没有给出$\lambda$ 的作用, 我们姑且可以认为$\lambda$ 是为了平衡$\mathcal{L}_{\text{rec}}$ 与$\mathcal{L}_{\mathrm{GAN}}$ 的影响而存在的, 保证两个Loss的作用差不多:</p><ul><li>当$\mathcal{L}_{\text{rec}}$ 梯度大于$\mathcal{L}_{\mathrm{GAN}}$ 的梯度时, 说明模型生成的图像还不够好, 此时$\lambda &gt; 1$, $\mathcal{L}_{\text{rec}}$ 影响比较大, 所以需要加强$\mathcal{L}_{\mathrm{GAN}}$ 的权重.</li><li>当$\mathcal{L}_{\text{rec}}$ 梯度小于$\mathcal{L}_{\mathrm{GAN}}$ 的梯度时, 说明模型生成的图像足够好, 此时$\lambda &lt; 1$, $\mathcal{L}_{\text{rec}}$ 影响比较小, 所以需要削弱$\mathcal{L}_{\mathrm{GAN}}$ 的权重.</li></ul><h3 id="Learning-the-Composition-of-Images-with-Transformers"><a href="#Learning-the-Composition-of-Images-with-Transformers" class="headerlink" title="Learning the Composition of Images with Transformers"></a>Learning the Composition of Images with Transformers</h3><h4 id="Latent-Transformers"><a href="#Latent-Transformers" class="headerlink" title="Latent Transformers"></a>Latent Transformers</h4><p>在通过VQ将输入图像$x$ 转化为$z_{\mathbf{q}}=\mathbf{q}(E(x))$ 后, 它等价于由Codebook $\mathcal{Z}$ 中Token下标组成的序列$s \in\{0, \ldots,|\mathcal{Z}|-1\}^{n \times w}$:</p><p>$$<br>s_{i j}=k \text { such that }\left(z_{\mathbf{q}}\right)_{i j}=z_k<br>$$</p><p>所以可以直接用Decoder - Only的Transformer对离散序列的自回归生成:</p><p>$$<br>\mathcal{L}_{\text {Transformer }}=\mathbb{E}_{x \sim p(x)}[-\log {\prod_i p\left(s_i \mid s_{&lt;i}\right)}]<br>$$</p><h4 id="Conditioned-Synthesis"><a href="#Conditioned-Synthesis" class="headerlink" title="Conditioned Synthesis"></a>Conditioned Synthesis</h4><p>如果需要对图像合成附加上条件$c$, 只需要在Transformer生成序列前加上条件信息$c$, 接下来继续做自回归生成就好:</p><p>$$<br>p(s \mid c)=\prod_i p\left(s_i \mid s_{&lt;i}, c\right)<br>$$</p><p>如果$c$ 只是简单的类别信息, 那么$c$ 就是一个简单的Embedding. 但如果是有约束条件$r$, 例如一个要生成图像的草图或框架, 那么$c$ 是一由一个新的Codebook和另一个专门编码条件的VQ - GAN得到的Conditional Sequence, 然后继续以Autoregressive Manner生成图像就可以了.</p><h4 id="Generating-High-Resolution-Images"><a href="#Generating-High-Resolution-Images" class="headerlink" title="Generating High - Resolution Images"></a>Generating High - Resolution Images</h4><p>生成高清图像的话需要扩大每个图片对应的Code Embedding数量, 这样就会给Transformer做序列生成时候带来压力. 作者提出了一种基于<strong>Sliding window</strong>的方法, 每次生成Token的时候只关注一个局部的小窗来生成下一个Code:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/introduction vq5.png" style="zoom: 33%;" /><p>只要数据集上满足近似平移不变, 或者附带空间信息, 就可以做到高清的图像生成了. 如果没有空间信息, 人为附加上一个空间信号, 依然可以用这种方法生成高清图像.</p><h2 id="Recommended"><a href="#Recommended" class="headerlink" title="Recommended"></a>Recommended</h2><p>VQ - VAE:</p><ul><li><p><a href="https://www.spaces.ac.cn/archives/6760" target="_blank" rel="noopener">VQ-VAE的简明介绍：量子化自编码器 - 科学空间|Scientific Spaces</a>.</p></li><li><p><a href="https://zhuanlan.zhihu.com/p/633744455" target="_blank" rel="noopener">轻松理解 VQ-VAE：首个提出 codebook 机制的生成模型 - 周弈帆的文章 - 知乎</a>.</p></li><li><p><a href="https://zhuanlan.zhihu.com/p/388299884s" target="_blank" rel="noopener">漫谈VAE和VQVAE，从连续分布到离散分布 - 陀飞轮的文章 - 知乎</a>.</p></li></ul><p>VQ - GAN:</p><ul><li><a href="https://zhuanlan.zhihu.com/p/637705399" target="_blank" rel="noopener">VQGAN 论文与源码解读：前Diffusion时代的高清图像生成模型 - 周弈帆的文章 - 知乎</a>.</li></ul><p>VQ + Image Generation串讲: </p><ul><li><a href="https://zhuanlan.zhihu.com/p/681895334" target="_blank" rel="noopener">AutoEncoder与图像生成 - vasgaowei的文章 - 知乎</a>.</li></ul><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>在不同的Modality里, 都可以用VQ来做:</p><ul><li>CV: VQ最主要的作用是压缩图像的序列长度.</li><li>NLP: Token Embedding的生成本身就遵循Autoregressive Manner, 本身就是离散的.</li><li>Audio: 在语音领域中, 语音信号也需要做离散化处理, 用一个Quantizer把连续语音信号转化成离散的Token是一种常用的操作.</li></ul><p>在以VL为主的MLLM时代, VQ因为使用Codebook能够统一离散形式的文本Token和图像生成而重新被人提起. 这种<strong>Multimodal Tokenization</strong>的方式或许能帮助MLLM更好的理解不同Modality之间的关系, 因此VQ或许能够在MLLM时代继续发挥它的光和热.</p><blockquote><p>另外, Codebook中的每个Code很容易出现利用率不均, 这里抛砖引玉的给出一篇文章, 是对VQ - GAN的改进:</p><ul><li><a href="https://arxiv.org/abs/2110.04627" target="_blank" rel="noopener">Vector-Quantized Image Modeling With Improved VQGAN</a>.</li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> VQ </tag>
            
            <tag> VQ-VAE </tag>
            
            <tag> VQ-GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Multimodal Large Language Model 总结</title>
      <link href="/posts/64567.html"/>
      <url>/posts/64567.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识: <a href="https://adaning.github.io/posts/44986.html">Vision &amp; Language Pretrained Model 总结</a>.</p></blockquote><h1 id="Multimodal-Large-Language-Model-总结"><a href="#Multimodal-Large-Language-Model-总结" class="headerlink" title="Multimodal Large Language Model 总结"></a>Multimodal Large Language Model 总结</h1><p>最近MLLM的进展实在是太快了, 必须得赶紧写一篇博客出来了… 再不写这些知识就要过期了…</p><p>所以, 本文只是以<strong>总结</strong>的形式梳理了近期比较有代表性的MLLM, 推荐有基础后再阅读.</p><h2 id="Revolution-of-Visual-Language-Adapter"><a href="#Revolution-of-Visual-Language-Adapter" class="headerlink" title="Revolution of Visual-Language Adapter"></a>Revolution of Visual-Language Adapter</h2><p>目前的MLLM基本组成有三部分, <strong>Visual Backbone</strong>, <strong>V-L Adapter</strong>, <strong>LLM</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结1.png" style="zoom: 50%;" /><blockquote><p>该图出自<a href="https://arxiv.org/abs/2402.12451" target="_blank" rel="noopener">The Revolution of Multimodal Large Language Models: A Survey</a>.</p></blockquote><p>之前的MLLM基本在LLM内部没有什么变化, Visual Encoder基本也用的CLIP的Vision Encoder, 主要区别在于Adapter上.</p><h3 id="Flamingo"><a href="#Flamingo" class="headerlink" title="Flamingo"></a>Flamingo</h3><p>Flamingo代表了在LLM主干中加入Cross Attention从而用视觉增强文本表示的一派.</p><ul><li>论文: <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html" target="_blank" rel="noopener">Flamingo: a Visual Language Model for Few-Shot Learning</a>.</li></ul><p>Flamingo将视觉信息融入LLM的方式是在LM Block的主干上<strong>串行</strong>的加入一个用Cross Attention增强文本表示的模块, 从而让文本表示中能融入视觉信息:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结2.png" style="zoom: 67%;" /><blockquote><p>预训练LM为Chinchilla 1.4 / 7 / 70B.</p></blockquote><p>作者在每个LM Block前面加上了一个Gated Cross - Attention Block. 结构如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结3.png" style="zoom: 67%;" /><p>以Language为Query, Vision input为Key和Value, 并用Tanh和残差做一下过滤, 决定视觉增强的文本表示流通率的门控系数为全0初始化, 跟LoRA有点类似.</p><p>其中<strong>Receiver Resampler</strong>是用类似<a href="https://adaning.github.io/posts/44986.html#toc-heading-6">BLIP</a>和<a href="https://adaning.github.io/posts/44986.html#toc-heading-7">CoCa</a>的<strong>Query和Cross-Attention</strong>汲取有效的视觉信息:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结4.png" style="zoom: 50%;" /><p>Resampler中Cross Attention的Key和Value<strong>是Visual Representation和Query Representation的拼接</strong>.</p><p>比较有趣的是作者提到了Flamingo对<strong>交错图文</strong>(Interleaved Image Text)的数据的处理方法:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm%E6%80%BB%E7%BB%935.png" alt=""></p><p>在一系列文本和一系列图像构成的图文交错数据中, 每个文本块中Token在Cross - Attention中只能对对应的Visual Token做Attention(深蓝色), 而无法对其他Visual Token做Attention(浅蓝).</p><blockquote><p>在作者的实验中, 将作者构建的交错图文数据集去掉后, 模型效果下降非常恐怖.</p></blockquote><h3 id="BLIP-2-InstructBLIP"><a href="#BLIP-2-InstructBLIP" class="headerlink" title="BLIP-2 / InstructBLIP"></a>BLIP-2 / InstructBLIP</h3><p>BLIP-2开创了以VL对齐的Q - Former抽取视觉信息送给LLM的先河.</p><h4 id="BLIP-2"><a href="#BLIP-2" class="headerlink" title="BLIP-2"></a>BLIP-2</h4><blockquote><p>BLIP-2我们在<a href="https://adaning.github.io/posts/44986.html#toc-heading-9">VLP总结</a>里面其实已经讲过了, 在这里只是简单的把它粘过来, 以保证内容完整性.</p></blockquote><ul><li>论文: <a href="https://proceedings.mlr.press/v202/li23q.html" target="_blank" rel="noopener">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a>.</li></ul><p>出发点: 当前大规模模型在预训练期间的高额计算消耗太大, 数据也用的特别多.</p><p>作者引入一个lightweight Querying Transformer (Q - Former)来完成Visual &amp; Language模态的桥接过程:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结6.png" style="zoom:75%;" /><p>作者把Q - Former的训练拆分为两个阶段:</p><ul><li>首阶段: 让Q - Former从Freeze Image Encoder中学习VL表示.</li><li>次阶段: 从Freeze LLM中学习VL表示.</li></ul><p>Q - Former结构和首阶段预训练如下:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm%E6%80%BB%E7%BB%937.png" alt=""></p><p>Q - Former实际上由<strong>双塔</strong>的两个Transformer组成, 分别被称为Image Transformer和Text Transformer. 结构上类似于<strong>BLIP</strong>中的Image - Grounded Text Encoder和Text Encoder.</p><p>Image Transformer的SA和Text Transformer的SA参数是共享的(这点和<strong>VLMo</strong>出奇的一致). Learnable Query从Image Transformer给入, 通过CA来从Frozen Image Encoder中获取视觉信号.</p><blockquote><p>作者还在文中补充了一个小细节, 实际上的Visual Key Value采用的是Image Encoder的倒数第二层输出, 而不是最后一层, 效果会稍微好一点, 这点与大家使用Stable Diffusion的时候取CLIP的倒数第二层输出有点类似.</p></blockquote><p>所以从结构上来看, 首阶段的训练目标是希望Query能够学到从Image Encoder中抽取对Text最有用的内容. 再看训练任务也是这样, 设计了三种:</p><ul><li><strong>ITC</strong>(Image - Text Contrastive Learning): 虽然说是老生常谈的Loss, 但因为Query经过Trm以后得到的表示有多个, 所以作者计算了多个Query与Text Transformer<code>[CLS]</code>的余弦相似度, 选择相似度最大的作为正样本. 为了避免<strong>信息泄露</strong>, 在做ITC的时候要保证Q和T之间是互相不可见的(最右侧Mask).</li><li><strong>ITG</strong>(Image - grounded Text Generation): 使得Q对T完全可见, T单独用Casual Mask, 然后生成图文匹配的文本段. 这就要求Query必须覆盖Image的全部信息, 且Query抽取出的信息必须是有效的(中间Mask). <code>[CLS]</code>也被换成<code>[DEC]</code>.</li><li><strong>ITM</strong>(Image - Text Matching): ITM也是常见Loss, Q必须拥有两个模态的信息才能一起判断图文是否匹配, 作者对所有Query都计算ITM Loss, 最后取平均作为Logits, 同时也使用Hard Negative.</li></ul><p>次阶段预训练, 直接用Q - Former完成图生文:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm%E6%80%BB%E7%BB%938.png" alt=""></p><p>由于在首阶段中Q - Former已经完成了Query从Image Encoder中抽取关键信息的学习, 这也就使得Visual Signal可以被Query以Soft Visual Prompt的形式传递给LLM. 所以Q - Former中的Text Transformer变得不再必要, 可以被<strong>丢弃</strong>. Query表示还需要过一层Linear Project和大模型输入维度对齐.</p><blockquote><p>如果不要首阶段直接硬学的话, 由于没有Text Transformer打辅助, 所以想要让Q - Former学到从Image中抽取出更多有关文本的信息会更难. 但文本模态在Q - Former首阶段训练中起到的实际上是一个Grounding的作用, 根据Language来让Learnable Query抽取更多有用的信息.</p></blockquote><h4 id="InstructBLIP"><a href="#InstructBLIP" class="headerlink" title="InstructBLIP"></a>InstructBLIP</h4><ul><li>论文: <a href="https://openreview.net/forum?id=vvoWPYqZJA" target="_blank" rel="noopener">InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning</a>.</li></ul><p>延续<strong>BLIP-2</strong>的Q - Former, 在Q - Former中添加了Instruct, 从而使得Q - Former能完成Instruction-aware Visual Feature Extraction, 从而将Visual Feature从静态的变为<strong>动态</strong>的, 能够做到<strong>instruction following</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结9.png" style="zoom:50%;" /><p>其余细节基本一致.</p><p>直接使用BLIP-2里面已经对齐好的Q - Former作为初始化(看做是预训练), 然后直接在Q - Former和LLM侧继续进行指令微调, 使Q - Former能够理解指令, 并完成指令引导的视觉特征抽取. 指令构造来自人工手动.</p><blockquote><p>与之类似的还有同样为BLIP系列续作的X-InstructBLIP, 但审稿人似乎认为这种方法并没有具备很大的贡献, 以及实验不够充分缺乏与当前的MLLM对比, 于是在ICLR 24被拒稿了.</p></blockquote><h3 id="LLaVA系列"><a href="#LLaVA系列" class="headerlink" title="LLaVA系列"></a>LLaVA系列</h3><p>LLaVA代表了整个使用MLP为Adapter的一派.</p><h4 id="LLaVA"><a href="#LLaVA" class="headerlink" title="LLaVA"></a>LLaVA</h4><ul><li>论文: <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/6dcf277ea32ce3288914faf369fe6de0-Abstract-Conference.html" target="_blank" rel="noopener">Visual Instruction Tuning</a>.</li></ul><p>与<strong>BLIP-2</strong>的Q - Former不同, LLaVA抛弃了沉重的Visual Extractor设计:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结10.png" style="zoom: 50%;" /><p>用预训练的<a href="https://adaning.github.io/posts/44986.html#toc-heading-3">CLIP</a>抽取的Visual Feature作为Vision Signal, 再用一次<strong>Linear Projection</strong>后送到LLM里面.</p><blockquote><p>在LLM中, Visual Token仍然是<strong>Autoregressive Encoding</strong>的.</p></blockquote><p>LLaVA训练的时候遵循多轮对话:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结11.png" style="zoom:50%;" /><p>在第一轮对话的时候把图像信息附加进去即可.</p><p>作者设计了两阶段微调, 让LLM能适配Visual Input:</p><ul><li><strong>Stage 1</strong>: Pre - training for Feature Alignment, 只调Linear Projection的参数, 使Visual Feature和LLM Embedding Space对齐.</li><li><strong>Stage 2</strong>: Fine - tuning End-to-End, 让Linear Projection和LLM一起调.</li></ul><p>比较有意思的是, LLaVA的指令数据集是用LLM(ChatGPT / GPT4)生成的, 通过把图像中的信息以自然语言描述出来从而传递给更高阶的LLM, 让LLM生成指令数据.</p><p>比如直接把图中物体的Caption和BBox都传进去, 然后让LLM生成三种类型的问题:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结12.png" style="zoom:50%;" /><p>LLaVA的表现超过了BLIP-2.</p><h4 id="LLaVA-1-5"><a href="#LLaVA-1-5" class="headerlink" title="LLaVA 1.5"></a>LLaVA 1.5</h4><ul><li>论文: <a href="https://arxiv.org/abs/2310.03744v1" target="_blank" rel="noopener">Improved Baselines with Visual Instruction Tuning (v1)</a>.</li></ul><p>LLaVA 1.5是LLaVA的改进版本:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结13.png" style="zoom: 67%;" /><p>主要做了如下改动:</p><ul><li>限制了LLM的输出格式, 让LLaVA直接以简短的方式回答, 有利于VQA任务.</li><li>从一层Linear Project变成了两层MLP.</li><li>加入了学术方面的数据集, 用于解锁LLaVA对视觉区域细粒度理解能力.</li><li>提高了图像分辨率, 并加入了额外数据源.</li></ul><h4 id="LLaVA-NeXT-LLaVA-1-5-HD"><a href="#LLaVA-NeXT-LLaVA-1-5-HD" class="headerlink" title="LLaVA-NeXT(LLaVA-1.5-HD)"></a>LLaVA-NeXT(LLaVA-1.5-HD)</h4><ul><li>论文: <a href="https://arxiv.org/abs/2310.03744v2" target="_blank" rel="noopener">Improved Baselines with Visual Instruction Tuning (v2)</a>.</li></ul><p>增强了推理, OCR和World Knowledge.</p><p>通过动态分辨率输入, 支持”任意”分辨率大小的图像作为输入(实际训练阶段最大支持4倍):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结14.png" style="zoom:67%;" /><p>把大图切分为小Patch, 分别用Visual Encoder编码, 再把一个下采样的图编码, 作为<strong>全局信息</strong>, 全拼接后一起送给LLM.</p><p>特性如下:</p><ul><li>支持动态分辨率输入.</li><li>采用更强的用户指令数据, 加入更多跟文档以及图片理解的数据.</li><li>增大LLM backbone的Scale.</li></ul><h2 id="Rethinking-Design-of-MLLM"><a href="#Rethinking-Design-of-MLLM" class="headerlink" title="Rethinking Design of MLLM"></a>Rethinking Design of MLLM</h2><p>从Flamingo并入LLM主路的Cross Attention, 再到BLIP-2里Q - Former, 再到LLaVA直接用简单的MLP就完成视觉信号输入, 确实可以得出一些关于MLLM设计的结论, 并引导MLLM的设计走向.</p><h3 id="Prismatic-VLMs"><a href="#Prismatic-VLMs" class="headerlink" title="Prismatic VLMs"></a>Prismatic VLMs</h3><ul><li>论文: <a href="https://arxiv.org/abs/2402.07865" target="_blank" rel="noopener">Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models</a>.</li></ul><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结16.png" style="zoom:50%;" /><p>作者探寻了四个维度的VLM设计, 并给出了一些结论:</p><ul><li><strong>Optimization Procedure</strong>:<ul><li>Multi - Stage Training: <strong>LLaVA</strong>中是分两个阶段训练, 第一个阶段训练Projection Layer, 第二个阶段全Finetune. 本文作者发现其实直接略过第一阶段也可以, 这样还可以节省训练时间.</li><li>Full Finetuning through Visual Backbones: 在上述单阶段过程中, <strong>必须要把Visual Backbone Freeze住</strong>, 不然模型会崩掉, 尤其是Location Task(这个结论和训练数据也可能有关).</li></ul></li><li><strong>Image Processing and Pretrained Visual Representations</strong>:<ul><li>Choosing a Pretrained Vision Representation: 同规模下, 用VL Contrastive训练过的<strong>CLIP</strong>, SigLIP比纯视觉无监督DINOv2和有监督的ViT效果要好.</li><li>Image Processing across Visual Backbones:  对于CLIP而言, 简单的Resize比Crop-Resize要好, SigLIP则在简单的Resize和Letterbox Padding上表现类似. 不同模型似乎不太一样, 无法得出一个确信的结论.</li><li>Scaling Image Resolution: 高分辨率确实好.</li><li>Ensembling Different Visual Representations: 集成DINOv2和SigLIP的特征效果最好.</li></ul></li><li><strong>Language Models</strong>:<ul><li>Base vs. Instruct - Tuned LMs: 没有经过指令微调的LLM和经过指令微调的LLM表现近似, 并且没有经过指令微调的base LLM出现幻觉更少.</li><li>Do Better LMs Lead to Better VLMs?: LLM本身的表现与VLM最终表现不一定有很强的关联.</li><li>Co-training on Language-only Safety Data: 加入Text-Only的安全数据后, 性能只有一点点下降, 但是增加了不少在对话过程中的安全性.</li></ul></li><li><strong>Scaling Properties Around Training Time and Data</strong>:<ul><li>Are we Undertraining?: 很多方法都只Train了一个epoch, 实际上有些模型没有充分训练, 还可以继续训练第二个epoch.</li><li>Adding Additional Vision-Language Data: 更多样性的数据堆Scaling VLM很重要.</li></ul></li></ul><h3 id="MM1"><a href="#MM1" class="headerlink" title="MM1"></a>MM1</h3><ul><li>论文: <a href="https://arxiv.org/abs/2403.09611" target="_blank" rel="noopener">MM1: Methods, Analysis &amp; Insights from Multimodal LLM Pre-training</a>.</li></ul><p>Apple团队的工作, 除了从模型角度揭示了一些MLLM的设计表现, 也从数据角度进行了分析.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结17.png" style="zoom:67%;" /><p>这里直接给出论文中得出的结论:</p><ul><li><strong>Visual Encoder</strong>: <strong>图像分辨率</strong>影响最大, 然后才是Visual Backbone的Size和数据.</li><li><strong>VL Adapter</strong>: <strong>Visual Token的数量和图像分辨率</strong>最关键, <strong>VL Adapter的类型反而不太重要</strong>.</li><li><strong>Data</strong>:<ol><li><strong>交错数据</strong>对提升Few - Shot和Text - Only影响很大, Caption数据对Zero - Shot提升比较大.</li><li><strong>Text-Only</strong>数据对Few - Shot和Text - Only性能提升有帮助.</li><li><strong>合理图文数据配比</strong>能带来更好的MM性能并保持住纯文本任务上的性能.</li><li><strong>合成数据</strong>有助于Few - Shot Learning.</li></ol></li></ul><h3 id="Idefics2"><a href="#Idefics2" class="headerlink" title="Idefics2"></a>Idefics2</h3><ul><li>论文: <a href="http://arxiv.org/abs/2405.02246" target="_blank" rel="noopener">What matters when building vision-language models?</a>.</li></ul><p>来自抱抱脸团队的工作. 文中对MLLM的可能设计做了探索, 最终探索得到的模型结构如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结15.png" style="zoom:45%;" /><p>输入图像 -&gt; Visual Encoder -&gt; Projection + Pooling -&gt; 大图分割成小图, 且可能和文本交错 -&gt; LLM -&gt; 输出文本.</p><p>一些关键结论:</p><ol><li>固定参数量下, <strong>LLM的质量</strong>相比于Visual Backbone的质量对VLM的影响更大(这点和Prismatic VLMs有冲突).</li><li>Cross - Attention(Flamingo并入LLM主干的Cross Attention)比Autoregressive(指不改动Decoder Only的LLM本身, 比如LLaVA)的表现在单模态Backbone冻结的时候效果要好, 但当Backbone解冻的时候, 反而<strong>Autoregressive Manner会更好一些</strong>, 即使Cross - Attention吃了更多参数.</li><li>Fully Autoregressive的架构的预训练Backbone<strong>如果都解冻会导致模型发散</strong>, 用LoRA可以缓解这个问题.</li><li><strong>Visual Token过多</strong>时, 通过<strong>Learnable Pooling来减少Visual Token的数量</strong>可以很明显的增加下游任务上的推理效果和训练 / 推理速度.</li><li>对在固定大小的正方形图像预训练的Visual Backbone, 采用LoRA等方式来<strong>保留图像的原始长宽比和分辨率</strong>, 不会降低性能, 并且同时还能加速训练和推理并减少显存.</li><li>在训练时将<strong>大图变为拆分出的多张小子图和大图本身</strong>, 都作为Visual Token输入到模型当中, 可以在推理阶段用推理效率换来更多的推理性能, 尤其是在涉及到阅读图像中文本的任务中更为明显.</li></ol><hr><p>经过一系列摸索以后发现, 之前纠结的VL Adapter形式似乎并不是很重要, <strong>图像分辨率和Visual Token数</strong>是重要的, 这也会成为未来(现在已经)的主流发展方向.</p><p>除去上述三篇论文以外, 还建议大家阅读一下这个知乎问题, 里面也有很多真知灼见:</p><ul><li><a href="https://www.zhihu.com/question/626796690" target="_blank" rel="noopener">多模态大语言模型（MLLM）为什么最近的工作中用BLIP2中Q - Former结构的变少了？</a>.</li><li>也有一些工作对Q - Former的问题做了一些分析:<ul><li><a href="https://arxiv.org/abs/2405.20985" target="_blank" rel="noopener">DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models</a>.</li><li><a href="https://arxiv.org/abs/2406.08487" target="_blank" rel="noopener">Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models</a>.</li></ul></li></ul><h2 id="Do-not-Stop-Generation"><a href="#Do-not-Stop-Generation" class="headerlink" title="Do not Stop Generation!"></a>Do not Stop Generation!</h2><p>LLM主要还是面向AIGC, 生成还是有很多要做的工作.</p><h3 id="NExT-GPT"><a href="#NExT-GPT" class="headerlink" title="NExT-GPT"></a>NExT-GPT</h3><ul><li>论文: <a href="https://arxiv.org/abs/2309.05519" target="_blank" rel="noopener">NExT-GPT: Any-to-Any Multimodal LLM</a>.</li></ul><p>上面讲的都是一些VLM, 有没有扩展到更多模态的工作?</p><p>NExT-GPT提出了从<strong>文本 / 图像 / 语音 / 视频</strong>作为输入再到上述四种模态生成的LLM框架:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结18.png" style="zoom: 33%;" /><p>能非常明显的从图中观察到, <strong>NExT-GPT是纯粹的LLM Centric</strong>, 所有其他模态的编码和生成都围绕LLM来构建. <strong>所有模态都跟Language进行了对齐</strong>, 所以可以完成任意模态到Text再到任意模态的生成.</p><p>当进行对话时, NExT-GPT能够根据对话过程中生成的<strong>特殊Token及其表示</strong>来生成对应模态的内容:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结19.png" style="zoom:40%;" /><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结20.png" style="zoom:40%;" /><p>特殊的Token有<code>&lt;IMG_i&gt;</code>, <code>&lt;AUD_i&gt;</code>, <code>&lt;VID_i&gt;</code>, 这些特殊Token的表示会被作为每种模态的Diffusion里面的<strong>Condition</strong>来引导对应模态生成.</p><p>作者用ImageBird当做编码器, 同时编码Image, Audio, Video作为输入.</p><p>在优化模型时, 主要做了Encoder和Decoder两侧的模态对齐.</p><p>因为要将所有模态都和Text对齐, 所以可以拿除文本以外的模态Encoder抽取出的表示经过Projection, 在各模态Caption的过程中完成<strong>Encoder侧对齐</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结21.png" style="zoom:40%;" /><p>同样的, 由于有各模态Diffusion的Caption, 可以将Diffusion的Text Encoder对Caption编码后的表示(Condition)与LLM输出的特殊Token经过Projection后的表示对齐, 从而完成<strong>Decoder侧对齐</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结22.png" style="zoom:40%;" /><p>为了让NExT-GPT在对话过程中拥有模态迁移的能力, 还需要进行Modality-switching Instruction Tuning. 对LLM做LoRA Tuning, 并设计几种<strong>模态切换</strong>的对话场景:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm总结23.png" style="zoom:50%;" /><p>指令数据集根据GPT4和各种模态的Caption共同构造, 之后再人工筛选一下.</p><h3 id="DreamLLM"><a href="#DreamLLM" class="headerlink" title="DreamLLM"></a>DreamLLM</h3><ul><li>论文: <a href="https://arxiv.org/abs/2309.11499" target="_blank" rel="noopener">DREAMLLM: SYNERGISTIC MULTIMODAL COMPREHENSION AND CREATION</a>.</li></ul><p>DreamLLM提出了一种能够处理图文交错数据的方法, 并且这种方法能在原始多模态空间中采样生成:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mllm%E6%80%BB%E7%BB%9324.png" alt=""></p><p>当DreamLLM需要生成图像的时候, 会生成一个特殊的Dream Token <code>&lt;dream&gt;</code>.</p><p>一旦生成这个Token, 就说明LLM需要生成图像了. 接着会有若干个Dream Query以Autoregressive Manner喂给到LLM里, 最后这组Dream Query的表示会被用于生成Diffusion Model的Condition. 生成的图像会被Visual Encoder和Projection重新提取为Visual Embedding输入到模型中.</p><p>由于这种方式是直接生成或处理的图像, 不需要用CLIP之类的模型对Visual Embedding进行对齐, 所以整个生成过程都是在原始多模态空间中进行的.</p><p>训练分为三个阶段, 整个过程Diffusion Model都是冻住的:</p><ol><li><strong>Alignment Training</strong>: Linear Visual Projector, Linear Condition Projector和Learnable Dream Embedding解冻, 用30M图文对对齐所有Projector和Dream Query, 在这个期间做图文理解和文图生成. 这样就可以把除了LLM以外的所有组件初步对齐到一个偏于LLM的Multimodal Space.</li><li><strong>I-GPT Pretraining</strong>: 解冻LLM, 做大规模文本图像的联合生成式建模, 将所有组件对齐到真正的Multimodal Space.</li><li><strong>Supervised Fine-tuning</strong>: 加上指令按下游任务Finetune.</li></ol><h3 id="Joint-Interleaved-Image-Text-Generation"><a href="#Joint-Interleaved-Image-Text-Generation" class="headerlink" title="Joint / Interleaved Image-Text Generation"></a>Joint / Interleaved Image-Text Generation</h3><p><a href="https://arxiv.org/abs/2310.12973" target="_blank" rel="noopener">本身LLM其实是一个Universal Decoder</a>, 它的能力一定不止于Text Generation. 如果能够<strong>直接统一Visual Token和Text Token生成的方式</strong>, 则会带来更灵活的生成效果. <strong>有多种方式可以达到这个效果</strong>, 其中一种就是用VQ(Vector Quantization)把视觉图像变成离散化Token, 将Codebook里的离散Token.</p><blockquote><p>当然, VQ这个过程不是必要的, 因为有很多种方式可以达到这个效果, 从DreamLLM里就可以看出来.</p></blockquote><p>因此, 在需要生成图像的时候, 只需要MLLM预测出对应的Visual Token, 然后再交给Diffusion之类的Visual Decoder做个解码, 就能拿到需要的图像了. 这样做的好处是整个Token Space对Vision和Language都是Joint Training的, 能达到非常灵活的输出效果…</p><p>比如说在对话过程中, 模型发现用户难理解就直接出个图, 甚至也可以从用户输入的图像中截取一部分做辅助说明(生成Visual Token, 然后再Decode回去), 也可以同时在对话中处理图像并返还给用户.</p><p>再结合Flamingo, Idefics2和MM1中发现的交错数据带来的提升, 以及DreamLLM这种交错生成框架的出现, 如果这条线能做Work的话, 能够推测出Joint / Interleaved Generation的时代就要来了, 我比较看好它和Diffusion结合在一起的发挥.</p><p>这个部分暂时没什么时间, 没法串起来讲一下, 因为补这条线需要从VQ-VAE先开始讲起, 而且它还在不断演进中… 所以只能给大家一个阅读思路:</p><ul><li>预备知识:<ul><li>VQ-VAE: <a href="https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html" target="_blank" rel="noopener">Neural Discrete Representation Learning</a>.</li><li>VQ-GAN: <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.html?ref=" target="_blank" rel="noopener">Taming Transformers for High-Resolution Image Synthesis</a>.</li></ul></li><li>SPAE: <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/a526cc8f6ffb74bedb6ff313e3fdb450-Abstract-Conference.html" target="_blank" rel="noopener">SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs</a>.</li><li>VLTokenizer: <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhu_Beyond_Text_Frozen_Large_Language_Models_in_Visual_Signal_Comprehension_CVPR_2024_paper.html" target="_blank" rel="noopener">Beyond Text: Frozen Large Language Models in Visual Signal Comprehension</a>.</li><li>LaVIT: <a href="https://arxiv.org/abs/2309.04669" target="_blank" rel="noopener">UNIFIED LANGUAGE-VISION PRETRAINING IN LLM WITH DYNAMIC DISCRETE VISUAL TOKENIZATION</a>.</li><li>SEED系列:<ul><li>SEED: <a href="https://arxiv.org/abs/2307.08041" target="_blank" rel="noopener">Planting a SEED of Vision in Large Language Model</a>.</li><li>SEED-LLaMA: <a href="https://arxiv.org/abs/2310.01218" target="_blank" rel="noopener">Making llama see and draw with seed tokenizer</a>.</li><li>SEED-X: <a href="https://arxiv.org/abs/2404.14396" target="_blank" rel="noopener">SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation</a></li></ul></li><li>Chameleon: <a href="https://arxiv.org/abs/2405.09818" target="_blank" rel="noopener">Chameleon: Mixed-Modal Early-Fusion Foundation Models</a>.</li><li>TiTok: <a href="https://arxiv.org/abs/2406.07550" target="_blank" rel="noopener">An Image is Worth 32 Tokens for Reconstruction and Generation</a>.</li><li>SETOKIM: <a href="https://arxiv.org/abs/2406.05127" target="_blank" rel="noopener">Towards Semantic Equivalence of Tokenization in Multimodal LLM</a>.</li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MLLM </tag>
            
            <tag> MM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>通用信息抽取(下) - UniEX, Mirror, RexUIE</title>
      <link href="/posts/683.html"/>
      <url>/posts/683.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li><a href="https://adaning.github.io/posts/11838.html">通用信息抽取(上) - UIE, USM, InstructUIE</a>.</li></ul></blockquote><h1 id="通用信息抽取-下-UniEX-Mirror-RexUIE"><a href="#通用信息抽取-下-UniEX-Mirror-RexUIE" class="headerlink" title="通用信息抽取(下) - UniEX, Mirror, RexUIE"></a>通用信息抽取(下) - UniEX, Mirror, RexUIE</h1><p>本文为介绍<strong>通用信息抽取领域</strong>经典模型的下篇, 将会介绍了<strong>UniEX</strong>, <strong>Mirror</strong>, <strong>RexUIE</strong>三个模型:</p><ul><li><strong>UniEX</strong>: <strong>ACL 2023</strong>, <a href="https://aclanthology.org/2023.acl-long.907/" target="_blank" rel="noopener">UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective</a>.</li><li><strong>Mirror</strong>: <strong>EMNLP 2023</strong>, <a href="https://aclanthology.org/2023.emnlp-main.548/" target="_blank" rel="noopener">Mirror: A Universal Framework for Various Information Extraction Tasks</a>.</li><li><strong>RexUIE</strong>: <strong>EMNLP Findings 2023</strong>,  <a href="https://aclanthology.org/2023.findings-emnlp.1024/" target="_blank" rel="noopener">RexUIE: A Recursive Method with Explicit Schema Instructor for Universal Information Extraction</a>.</li></ul><p>碍于篇幅原因, <strong>本文不包含对实验部分的解读</strong>, 对实验感兴趣的读者还请自行阅读. </p><p>也可以太长不看. 单从性能角度来讲, 全监督任务上RexUIE &gt; Mirror ≈ USM ≈ UniEX, Few shot设置中RexUIE &gt; Mirror &gt; USM.</p><h2 id="UniEX-An-Effective-and-Efficient-Framework-for-Unified-Information-Extraction-via-a-Span-extractive-Perspective"><a href="#UniEX-An-Effective-and-Efficient-Framework-for-Unified-Information-Extraction-via-a-Span-extractive-Perspective" class="headerlink" title="UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective"></a>UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective</h2><p>本篇第一个模型为<strong>UniEX</strong>, 论文出自<strong>ACL 2023</strong>, <a href="https://aclanthology.org/2023.acl-long.907/" target="_blank" rel="noopener">UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective</a>.</p><p>UniEX将信息抽取任务建模为一个Token Pair的解析问题, 即将所有IE Task目标的抽取都拆分为<strong>Span Detection</strong>, <strong>Classification</strong>, <strong>Association</strong>这三个问题. 在解决这三个子问题后, 直接通过解码获得IE Task Target:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie-next1.png" style="zoom: 60%;" /><p>相较于Task - Specialized IE, UniEX可以处理更多的IE任务, 相较于生成式UIE, UniEX也可以更快更好地完成:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie-next2.png" style="zoom: 50%;" /><h3 id="The-UniEX-Framework"><a href="#The-UniEX-Framework" class="headerlink" title="The UniEX Framework"></a>The UniEX Framework</h3><h4 id="Unified-Input"><a href="#Unified-Input" class="headerlink" title="Unified Input"></a>Unified Input</h4><p>在UIE任务中, 大多模型都需要以IE Task的Schema作为Prompt, 来指导模型要抽取的内容.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie-next3.png" style="zoom: 67%;" /><p>在UniEX中有三种特殊Token: </p><ul><li><code>[D-TOK]</code> 承担类似<code>[CLS]</code>的任务, 来捕获全局语义信息, 由基于任务的Schema $s_d$ 组成, 用于Span Detection.</li><li><code>[C-TOK]</code> 和<code>[A-TOK]</code> 则扮演类似<code>[SEP]</code>的角色, 分别分隔用于Span Classification的Schema $s_c$ 和用于Span association的Schema $s_a$. </li></ul><p>对于输入文本$x$, 和上面提到的特殊Token构成的集合$s$, 整个模型输入序列$x_{inp}$ 是上述内容的拼接:</p><p>$$<br>\begin{aligned}<br>x_{i n p}= &amp; \left\{[\mathrm{D}-\mathrm{TOK}]^i s_d^i\right\}_{i=1}^{N_{s d}}\left\{[\mathrm{C}-\mathrm{TOK}]^i s_c^i\right\}_{i=1}^{N_{s c}} \\<br>&amp; \left\{[\mathrm{A}-\mathrm{TOK}]^i s_a^i\right\}_{i=1}^{N_{s a}}[\mathrm{SEP}] x[\mathrm{SEP}] .<br>\end{aligned}<br>$$</p><p>例如, 要抽取句子<code>Church &#39;s is headquartered in San Antonio</code> 中的所有关系, 那就将上述输入序列转换为: </p><pre><code>[D-TOK] Relation Extraction [C-TOK-0] Organization [C-TOK-1] Location ... [A-TOK-0] Organization in ... [SEP] Church &#39;s is headquartered in San Antonio [SEP]</code></pre><p>能够看到, 前面的<code>Organization</code>, <code>Location</code> 都是实体类型的Text Prompt, <code>Organization in</code> 则为关系的Text Prompt.</p><p>当Text Prompt比较复杂时, 输入序列中的Prompt就会对Prompt之间和输入句子产生<strong>干扰</strong>.</p><p>因此, 作者先将每个Schema Token都标注成<strong>独立的位置编码</strong>, 例如下图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie-next4.png" style="zoom: 67%;" /><p>每个<code>[C-TOK]</code> 和<code>[A-TOK]</code> 位置编码都是2, 对应的Schema Token都是顺次, 从而消除前面一大长串Schema对文本位置的不利影响.</p><p>其次, 由于不同Schema Token会互相影响, <strong>Mask需要保证不相关的Token之间互相不可见</strong>. 例如, <strong>有些关系是绑定实体类型的, 则关系表示不应该收到不相关的实体类型表示影响</strong>. 假设存在三元组类型为$(e^1, r^1, e^2)$, $(e^1, r^2, e^3)$, 那么Schema对应的Mask应该是这样的:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie-next5.png" style="zoom:67%;" /><p>即在三元组类型$(e^1, r^1, e^2)$中, 与关系类型$r^1$ 相对应的实体类型$e^1, e^2$ 对于$r^1$ 是可见的, $r^1$ 对于$e^1, e^2$ 来说也是可见的. $(e^1, r^2, e^3)$ 同理. </p><p>更加具体的对于各类信息抽取任务来说, 只有关系类型和事件类型存在上述情况, 因为它们可以和实体类型或者事件角色 / 触发词绑定. 除此以外, 实体, 关系, 事件类型中均互相不可见.</p><h4 id="Backbone-Network"><a href="#Backbone-Network" class="headerlink" title="Backbone Network"></a>Backbone Network</h4><p>UniEX用RoBERTa或者ALBERT这样的BERT - like PLM对输入序列进行编码.</p><p>形式化的, 用Encoder输入序列$x_{inp}$, , 基于Schema的Mask $M_{mask}$ 对所有输入编码:</p><p>$$<br>H_s, H_x=\text { Encoder }\left(x_{i n p}, p o s, M_{\text {mask }}\right)<br>$$</p><p>$H_s, H_x$ 分别为Schema表示和文本表示.</p><h4 id="Triaffine-Attention-for-Span-Representation"><a href="#Triaffine-Attention-for-Span-Representation" class="headerlink" title="Triaffine Attention for Span Representation"></a>Triaffine Attention for Span Representation</h4><p>在前文提过, UniEX将UIE任务拆分为Span Detection, Span Classification, Span Association三个子任务. 从Span视角来看, 上述三个子任务都可以分别在不同由Span构成的二维表上完成.  </p><p>那么具体要如何标注这张表呢:</p><ul><li>对于Span Detection, 则直接将<code>[D-TOK]</code>, 也就是<code>[CLS]</code> 的表示拿来构建一张二维表, 直接标注第$i$ 个Span的起止位置$(s_i, e_i)$ 即代表该Span有效.</li><li>对于Span Classification, 目标为判断每个Span的类型(实体 / 论元 / 触发词 / 事件类型). 将每种类型的Token<code>[C-TOK]</code> 都构建一张对应的二维表, 在每张特定类型的表上标注第$i$ 个Span的起止位置$(s_i, e_i)$, 即可得到该Span的类型.</li><li>对于Span Association, 第$i$ 和第$j$ 个Span之间如果存在关联(关系类型 / 情感类型), 则需要将对应的<code>[A-TOK]</code> 的表示拿来构建一张二维表, 在每张特定类型的表上标注Span对的交错起始位置$(s_i, s_j)$ 和Span对的交错结束位置$(e_i, e_j)$.</li></ul><p>仅当Span能够被检测到时, Span Classification和Span Association的预测结果才是有效的.</p><p>多说无益, 可以直接看作者附录中给出的一个关系抽取的例子:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie-next6.png" style="zoom: 50%;" /><p>下标为5, 6, 10的单词分别为<code>Besty</code>, <code>Ross</code> 和<code>Philadelphia</code>. 蓝色, 绿色, 黄绿色分别为用于完成Span Detection, Span Classification, Span Association三个子任务的矩阵:</p><ul><li>在<code>[CLS]</code> 构成的二维表中, (5, 6)位置为1, 标注出了Span <code>Besty Ross</code> 和<code>Philadelphia</code>.</li><li>在<code>[C-TOK]</code> 对应实体类型Location和Person构成的表中, Person表(5, 6)为1, Location表的(10, 10)为1, 分别标注出了<code>Philadelphia</code> 为Location, <code>Besty Ross</code> 为Person.</li><li>在<code>[A-TOK]</code> 对应关系类型为Live in构成的表中, (5, 10), (6, 10)为1, 标注出了(<code>Besty Ross</code>, Live in, <code>Philadelphia</code>).</li></ul><p>如何用模型完成上述标注方法呢? </p><p>假设输入文本Token数量为$N_x$, Schema Token的数量为$N_s$, 作者使用三仿射来同时建模Schema表示$H_s \in \mathbb{R}^{N_s \times d}$ 与输入文本的起始表示$H_x^s \in \mathbb{R}^{N_x \times d}$ 和结束表示$H_x^e \in \mathbb{R}^{N_x \times d}$ 的交互, 得到最终的标注得分$S \in \mathbb{R}^{N_s \times N_x \times N_x}$:</p><p>$$<br>\begin{gathered}<br>H_x^s=\mathrm{FFN}_s\left(H_x\right) \\<br>H_x^e=\mathrm{FFN}_e\left(H_x\right) \\<br>S=\sigma\left(\mathcal{W} \times_1 H_s \times_2 H_x^s \times_3 H_x^e\right)<br>\end{gathered}<br>$$</p><p>其中, $\mathcal{W} \in \mathbb{R}^{d \times d \times d}$ 为三仿射打分权重, $\sigma (\cdot)$ 为Sigmoid函数.</p><h3 id="EX-Training-Procedure"><a href="#EX-Training-Procedure" class="headerlink" title="EX Training Procedure"></a>EX Training Procedure</h3><p>UniEX构造的是三仿射打分矩阵, 直接用BCE完成优化:</p><p>$$<br>\begin{aligned}<br>\operatorname{BCE}(y, \hat{y}) &amp; =-(y \cdot \log (\hat{y})+(1-y) \cdot \log (1-\hat{y})) \\<br>\mathcal{L} &amp; =\sum_{r=1}^{N_s} \sum_{p=1}^{N_x} \sum_{q=1}^{N_x} \operatorname{BCE}\left(Y_{r, p, q}, S_{r, p, q}\right)<br>\end{aligned}<br>$$</p><p>此外, UniEX在有监督设置下不需要在远程监督数据集和其他数据集上做预训练, 只需要在IE数据集上一起训就好.</p><p>UniEX与<a href="https://adaning.github.io/posts/11838.html#USM-Universal-Information-Extraction-as-Unified-Semantic-Matching">通用信息抽取(上)中提到的USM</a> 在全监督任务上性能表现不分伯仲.</p><h2 id="Mirror-A-Universal-Framework-for-Various-Information-Extraction-Tasks"><a href="#Mirror-A-Universal-Framework-for-Various-Information-Extraction-Tasks" class="headerlink" title="Mirror: A Universal Framework for Various Information Extraction Tasks"></a>Mirror: A Universal Framework for Various Information Extraction Tasks</h2><p>第二个模型<strong>Mirror</strong>, 论文来自<strong>EMNLP 2023</strong>, <a href="https://aclanthology.org/2023.emnlp-main.548/" target="_blank" rel="noopener">Mirror: A Universal Framework for Various Information Extraction Tasks</a>.</p><p>据作者所述, Mirror这个名字的来源:</p><blockquote><p>The name, Mirror, comes from the classical story <em>Snow White and the Seven Dwarfs</em>, where a magic mirror knows everything in the world. We aim to build such a powerful tool for the IE community.</p></blockquote><p>Mirror将IE Task统一为多槽元组的抽取问题, 并将要抽取的信息转换为找Multi-Span Cyclic Graph, 这样使得连续或不连续的Span都可以被确定:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie-next7.png" style="zoom: 50%;" /><p>此外, Mirror和之前的UIE方法(<a href="https://adaning.github.io/posts/11838.html">UIE, USM, InstructUIE</a>)存在一些区别, 因为作者将它扩展到了Multi Span, N元组抽取, 分类和MRC任务上. 之前介绍的UIE模型大多不能处理扩展出来的这些任务:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie-next8.png" style="zoom:50%;" /><h3 id="Mirror-Framework"><a href="#Mirror-Framework" class="headerlink" title="Mirror Framework"></a>Mirror Framework</h3><h4 id="Unified-Data-Interface"><a href="#Unified-Data-Interface" class="headerlink" title="Unified Data Interface"></a>Unified Data Interface</h4><p>作者将各类IE任务统一为Instruction + Schema Labels + Text的形式, 使得模型能处理任何形式IE任务:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie-next9.png" style="zoom: 55%;" /><p>图中的绿色Token代表分隔不同内容:</p><ul><li><code>[I]</code>: Instruction.</li><li><code>[LM]</code>: Label Mentions.</li><li><code>[LR]</code>: Label Relations.</li><li><code>[LC]</code>: Classifications.</li><li><code>[TL]</code>: 分隔Text和Schema Label.</li><li><code>[TP]</code>: 后面是MRC或者QA, 没有Schema.</li><li><code>[B]</code>: 分类任务里的背景类.</li></ul><p>前文提过, 任何IE任务都可以建模为抽取多槽元组的问题, 进而变为找到Multi Span Cyclic Graph的问题. 例如:</p><ul><li><strong>NER</strong>: 抽取(entity type, entity mention)的二元组. entity type为特殊Token <code>[LM]</code>.</li><li><strong>RE</strong>: 抽取(relation, head entity, tail entity)的三元组. relation为特殊Token <code>[LR]</code>.</li><li><strong>EE</strong>: 有些特殊, 需要区分事件触发词和事件角色. 触发词为抽取(event type, trigger), event type为特殊Token <code>[LM]</code>, 事件角色为抽取(event role, trigger, mention), event role为特殊Token <code>[LR]</code>. 因为同一个事件的触发词是相同的, 因此触发词和不同事件角色可以被解码到同一事件里面.</li><li><strong>MRC &amp; QA</strong>: 抽取(answer)元组, 简单粗暴.</li><li><strong>Classification</strong>: 抽取(class)元组, 即特殊Token <code>[LC]</code>.</li></ul><h4 id="Multi-slot-Tuple-and-Multi-span-Cyclic-Graph"><a href="#Multi-slot-Tuple-and-Multi-span-Cyclic-Graph" class="headerlink" title="Multi-slot Tuple and Multi-span Cyclic Graph"></a>Multi-slot Tuple and Multi-span Cyclic Graph</h4><p>作者设计了三种连接来确定循环图, 从而抽取Multi-Slot Tuple:</p><ul><li><strong>Jump Connection</strong>: 蓝虚线, 用于连接Tuple里的不同槽位.</li><li><strong>Consecutive Connection</strong>: 黑实线, 用于连接同一个实体的不连续Span.</li><li><strong>Tail-to-Head Connection</strong>: 粉实线, 用于构成闭环, 从Tuple的最后一个元素指向首个元素, 它决定了图的边界.</li></ul><blockquote><p>其实Jump Connection和Tail-to-Head  Connection就是<a href="https://adaning.github.io/posts/37376.html#NER-as-Word-Word-Relation-Classification">W2NER</a>里面的NNW和THW.</p></blockquote><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie-next10.png" style="zoom: 55%;" /><h4 id="Model-Structure"><a href="#Model-Structure" class="headerlink" title="Model Structure"></a>Model Structure</h4><p>模型结构是比较简单的. 用BERT-like的PLM获得每个Token的表示$h_i$ 后, 用两个FFNN和双仿射构成三张二维表, 用于分别得到作者设计的三种连接:</p><p>$$<br>\begin{gathered}<br>\tilde{h}_i=\operatorname{FFNN}_s\left(h_i\right), \quad \tilde{h}_j=\mathrm{FFNN}_e\left(h_j\right) \\<br>p_{i j}^k=\operatorname{sigmoid}\left(\tilde{h}_i^{\top} U \tilde{h}_j / \sqrt{d_h}\right),<br>\end{gathered}<br>$$</p><p>其中, FFNN为附带<a href="https://spaces.ac.cn/archives/8265" target="_blank" rel="noopener">苏神RoPE</a>的FFNN. $U \in \mathbb{R}^{d \times 3 \times d }$ 为双仿射矩阵, 3为构成循环图的三种连接.</p><p>由于是二分类任务, 用<a href="https://spaces.ac.cn/archives/7359" target="_blank" rel="noopener">苏神的多标签损失函数</a>优化模型:</p><p>$$<br>\mathcal{L}(i, j)=\log \left(1+\sum_{\Omega_{\mathrm{neg}}} e^{p_{i j}^k}\right)+\log \left(1+\sum_{\Omega_{\mathrm{pos}}} e^{-p_{i j}^k}\right)<br>$$</p><p>其中$\Omega_{\mathrm{neg}}$ 为负例($\mathcal{A}_{ij}^k=0$), $\Omega_{\mathrm{pos}}$ 为正例($\mathcal{A}_{ij}^k=1$).</p><blockquote><p>虽然性能也不够亮眼, 但Mirror有自己的优势, 比如可以处理不连续的Span, 这是很多基于Span的UIE方法没有考虑到的.</p></blockquote><h2 id="RexUIE-A-Recursive-Method-with-Explicit-Schema-Instructor-for-Universal-Information-Extraction"><a href="#RexUIE-A-Recursive-Method-with-Explicit-Schema-Instructor-for-Universal-Information-Extraction" class="headerlink" title="RexUIE: A Recursive Method with Explicit Schema Instructor for Universal Information Extraction"></a>RexUIE: A Recursive Method with Explicit Schema Instructor for Universal Information Extraction</h2><p>最后一个方法<strong>RexUIE</strong>, 论文出自<strong>EMNLP Findings 2023</strong>,  <a href="https://aclanthology.org/2023.findings-emnlp.1024/" target="_blank" rel="noopener">RexUIE: A Recursive Method with Explicit Schema Instructor for Universal Information Extraction</a>.</p><p><strong>RexUIE</strong>(<strong>R</strong>ecursive Method with <strong>Ex</strong>plicit Schema Instructor for <strong>UIE</strong>)单从名字上便可以看出它的特性, 它能够<strong>递归</strong>的根据任意形式的Schema抽取信息. </p><p>先前的UIE方法只能判断两个Span之间的关系, 而不能处理更多元(例如四元组, 甚至是五元组)的关联. </p><p>并且, 对于显式的Schema限制, 之前的一些UIE方法也没有做干涉. 比如关系类型”work for”的头尾实体类型必须分别为”person”和”organization”. 这对模型泛化和低资源场景有害.</p><p>综上, 作者提出RexUIE, 可以处理任何形式的Schema:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie-next11.png" style="zoom:50%;" /><p>根据Schema, RexUIE有:</p><ol><li>抽取出实体类型为”Person”的实体<code>Leonard Parker</code>.</li><li>抽取出关系类型为”Educated At”的尾实体<code>Harvard University</code>, 并根据关系类型对实体类型的限制, 同时得到<code>Harvard University</code> 的实体类型为”University”.</li><li>继续根据Schema和已知的元组(<code>Leonard Parker</code>, “Person”), (<code>Harvard University</code> , “Educated At (University)”), 继续抽取出Span <code>Phd</code> 对应的类型为”Academic Degree”.</li></ol><blockquote><p>其实这种层次化抽取的思路并不稀奇, <a href="https://adaning.github.io/posts/11838.html#Paddle-UIE">Paddle-UIE</a>实现里面用的就是这种.</p></blockquote><h3 id="Redefine-Universal-Information-Extraction"><a href="#Redefine-Universal-Information-Extraction" class="headerlink" title="Redefine Universal Information Extraction"></a>Redefine Universal Information Extraction</h3><p>对于$n$ 个Span的文本$\mathbf{s} = [s_1, s_2, \dots, s_n]$ 和其对应的Schema定义的类型$\mathbf{t} = [t_1, t_2, \dots, t_n]$, UIE目标是要抽取出每个$(s_i, t_i)$ Pair. 形式化的, 需要最大化如下概率:</p><p>$$<br>\begin{aligned}<br>&amp; \prod_{(\mathbf{s}, \mathbf{t}) \in \mathbb{A}} p\left((\mathbf{s}, \mathbf{t}) \mid \mathbf{C}^n, \mathbf{x}\right) \\<br>= &amp; \prod_{(\mathbf{s}, \mathbf{t}) \in \mathbb{A}} \prod_{i=1}^n p\left((s, t)_i \mid(\mathbf{s}, \mathbf{t})_{&lt;i}, \mathbf{C}^n, \mathbf{x}\right) \\<br>= &amp; \prod_{i=1}^n\left[\prod_{(s, t)_i \in \mathbb{A}_i \mid(\mathbf{s}, \mathbf{t})_{&lt;i}} p\left((s, t)_i \mid(\mathbf{s}, \mathbf{t})_{&lt;i}, \mathbf{C}^n, \mathbf{x}\right)\right]<br>\end{aligned}<br>$$</p><p>其中, $\mathbf{C}^n$ 为深度为$n$ 的树形层次化Schema, $\mathbb{A}$ 为标注了信息的序列. 所以从公式中来看, 其实就是按照层次结构抽取每一层的信息, 更深层的信息可以在浅层Schema的约束下得到.</p><h3 id="RexUIE"><a href="#RexUIE" class="headerlink" title="RexUIE"></a>RexUIE</h3><p>RexUIE用循环递归的Query来不断抽取信息, 并将每层Schema对应Span类型的预测结果添加到最终结果集中, 即图中的ESI:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie-next12.png" style="zoom: 45%;" /><h4 id="Framework-of-RexUIE"><a href="#Framework-of-RexUIE" class="headerlink" title="Framework of RexUIE"></a>Framework of RexUIE</h4><p>对于第$i$ 个Query $Q_i$, 其表征$h_i$ 可以用PLM作为Encoder得到:</p><p>$$<br>h_i=\operatorname{Encoder}\left(Q_i, P_i, M_i\right)<br>$$</p><p>$P_i, M_i$ 分别为$Q_i$ 对应的Prompt和Mask. $Q_i$ 的构成在下面会提到.</p><p>接着用两个FFNN来构成一张Query $Q_i$ 对应的二维得分矩阵$Z_i$:</p><p>$$<br>Z_i^{j, k}=\left(\mathbf{F F N N}_q\left(h_i^j\right)^{\top} \mathbf{R}\left(P_i^k-P_i^j\right)\mathbf{F F N N}_k\left(h_i^k\right)\right) \otimes M_i^{j, k}<br>$$</p><p>$\mathbf{R}\left(P_i^k-P_i^j\right)$为<a href="https://spaces.ac.cn/archives/8265" target="_blank" rel="noopener">苏神提出的RoPE</a>, 用来表征位置$P_i^k$, $P_i^j$ 之间的相对位置信息.</p><p>在获得第$i$ 个Query对的得分矩阵$Z_i$ 后, 直接应用解码, 并将解码得到的结果$Y_i$ 添加到信息抽取结果集$\mathcal{Y} = \set{Y_1, Y_2, \dots}$ 中. $Y_i$ 会被用于构成下一轮迭代的Query $Q_{i+1}$.</p><p>最后, 用<a href="https://spaces.ac.cn/archives/7359" target="_blank" rel="noopener">苏神的类别不平衡Loss</a>来优化RexUIE:</p><p>$$<br>\begin{gathered}<br>\mathcal{L}_i=\log \left(1+\sum_{\hat{Z}_i^j=0} e^{\bar{Z}_i^j}\right)+\log \left(1+\sum_{\hat{Z}_i^k=1} e^{-\bar{Z}_i^k}\right) \\<br>\mathcal{L}=\sum_i \mathcal{L}_i<br>\end{gathered}<br>$$</p><p>$\bar{Z}_i$ 为展平后的$Z_i$ , $\hat{Z}_i$ 则为展平后的Binary Ground Truth.</p><h4 id="Explicit-Schema-Instructor"><a href="#Explicit-Schema-Instructor" class="headerlink" title="Explicit Schema Instructor"></a>Explicit Schema Instructor</h4><p>对于输入的文本$\mathbf{x}$ 和第$i$ 个Query抽取出的结果$(\mathbf{s}, \mathbf{t})_{&lt;i}$, 直接将抽取结果$p_i$ 和一个特殊Token <code>[P]</code>作为Prefix拼在Type之前, 形成$Q_i$, 如下所示:</p><p>$$<br>Q_i=[\mathrm{CLS}][\mathrm{P}] p_i[\mathrm{T}] t_i^1[\mathrm{T}] t_i^2 \ldots[\mathrm{Text}] x_0 x_1 \ldots<br>$$</p><blockquote><p>如果觉得有点困惑, 可以阅读完下面两小节后看例子.</p></blockquote><h4 id="Token-Linking-Operations"><a href="#Token-Linking-Operations" class="headerlink" title="Token Linking Operations"></a>Token Linking Operations</h4><p>下面该说说如何在二维表上完成标注, 从而抽取Span对应的Type, 即抽取每个$(s_i, t_i)$了.</p><p>在RexUIE中, 表填充的结果$\hat{Z}$ 是由二维得分矩阵$Z$ 通过阈值$\delta$ 得到的一个二元标注矩阵:</p><p>$$<br>\tilde{Z}^{i, j}=\left\{\begin{array}{cc}<br>1 &amp; \text { if } Z^{i, j} \geq \delta \\<br>0 &amp; \text { otherwise }<br>\end{array}\right.<br>$$</p><p>当Linking满足作者设计的三种Linking的方式时, 二元标注矩阵中的对应位置为1, 即Linking有效:</p><ul><li><strong>Token Head-Tail Linking</strong>: 还是用来检测Span. 判断Span的起止位置.</li><li><strong>Token Head-Type Linking</strong>: Span的起始位置链接到对应类别的特殊Token <code>[T]</code>.</li><li><strong>Type-Token Tail Linking</strong>: 类别特殊Token<code>[T]</code> 链接到对应Span的结束位置.</li></ul><p>下面来看一个例子, 假如任务给定的Schema如下:</p><pre><code>{“person”: {“work for (organization)”: null}, “organization”: null }</code></pre><p>对应的得分矩阵的Ground Truth如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie-next13.png" style="zoom:50%;" /><p>绿色区域为Token Head-Type Linking, 蓝色为Type-Token Tail Linking, 橘色为Token Head-Tail Linking. </p><p>而左侧的矩阵为抽取第一层Schema时的序列, 即没有Prefix, 并且要抽取的类型就是Schema的最外圈, “person”和”organization”. 所以第一步实际上在做NER.</p><p>右侧矩阵为抽取Schema的第二层, 即”work for (organization)”的抽取, 能看到上一层的抽取结果”person: <code>Steve Jobs</code>“ 被补充到了Prefix里面, 并将要抽取的类型做了替换. 第二步实际上在做RE.</p><blockquote><p>USM的标注方法比RexUIE要更加复杂, 这是因为USM要求所有Linking被一次性并行标注, 但实际上解决”Span之间的关联”的问题被RexUIE在递归的过程中被巧妙的分解了, 拆解为Span本身和其类型的抽取过程.</p></blockquote><h4 id="Prompts-Isolation"><a href="#Prompts-Isolation" class="headerlink" title="Prompts Isolation"></a>Prompts Isolation</h4><p>若在结构化Schema的某一层中有多条预测结果, 那在构成Query的时候就会有多条Query, 编码和预测很耗时. 为了节省时间, RexUIE索性直接简单的将多个Prefix Group放到同一个Query中.</p><p>除此外, Prefix和Prompt比较混乱, 都会对其他内容产生互相影响. 所以RexUIE中做了如下限制:</p><ul><li><strong>Token Type ids</strong>: 将用Token Type ids区分Prompt和正文$\mathbf{x}$.</li><li><strong>Position ids</strong>: 对不同的Type给予相同的起始位置编码. 不同的Prefix Group的位置编码都是独立的. </li><li><strong>Attention Mask</strong>: 对于Mask来说, 不同的Type之间是不可见的, 不同Prefix Group之间的Token也是不可见的.</li></ul><p>上述过程如下所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie-next14.png" style="zoom:50%;" /><p>明显的观察到两个Prefix Group之间是不可见的, 并且具有均为起始为1的位置编码. 每组中构成Query的上次预测结果$p, q$ 是能够对所在组的Type给予注意力的.</p><p>因此, 在打分矩阵完成预测时, 可以直接同时处理多组Prefix构成的Query.</p><p>下面可以举个作者附录中的例子, 比如在CoNLL04上完成RE. </p><p>结构化Schema为:</p><pre><code>{“organization”: {“organization in ( location )”: null}, “other”: null, “location”: {“located in ( location )”: null}, “people”: {“live in ( location )”: null, “work for ( organization )”: null, “kill ( people )”: null}}</code></pre><p>首先按最外层Schema判断Span的类型, 其实在这里先进行的是NER:</p><pre><code>[CLS][P][T] location[T] organization[T] other[T] people[Text] The self-propelled rig Avco 5 was headed to shore with 14 people aboard early Monday when it capsized about 20 miles off the Louisiana coast , near Morgan City , Lifa said.[SEP]</code></pre><p>抽取得到(<code>Morgan City</code>, “location”), (<code>Louisiana</code>, “location”), (<code>Lifa</code>, “people”). 如此的三个元组便对应了三个Prefix Group. 接着根据关系类型对实体类型的限制, 判断第一层抽取结果对应的第二层Schema类型. 例如”location”对应的Schema第二层为”located in ( location )”, 而”person”对应的有三个类型”live in ( location )”, “work for ( organization )”和”kill ( people )”. 将第二层抽取组成如下的Query, 再次进行抽取:</p><pre><code>[CLS][P] location: Morgan City[T] located in ( location )[P] location: Louisiana[T] located in ( location )[P] people: Lifa[T] kill ( people )[T] live in ( location )[T] work for ( organization )[Text] The self-propelled rig Avco 5 was headed to shore with 14 people aboard early Monday when it capsized about 20 miles off the Louisiana coast , near Morgan City , Lifa said.[SEP]</code></pre><p>从例子中不难看到, RexUIE的抽取内容全部来自Schema, 所以它对Span的抽取是<strong>严格遵守Schema限制</strong>的.</p><h4 id="Pre-training"><a href="#Pre-training" class="headerlink" title="Pre-training"></a>Pre-training</h4><p>RexUIE预训练数据由三部分组成:</p><ul><li>$\mathcal{D}_{distant}$: WikiPedia的远程监督数据.</li><li>$\mathcal{D}_{superv}$: IE任务的公开数据集.</li><li>$\mathcal{D}_{mrc}$: 和USM一样, 采用了MRC的数据来增强模型对Prompt的理解能力.</li></ul><p>RexUIE在性能上要优于<strong>USM</strong>. 尤其是在Few - Shot上比USM提升要更显著. </p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>由于信息抽取领域存在大量的共性, 因此各类IE任务被统一是大势所趋. 它们都可以被抽象为找Span, 判断Span之间的关联, 进而转换为各类花式分类问题, 当然也可以通过Generative的方法来完成. 全监督设置已经不能满足大家疯狂刷点, 由于各类任务大一统带来的多任务学习共享知识的能力, 也带着这个领域朝着Zero - Shot和Few - Shot迈进. 至此, 通用信息抽取篇结束.</p><p>如果需要了解更多关于通用信息抽取领域前沿, 还可以阅读:</p><ul><li><a href="https://github.com/zjunlp/Low-resource-KEPapers" target="_blank" rel="noopener">GitHub - zjunlp/Low-resource-KEPapers: A Paper List of Low-resource Information Extraction</a>.</li></ul><hr><p>最后, 是一些个人想法. </p><p>其实按照正向思路, 是在LLM时代期待一下LLM在IE Task上的应用, 最起码在目前来看LLM在IE任务上做的并不够好, 还有提升空间. 但是更多时候, 如果下游任务的知识已经蕴含在LLM内部了, 真的有必要做IE吗? </p><p>如果反过来思考一下, IE Task经历了这么长的时间, 早就已经相对趋于成熟. </p><p><strong>所以在这里说一个暴论, IE任务从文本而来, 那最终形态还得再回到文本里去. 从IE Task能为LLM带来什么的角度思考问题会更有价值, 也更有可能是未来IE的出路.</strong> 比如在IE有监督指导下的数据合成, 或许能够保证LLM的训练数据更加安全可靠, 从而体现出IE本身的价值.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NER </tag>
            
            <tag> EE </tag>
            
            <tag> IE </tag>
            
            <tag> UIE </tag>
            
            <tag> RE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>通用信息抽取(上) - UIE, USM, InstructUIE</title>
      <link href="/posts/11838.html"/>
      <url>/posts/11838.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>2024.5.27: 稍微补充了UIE的其中一个改进版MetaRetriever.</p><p>本文前置知识:</p><ul><li><a href="https://adaning.github.io/posts/58173.html">T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>.</li></ul><p>扩展阅读:</p><ul><li><a href="https://adaning.github.io/posts/4431.html">UniRel: Unified Representation and Interaction for Joint Relational Triple Extraction</a>. </li></ul></blockquote><h1 id="通用信息抽取-上-UIE-USM-InstructUIE"><a href="#通用信息抽取-上-UIE-USM-InstructUIE" class="headerlink" title="通用信息抽取(上) - UIE, USM, InstructUIE"></a>通用信息抽取(上) - UIE, USM, InstructUIE</h1><p>本文为介绍<strong>通用信息抽取领域</strong>经典模型的上篇, 介绍了<strong>UIE</strong>, <strong>USM</strong>, <strong>InstructUIE</strong>三个模型:</p><ul><li><strong>UIE</strong>: <strong>ACL 2022</strong>, <a href="https://aclanthology.org/2022.acl-long.395/" target="_blank" rel="noopener">Unified Structure Generation for Universal Information Extraction</a>. </li><li><strong>USM</strong>: <strong>AAAI 2023</strong>, <a href="https://ojs.aaai.org/index.php/AAAI/article/view/26563" target="_blank" rel="noopener">Universal Information Extraction as Unified Semantic Matching</a>.</li><li><strong>InstructUIE</strong>: 暂挂arxiv, <a href="https://arxiv.org/abs/2304.08085" target="_blank" rel="noopener">InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction</a>.</li></ul><p>三个模型文中报告的实验效果依次递增, 碍于篇幅原因, <strong>本文不包含对实验部分的解读</strong>, 对实验感兴趣的读者还请自行阅读.</p><h2 id="UIE-Unified-Structure-Generation-for-Universal-Information-Extraction"><a href="#UIE-Unified-Structure-Generation-for-Universal-Information-Extraction" class="headerlink" title="UIE: Unified Structure Generation for Universal Information Extraction"></a>UIE: Unified Structure Generation for Universal Information Extraction</h2><p>第一个模型叫做UIE, 论文出自<strong>ACL 2022</strong>, <a href="https://aclanthology.org/2022.acl-long.395/" target="_blank" rel="noopener">Unified Structure Generation for Universal Information Extraction</a>. </p><p>UIE算是打开了近年通用信息抽取的新时代. </p><p>我印象中, 当时是Generative Information Extraction刚冒出头的时候. 比较出名的有NER里面的<a href="https://aclanthology.org/2021.acl-long.451/" target="_blank" rel="noopener">BARTNER</a>, 后来这种方法也被迁移到ABSA中叫<a href="https://aclanthology.org/2021.acl-long.188" target="_blank" rel="noopener">BARTABSA</a>, 它们可以直接从索引中抽取Span, 得到Target. 还有一类是直接做Text2Text的, 比如在EE里面的<a href="https://aclanthology.org/2021.acl-long.217/" target="_blank" rel="noopener">TEXT2EVENT</a>, <a href="https://arxiv.org/abs/2101.05779" target="_blank" rel="noopener">TANL</a>, RE里的<a href="https://aclanthology.org/2021.findings-emnlp.204/" target="_blank" rel="noopener">REBEL</a> 等等, 按照结构化的规则用<strong>Language Modeling</strong>的方式生成Target. 上述两种方法虽然略有不同, 但它们都遵循着将<strong>输出结构线性化, 并结构化生成</strong>的原则, 来处理各类IE task.</p><blockquote><p>这种生成式IE的发展, 和当时<strong>T5</strong>走上舞台以及人们对<strong>In Context Learning</strong>的探索脱不开关系.</p></blockquote><p>那么, 各类任务都可以通过Generative的方式来实现, 这些任务是不是也可以统一到一起呢? </p><p>作者认为, 各类任务是可以在任务形式上得到统一的:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie1.png" style="zoom: 80%;" /><p>很明显的, 对于特定任务下的IE, 它们可以通过作者设定的结构化生成方式, 来将它们完成统一, 以此来利用不同IE task下跨任务的知识, 从而达到只用一个模型来兼顾各种任务的效果.</p><h3 id="Unified-Structure-Generation-for-Universal-Information-Extraction"><a href="#Unified-Structure-Generation-for-Universal-Information-Extraction" class="headerlink" title="Unified Structure Generation for Universal Information Extraction"></a>Unified Structure Generation for Universal Information Extraction</h3><h4 id="Structured-Extraction-Language-for-Uniform-Structure-Encoding"><a href="#Structured-Extraction-Language-for-Uniform-Structure-Encoding" class="headerlink" title="Structured Extraction Language for Uniform Structure Encoding"></a>Structured Extraction Language for Uniform Structure Encoding</h4><p>各类IE任务都可以被转化为text-to-structure的形式, 而任意IE结构生成的过程可以被拆分为两个原子操作:</p><ul><li><strong>Spotting</strong>: 从句子中<strong>定位</strong>任务需要的目标信息片段, 比如一个实体或者触发词的位置.</li><li><strong>Associating</strong>: 判断两个片段之间的<strong>关联</strong>, 比如实体之间的关系, 或者是事件中每个论元扮演的事件角色.</li></ul><p>作者将这两个操作在文本结构化生成中体现出来:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie2.png" style="zoom:67%;" /><p>上述结构化的文本被作者称为<strong>S</strong>tructured <strong>E</strong>xtraction <strong>L</strong>anguage(<strong>SEL</strong>). 所以SEL就是要找到<code>Spot Name</code>的Span, 并且生成Span和Span之间的<code>Asso Name</code>. 作者通过这种<strong>嵌套的结构化文本</strong>来表征任意的IE任务目标.</p><p>作者在这里展示了一个例子, 对于句子<code>Steve became CEO of Apple in 1997.</code>, 我们来一起完成RE, EE, NER三个经典IE任务:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie3.png" style="zoom:67%;" /><ul><li>蓝色: RE, 抽取出头实体<code>Steve</code>, 尾实体<code>Apple</code>, 他俩之间的关系, 也就是<code>Asso Name</code> 是<code>work for</code>.</li><li>红色: EE, 抽取出事件类型<code>start-position</code>, 触发词是<code>became</code>, 这个事件下对应的三个论元<code>Steve</code>, <code>Apple</code>, <code>1997</code> 分别扮演<code>employee</code>, <code>employer</code>, <code>time</code> 的事件角色. <strong>所以这里的触发词是以SPOT结构存在的</strong>.</li><li>黑色: NER, 实际上还有蓝色中也有一部分. 能抽取出<code>Steve</code>, <code>Apple</code>, <code>1997</code> 分别为<code>person</code>, <code>organization</code>, <code>time</code> 类型的实体.</li></ul><p>所以, SEL可以把任意的IE任务拆解为<strong>找Span</strong>, 和<strong>判断Span之间的关系</strong>这两个最小的操作.</p><h4 id="Structural-Schema-Instructor-for-Controllable-IE-Structure-Generation"><a href="#Structural-Schema-Instructor-for-Controllable-IE-Structure-Generation" class="headerlink" title="Structural Schema Instructor for Controllable IE Structure Generation"></a>Structural Schema Instructor for Controllable IE Structure Generation</h4><p>在上面一节中可以发现, 不同的任务有不同的Schema, 所以必须要用某种方式把Schema引入, 并令它控制要抽取的内容. 所以作者提出了<strong>S</strong>tructural <strong>S</strong>chema <strong>I</strong>nstructor(<strong>SSI</strong>), 很自然的就能想到把Schema作为Prompt(在这里也可以称为<strong>Prefix</strong>)放到模型当中. </p><p>UIE整体的形式化为, 对于输入文本序列$x=[x_1, \dots, x_{|x|}]$, 和结构化的Schema $s=[s_1, \dots, s_{|s|}]$ , UIE直接将Schema和文本拼接到一起作为输入:</p><p>$$<br>y = \text{UIE}(s \oplus x)<br>$$</p><p>$y=[y_1, \dots, y_{|y|}]$ 为SEL序列.</p><h5 id="Structural-Schema-Instructor"><a href="#Structural-Schema-Instructor" class="headerlink" title="Structural Schema Instructor"></a>Structural Schema Instructor</h5><p>SSI的目的是描述任务, 并引导UIE抽取出任务需要的SEL. 所以它包含三个部分:</p><ul><li>SPOTNAME: 比如NER中的<code>person</code>.</li><li>ASSONAME: 比如RE中的<code>work for</code>.</li><li>特殊Token: 比如<code>[spot]</code>, <code>[asso]</code>, <code>[text]</code>, 用来区分输入文本中的内容到底是归属于哪个部分. </li></ul><p>输入文本Text加上SSI以后的UIE在做RE, EE, NER的例子如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie4.png" style="zoom:67%;" /><p>UIE的完整输入形式化描述如下:</p><p>$$<br>\begin{aligned}<br>s \oplus x= &amp; {\left[s_1, s_2, \ldots, s_{|s|}, x_1, x_2, \ldots, x_{|x|}\right] } \\<br>= &amp; {[[\text{ spot }], \ldots[\text { spot }] \ldots, } \\<br>&amp; {[\text { asso }], \ldots,[\text { asso }] \ldots, } \\<br>&amp; {\left.[\text { text }], x_1, x_2, \ldots, x_{|x|}\right] }<br>\end{aligned}<br>$$</p><blockquote><p>这种SSI的方式在面对特别多数量的Spot或者Asso的时候, 会导致序列长度变得特别长, 附带一大坨东西, 详情可以看原论文的附录Table 11. 如果碰到这种情况, 对显存是不太友好的.</p></blockquote><h5 id="Structure-Generation-with-UIE"><a href="#Structure-Generation-with-UIE" class="headerlink" title="Structure Generation with UIE"></a>Structure Generation with UIE</h5><p>对于给定的SSI $s$ 和输入句子$x$, UIE用<strong>Encoder-Decoder</strong>架构(T5, BART等为代表)来实现从 <code>SSL + Text -&gt; SEL</code> 的转换. 作者在这里也是认为, 直接将结构化结果以生成式范式输出的好处在于, 可以直接<strong>共享预训练模型中的知识</strong>. 比如<code>location</code> 既可以表征它的自然语义, 也作为<code>[asso]</code>, 所以文本和标签的知识是可以被共享的. </p><p>首先用Encoder得到输入句子的每个Token的表示$\mathbf{H}=\left[\mathbf{s}_1, \ldots, \mathbf{s}_{|s|}, \mathbf{x}_1, \ldots, \mathbf{x}_{|x|}\right]$:</p><p>$$<br>\mathbf{H}=\operatorname{Encoder}\left(s_1, \ldots, s_{|s|}, x_1, \ldots, x_{|x|}\right)<br>$$</p><p>然后是用Decoder自回归的完成解码:</p><p>$$<br>y_i, \mathbf{h}_i^d=\operatorname{Decoder}\left(\left[\mathbf{H} ; \mathbf{h}_1^d, \ldots, \mathbf{h}_{i-1}^d\right]\right)<br>$$</p><p>在输出<code>&lt;eos&gt;</code> 后表示输出终止.</p><h3 id="Pre-training-and-Fine-tuning-for-UIE"><a href="#Pre-training-and-Fine-tuning-for-UIE" class="headerlink" title="Pre-training and Fine-tuning for UIE"></a>Pre-training and Fine-tuning for UIE</h3><h4 id="Pre-training-Corpus-Construction"><a href="#Pre-training-Corpus-Construction" class="headerlink" title="Pre-training Corpus Construction"></a>Pre-training Corpus Construction</h4><p>在预训练阶段的主要语料由以下三部分组成:</p><ul><li>$\mathcal{D}_{\text{pair}}$ 是按照Text-Structure准备好的平行语料, 记为$\mathcal{D}_{\text{pair}}=\set{(x, y)}$. 来自Wikipedia, 用于UIE的预训练, 赋予UIE这种文本转结构化输出的能力.</li><li>$\mathcal{D}_{\text{record}}$ 是只有$y$ 的结构化数据(也就是文中所说的SEL), 来自ConceptNet和Wikidata. </li><li>$\mathcal{D}_{\text{text}}$ 是无结构化文本, 来自Wikipedia.</li></ul><h4 id="Pre-training"><a href="#Pre-training" class="headerlink" title="Pre-training"></a>Pre-training</h4><p>作者以<strong>T5-v1.1</strong>为预训练模型, 完成Seq2Seq任务. 对于上述三种不同的数据格式构成的数据源, 作者分别拿它们过来做了不同的预训练:</p><ul><li>Text-to-Structure Pre-training using $\mathcal{D}_{\text{pair}}$: 作者认为, 只用Positive Schema可能会导致UIE的泛化能力不足, 所以作者还构建了<strong>Negative Schema</strong>来增强UIE的泛化能力. 记Positive Schema为$s_{+}=s_{\mathrm{S}+} \cup s_{\mathrm{a}+}$, 作者随机采样出Negative Spot $s_{\mathrm{S}-}$ 和Negative Asso $s_{\mathrm{a}-}$, 从而构建出Meta Schema $s_{\text{meta}}= s_{+} \cup s_{\mathrm{S}-} \cup s_{\mathrm{a}-}$, 然后在Meta Schema上完成训练:</li></ul><p>$$<br>\mathcal{L}_{\text {Pair }}=\sum_{(x, y) \in \mathcal{D}_{\text {pair }}}-\log p\left(y \mid x, s_{\text {meta }} ; \theta_e, \theta_d\right)<br>$$</p><p>​        其中$\theta_e, \theta_d$ 分别为Encoder和Decoder的参数.</p><ul><li>Structure Generation Pre-training with $\mathcal{D}_{\text{record}}$: 因为$\mathcal{D}_{\text{record}}$ 只有$y$, 所以UIE Decoder这时候直接单独拿出来用, 当成传统意义上的Language Model来用, 直接生成SEL:</li></ul><p>$$<br>\mathcal{L}_{\text {Record }}=\sum_{y \in \mathcal{D}_{\text {record }}}-\log p\left(y_i \mid y_{&lt;i} ; \theta_d\right)<br>$$</p><ul><li>Retrofitting Semantic Representation using $\mathcal{D}_{\text{text}}$: $\mathcal{D}_{\text{text}}$ 是无结构化文本$x$. 在text-to-structure pre-training时, 作者也像T5一样使用了MLM的Denoising task, 来将损坏的文本恢复出来: </li></ul><p>$$<br>\mathcal{L}_{\text {Text }}=\sum_{x \in \mathcal{D}_{\text {text }}}-\log p\left(x^{\prime \prime} \mid x^{\prime} ; \theta_e, \theta_d\right)<br>$$</p><p>​        其中$x^\prime$ 为Corrupted Text, $x^{\prime\prime}$ 为重建的目标Span. </p><blockquote><p><strong>作者提到, 这个任务可以有效的解决对<code>SPOTNAME</code> 和 <code>ASSONAME</code> Token语义的灾难性遗忘</strong>. </p><p>作者没有详细解释, 我个人的理解是, 因为Label和Text的知识被共享, 也确实插入了<code>[spot], [asso], [text]</code> 这样的Token来区分某个Token到底是Text还是SSI, 但<strong>Text仍然可能被插入的SSI所污染</strong>, 而且也<strong>容易导致Token在SSI和Text之间的语义混淆</strong>. <strong>MLM任务执行的时候是没有SSI插入的</strong>, 所以我更倾向于认为, 这里用一个MLM的任务是为了保证<strong>Token作为纯文本时的语义</strong>能被一定程度的保留. 而且这个任务是继承T5的, 感觉还是为了保持预训练时候的一些能力.</p></blockquote><p>如果把<code>(SSI, Input, Output)</code>看成一个三元组, 可以将上面三个任务归纳一下:</p><table><thead><tr><th>Data</th><th>SSI</th><th>Input</th><th>Output</th></tr></thead><tbody><tr><td>$\mathcal{D}_{\text{pair}}$</td><td>$s$</td><td>$x$</td><td>$y$</td></tr><tr><td>$\mathcal{D}_{\text{record}}$</td><td>None</td><td>None</td><td>$y$</td></tr><tr><td>$\mathcal{D}_{\text{text}}$</td><td>None</td><td>$x^\prime$</td><td>$x^{\prime\prime}$</td></tr></tbody></table><p>最后的Loss就是上面说的三者加在一起:</p><p>$$<br>\mathcal{L}=\mathcal{L}_{\text {Pair }}+\mathcal{L}_{\text {Record }}+\mathcal{L}_{\text {Text }}<br>$$</p><h4 id="On-Demand-Fine-tuning"><a href="#On-Demand-Fine-tuning" class="headerlink" title="On-Demand Fine-tuning"></a>On-Demand Fine-tuning</h4><p>在预训练过后, UIE需要在某个IE Task上做微调. 对于给定的有标签的语料$\mathcal{D}_{\text {Task }}=\set{(s, x,  y)}$, 直接在语料上用Teacher-Forcing做Finetune即可:</p><p>$$<br>\mathcal{L}_{\mathrm{FT}}=\sum_{(s, x, y) \in \mathcal{D}_{\text {Task }}}-\log p\left(y \mid x, s ; \theta_e, \theta_d\right)<br>$$</p><p>为了<strong>缓解曝光偏差</strong>问题, 作者提出了<strong>拒绝机制</strong>. 即在SEL $y$ 中, 以概率$p_\epsilon$ 随机的插入一些Negative spot和Negative asso, 并将它们对应的info span设定为<code>[NULL]</code>: </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie5.png" style="zoom:80%;" /><p>如上例中, 如果模型错误的生成了<code>SPOTNAME: facility</code>, 模型仍然在自回归生成的下一步有可能生成<code>[NULL]</code> 来<strong>撤销</strong>这次错误的生成.</p><h3 id="Paddle-UIE"><a href="#Paddle-UIE" class="headerlink" title="Paddle-UIE"></a>Paddle-UIE</h3><p>非常值得一提的是, 这篇工作的一部分是本文的一作和二作在百度实习时完成的, 所以百度在这篇文章中稿后, 也发布了<a href="https://github.com/PaddlePaddle/PaddleNLP/tree/develop/model_zoo/uie" target="_blank" rel="noopener">Paddle版本的UIE</a>, 是一个开箱即用的模型, 并且有各种不同的规模, 非常的方便, 读者可以自行尝试.</p><p>不过Paddle版本的UIE的实现方式并不同, 只是利用了UIE的大致思想, 通过构建Prompt的方式, 并用两个<a href="https://github.com/PaddlePaddle/PaddleNLP/blob/17acf221b44fb5c6284acd5e45ffeac243ead13c/paddlenlp/transformers/ernie/modeling.py#L1222C7-L1222C7" target="_blank" rel="noopener">FFN</a>来<a href="https://github.com/PaddlePaddle/PaddleNLP/blob/17acf221b44fb5c6284acd5e45ffeac243ead13c/model_zoo/uie/deploy/python/infer.py#L71" target="_blank" rel="noopener">轮询每个Schema Element在文中对应的Span</a>, 基座用的则是<strong>ERNIE</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie6.png" style="zoom:80%;" /><blockquote><p>虽然UIE非常简单, 说白了就是T5 + 结构化生成来做IE任务, 但它确实成为了近两年通用IE中非常具有里程碑意义的方法.</p><p>不得不说, 在LLM兴起的前夕这个节骨眼下, UIE算是给信息抽取任务统一打了个样.</p></blockquote><p>另外, 在<strong>ACL Findings 2023</strong>上也有一个基于检索的UIE改进方法<a href="https://aclanthology.org/2023.findings-acl.251/" target="_blank" rel="noopener">MetaRetriever</a>, 用Meta Learning教会模型从Knowledge Base里面检索知识, 然后用检索到的知识作为上下文再做UIE:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/uie7.png" style="zoom:80%;" /><p>所以它是一种检索增强的方法. 如果单从效果上来讲, 不如我们接下来要讲的UIE续作USM.</p><h2 id="USM-Universal-Information-Extraction-as-Unified-Semantic-Matching"><a href="#USM-Universal-Information-Extraction-as-Unified-Semantic-Matching" class="headerlink" title="USM: Universal Information Extraction as Unified Semantic Matching"></a>USM: Universal Information Extraction as Unified Semantic Matching</h2><p>第二个模型叫做USM, 论文出自<strong>AAAI 2023</strong>, <a href="https://ojs.aaai.org/index.php/AAAI/article/view/26563" target="_blank" rel="noopener">Universal Information Extraction as Unified Semantic Matching</a>.</p><p>名字上一看就知道, USM和UIE一脉相承, 事实上USM是<strong>UIE</strong>的续作.</p><p>USM认为, UIE是一个Seq2Seq的黑盒, 非常难以确定什么时候UIE的知识迁移有效, 什么时候无效, 所以需要对知识的显式建模, 以明确知识迁移的有效性, 鲁棒性和可解释性.</p><p>因此, USM将所有IE任务解耦为两个操作, Structuring和Conceptualizing:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/usm1.png" alt=""></p><ul><li><strong>Structuring</strong>: 从句子中抽取和标签无关的基本子结构, 其实就是<strong>抽取文本和文本对</strong>. 比如抽取Entity Mention <code>Monet</code>, 事件触发词<code>born in</code>, 或者某种关系的实体对<code>(Monet, Paris)</code>, 或者事件的论元<code>(born in, Paris)</code>.</li><li><strong>Conceptualizing</strong>: 将子结构与目标语义标签对应, 也就是<strong>对文本或者文本对的标签做判定</strong>. 比如说使得语义标签<code>person</code> 和Entity Mention <code>Monet</code> 相对应, 它的含义就是判断<code>Monet</code> 的实体类型为 <code>person</code>.</li></ul><blockquote><p>其实这两个操作和UIE中的Spotting和Associating的本质是一样的, 都是<strong>找Span</strong>, 和<strong>判断Span之间的关系</strong>. 或者甚至可以说, 信息抽取各类任务的目标就是在找Span和Span之间的关系. </p></blockquote><p>当标签以<strong>语义Token</strong>的形式给定的时候, 上述两个操作可以统一的被抽象成一个<strong>Directed Token Linking</strong>的操作, 并且可以使得所有IE Task共享知识. </p><h3 id="Unified-Semantic-Matching-via-Directed-Token-Linking"><a href="#Unified-Semantic-Matching-via-Directed-Token-Linking" class="headerlink" title="Unified Semantic Matching via Directed Token Linking"></a>Unified Semantic Matching via Directed Token Linking</h3><p>USM的Linking包含三种类型, 再通过Schema约束的解码就可以得到目标输出:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/usm2.png" style="zoom:67%;" /><h4 id="Schema-Text-Joint-Embedding"><a href="#Schema-Text-Joint-Embedding" class="headerlink" title="Schema-Text Joint Embedding"></a>Schema-Text Joint Embedding</h4><p>首先, 我们用一个Encoder来同时编码<strong>Schema Token</strong> $l=\set{l_1, l_2, \dots, l_{||}}$ 和<strong>Text Token</strong> $t=\set{t_1, t_2, \dots, t_{|t|}}$ 一并拼接后的序列, 得到一个Label-Text联合编码的表示$\mathbf{H}=\left[\mathbf{h}_1, \mathbf{h}_2, \dots, \mathbf{h}_{|l|+|t|}\right]$: </p><p>$$<br>\mathbf{H}=\operatorname{Encoder}\left(l_1, l_2, \ldots, l_{|l|}, t_1, t_2, \ldots, t_{|t|}, \mathbf{M}\right)<br>$$</p><p>其中$\mathbf{M} \in \mathbb{R}^{(|l| + |t|) \times (|l| + |t|)}$ 为Attention的Mask矩阵, 来控制每个Token是否能给予其他Token注意力.</p><h4 id="Directed-Token-Linking"><a href="#Directed-Token-Linking" class="headerlink" title="Directed Token Linking"></a>Directed Token Linking</h4><h5 id="Token-Token-Linking-for-Structuring"><a href="#Token-Token-Linking-for-Structuring" class="headerlink" title="Token-Token Linking for Structuring"></a>Token-Token Linking for Structuring</h5><p>我们在之前说过, Structuring需要抽取句中的<strong>文本</strong>和<strong>文本对</strong>, 它们分别被称为<strong>Utterance</strong>和<strong>Association Pair</strong>(也是Span Pair, 记为<code>(Subject, Object)</code>), 它们均可以用Token Linking的方式实现:</p><ul><li>Utterance的抽取即同一个连续Span的<strong>Head</strong> Token和<strong>Tail</strong> Token的Linking(<strong>H2T</strong>). 比如某个实体的Entity Mention.</li><li>Association Pair的抽取即<strong>两个存在关联的Span</strong>的<strong>Head</strong> Token的Linking(<strong>H2H</strong>)和<strong>Tail</strong> Token的Linking(<strong>T2T</strong>), 也就是<strong>头对头尾对尾</strong>. 比如关系三元组的实体对, 或者是事件中的触发词和论元对.</li></ul><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/usm3.png" style="zoom:67%;" /><p>综上, 每个Token Pair $\left\langle t_i, t_j\right\rangle$ 是否存在Token-Token Linking (H2T, H2H, T2T), 可以通过计算得分$\mathbf{s}_{\mathrm{TTL}}\left(t_i, t_j\right)$ 来判断:</p><p>$$<br>\mathbf{s}_{\mathrm{TTL}}\left(t_i, t_j\right)=\operatorname{FFNN}_{\mathrm{TTL}}^l\left(\mathbf{h}_t^i\right)^T \mathbf{R}_{j-i} \operatorname{FFNN}_{\mathrm{TTL}}^r\left(\mathbf{h}_t^j\right)<br>$$</p><p>其中, $\operatorname{FFNN}^{l/r}$ 为输出大小为$d$ 的前馈神经网络, $\mathbf{R}_{j-i} \in \mathbb{R}^{d \times d}$ 为<a href="https://spaces.ac.cn/archives/8265" target="_blank" rel="noopener">苏神提出的RoPE</a>, 用于编码Token之间的相对位置关系.</p><h5 id="Label-Token-Linking-for-Utterance-Conceptualizing"><a href="#Label-Token-Linking-for-Utterance-Conceptualizing" class="headerlink" title="Label-Token Linking for Utterance Conceptualizing"></a>Label-Token Linking for Utterance Conceptualizing</h5><p>下面需要对Label和Token之间做Token Linking, 来完成前面提到的Conceptualizing, 这一小节中先处理Utterance.</p><p>对于给定的Label Token Embedding $\mathbf{h}^l_1, \mathbf{h}^l_2, \dots, \mathbf{h}^l_{|l|}$ 和Text Token Embedding $\mathbf{h}^t_1, \mathbf{h}^t_2, \dots, \mathbf{h}^t_{|t|}$, Utterance Conceptualizing需要判断Label Token和Utterance Token或者Association Pair中的<strong>候选Object</strong>之间是否有链接. </p><p>这一操作的就把Text Mention指向了Label, 比如NER中的<code>(person, Monet)</code>, <code>(country, France)</code>, 或者EE中事件类型和触发词构成的元组<code>(born, born in)</code>, RE中关系类型和<strong>尾实体</strong>构成的元组<code>(birth place, Paris)</code>.</p><p>其实上面的例子包含了两种情况:</p><ul><li>把<strong>Span的标签</strong>直接分配给Span, 比如NER的Entity Type分配给Entity Mention.</li><li>把<strong>Span Pair之间的关系标签</strong>分配给<strong>Association Pair中的Object</strong>, 比如RE中把关系类型分配给Object.</li></ul><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/usm4.png" style="zoom:67%;" /><p>如果做Label和Span的Linking, 则需要分别链接<strong>Label Head</strong> Token和<strong>Span Head</strong> Token(<strong>L2H</strong>), <strong>Label Tail</strong> Token和<strong>Span Tail</strong> Token(<strong>L2T</strong>).</p><p>与前面类似的, 每个Label-Text Token Pair $\left\langle l_i, t_j\right\rangle$ 是否存在Label-Token Linking(L2H, L2T), 通过计算得分$\mathbf{s}_{\mathrm{LTL}}\left(l_i, t_j\right)$ 来判断:</p><p>$$<br>\mathbf{s}_{\mathrm{LTL}}\left(l_i, t_j\right)=\operatorname{FFNN}_{\mathrm{LTL}}^{\text {label }}\left(\mathbf{h}_i^l\right)^T \mathbf{R}_{j-i} \operatorname{FFNN}_{\mathrm{LTL}}^{\text {text }}\left(\mathbf{h}_j^t\right)<br>$$</p><h5 id="Token-Label-Linking-for-Pairing-Conceptualizing"><a href="#Token-Label-Linking-for-Pairing-Conceptualizing" class="headerlink" title="Token-Label Linking for Pairing Conceptualizing"></a>Token-Label Linking for Pairing Conceptualizing</h5><p>这节来处理Association Pair的概念化. </p><p>因为在Utterance Conceptualizing中, 我们已经建立了Label到Association Pair中的Object的链接了, 所以我们只需要将Association Pair中的Subject再有向链接到Label, 就可以根据多条路径来确定一个三元组<code>(Subject, Association, Object)</code>. </p><p>这三条路径分别是:</p><ul><li>Token-Token Linking构造的<code>(Subject, Object)</code>.</li><li>Label-Token Linking构造的<code>(Association, Object)</code>.</li><li>即将在Token-Label Linking中构造的<code>(Subject, Association)</code>.</li></ul><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/usm5.png" style="zoom:67%;" /><p>如果要从Span链接到Label, 则需要分别链接<strong>Span Head</strong> Token和<strong>Label Head</strong> Token(<strong>H2L</strong>), 以及<strong>Span Tail</strong> Token和<strong>Label Tail</strong> Token(<strong>T2L</strong>).</p><p>类似的, 每个Text-Label Token Pair $\left\langle t_i, l_j\right\rangle$ 是否存在Token-Label Linking(H2L, T2L), 通过计算得分$\mathbf{s}_{\mathrm{TLL}}\left(t_i, l_j\right)$ 来判断:</p><p>$$<br>\mathbf{s}_{\mathrm{TLL}}\left(t_i, l_j\right)=\operatorname{FFNN}_{\mathrm{TLL}}^{\mathrm{text}}\left(\mathbf{h}_i^l\right)^T \mathbf{R}_{j-i} \mathrm{FFNN}_{\mathrm{TLL}}^{\text {label }}\left(\mathbf{h}_j^t\right)<br>$$</p><h4 id="Schema-constraint-Decoding-for-Structure-Composing"><a href="#Schema-constraint-Decoding-for-Structure-Composing" class="headerlink" title="Schema-constraint Decoding for Structure Composing"></a>Schema-constraint Decoding for Structure Composing</h4><p>跟着作者来看一个具体的例子, 感受一下Linking和解码的流程:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/usm6.png" style="zoom:67%;" /><p>对于输入的Schema<code>[L] person [L] country [L] birth place [L] capital [T]</code> 和输入句子 <code>Monet was born in Paris, the capital of France</code>, 有:</p><ol><li>首先经过Token-Token Linking, 抽取得到<code>Monet</code>, <code>Paris</code>, <code>France</code>, <code>(Monet, Pairs)</code>, <code>(France, Pairs)</code>. </li><li>然后通过Label-Token Linking, 得到<code>(person, Monet)</code>, <code>(country, France)</code>, <code>(birth place, Paris)</code>, <code>(capital, Paris)</code>, 这样就得到了Label对应的候选Object.</li><li>最后用Token-Label Linking, 得到<code>(Monet, birth place)</code>, <code>(France, capital)</code>, 完成Subject和Label的链接.</li><li>由于第一步, 抽取得到<code>(Monet, birth place)</code>, <code>(France, capital)</code>. 并且基于第一步的结果, 抽取得到两个三元组<code>(Monet, birth place, Paris)</code>,<code>(France, capital, Paris)</code>.</li></ol><p>该过程中, 抽取是互不干扰且高度并行的.</p><blockquote><p>到这里, USM的模型部分就全部讲解完了. 不知道读者是否能联想到, USM实际上是<a href="https://adaning.github.io/posts/4431.html">UniRel</a>的超集.</p></blockquote><h3 id="Learning-from-Heterogeneous-Supervision"><a href="#Learning-from-Heterogeneous-Supervision" class="headerlink" title="Learning from Heterogeneous Supervision"></a>Learning from Heterogeneous Supervision</h3><h4 id="Pre-training-1"><a href="#Pre-training-1" class="headerlink" title="Pre-training"></a>Pre-training</h4><p>USM采用了三种有监督信号来预训练:</p><ul><li>$\mathcal{D}_{\text{task}}$: <strong>有标训练数据</strong>, 也就是IE特定的数据.</li><li>$\mathcal{D}_{\text{distant}}$: <strong>远程监督数据</strong>, 有文本和知识库对齐的数据, 在这个过程中也使用了UIE中的Meta Schema.</li><li>$\mathcal{D}_{\text{indirect}}$: <strong>间接监督数据</strong>, 由其他可能与IE相关任务的数据组成, 比如MRC相关, KBQA相关的数据. </li></ul><blockquote><p>我感觉这个间接监督用的挺巧, 我估计作者这里用的MRC数据大多是<strong>抽取式MRC</strong>的数据, 任务形式其实和信息抽取是完全一样的. </p><p>作者在文中写到, 每条MRC数据由<code>(question, context, answer)</code> 组成, question就可以当做Label Schema, answer当做Mention, Context当做输入文本. 这样还增强了USM的泛化能力.</p></blockquote><h4 id="Learning-function"><a href="#Learning-function" class="headerlink" title="Learning function"></a>Learning function</h4><p>在预训练时, 所有数据集都可以被表示为$\set{(x_i, y_i)}$, $x_i, y_i$  分别为文本和USM的Linking标签(就是TTM, LTM, TLM对应的三张表). 因为表中会出现很多负样本Token Pair, 非常稀疏, 所以作者在这里采用了<a href="https://spaces.ac.cn/archives/7359" target="_blank" rel="noopener">苏神的了类别不平衡Loss</a>作为损失函数:</p><p>$$<br>\begin{aligned}<br>\mathcal{L}=\sum_{m \in \mathcal{M}} &amp; \log \left(1+\sum_{(i, j) \in m^{+}} e^{-s_m(i, j)}\right) +\log \left(1+\sum_{(i, j) \in m^{-}} e^{s_m(i, j)}\right)<br>\end{aligned}<br>$$</p><p>其中$\mathcal{M}$ 代表USM的三种Linking Type, $m^+$ 代表有链接的Token Pair, $m^-$ 代表没有链接的Token Pair. $s_m(i, j)$ 代表Linking操作$m$ 下Token Pair $(i, j)$ 之间的Linking得分.</p><p>在特定IE Task上使用时, 还需要继续Finetune.</p><h2 id="InstructUIE-Multi-task-Instruction-Tuning-for-Unified-Information-Extraction"><a href="#InstructUIE-Multi-task-Instruction-Tuning-for-Unified-Information-Extraction" class="headerlink" title="InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction"></a>InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction</h2><p>第三篇论文是InstructUIE, 论文暂挂了arxiv, <a href="https://arxiv.org/abs/2304.08085" target="_blank" rel="noopener">InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction</a>.</p><p>在LLM时代, 有UIE必有InstructUIE.</p><p>作者首先是对比了之前的<strong>UIE</strong>, <strong>USM</strong>, 和本文提出的InstructUIE的区别:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/instructuie1.png" style="zoom: 50%;" /><blockquote><p>作者对于UIE和USM中写的”LLM”都是<strong>广义LLM</strong>, 比如T5-base / large或者BERT-base / large这种级别的模型, 而非LLaMA这样的狭义LLM, InstructUIE采用的才是7B及以上的这种狭义LLM.</p></blockquote><p>作者认为:</p><ul><li>UIE需要在每种任务上进行Finetune, 导致对没见过的Schema或者在低资源场景下表现会差.</li><li>USM有两个缺陷, 第一个是将IE转化为语义匹配任务, 使得USM难以应用在生成类模型下. 第二是需要对每个词都做语义匹配, 导致了训练和推理时间的增加.</li></ul><blockquote><p>我感觉, UIE的缺陷是因为它采用的T5-v1.1 base / large规模不够大, 而作者描述的USM的这两个缺陷也并不存在, 十几B的LLM自回归生成难道会比USM的二三百M的模型时间短吗?</p></blockquote><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>其实LLM来做UIE, 直接用图里的内容就能完全概括:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/instructuie2.png" style="zoom:50%;" /><p>就是用Instruct Tuning指挥LLM, 在自然语言驱动下完成各类IE任务. InstructUIE采用<strong>FlanT5 11B</strong>作为预训练基座.</p><h3 id="Task-Schema"><a href="#Task-Schema" class="headerlink" title="Task Schema"></a>Task Schema</h3><p>在预训练阶段, InstructUIE采用所有有标数据作为训练数据. 输入到LLM中的内容由以下内容组成:</p><ul><li><strong>Task Instruction</strong>: 用自然语言描述的能指挥LLM完成指定IE任务的Instruct. 比如NER任务中的其中一条Instruct为 <code>Please list all entity words in the text that fit the category. Output format is &quot;type1: word1; type2: word2&quot;.</code>.</li><li><strong>Options</strong>: 对输出标签做的约束, 比如NER的实体类型仅能为<code>person</code>, <code>organization</code> , <code>location</code> 等等.</li><li><strong>Text</strong>: 要预测的句子文本.</li></ul><p>LLM要输出的内容就是任务的直接结果. 不同任务的预期输出不同:</p><ul><li>NER: <code>entity tag: entity span</code>.</li><li>RE: <code>relationship: head entity, tail entity</code>.</li><li>EE: <code>event tag: trigger word, argument tag: argument span</code>. </li><li>当没有任何结构化信息时则输出<code>None</code>.</li></ul><h3 id="Auxiliary-Tasks"><a href="#Auxiliary-Tasks" class="headerlink" title="Auxiliary Tasks"></a>Auxiliary Tasks</h3><p>作者还提出了一系列辅助任务来保证InstructUIE的性能:</p><ul><li><strong>NER</strong>: 额外加入抽取Span, 对实体类型的判断.</li><li><strong>RE</strong>: 抽取有关系存在的实体对, 对实体对之间的关系进行判断.</li><li><strong>EE</strong>: 识别事件的触发词, 识别事件的论元.</li></ul><blockquote><p>其实就是把各类IE任务拆解为粒度更细的子任务, 来提升LLM对这个任务的理解能力.</p></blockquote><h3 id="IE-INSTRUCTIONS"><a href="#IE-INSTRUCTIONS" class="headerlink" title="IE INSTRUCTIONS"></a>IE INSTRUCTIONS</h3><p>作者提出了一个新的数据集IE INSTRUCTIONS, 该数据集收集了在NER, RE, EE上共32个公开可用的数据集, 并保证了它们的多样性, 数据来自于科学, 医疗, 社交媒体, 交通, 新闻百科等领域:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/instructuie3.png" style="zoom:80%;" /><p>作者主要针对上述数据做出了三项改动:</p><ol><li>统一了各类数据集中语义相同但表述不同的标签名称.</li><li>将特殊格式的标签转换为自然语言的语义标签, 可能需要去掉下划线, 转换缩写等.</li><li>保证了所有任务的输入输出格式均为文本到文本.</li></ol><blockquote><p>注: 读者在阅读InstructUIE的文章时, 需注意实验部分结果分为两部分, 一种是Supervised Settings, 另一种是Zero-shot Settings. </p><p>在Supervised Settings下, InstructUIE先在IE INSTRUCTIONS上做多任务预训练, 后仍会在特定数据集上<strong>全量微调</strong>. Zero-shot Settings下, InstructUIE则在与测试集不相交的数据集上训练, 没有微调.</p><p>所以实际上InstructUIE与UIE, USM相比, 在训练上并无优势, 因为它还是得在特定任务上Finetune.</p></blockquote><p>通用信息抽取未完待续, 挖的坑要填.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NER </tag>
            
            <tag> EE </tag>
            
            <tag> IE </tag>
            
            <tag> UIE </tag>
            
            <tag> RE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2024-元旦</title>
      <link href="/posts/4539.html"/>
      <url>/posts/4539.html</url>
      
        <content type="html"><![CDATA[<h1 id="2024-元旦"><a href="#2024-元旦" class="headerlink" title="2024-元旦"></a>2024-元旦</h1><p>都有小半年没更新博客了, 已经鸽了好久了… 首先祝大家元旦快乐! 这半年来, 找工作和申博我都试了试, 最后是选择了自己觉得更合适的一条路, 也算是人生中做的一个关键的节点吧.</p><p>2023年是LLM横行霸道的一年, 我印象中光是两个月没跟进LLM的进度, 就已经跟原始人差不多了… 和现在的三体人完全没法比, RAG, Agent, Alignment之类的方向在短时间内取得了很大进展, 光是有段时间关注就已经天翻地覆了. </p><p>2024年, 我想恢复博客的更新. 之后博客的更新方向的话, 有下面这几种:</p><ul><li>跟<strong>AIGC相关</strong>的, <strong>LLM</strong>, <strong>DDPM</strong>相关, 我一直对<strong>SVC</strong>(<strong>S</strong>inging <strong>V</strong>oice <strong>C</strong>onversion)都挺感兴趣, 希望以后能有时间做点有趣的东西.</li><li>跟<strong>毕设相关</strong>的, 可能会做一些跟多模态在NLP上的传统任务相关的内容? 比如<strong>MRE / MERE</strong>之类的.</li><li>跟<strong>信息抽取</strong>相关的, 跟NLP里面大多数任务一样, <del>基本上死完了</del>, 这个领域是基本做到头了, 任务基本被LLM全部吞并了, 应该还有几篇Universal Information Extraction Method还没有讲过, 后续可能考虑补上.</li><li>跟<strong>工作相关</strong>的, 可能也会遇到一些软工方面的问题, 虽然说这个还为时尚早.</li><li>跟<strong>其他方向相关</strong>的, 我平时论文看的比较杂, 无论是不是和我方向相关的, 凡是我感兴趣的我都会看, 虽然组里一直做的NLP, 但CV, 多模态平时都会一直关注. 但是往往不是自己研究领域的, 可能了解会比较粗浅, 解读的深度不会超过专门的研究人员. </li></ul><blockquote><p>写完再看看上面这几条, 我给自己挖的坑还挺多的.</p></blockquote><p>在以前写博客的时候, 很多论文都是精读的, 以后在的博客在做论文解读的时候, 应该会多点输出自己的理解和内容, 尽可能的精简一点. 有空可能会把博客主题换下, 就看有没有时间继续折腾了.</p>]]></content>
      
      
      <categories>
          
          <category> 心情随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 心情随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Vision &amp; Language Pretrained Model 总结</title>
      <link href="/posts/44986.html"/>
      <url>/posts/44986.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>2024.4.21: 添加CoCa, 并修改对WPA的描述.</p><p>2024.4.23: 增加了BLIP-2的部分描述.</p></blockquote><h1 id="Vision-amp-Language-Pretraining-总结"><a href="#Vision-amp-Language-Pretraining-总结" class="headerlink" title="Vision &amp; Language Pretraining 总结"></a>Vision &amp; Language Pretraining 总结</h1><p>本文只是以<strong>总结</strong>的形式梳理了近期比较有代表性的VLP模型结构和预训练任务, 推荐有基础后再阅读.</p><h2 id="UNITER"><a href="#UNITER" class="headerlink" title="UNITER"></a>UNITER</h2><ul><li>论文: <a href="https://arxiv.org/abs/1909.11740" target="_blank" rel="noopener">UNITER: UNiversal Image-TExt Representation Learning</a>.</li></ul><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%931.png" alt=""></p><p>UNITER是典型的单塔模型, 直接把Region Feature也变成Token Level Feature和Token Emebdding(Text)一并交给Transformer完成各类预训练任务.</p><p>UNITER有四个预训练任务: </p><ul><li><strong>MLM</strong> - Masked Language Modeing: 和BERT相同, 对文本打<code>[MASK]</code> 预测原来的Token.</li><li><strong>MRM</strong> - Masked Region Modeling: 把Region Vector变成全0, 但是由于Visual Feature是连续的, 没法像NLP的Token一样被多分类预测出来, 所以作者提出了三个MRM的变体:<ul><li><strong>MRFR</strong> - Masked Region Feature Regression: 用一个Linear层把被Mask的ROI Feature直接恢复出来, 用L2回归损失.</li><li><strong>MRC</strong> - Masked Region Classification: 用Linear预测ROI Feature的Semantic Class. 这里没有Ground Truth Label, 直接用Fast RCNN预测的物体类别当做Ground Truth Label.</li><li><strong>MRC-kl</strong> - Masked Region Classification with KL-Divergence: 由于MRC中的Label过于硬了, 所以MRC-kl用Soft Label当做Ground Truth.</li></ul></li><li><strong>ITM</strong> - Image-Text Matching: 采样若干负样本, 用<code>[CLS]</code>接一个Linear来判断图文是否匹配.</li><li><strong>WRA</strong> - Word Region Aligment: 粒度比ITM更细, 对齐每个Text Token和Region. 由于图文在正样本里是匹配的, 所以可以最小化<strong>最优传输</strong>的分配代价, 来达到细粒度对齐的目的. 可以把最优传输看成是计算Text Modality到Vision Modality的距离, 最小化这两个分布的距离即可.</li></ul><p>当预训练的时候, 每次只Mask掉一个模态的Token, 使另一个模态的信息能充分交互.</p><h2 id="Oscar"><a href="#Oscar" class="headerlink" title="Oscar"></a>Oscar</h2><ul><li>论文: <a href="https://arxiv.org/abs/2004.06165" target="_blank" rel="noopener">Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks</a>.</li></ul><p>出发点: </p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%932.png" alt=""></p><p>Region Feature可能不能很好的区分开不同的物体, 例如图中的dog和couch实际上很大部分重叠到了一起, 但是在Word Embedding Space上二者是分开的.</p><p>单塔, 把Region Tag的语义标签也加入到模型的预训练过程中: </p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%933.png" alt=""></p><p>Tag本身具备Language的语义, 但实际上描述的是Visual信息, 所以<strong>Tag打通了V&amp;L的桥梁</strong>.</p><blockquote><p>和现在普遍使用”<strong>中间过渡模态</strong>“的思想非常像.</p></blockquote><p>两个预训练任务, 都与Tag相关</p><ul><li>Dictionary View: Tag和Text都属于同一个Word Dictionary下, 所以沿用BERT的MLM任务, 要求模型用剩余的Tag或Text, Region Feature来恢复.</li><li>Modality View: Tag和Region Feature同属Visual Signal, 作者令训练阶段有50%的概率替换Tag为负样本, 用<code>[CLS]</code>来预测Tag是否被替换.</li></ul><h2 id="CLIP"><a href="#CLIP" class="headerlink" title="CLIP"></a>CLIP</h2><ul><li>论文: <a href="http://proceedings.mlr.press/v139/radford21a" target="_blank" rel="noopener">Learning Transferable Visual Models From Natural Language Supervision</a>.</li></ul><p>CLIP的模型和训练方法非常简单, A picture is worth a thousand words:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%934.png" alt=""></p><p>采用<strong>双塔</strong>结构, 最大化主对角线上的匹配的图文对之间的相似度, 最小化对角线两侧其他文本的相似度即可, 力大砖飞.</p><p>在做Zero Shot Image Classification时, 直接用prompt <code>A photo of a {object}</code> 来预测Image和每个Class的Prompt之间的相似度.</p><p>伪代码:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># image_encoder - ResNet or Vision Transformer </span><span class="token comment" spellcheck="true"># text_encoder - CBOW or Text Transformer </span><span class="token comment" spellcheck="true"># I[n, h, w, c] - minibatch of aligned images </span><span class="token comment" spellcheck="true"># T[n, l] - minibatch of aligned texts </span><span class="token comment" spellcheck="true"># W_i[d_i, d_e] - learned proj of image to embed </span><span class="token comment" spellcheck="true"># W_t[d_t, d_e] - learned proj of text to embed </span><span class="token comment" spellcheck="true"># t - learned temperature parameter </span><span class="token comment" spellcheck="true"># extract feature representations of each modality </span>I_f <span class="token operator">=</span> image_encoder<span class="token punctuation">(</span>I<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#[n, d_i] </span>T_f <span class="token operator">=</span> text_encoder<span class="token punctuation">(</span>T<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#[n, d_t] </span><span class="token comment" spellcheck="true"># joint multimodal embedding [n, d_e] </span>I_e <span class="token operator">=</span> l2_normalize<span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>I_f<span class="token punctuation">,</span> W_i<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> T_e <span class="token operator">=</span> l2_normalize<span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>T_f<span class="token punctuation">,</span> W_t<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># scaled pairwise cosine similarities [n, n] </span>logits <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>I_e<span class="token punctuation">,</span> T_e<span class="token punctuation">.</span>T<span class="token punctuation">)</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>t<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># symmetric loss function </span>labels <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>n<span class="token punctuation">)</span> loss_i <span class="token operator">=</span> cross_entropy_loss<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span> loss_t <span class="token operator">=</span> cross_entropy_loss<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> loss <span class="token operator">=</span> <span class="token punctuation">(</span>loss_i <span class="token operator">+</span> loss_t<span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">2</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="ViLT"><a href="#ViLT" class="headerlink" title="ViLT"></a>ViLT</h2><ul><li>论文: <a href="https://proceedings.mlr.press/v139/kim21k.html" target="_blank" rel="noopener">ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision</a>.</li></ul><p>ViLT的核心思想: <strong>弱Embed重交互</strong>:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%935.png" alt=""></p><p>先前的模型在Embedding上是不均衡或者都非常重的, 反而是在交互上设计的比较弱. 作者认为弱Embedding重交互也可以得到很好的性能, 并且非常省时间.</p><p>模型非常简单, 直接单塔大一统:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%936.png" alt=""></p><p>经典预训练任务ITM, MLM. WPA与UNITER里的WRA类似, 算作是ITM Loss的一个附加Loss. 由于ViLT从输入的Region变成了Patch, 所以作者希望Text Token和Image Patch的匹配程度越高越好, 即总体距离越小越好.</p><h2 id="ALBEF"><a href="#ALBEF" class="headerlink" title="ALBEF"></a>ALBEF</h2><ul><li>论文: <a href="https://proceedings.neurips.cc/paper_files/paper/2021/hash/505259756244493872b7709a8a01b536-Abstract.html" target="_blank" rel="noopener">Align before Fuse: Vision and Language Representation Learning with Momentum Distillation</a>.</li></ul><p>出发点:</p><p>先前的模型(<strong>UNITER</strong>, <strong>Oscar</strong>)仍然在用Region based视觉特征, 整个模型性能受限于Detector的性能. 并且因为Detector是<strong>Freeze</strong>住的, 没有参与E2E的训练, 所以Visual Feature和Text Feature一起扔给一个单塔模型会导致Visual和Text没有<strong>对齐</strong>, 单塔学起来就比较困难.</p><p>模型框架:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%937.png" alt=""></p><p>双塔和单塔融合结构, 先双塔, 后单塔融合. BERT被拆成两半, 一半给文本用, 一半给多模态用. </p><p>至于为什么劈开的是BERT而不是ViT, 有些人认为在多模态任务中往往是视觉占主导地位, 所以对Visual这边要给的重一些.</p><blockquote><p>虽然叫Multimodal Encoder, 但实际上是个Decoder.</p></blockquote><p>ALBEF用了三个Loss: <strong>ITC</strong>, <strong>ITM</strong>, <strong>MLM</strong>, 都是多模态老传统了.</p><p>作者认为, 从Web上爬下来的数据都太脏了, 很有可能出现图文不匹配的情况. 所以直接用One Hot的Cross Entropy去计算对模型的伤害会比较大. 所以要用Momentum Distillation, 也就是<a href="https://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html" target="_blank" rel="noopener">MoCo</a>中提到的方式来缓解这种强惩罚.</p><p>假设图像Token为$\boldsymbol{v}$, 文本Token为$\boldsymbol{w}$. $g_v, g_w, g_v^\prime, g_w^\prime$ 都是Linear Projection. Momentum Encoder得到的V &amp; L归一化以后的Feature为$g_v^{\prime}\left(\boldsymbol{v}_{\mathrm{cls}}^{\prime}\right), g_w^{\prime}\left(\boldsymbol{w}_{\mathrm{cls}}^{\prime}\right)$.</p><p>ITC Loss计算如下. 计算主干和Momentum Encoder的图文匹配相似度:</p><p>$$<br>\begin{aligned}<br>s(I, T)=g_v\left(\boldsymbol{v}_{\mathrm{cls}}\right)^{\top} g_w^{\prime}\left(\boldsymbol{w}_{\mathrm{cls}}^{\prime}\right) \\<br>s(T, I)=g_w\left(\boldsymbol{w}_{\mathrm{cls}}\right)^{\top} g_v^{\prime}\left(\boldsymbol{v}_{\mathrm{cls}}^{\prime}\right)<br>\end{aligned}<br>$$</p><blockquote><p>其实和<a href="https://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html" target="_blank" rel="noopener">MoCo</a>是完全一样的, 拿Momentum Encoder和主干Encoder投影完做点积. 主要是为了<strong>扩大Dictionary Size</strong>.</p></blockquote><p>$M$ 为图文匹配对数量, 计算InfoNCE:</p><p>$$<br>\begin{aligned}<br>p_m^{\mathrm{i} 2 \mathrm{t}}(I)=\frac{\exp \left(s\left(I, T_m\right) / \tau\right)}{\sum_{m=1}^M \exp \left(s\left(I, T_m\right) / \tau\right)} \\<br>p_m^{\mathrm{t} 2 \mathrm{i}}(T)=\frac{\exp \left(s\left(T, I_m\right) / \tau\right)}{\sum_{m=1}^M \exp \left(s\left(T, I_m\right) / \tau\right)}<br>\end{aligned}<br>$$</p><p>如果使用Momentum Distillation, 则需要从计算One Hot损失变为加权计算Momentum Encoder得到的概率分布$\boldsymbol{q}^{\mathrm{i} 2 \mathrm{t}}, \boldsymbol{q}^{\mathrm{t} 2 \mathrm{i}}$ 和主干预测结果$\boldsymbol{p}_m^{\mathrm{i} 2 \mathrm{t}}, \boldsymbol{p}_m^{\mathrm{t} 2 \mathrm{i}}$ 之间的KL散度:<br>$$<br>\begin{aligned}<br>s(I, T)=&amp;g_v\left(\boldsymbol{v}_{\mathrm{cls}}\right)^{\top} g_w^{\prime}\left(\boldsymbol{w}_{\mathrm{cls}}^{\prime}\right) \\<br>s(T, I)=&amp;g_w\left(\boldsymbol{w}_{\mathrm{cls}}\right)^{\top} g_v^{\prime}\left(\boldsymbol{v}_{\mathrm{cls}}^{\prime}\right) \\<br>&amp;\Downarrow \\<br>s^\prime(I, T)=&amp;g_v^\prime\left(\boldsymbol{v}^{\prime}_{\mathrm{cls}}\right)^{\top} g_w^{\prime}\left(\boldsymbol{w}^{\prime}_{\mathrm{cls}}\right) \\<br>s^\prime(T, I)=&amp;g_w^\prime\left(\boldsymbol{w}^{\prime}_{\mathrm{cls}}\right)^{\top} g_v^{\prime}\left(\boldsymbol{v}^{\prime}_{\mathrm{cls}}\right) \\<br>\end{aligned}<br>$$</p><p>$$<br>\begin{aligned}<br>\mathcal{L}_{\mathrm{itc}}=&amp;\frac{1}{2} \mathbb{E}_{(I, T) \sim D}\left[\mathrm{H}\left(\boldsymbol{y}^{\mathrm{i} 2 \mathrm{t}}(I), \boldsymbol{p}^{\mathrm{i} 2 \mathrm{t}}(I)\right)+\mathrm{H}\left(\boldsymbol{y}^{\mathrm{t} 2 \mathrm{i}}(T), \boldsymbol{p}^{\mathrm{t} 2 \mathrm{i}}(T)\right)\right] \\<br>&amp;\Downarrow \\<br>\mathcal{L}_{\text {itc }}^{\mathrm{mod}}=&amp;(1-\alpha) \mathcal{L}_{\mathrm{itc}}+\frac{\alpha}{2} \mathbb{E}_{(I, T) \sim D}\left[\mathrm{KL}\left(\boldsymbol{q}^{\mathrm{i} 2 \mathrm{t}}(I) | \boldsymbol{p}^{\mathrm{i} 2 \mathrm{t}}(I)\right)+\mathrm{KL}\left(\boldsymbol{q}^{\mathrm{t} 2 \mathrm{i}}(T) | \boldsymbol{p}^{\mathrm{t} 2 \mathrm{i}}(T)\right)\right]<br>\end{aligned}<br>$$</p><p>MLM任务也做相应的改动:<br>$$<br>\begin{aligned}<br>\mathcal{L}_{\mathrm{mlm}}=&amp;\mathbb{E}_{(I, \hat{T}) \sim D} \mathrm{H}\left(\boldsymbol{y}^{\mathrm{msk}}, \boldsymbol{p}^{\mathrm{msk}}(I, \hat{T})\right) \\<br>&amp;\Downarrow \\<br>\mathcal{L}_{\mathrm{mlm}}^{\mathrm{mod}}=&amp;(1-\alpha) \mathcal{L}_{\mathrm{mlm}}+\alpha \mathbb{E}_{(I, \hat{T}) \sim D} \mathrm{KL}\left(\boldsymbol{q}^{\mathrm{msk}}(I, \hat{T}) | \boldsymbol{p}^{\mathrm{msk}}(I, \hat{T})\right)<br>\end{aligned}<br>$$<br>在ITM Task中, 作者采用前面计算ITC时除去匹配对相似度最高的图文对作为<strong>Hard Negative</strong>, 作为一个比较难的负样本让模型学习.</p><h2 id="BLIP"><a href="#BLIP" class="headerlink" title="BLIP"></a>BLIP</h2><ul><li>论文: <a href="https://proceedings.mlr.press/v162/li22n.html" target="_blank" rel="noopener">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a>.</li></ul><p>出发点: </p><ol><li>模型角度: encoder only的模型不擅长文本生成, 而encoder-decoder的模型又不擅长检索.</li><li>数据角度: Web端获得的图文匹配对有大量<strong>噪声</strong>.</li></ol><p>模型结构图:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%938.png" alt=""></p><p>不难看出, BLIP实际上也是延续<strong>ALBEF</strong>的一种单双塔融合模型(两篇论文均出自Salesforce之手), 在Image Encoder和Text Encoder完成了两个模态的对齐, 在这个基础上额外加了一个Text Decoder完成Language Modeling的任务, 取代了MLM.</p><p>作者共享了所有的Text Encoder的部分参数, 除了SA是独有的, CA和FFN在二者之间共享.</p><p>所以BLIP的预训练任务也是ITC, ITM, 以及新加入的LM.</p><p>在计算ITC的时候同样是延续ALBEF对ITM用了Hard Negative, 而知识蒸馏以另一种方式CapFilt在BLIP中体现.</p><blockquote><p>至于BLIP为什么要这么设计, 请看后面的CapFilt.</p></blockquote><p>由于大部分图文对都来自网络, 质量很低, 作者希望通过Captioning and Filtering来得到质量更高的数据:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%939.png" alt=""></p><p>在数据中, 人为标注的数据$\{(I_h, T_h)\}$ 肯定是质量非常高的, 但是从网上爬下来的图文对 “image and <strong>alt-text</strong> pairs” $\{(I_w, T_w)\}$ 质量就差很多, 包含了大量<strong>噪声</strong>. </p><p>这时候再看看作者设计的模型, 既然Image - grounded Text Decoder实现的是对图像的描述, Image - grounded Text Encoder完成的是对图文是否匹配的判断, 那么Captioning and Filtering的框架就呼之欲出了:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%9310.png" alt=""></p><p>作者首先在所有数据$\{(I_h, T_h)\} + \{(I_w, T_w)\}$ 上完成BLIP的预训练, 然后在人为标注的数据$\{(I_h, T_h)\}$ 上对Captioner和Filter进行微调. </p><p>微调后, Captioner为网上爬下来的图像$I_w$ 生成描述$T_w$, 再把这部分生成的图文对交给Filter. Filter将网上爬下来的图文对$\{(I_w, T_w)\}$ 和Captioner生成的图文对进行过滤, 最后将这两部分和人为标注的数据$\{(I_h, T_h)\}$ 一起作为新的数据集重新训练一个BLIP.</p><blockquote><p>参数共享在CapFilt阶段是不启用的, 作者在文中有实验说明.</p></blockquote><p>BLIP和ALBEF其实思想上非常相似, 从出发点到模型再到左脚踩右脚上天的思想都是一致的.</p><h2 id="CoCa"><a href="#CoCa" class="headerlink" title="CoCa"></a>CoCa</h2><ul><li>论文: <a href="https://arxiv.org/abs/2205.01917" target="_blank" rel="noopener">CoCa: Contrastive Captioners are Image-Text Foundation Models</a>.</li></ul><p>现有工作的缺点:</p><ol><li>Single Encoder: 代指纯视觉预训练模型, 这些模型不能处理VL Task.</li><li>Dual Encoder: 类似CLIP的双塔, 有利于检索, 但对VL的融合能力不足, 无法直接迁移到VL Understanding任务里.</li><li>Encoder - Decoder: Generative Pretraining, 在VL Alignment上很差.</li></ol><p>模型图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp总结17.png" style="zoom: 67%;" /><p>Attentional Pooling就是一层Cross Attention, 将Query作为Q, Image Feature作为KV, 从而使得Query提炼图像中的特征.</p><p>Image Encoder可以是预训练的Visual Encoder, 在计算Contrastive Loss的时候, Query(Contrastive Query)数量设定为1, 这时候作用和[CLS]类似. 在计算Image Caption Loss的时候, 可能需要更细粒度的Visual Feature, 所以设定这种Query(Cap Query)有256个.</p><p>文中写到, 整个Text Decoder的前半部分的Cross Attention全部被忽略, 以便于编码文本单独的特征. 所以文本模态的Text Decoder其实就是Casual Mask + Self Attention.</p><p>损失函数: </p><ul><li>Single-Encoder Classification: 只对Image Encoder做各类图像相关的分类任务的损失, 在有标注数据下进行.</li><li>Dual-Encoder Contrastive Learning: 在Image Encoder和纯文本模态的Text Decoder上做的ITC Loss.</li><li>Encoder-Decoder Captioning: 两个Text Decoder都用上的Next Token Prediction Loss.</li></ul><p>伪代码:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp总结18.png" style="zoom:80%;" /><p>综上, 个人认为CoCa和上面讲过的BLIP很像, 甚至在对视觉信息的处理上是下面要说的BLIP-2里Q-Former的雏形.</p><h2 id="VLMo"><a href="#VLMo" class="headerlink" title="VLMo"></a>VLMo</h2><ul><li>论文: <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/d46662aa53e78a62afd980a29e0c37ed-Abstract-Conference.html" target="_blank" rel="noopener">VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts</a>.</li></ul><p>VLMo对不同模态使用了<strong>专家系统</strong>(MOE), 大框架如下:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%9311.png" alt=""></p><p>SA是可以被共享的, 当VLMo执行不同任务时, 不同的FFN会被启用:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%9312.png" alt=""></p><p>如图, VLMo预训练一共使用了三个Loss: ITC, ITM, MLM, 同样ITM也带有Hard Negative, 和<strong>ALBEF</strong>, <strong>BLIP</strong>一样. 做ITC时候, 只走V-FFN和L-FFN, 不使用VL-FFN, 此时VLMo更像双塔一些. 当做ITM和MLM时候, VLMo的前(L-F)层用V-FFN和L-FFN对Image Patch和Word Token分别编码, 后F层用VL-FFN把两种模态的数据整合, 此时VLMo更像ALBEF类的单双塔融合模型. 在Base和Large中F分别取2和3.</p><blockquote><p>实验中证明Shared SA比不Share性能要高很多, 这可能说明SA对模态并不是那么敏感, 而且只是用来控制数据流的一个组件而已, 与模态无关.</p></blockquote><p>由于VLMo使用了专家系统, 所以VLMo的使用方式可以十分灵活. VLMo将自己的训练过程拆分为多个阶段:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%9313.png" alt=""></p><p>模型首先在Image dataset上训练SA和V-FFN, 然后再把它们冻住, 去训练L-FFN(用MLM), 此时可以认为VLMo在VL Task上已经有一个良好的初始化, 并且L-FFN是兼容SA输入的. 所以在Image - Text Pair上进行预训练时, 直接开放全量微调.</p><h2 id="BLIP-2"><a href="#BLIP-2" class="headerlink" title="BLIP-2"></a>BLIP-2</h2><ul><li>论文: <a href="https://arxiv.org/abs/2301.12597" target="_blank" rel="noopener">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a>.</li></ul><p>出发点: 当前大规模模型在预训练期间的高额计算消耗太大, 数据也用的特别多.</p><p>作者引入一个lightweight Querying Transformer (Q-Former)来完成Visual &amp; Language模态的桥接过程:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%9314.png" alt=""></p><p>作者把Q-Former的训练拆分为两个阶段:</p><ul><li>首阶段: 让Q-Former从Freeze Image Encoder中学习VL表示.</li><li>次阶段: 从Freeze LLM中学习VL表示.</li></ul><p>Q-Former结构和首阶段预训练如下:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%9315.png" alt=""></p><p>Q-Former实际上由<strong>双塔</strong>的两个Transformer组成, 分别被称为Image Transformer和Text Transformer. 结构上类似于<strong>BLIP</strong>中的Image-Grounded Text Encoder和Text Encoder.</p><p>Image Transformer的SA和Text Transformer的SA参数是共享的(这点和<strong>VLMo</strong>出奇的一致). Learnable Query从Image Transformer给入, 通过CA来从Frozen Image Encoder中获取视觉信号. </p><blockquote><p>作者还在文中补充了一个小细节, 实际上的Visual Key Value采用的是Image Encoder的倒数第二层输出, 而不是最后一层, 效果会稍微好一点, 这点与大家使用Stable Diffusion的时候取CLIP的倒数第二层输出有点类似.</p></blockquote><p>所以但从结构上来看, 首阶段的训练目标是希望Query能够学到从Image Encoder中抽取对Text最有用的内容. 再看训练任务也是这样, 设计了三种:</p><ul><li><strong>ITC</strong>(Image-Text Contrastive Learning): 虽然说是老生常谈的Loss, 但因为Query经过Trm以后得到的表示有多个, 所以作者计算了多个Query与Text Transformer<code>[CLS]</code>的余弦相似度, 选择相似度最大的作为正样本. 为了避免<strong>信息泄露</strong>, 在做ITC的时候要保证Q和T之间是互相不可见的(最右侧Mask).</li><li><strong>ITG</strong>(Image-grounded Text Generation): 使得Q对T完全可见, T单独用Casual Mask, 然后生成图文匹配的文本段. 这就要求Query必须覆盖Image的全部信息, 且Query抽取出的信息必须是有效的(中间Mask). <code>[CLS]</code>也被换成<code>[DEC]</code>.</li><li><strong>ITM</strong>(Image-Text Matching): ITM也是常见Loss, Q必须拥有两个模态的信息才能一起判断图文是否匹配, 作者对所有Query都计算ITM Loss, 最后取平均作为Logits, 同时也使用Hard Negative.</li></ul><p>次阶段预训练, 直接用Q-Former完成图生文:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vlp%E6%80%BB%E7%BB%9316.png" alt=""></p><p>由于在首阶段中Q-Former已经完成了Query从Image Encoder中抽取关键信息的学习, 这也就使得Visual Signal可以被Query以Soft Visual Prompt的形式传递给LLM. 所以Q-Former中的Text Transformer变得不再必要, 可以被<strong>丢弃</strong>. Query表示还需要过一层Linear Project和大模型输入维度对齐.</p><blockquote><p>如果不要首阶段直接硬学的话, 由于没有Text Transformer打辅助, 所以想要让Q-Former学到从Image中抽取出更多有关文本的信息会更难. 但文本模态在Q-Former首阶段训练中起到的实际上是一个Grounding的作用, 根据Language来让Learnable Query抽取更多有用的信息.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MM </tag>
            
            <tag> VLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Large Model并行优化</title>
      <link href="/posts/8982.html"/>
      <url>/posts/8982.html</url>
      
        <content type="html"><![CDATA[<h1 id="Large-Model并行优化"><a href="#Large-Model并行优化" class="headerlink" title="Large Model并行优化"></a>Large Model并行优化</h1><h2 id="为什么要并行优化"><a href="#为什么要并行优化" class="headerlink" title="为什么要并行优化?"></a>为什么要并行优化?</h2><p><strong>大就是好</strong>,  虽然丛2019年人们的认识普遍就是大就是好,  这个概念在当今依然没有被改变,  只是有了更深刻的认识.</p><p>所以,  为什么要并行?</p><ul><li>虽然大就是好,  模型太大<strong>显存</strong>吃不消(空间).</li><li>虽然大就是好,  但是模型太大<strong>速度</strong>也吃不消(时间).</li></ul><p>目标即<strong>难点</strong>:</p><ul><li>大模型训练时,  中间过程保存的<strong>变量</strong>及<strong>参数</strong>成为负担.</li><li>大模型训练时,  <strong>通信开销</strong>不可忽视.</li></ul><p>当今经典分布式并行优化方式主要有三种:</p><ul><li>流水线并行(Pipeline Parallelism).</li><li>数据并行(Data Parallelism).</li><li>张量并行(Tensor Parallelism).</li></ul><h2 id="数据并行-Data-Parallelism"><a href="#数据并行-Data-Parallelism" class="headerlink" title="数据并行 Data Parallelism"></a>数据并行 Data Parallelism</h2><blockquote><p>请参考<a href="https://zhuanlan.zhihu.com/p/617133971" target="_blank" rel="noopener">图解大模型训练之：数据并行上篇(DP,  DDP与ZeRO) - 知乎</a>.</p></blockquote><p>顾名思义,  数据并行(DP)是直接在Batch的维度上进行划分,  将多个Batch拆分到多个节点上进行计算. </p><h3 id="参数服务器-Parameter-Server"><a href="#参数服务器-Parameter-Server" class="headerlink" title="参数服务器 Parameter Server"></a>参数服务器 Parameter Server</h3><p>数据并行最经典的例子是参数服务器(Server),  会在每个节点(Wokrer)上都存储同一份模型,  然后将Batch下放到每个不同的Wokrer上,  完成Forward和Backward,  最后将每个Wokrer算完的梯度回传到一个参数服务器上,  由参数服务器聚合各节点的梯度,  再将聚合后的梯度 / 新的模型参数 <strong>广播</strong>到各个Wokrer上:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lmparallel1.png" style="zoom: 67%;" /><p>各个计算节点(Wokrer) 将梯度上传到参数服务器之后,  参数服务器可能会有两种实现:</p><ol><li>PS计算平均梯度(或加权梯度),  并代替各Worker完成模型参数更新,  之后将参数下放到各计算节点中.</li><li>PS代替仅仅计算平均梯度(或加权梯度),  但不更新模型参数,  而是将计算完的梯度下放到各个Worker当中,  由各个节点自主更新各节点上的模型参数.</li></ol><p>而聚合梯度外加下放梯度这个过程,  被称为”<strong>AllReduce</strong>“.</p><p>由于计算体系内的带宽各不同,  主要考虑AllReduce的开销,  不同的参数服务器聚合方式可能会产生不同的耗时.</p><p>数据并行在每个Worker上都存放了一份模型参数,  所以其实造成了大量<strong>冗余</strong>,  并且Server需要向每个Worker都传输一份梯度 / 模型参数. </p><p>所以,  每当Worker在接收参数或者梯度的时候,  一直在<strong>空转</strong>,  造成了利用率不高. 为了避免这种情况,  可以将<strong>梯度异步更新</strong>,  让Worker拿旧的模型参数来跑新的数据,  但是异步也不能太异步,  可以设定一个延迟步数来保证权重不会太久没有发生更新.</p><p>异步更新由于拿到的梯度不稳定,  会<strong>减缓收敛速度</strong>,  发散的风险也提高了.</p><h3 id="Ring-AllReduce"><a href="#Ring-AllReduce" class="headerlink" title="Ring - AllReduce"></a>Ring - AllReduce</h3><p>Ring - AllReduce,  现在<a href="https://www.youtube.com/watch?v=Cvdhwx-OBBo&ab_channel=PyTorch" target="_blank" rel="noopener">Pytorch的分布式数据并行(DDP)</a>用的就是这种实现方式,  用于多机训练场景.</p><p>DP中最大的缺点就是在AllReduce中,  Server需要和其他所有的Worker通信,  这个通信过程使得每个Worker的计算通信比不高. Server有问题,  那就把所有的通信压力全部转移到Worker上,  <strong>人人都是Worker</strong>,  <strong>人人又都是Server</strong>.</p><p>Ring-AllReduce将该过程拆分为两个部分,  <strong>Reduce - Scatter</strong>和<strong>All - Gather</strong>.</p><h4 id="Reduce-Scatter"><a href="#Reduce-Scatter" class="headerlink" title="Reduce - Scatter"></a>Reduce - Scatter</h4><p>在Reduce - Scatter中,  所有Worker都在拓扑结构上与相邻的两个Worker通信,  因此构成一个<strong>拓扑环</strong>(Ring):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lmparallel3.png" style="zoom:40%;" /><p>假设一共有$N$ 块GPU,  每块GPU记为$i$,  且$i = 1,  2,  \dots,  N$. 那么我们把每块GPU上计算得到的梯度拆分为$N$ 份,  称为 $i$ 的 $N$ 个Gradient Chunk $G_i$.</p><p>每次通信时,  每块GPU $i$ 都会将自己的某个梯度块 $G_i[i]$ 传递到拓扑环上相邻的下一块GPU $i+1$ 上,   使得下一块GPU的梯度块$G_{i+1}[i] = G_{i+1}[i] + G_i[i]$. </p><p>$i$ 同时接收拓扑环中上一块GPU $i-1$ 的某个梯度块$G_{i-1}[i-1]$,  加到自己的对应位置梯度块$G_{i}[i-1]$ 上面,  使得$i$ 的GPU的梯度块$G_{i}[i-1] = G_{i}[i-1] + G_{i-1}[i-1]$:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lmparallel4.png" style="zoom:40%;" /><p>如此反复,  每块GPU都会发送出上次自己接收到梯度块的位置的梯度块到下一个相邻节点,  并接收上个节点送来的梯度块:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lmparallel5.png" style="zoom:40%;" /><p>经过$N-1$ 次传递后,  每块GPU上都拥有了一个完整的梯度块,  这个梯度块被累加过$N$ 次,  也就是经过所有GPU运算得到的梯度之和. 即对于每块GPU $i$,  都应有$G[i]=G_{i}[i]= \sum_{j=1}^{N} G_j[i]$.</p><h4 id="All-Gather"><a href="#All-Gather" class="headerlink" title="All - Gather"></a>All - Gather</h4><p>All - Gather与Reduce - Scatter过程几乎完全一样,  只不过把累加操作变为了<strong>直接替换</strong>的操作,  将每块GPU上得到的一块”完整的梯度块”发送到拓扑环上下个相邻节点:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lmparallel6.png" style="zoom:60%;" /><p>同样是经过$N-1$ 次操作后,  每块GPU上便拥有了经过所有数据计算得到的完整梯度:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lmparallel7.png" style="zoom: 40%;" /><p>此时在每块GPU上分别完成模型参数更新.</p><h4 id="通信量分析"><a href="#通信量分析" class="headerlink" title="通信量分析"></a>通信量分析</h4><p>假设模型参数大小为$\Phi$,  则梯度大小也为$\Phi$,  每个梯度块的大小为$\frac{\Phi}{N}$,  对于单块GPU来说有:</p><ul><li>Reduce - Scatter的通信量为$(N-1)\frac{\Phi}{N}$.</li><li>All - Gather的通信量也为$(N-1)\frac{\Phi}{N}$.</li></ul><p>所以单卡通信总量为$2(N-1)\frac{\Phi}{N}$,  当$N \rightarrow \infty$ 时,  全卡通信总量可以近似为$2N\Phi$. 虽然通信量与DP相同,  但Ring - AllReduce把负载均摊到了每个Worker上.</p><blockquote><p>Reduce - Scatter的本质是从通信角度把Server - Worker之间的串行通信变为了Worker之间的并行通信,  同时利用了所有GPU的计算资源.<br>之所以对每块GPU上的梯度分块就是这个原因,  如果梯度不分块,  又从环退化回了串行通信.</p></blockquote><h2 id="模型并行-Model-Parallelism"><a href="#模型并行-Model-Parallelism" class="headerlink" title="模型并行 Model Parallelism"></a>模型并行 Model Parallelism</h2><p>单卡装不下模型的时候,  最自然的想法就是把模型的各个部分拆分到每个GPU上分别做Forward和Backward,  然后最后再汇总起来:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lmparallel2.png" style="zoom:67%;" /><p>说起来轻巧,  怎么拆呢?</p><h3 id="流水线并行-Pipeline-Parallelism"><a href="#流水线并行-Pipeline-Parallelism" class="headerlink" title="流水线并行 Pipeline Parallelism"></a>流水线并行 Pipeline Parallelism</h3><blockquote><p>请参考<a href="https://zhuanlan.zhihu.com/p/613196255" target="_blank" rel="noopener">图解大模型训练之：流水线并行（Pipeline Parallelism）, 以Gpipe为例 - 知乎</a>.</p></blockquote><p>最简单的,  按层拆呗,  流水线并行也可以被看做是<strong>层间并行</strong>. 把模型的所有层分成多份,  <strong>分别拆到每块GPU去算</strong>. 但是这样在Forward和Backward时都会有问题,  由于模型Forward是顺序串行的,  所以Forward和Backward也是顺序串行的. 即使是这样做了,  还是会存在两个问题:</p><ol><li>串行导致GPU<strong>利用率</strong>很低,  大部分时间在空转.</li><li>随着模型规模的增大,  每块GPU上每层的<strong>中间状态</strong>的显存开销也非常大. 虽然这个原因不是流水线并行本身导致的,  但它会因模型大小而削弱模型并行所带来的优势.</li></ol><p>针对上述两点, 有两种解决办法. </p><p>其中一种缓解的方法,  就是把数据并行也引入. 把所有数据再划分为若干个Batch给到GPU训练,  之前的Batch叫做Mini Batch,  那再次划分的Batch叫做<strong>Micro Batch</strong>. </p><p>在引入Micro Batch以后,  每个GPU可以直接进行<strong>流水线作业</strong>,  将自己的计算结果提交到模型下一层对应的GPU中,  然后再计算下一个Micro Batch的梯度.</p><p>另一种解决办法被称为<strong>Re - Materialization</strong>(Activation Checkpoint),  <strong>直接用时间换空间</strong>. 几乎不存储中间结果,  除了每块GPU的最终输出,  其余的Activation等到Backward用到的时候直接再让模型Forward一遍就行了.</p><h3 id="张量并行-Tensor-Parallelism"><a href="#张量并行-Tensor-Parallelism" class="headerlink" title="张量并行 Tensor Parallelism"></a>张量并行 Tensor Parallelism</h3><blockquote><p>请参考<a href="https://zhuanlan.zhihu.com/p/622212228" target="_blank" rel="noopener">图解大模型训练之：张量模型并行(TP), Megatron-LM - 知乎</a>.</p></blockquote><p><a href="https://arxiv.org/abs/1909.08053" target="_blank" rel="noopener">Megatron</a>是19年遵循张量并行搞出的大模型. 张量并行并不像流水线并行一样,  拆分各层到各块GPU上,  而是对每层里面的矩阵进行拆分,  下放到每块GPU上. 也就是<strong>将模型每层操作的一部分放到不同GPU上完成</strong>,  所以张量并行也被看做为<strong>层间并行</strong>. 最基本的有<strong>按行切分</strong>和<strong>按列切分</strong>,  并且对于不同的操作,  有不同的切分方式,  在此不详细展开.</p><h2 id="ZeRO"><a href="#ZeRO" class="headerlink" title="ZeRO"></a>ZeRO</h2><blockquote><p>请参考:</p><ul><li><a href="https://zhuanlan.zhihu.com/p/618865052" target="_blank" rel="noopener">图解大模型训练之：数据并行下篇( DeepSpeed ZeRO, 零冗余优化) - 知乎</a>.</li><li><a href="https://www.bilibili.com/video/BV1tY411g7ZT" target="_blank" rel="noopener">Zero 论文精读【论文精读】_哔哩哔哩_bilibili</a>.</li></ul></blockquote><p>在数据并行(DP)和分布式数据并行(DDP)中,  都针对通信上负载不均的问题做了优化,  但是仍然没有解决<strong>爆显存</strong>的问题.</p><p><strong>微软</strong>的<a href="https://arxiv.org/abs/1910.02054" target="_blank" rel="noopener">ZeRO</a> 解决了显存上的困难. ZeRO全称为<strong>Ze</strong>ro <strong>R</strong>edundancy <strong>O</strong>ptimizer,  从名字上来看就主要是解决的显存开销,  Zero Redundancy.</p><p>训练过程中的显存占用主要包含以下几个方面:</p><ul><li><strong>Model State Memory</strong>:<ul><li>参数<strong>梯度</strong>.</li><li>模型<strong>参数</strong>.</li><li><strong>优化器状态</strong>. 尤其是<strong>Adam</strong>这样的优化器,  对于每个参数需要保存Momentum和Variance,  也是大头.</li></ul></li><li><strong>Activation Memory</strong>: 在Forward之后,  通常会保存部分输入输出(Activation)的值,  来方便Backward. 当然这个保存不是必须的,  可以通过重新Forward来再次得到.</li><li><strong>Fragmented Memory</strong>: 碎片化存储空间.</li></ul><p>由于FP16的计算效率比FP32要高得多,  所以大模型往往是使用<a href="https://www.jianshu.com/p/58a68fa69332" target="_blank" rel="noopener">混合精度训练</a>的:</p><ol><li>模型参数$W$ 是FP32,  Momentum和Variance也是FP32,  统称为Model States.</li><li>Forward时将FP32的Parameter新建一份FP16备份,  然后用FP16的正常做Forward和Backward. 产生的Activation全部用FP16存储.</li><li>用FP16的Gradient,  更新FP32的Model States(涉及到Loss Scaling).</li></ol><p>最终输出的模型权重应该是<strong>FP32</strong>而不是FP16.</p><p>在权重更新时,  采用FP32而不是FP16,  原因是FP16训练时的<strong>精度可能会不够</strong>,  容易炸,  特别小的数可能会直接变成0. 而且如果模型参数是FP16而不是FP32,  可能出现参数半天也不变的情况.</p><p>根据对混合精度训练的描述,  可以知道模型训练时所需的空间大小,  假设模型参数数量为$\Phi$,  假如以Bytes为单位,  需要的空间如下:</p><ol><li>FP32: <ol><li>Parameter: $4\Phi$.</li><li>Momentum: $4\Phi$.</li><li>Variance: $4\Phi$.</li></ol></li><li>FP16:<ol><li>Parameter: $2\Phi$.</li><li>Gradients: $2\Phi$.</li></ol></li></ol><p>总共$16\Phi$,  当然这个值没有包含Activation在内,  因为Activation的存在比较灵活,  所以在此暂不做考虑.</p><h3 id="ZeRO-DP"><a href="#ZeRO-DP" class="headerlink" title="ZeRO - DP"></a>ZeRO - DP</h3><p>很多States在自己的大多数时间内,  都不会被一直使用,  而是一直拿着,  直到某个被调用的一刻才会用到. ZeRO对这部分States做了优化,  <strong>用到时再拿</strong>,  而不是一直在每块GPU上拿着.</p><h4 id="ZeRO-Stage-1"><a href="#ZeRO-Stage-1" class="headerlink" title="ZeRO Stage 1"></a>ZeRO Stage 1</h4><p>参考Ring - AllReduce,  每块GPU上都有完整的模型参数$W$. 对梯度做一次AllReduce($2\Phi$,  特指单卡通信量,  下同),  所有GPU都能拿到完整的梯度$G$.</p><p>在ZeRO Stage 1中,  所有<strong>Optimizer States</strong> $O$ 被平均拆到了每块GPU上. 模型参数的更新取决于梯度和Optimizer States,  但是现在Optimizer States分布在各块GPU上, 记作$O_i$,  所以只能先结合完整梯度$G$ 来更新一部分模型参数$W_i$,  然后将更新完的这部分$W_i^\prime$ 做一次All - Gather($\Phi$),  所有GPU的模型参数就都是更新完成的了.</p><h4 id="ZeRO-Stage-2"><a href="#ZeRO-Stage-2" class="headerlink" title="ZeRO Stage 2"></a>ZeRO Stage 2</h4><p>在Stage 1的基础上,  把<strong>梯度</strong>也拆分到每块GPU上. 与Ring - AllReduce相似的,  如果每块GPU的最终目标是只维护完整梯度的某一块$G_i$,  那么每块GPU不需要维护除该块以外的梯度,  这是与Ring - AllReduce最大不同的地方,  这节省大量的梯度显存占用.</p><p>对梯度做一次Reduce - Scatter($\Phi$),  每块GPU用自己维护的梯度块$G_i$ 来和部分Optimizer States $O_i$ 来更新对应的$W_i \rightarrow W_i^\prime$,  然后再仿照Stage 1的方式将自己更新好的$W^\prime$ 发送出去,  做一次All - Gather($\Phi$),  所有GPU上的参数就都更新完成了.</p><h4 id="ZeRO-Stage-3"><a href="#ZeRO-Stage-3" class="headerlink" title="ZeRO Stage 3"></a>ZeRO Stage 3</h4><p>在Stage2的基础上,  <strong>模型参数</strong>也全部都拆分到每块GPU上,  每块GPU只维护自己的$W_i$. 在做Forward时,  对$W$ 做一次All - Gather($\Phi$),  做完Forward以后立即把不属于自己管理的$W$ 删除.</p><blockquote><p>这样All - Gather并不会导致峰值过高,  做Forward时也可以是<strong>分批</strong>慢慢做的.</p></blockquote><p>做Backward时,  对$W$ 做All - Gather($\Phi$),  做完以后再删除.</p><p>做完Backward以后,  对梯度$G$ 做一次Reduce - Scatter($\Phi$), 以确保自己能拿到自己应该维护的那部分梯度, 聚合后把不属于自己的梯度删除.</p><p>之后更新自己应该维护的权重$W_i$, 由于每块GPU只需要维护部分权重$W^\prime$, 所以不需要对$W^\prime$ 再重新All - Gather.</p><p>所以其实从ZeRO的Stage1 - 3, 思想都是完全一样的, 不过是分别把Optimizer States, Gradient, Model Parameters分别拆到了每块GPU上, 然后解决它们的通信问题:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lmparallel8.png" style="zoom: 33%;" /><h4 id="ZeRO-Stage-3-VS-模型并行"><a href="#ZeRO-Stage-3-VS-模型并行" class="headerlink" title="ZeRO Stage 3 VS 模型并行"></a>ZeRO Stage 3 VS 模型并行</h4><blockquote><p>引用<a href="https://zhuanlan.zhihu.com/p/618865052" target="_blank" rel="noopener">原话</a>:<br>其实<strong>ZeRO是模型并行的形式, 数据并行的实质</strong>.<br>模型并行, 是指在Forward和Backward的过程中, 我只需要用自己维护的那块W来计算就行. 即<strong>同样的输入X, 每块GPU上各算模型的一部分, 最后通过某些方式聚合结果</strong>.<br>但对ZeRO来说, 它做Forward和Backward的时候, 是需要把各GPU上维护的W聚合起来的, 即本质上还是用完整的W进行计算. <strong>它是不同的输入X, 完整的参数W, 最终再做聚合</strong>. </p></blockquote><h3 id="ZeRO-R"><a href="#ZeRO-R" class="headerlink" title="ZeRO - R"></a>ZeRO - R</h3><p>ZeRO - R是对模型训练过程中<strong>额外产生的内容</strong>做的优化, 这个R指的就是Residual States:</p><ul><li>Partitioned Activation Checkpointing: 灵活的存储Activation.</li><li>Constant Size Buffer: 固定内存大小Buffer,  减少GPU之间的通讯次数,  当积攒足够的数据时才进行GPU通讯,  使得带宽利用率更高,  也使得存储大小已知.</li><li>Memory Defragmentation: 对碎片化存储空间重新整合成连续存储空间.</li></ul><h3 id="ZeRO-Offload"><a href="#ZeRO-Offload" class="headerlink" title="ZeRO-Offload"></a>ZeRO-Offload</h3><p>显存再不够,  实在不行只能扔<strong>CPU</strong>上了. 因此ZeRO - Offload把Update相关的不需要频繁计算的东西全部扔到了CPU上,  比如FP32的Parameter,  FP32的Optimizer States,  FP16的Gradient.</p><p>剩下Forward和Backward这种频繁需要的部分就全放在GPU上,  比如FP16的Parameter,  FP16的Activation.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 并行计算 </tag>
            
            <tag> 分布式 </tag>
            
            <tag> ZeRO </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>QIDN: Query-based Instance Discrimination Network for Relational Triple Extraction</title>
      <link href="/posts/33099.html"/>
      <url>/posts/33099.html</url>
      
        <content type="html"><![CDATA[<h1 id="Query-based-Instance-Discrimination-Network-for-Relational-Triple-Extraction"><a href="#Query-based-Instance-Discrimination-Network-for-Relational-Triple-Extraction" class="headerlink" title="Query-based Instance Discrimination Network for Relational Triple Extraction"></a>Query-based Instance Discrimination Network for Relational Triple Extraction</h1><p>本文是论文<a href="https://arxiv.org/abs/2211.01797" target="_blank" rel="noopener">Query-based Instance Discrimination Network for Relational Triple Extraction</a> 的阅读笔记和个人理解, 论文来自<strong>EMNLP 2022</strong>.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者认为, 现有的方法在抽取三元组时要么是通过<strong>立体透视图</strong>的方法, 要么是学到每一种关系的独立分类器来完成Tagging:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/qidn1.png" style="zoom: 75%;" /><p>几乎之前的所有方法都可以归类到这类中, 这大类方法仍然导致有<strong>误差错误传播</strong>, <strong>关系冗余</strong>, 以及<strong>缺少三元组之间的高级连接</strong>的问题.</p><p>作者提出了一种对三元组<strong>Instance Level</strong>的表示方法, 通过<strong>Query Embedding</strong>对<strong>Token Embedding</strong>完成一步抽取, 从而规避上述问题:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/qidn2.png" style="zoom:75%;" /><blockquote><p>图示中的星星代表Relation Type Embedding, 而圆圈代表Query Embedding, 不难看出作者的应该要尽可能让同关系下的Query Embedding环绕在Relation Embedding周围. 这种方法在空间中可以保留关系之间的语义信息, <strong>对比学习</strong>可以达到这种目的.</p><p>另外, 看到Query Embedding这个词的时候, 对CV略有了解的小伙伴可能会想到近年在CV中影响力很大的<strong>DETR</strong>: <a href="https://arxiv.org/abs/2005.12872" target="_blank" rel="noopener">End-to-End Object Detection with Transformers</a>, 事实上本文也是DETR在NLP中的应用. 如果对DETR还不了解, 建议花几分钟时间阅读一下DETR的架构(其实就是标准的Transformer)和基本思想, 非常简洁也非常简单. 如果你只知道<a href="https://adaning.github.io/posts/50175.html">SPN</a>而不知道DETR, 也可以不看DETR, 因为SPN和DETR几乎完全一致.</p></blockquote><h2 id="QIDN"><a href="#QIDN" class="headerlink" title="QIDN"></a>QIDN</h2><p>QIDN的概览模型图如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/qidn3.png" style="zoom: 25%;" /><p>在经过Sentence Encoder编码后, 结合Query在Decoder中完成Triple Prediction. 此外, 使用Instance Discriminator在空间中用<strong>对比学习约束</strong>预测头得到的三元组表示.</p><h3 id="Task-Formulation"><a href="#Task-Formulation" class="headerlink" title="Task Formulation"></a>Task Formulation</h3><p>对于输入句子$X=x_1, x_2, \ldots, x_n$, 实体集$\mathcal{E}$ 中的实体可以被表示为$(x_i, x_j, t_e)$, $x_i, x_j$ 分别为实体的左右边界Token. 而$t_e$ 代表预定义好的实体类型集合$\mathcal{Y}_e$ 中的实体类型.</p><p>对于关系集合$\mathcal{R}$, 每种关系被表示为$(e_1, e_2, t_r)$, $e_1, e_2 \in \mathcal{E}$ 分别为头实体和尾实体, $t_r$ 为预定义好的关系集合$\mathcal{Y}_r$ 中的关系类型. 除此外, 在$\mathcal{Y}_e, \mathcal{Y}_r$ 中还有$\varnothing$ 代表没有识别到任何关系或者任何实体.</p><h3 id="Sentence-Encoder"><a href="#Sentence-Encoder" class="headerlink" title="Sentence Encoder"></a>Sentence Encoder</h3><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/qidn4.png" style="zoom: 67%;" /><p>对于输入句子$X$, 作者用BERT获取它每个Token的上下文表示, 然后送入一个双层LSTM获得最终的句子表示$H \in \mathbb{R}^{n\times d}$, 其中$n, d$ 分别为句子长度和Hidden size.</p><h3 id="Triple-Prediction"><a href="#Triple-Prediction" class="headerlink" title="Triple Prediction"></a>Triple Prediction</h3><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/qidn5.png" style="zoom:67%;" /><p>作者使用$M$ 个Instance Level Query $Q=\mathbb{R}^{M\times d}$来查询句子中所包含的所有三元组.</p><blockquote><p><strong>DETR最早被用于目标检测</strong>, 每个Query都对应了图像中的一个<strong>目标</strong>(Bounding Box和物体类别). 与之类似的, <strong>在QIDN中, 每个Query对应着一个关系三元组</strong>.</p></blockquote><p>由于RTE任务需要抽取出关系特定下的头尾实体, 这意味着每个Query Embedding不但能指出对应的头尾实体, 还有头尾实体间关系, 所以在这个模块中作者需要构建对实体和关系的两种Query.</p><h4 id="Transformer-based-Decoder"><a href="#Transformer-based-Decoder" class="headerlink" title="Transformer - based Decoder"></a>Transformer - based Decoder</h4><p>Decoder是由$L$ 层的Transformer Decoder的堆叠, 其中Attention计算方式如下:</p><p>$$<br>\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V<br>$$</p><p>其中$Q, K, V$ 分别为Query, Key, Value矩阵, $1/\sqrt{d_k}$ 为缩放因子.</p><p>Transformer Decoder的堆叠可以记为:</p><p>$$<br>\operatorname{Decoder}(Q, H^\prime) = \operatorname{Attention}(Q, H^\prime, H^\prime)<br>$$</p><p>在DETR的架构中, Decoder端的Query便是<strong>Query Embedding</strong>, Key和Value则是原图信息.</p><p>在QIDN中, 作者没有直接使用Sentence Encoder中的$H$ 作为Decoder中需要的Key和Value, 而是构造了一种<strong>Span Level</strong>的表示来获得层次语义信息. 令$S=s_1, s_2, \ldots, s_{n_s}$ 为句子$X$ 中所有的Span, 对于任意一Span $s_i \in S$, 其Span表示$H_i^{span}$ 为$H$ 中的Span边界表示和长度Embedding的拼接:</p><p>$$<br>H_{\mathrm{i}}^{\text {span }}=\left[H_{\text {start }(\mathrm{i})} ; H_{\mathrm{end}(\mathrm{i})} ; \phi\left(s_{\mathrm{i}}\right)\right]<br>$$</p><p>$[;]$ 为拼接操作, $H_{\text {start }(\mathrm{i})} , H_{\mathrm{end}(\mathrm{i})}$ 分别为起始和结束Token的表示, $\phi(s_i)$ 为NER中Span based方法常用的长度Embedding, 可以加入一些Span的长度信息. 最终Span Level的表示为$H^{span} \in \mathbb{R}^{n_s \times d}$.</p><blockquote><p>我认为在这里选择Span作为单位构建表示, 可以获得大量的<strong>负样本</strong>, 有利于<strong>对比学习</strong>.</p></blockquote><p>前面提到过, 同一个Query需要同时能够抽取关系和实体, 因此作者将每个Query兵分两路, 变为Relation Query $Q_r$和Entity Query $Q_e$:</p><p>$$<br>\left[Q_r ; Q_e\right]=\operatorname{Decoder}\left(\left[Q W_r ; Q W_e\right], H^{\text {span }}\right)<br>$$</p><p>其中$W_r, W_e \in \mathbb{R}^{d \times d}$ 为可训练参数.</p><p>更直观一点的话, 作者将Decoder的结构放在附录里:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/qidn6.png" style="zoom: 25%;" /><blockquote><p>其实就是普通Transformer Decoder, 主要区别在于Query的生成拆分为了Entity Query和Relation Query两个branch, 并且在Cross Attention中采用的Key和Value是Span Level的.</p></blockquote><p>综上, 如果有了成对的$Q_r, Q_e$, 就可以用简单的预测头将每个$Q$ 所对应的三元组预测出来.</p><h4 id="Relation-Head"><a href="#Relation-Head" class="headerlink" title="Relation Head"></a>Relation Head</h4><p>对于Relation Query $Q_r$, 将其送入一个FFN中来预测第$i$ 个Query $Q_r^i$ 对应三元组的关系类型$c$ 的概率$P_{i c}^t$:</p><p>$$<br>P_{i c}^t=\frac{\exp \left(Q_r^i W_t^c+b_t^c\right)}{\sum_{c^{\prime}}^{\left|\mathcal{Y}_r\right|} \exp \left(Q_r^i W_t^{c^{\prime}}+b_t^{c^{\prime}}\right)}<br>$$</p><p>其中$W_t \in \mathbb{R}^{|\mathcal{Y}_r| \times d}, b_t \in \mathbb{R}^{|\mathcal{Y}_r|}$ 为可学习参数.</p><h4 id="Entity-Head"><a href="#Entity-Head" class="headerlink" title="Entity Head"></a>Entity Head</h4><p>为了预测三元组中实体的边界, 作者将Entity Query $Q_e$ 和Token表示$H$ 都经过FFN变换:</p><p>$$<br>\begin{aligned}<br>E_\delta=Q_e<br>W_{\delta}\\<br>H_s=H W_s<br>\end{aligned}<br>$$</p><p>其中$\delta \in \mathcal{C} =\set{l_{sub}, r_{sub}, l_{obj}, r_{obj}}$ 代表Subject或者Object的左右边界, $W_\delta, W_s \in \mathbb{R}^{d \times d}$ 为可训练参数.</p><p>接着用<strong>余弦相似度</strong>$S(\cdot)$ 来衡量Entity Query生成的新表示$E_\delta$ 和原文表示$H_s$ 之间的相似度得分:</p><p>$$<br>S\left(\mathbf{v}_i, \mathbf{v}_j\right)=\frac{\mathbf{v}_i}{\left|\mathbf{v}_i\right|} \cdot \frac{\mathbf{v}_j}{\left|\mathbf{v}_j\right|}<br>$$</p><p>根据得分, 做个Softmax就可以得到第$i$ 个Entity Query对应的边界Token是第$j$ 个Token的概率:</p><p>$$<br>P_{i j}^\delta=\frac{\exp S\left(E_\delta^i, H_s^j\right)}{\sum_{j^{\prime}}^n \exp S\left(E_\delta^i, H_s^{j^{\prime}}\right)}<br>$$</p><p>其中$n$ 为句子中Token的数量.</p><p>通过Relation Head, 可以得到Instance Query $Q$ 对应的三元组关系类型$c$, 通过Entity Head, 就可以用相似度得到三元组中Subject和Object的左右边界, 由此来确定Query对应的三元组.</p><h3 id="Instance-Discriminator"><a href="#Instance-Discriminator" class="headerlink" title="Instance Discriminator"></a>Instance Discriminator</h3><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/qidn7.png" style="zoom:67%;" /><p>在Instance Discriminator中, 作者希望把预测头中得到的表示进一步聚合, 当做最初的三元组表示, 并在空间中使这些三元组表示满足某种<strong>约束</strong>, 来建立三元组之间的全局链接, 并且让它们具有类别语义信息.</p><h4 id="Aggregation"><a href="#Aggregation" class="headerlink" title="Aggregation"></a>Aggregation</h4><p>首先要用关系头的表示$Q_r$, 然后简单的把它们相加到一起, <strong>聚合</strong>成三元组初始表示$\mathbf{v}$:</p><p>$$<br>\mathbf{v}=Q_r W+\sum_{\delta \in \mathcal{C}} E_\delta<br>$$</p><blockquote><p>这里对$Q_r$ 做一次变换是为了和Entity Head对齐, 因为$E_\delta$ 已经是由$Q_e$ 做了一次变换得到的.</p></blockquote><h4 id="Training-Objective"><a href="#Training-Objective" class="headerlink" title="Training Objective"></a>Training Objective</h4><p>对于关系集$\mathcal{R} = \set{\mathbf{r}_1, \cdots, \mathbf{r}_{|\mathcal{Y}_r|}}$, 对每种关系都建立一个随机初始化的Relation Type Embedding, 作为关系的表示.</p><p>作者希望三元组实例和关系嵌入在空间中满足下述两个特点:</p><ol><li>对于三元组<strong>实例和实例</strong>之间, 应满足同关系内更近, 不同关系的更远.</li><li>对于三元组<strong>实例和关系</strong>之间, 应满足三元组和自身关系对应的关系嵌入更近.</li></ol><p><strong>对比学习</strong>就是做这个的, 所以作者使用InfoNCE来建模上述两个要求.</p><p>对于第一个特点, 同关系三元组更近, 不同关系三元组更远:</p><p>$$<br>\mathcal{L}_{\mathrm{ins}}=-\sum_c \sum_{i, j} \log \frac{\exp S\left(\mathbf{v}_i^c, \mathbf{v}_j^c\right)}{\sum_{c^{\prime}, j^{\prime}} \exp S\left(\mathbf{v}_i^c, \mathbf{v}_{j^{\prime}}^{c \prime}\right)}<br>$$</p><p>其中$(\mathbf{v}_i^c, \mathbf{v}_j^c)$ 代表同种关系类型$c$ 下的实例对.</p><p>与之类似的, 第二个特点要求三元组和自身对应的关系更近:</p><p>$$<br>\mathcal{L}_{\mathrm{cls}}=-\sum_{i, c} \log \frac{\exp S\left(\mathbf{v}_i^c, \mathbf{r}_c\right)}{\sum_{c^{\prime}} \exp S\left(\mathbf{v}_i^c, \mathbf{r}_{c^{\prime}}\right)}<br>$$</p><p>其中, $\mathbf{v}_i^c$ 是关系$c$ 的三元组实例, $\mathbf{r}_c \in \mathcal{R}$ 是关系$c$ 对应的关系嵌入.</p><p>这样就建模了关系之间的语义信息, 而不是让不同关系独立学习, 并且使得不同关系的三元组之间存在全局链接.</p><h3 id="Training-and-Inference"><a href="#Training-and-Inference" class="headerlink" title="Training and Inference"></a>Training and Inference</h3><h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>在训练时, 记三元组预测Loss $\mathcal{L}_{tri}$ 为每个Instance Query自身所对应的<strong>最优匹配三元组</strong>的关系类型预测交叉熵和头尾实体左右边界的交叉熵之和:</p><p>$$<br>\mathcal{L}_{\text {tri }}=-\sum_{i=1}^M\left(\log P_{\sigma(i)}^t+\sum_{\delta \in \mathcal{C}} \log P_{\sigma(i)}^\delta\right)<br>$$</p><p>$M$ 为Query的数量, $\sigma$ 为<strong>最优匹配</strong>的三元组.</p><blockquote><p>如果还不清楚”最优匹配”, 可以看DETR中的Loss部分, 也可以看<a href="https://adaning.github.io/posts/50175.html#Bipartite-Matching-Loss">SPN的二部图匹配Loss</a>部分.</p></blockquote><p>最终的Loss为前面提到的对比学习Loss和三元组预测Loss之和:</p><p>$$<br>\mathcal{L} = \mathcal{L}_{tri} + \mathcal{L}_{ins} + \mathcal{L}_{cls}<br>$$</p><h4 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h4><p>在推理时:</p><ul><li>三元组可以由Instance Query$\mathcal{Y}_i=\left(\mathcal{Y}_i^t, \mathcal{Y}_i^\delta\right), \delta \in \mathcal{C}$ 得到.</li><li>关系类型预测可以由$\mathcal{Y}_i^t=\arg \max _c\left(P_{i c}^t\right)$ 得到.</li><li>头尾实体的左右边界可以由$\mathcal{Y}_i^\delta=\arg \max _k\left(P_{i k}^\delta\right)$ 得到.</li><li>预测类型为$\varnothing$ 的三元组直接被滤去.</li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文.</p><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>作者选用了RTE中常用的三个数据集NYT, WebNLG*, NYT*, 和ERE中常用的数据集ACE05和SciERC, 统计信息如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/qidn8.png" style="zoom:67%;" /><p>在作者的论文中, 部分匹配数据集为是不带*的, 而精准匹配是带*的. 和RTE论文的习惯刚好相反. 也就是说, 作者写的是WebNLG, 后续实验结果实际上是WebNLG*, NYT和NYT*也要颠倒一下, 下同.</p><h3 id="Overall-Performance"><a href="#Overall-Performance" class="headerlink" title="Overall Performance"></a>Overall Performance</h3><p>QIDN在RTE三个数据集NYT, WebNLG*, NYT* 上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/qidn9.png" style="zoom: 75%;" /><p>QIDN相较于作者给出的Baseline有非常大的进步, 这个性能其实也挺能打的.</p><p>在ERE上两个数据集ACE05和SciERC上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/qidn10.png" style="zoom: 25%;" /><p>这里的RE的实验结果均为严格标准, 即要求实体类型, 关系类型, 实体边界均正确时的F1. QIDN也取得了SOTA, 相较于之前的一些Table Filling based模型比如<a href="https://adaning.github.io/posts/27457.html">UniRE</a>, <a href="https://adaning.github.io/posts/8137.html">PFN</a>都有明显的提升.</p><h3 id="Analysis-on-Complex-Scenarios"><a href="#Analysis-on-Complex-Scenarios" class="headerlink" title="Analysis on Complex Scenarios"></a>Analysis on Complex Scenarios</h3><p>QIDN在NYT*和WebNLG*复杂场景表现如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/qidn11.png" style="zoom:75%;" /><p>根据Baseline来看, QIDN对于三元组数量比较多的情况似乎比较擅长, 有比较明显的提升, 对于一般三元组的情况抽取进步也比较大.</p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>作者尝试了以下设定:</p><ul><li>w/o $H_{span}$: 将Span Level表示替换为<strong>Token Level</strong>表示.</li><li>w/o $Q_e, Q_r$: 去掉对实体和关系Query的特化, 统一用<strong>同一种Query</strong>来代替.</li><li>w/o $\mathcal{L}_{ins}$: 去掉<strong>Instance Pair</strong>之间的对比学习Loss.</li><li>w/o $\mathcal{L}_{cls}$: 去掉<strong>Instance和Relation Embedding</strong>之间的对比学习Loss.</li><li>w/o $\mathcal{L}_{ins}, \mathcal{L}_{cls}$: 去掉<strong>所有对比学习</strong>Loss.</li></ul><p>实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/qidn12.png" style="zoom: 75%;" /><p>从中观察到, 影响比较大的是对比学习的两个Loss, 无论移除哪个都会产生较大的性能衰减. 将Query兵分两路也可以带来一些提升. Span Level的表示对WebNLG来说影响较大.</p><h3 id="Topology-of-Relations"><a href="#Topology-of-Relations" class="headerlink" title="Topology of Relations"></a>Topology of Relations</h3><p>在NYT上对Relation Embedding经过L2正则化后用PCA的可视化如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/qidn13.png" style="zoom:33%;" /><p>语义相近的关系嵌入都比较靠近. 与person相关的几个关系, 语义差别较大, 距离也比较远.</p><h3 id="Error-Analysis"><a href="#Error-Analysis" class="headerlink" title="Error Analysis"></a>Error Analysis</h3><p>作者将错误按NER和RE任务划分:</p><ul><li>NER: ECE(Entity Classification Error), ELE(Entity Localtion Error).</li><li>RE: RCE(Relation Classification Error), PCE(Entity-Pair Classification Error), PLE(Entity-Pair Location Error).</li></ul><p>在ACE05和SciERC这两个数据集上测试集错误比例如下:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/qidn14.png" alt=""></p><p>最高的是实体识别相关的ELE和PLE, 对关系分类RCE和PCE错误不明显, 证明了作者方法的有效性.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>QIDN用<strong>Query based</strong>方法, 从Instance Level的角度解决了RE任务. 将Instance Query拆分为<strong>Relation Query</strong>和<strong>Entity Query</strong>两个branch从而将Query对应的三元组抽取出来, 同时在空间中用<strong>对比学习约束</strong>三元组, 使得三元组之间存在全局关联, 并捕获了关系的语义信息.</p><p>说真的, 自上次SPN以来, 已经有很长时间没有看到Query based Method在联合抽取上的模型了. 可惜代码没有开源.</p><blockquote><p>我个人认为, 如果了解预训练模型<a href="http://arxiv.org/abs/2012.15022" target="_blank" rel="noopener">ERICA</a>, 会对理解QIDN的对比学习任务设计更有帮助. QIDN在对比学习的设计上其实与ERICA非常相似.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RTE </tag>
            
            <tag> ERE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>UniRel: Unified Representation and Interaction for Joint Relational Triple Extraction</title>
      <link href="/posts/4431.html"/>
      <url>/posts/4431.html</url>
      
        <content type="html"><![CDATA[<h1 id="UniRel-Unified-Representation-and-Interaction-for-Joint-Relational-Triple-Extraction"><a href="#UniRel-Unified-Representation-and-Interaction-for-Joint-Relational-Triple-Extraction" class="headerlink" title="UniRel: Unified Representation and Interaction for Joint Relational Triple Extraction"></a>UniRel: Unified Representation and Interaction for Joint Relational Triple Extraction</h1><p>本文是论文<a href="http://arxiv.org/abs/2211.09039" target="_blank" rel="noopener">UniRel: Unified Representation and Interaction for Joint Relational Triple Extraction</a> 的阅读笔记和个人理解, 论文来自<strong>EMNLP 2022</strong>.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者认为现有的RTE模型存在下述两个缺点:</p><ol><li>实体表示和关系表示是<strong>异质</strong>的.</li><li>异质的建模了<strong>实体 - 实体</strong>间交互和<strong>实体 - 关系</strong>间交互.</li></ol><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unirel1.png" style="zoom:33%;" /><p>因此, 作者尝试<strong>统一</strong>关系与实体的<strong>表示类型</strong>, 并同时对二者做<strong>同质交互建模</strong>.</p><h2 id="UniRel"><a href="#UniRel" class="headerlink" title="UniRel"></a>UniRel</h2><h3 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h3><p>对于给定的$N$ 个Token的句子$X=\set{x_1, x_2, \dots, x_N}$, 联合关系三元组抽取的目标是找到句子$X$ 中所有的三元组$T=[(s_l, r_l, o_l)]^L_{l=1}$, 其中$s_l, o_l, r_l$ 分别为Subject, Object和它们之间的关系, $L$ 为句子中的三元组总数.</p><p>关系来自于预定义好的集合$R=\set{R_1, R_2, \dots, R_M}$, $M$ 为关系类型数量.</p><h3 id="Unified-Representation"><a href="#Unified-Representation" class="headerlink" title="Unified Representation"></a>Unified Representation</h3><p>在作者的方法中, 将关系以它的<strong>实际语义Token</strong>引入进来, 比如说关系<code>/business/company/founders</code> 被手动的用<code>founders</code>代替.</p><p>然后以它们Embedding的形式当做关系表示:</p><p>$$<br>\begin{gathered}<br>T=\operatorname{Concat}\left(T_s, T_p\right) \\<br>H=E[T]<br>\end{gathered}<br>$$</p><p>其中$H \in \mathbb{R}^{(N+M) \times d_h}$ 为输入到BERT的向量, $d_h$ 为隐层维度, $T_s, T_p$ 分别为输入句子的Token ID和关系的Token ID.</p><blockquote><p>这种方法其实可以看做是一种Prompt的应用, 附录中有关于作者对手动选择关系Token(Prompt) 的分析.</p><p>另外, 这里其实有一个小细节, 作者用BERT中的Segment Embedding(也就是Token Type Embedding)来区分实体和关系的Token.</p></blockquote><p>接着用Self Attention来捕捉这些所有输入词之间的关系:</p><p>$$<br>\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^T}{\sqrt{d_h}}\right) V<br>$$</p><p>其中$Q, K, V$ 分别是由$H$ 得到的Query, Key, Value向量.</p><h3 id="Unified-Interaction"><a href="#Unified-Interaction" class="headerlink" title="Unified Interaction"></a>Unified Interaction</h3><p>由于作者认为异质建模实体和关系, 以及它们之间的交互, 是存在问题的. 所以作者尝试将它们的建模完全统一, 可以用如下交互图概括:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unirel2.png" style="zoom:33%;" /><p>作者说明了表格中这三种区域表述的实际意义:</p><ul><li>红色区域: <code>Entity - Entity Interaction</code>.</li><li>绿色区域: <code>Subject - Relation Interaction</code>.</li><li>蓝色区域: <code>Relation - Object Interaction</code>.</li></ul><p>交互图上的1便是<strong>行Token与列Token应该发生交互</strong>.</p><blockquote><p>从直观上来看, 这种设计实际上是将关系三元组拆分为若干条边来进行解码.</p></blockquote><h4 id="Entity-Entity-Interaction"><a href="#Entity-Entity-Interaction" class="headerlink" title="Entity - Entity Interaction"></a>Entity - Entity Interaction</h4><p>对于句子$X$ 中的两实体$e_a, e_b$ 形成的实体对$(e_a, e_b)$ 而言, 当它们之间存在某种关系$r$ 时, 两实体之间才会产生交互.</p><p>形式上来说, 实体 - 实体间是否交互的符号函数为:</p><p>$$<br>I_e\left(e_a, e_b\right)= \begin{cases}\text { True } &amp; \left(e_a, r, e_b\right) \in T \text { or } \\\ &amp; \left(e_b, r, e_a\right) \in T, \exists r \in R \\\ \text { False } &amp; \text { otherwise }\end{cases}<br>$$</p><p>由于$I_e(e_b, e_a) = I_e(e_a, e_b)$, 所以它是<strong>对称</strong>的.</p><blockquote><p><code>Entity - Entity Interaction</code> 建模了实体之间的对齐关系(实体之间的成对关系).</p></blockquote><h4 id="Entity-Relation-Interaction"><a href="#Entity-Relation-Interaction" class="headerlink" title="Entity - Relation Interaction"></a>Entity - Relation Interaction</h4><p>实体 - 关系交互仅当<strong>实体存在某关系</strong>时才会发生. 即无论实体$e$ 拥有关系$r$ 时自身的角色是Subject $s$ 还是Object $o$ 都应该产生交互.</p><p>实体关系间是否产生交互的符号函数形式如下:</p><p>$$<br>\begin{aligned}<br>&amp; I_r(e, r)= \begin{cases}\text { True } &amp; (e, r, o) \in T, \exists o \in E \\<br>\text { False } &amp; \text { otherwise }\end{cases} \\<br>&amp; I_r(r, e)= \begin{cases}\text { True } &amp; (s, r, e) \in T, \exists s \in E \\<br>\text { False } &amp; \text { otherwise }\end{cases}<br>\end{aligned}<br>$$</p><p>由于关系是有向的, 所以实体 - 关系间交互是<strong>反对称</strong>的.</p><blockquote><p>这种实体关系间交互的建模方式将表填充中的空间复杂度由$O(N\times M \times N)$ 优化到了$O((N+M)^2)$.</p></blockquote><h4 id="Interaction-Discrimination"><a href="#Interaction-Discrimination" class="headerlink" title="Interaction Discrimination"></a>Interaction Discrimination</h4><p>将BERT最后一层每个头$t$所生成的$Q_t, K_t$ 缩放点积后的结果取平均, 然后直接用Sigmoid做激活作为最终结果:</p><p>$$<br>\mathbf{I}=\operatorname{sigmoid}\left(\frac{1}{T} \sum_t^T \frac{Q_t K_t^T}{\sqrt{d_h}}\right)<br>$$</p><p>其中$\mathbf{I} \in \mathbb{R}^{(N+M)(N+M)}$ 是交互图中的交互矩阵, $T$ 是Self Attention头的数量, $W_t^Q, W_t^K$ 是可训练权重, 当$\mathbf{I}(\cdot)$ 超过阈值$\sigma$ 时, 认为该位置对应的行列Token存在交互.</p><blockquote><p>在作者设计的交互图中, 只需要算个$Q, K$ 之间的相似度得分就可以了, 不需要重新获得表格表示, 索性丢掉后面的步骤, 用$Q, K$ 就行了.</p><p>另外, 其实从模型角度来看, 整个模型没有引入任何的额外参数… 对… <strong>没有引入任何的额外参数</strong>.</p></blockquote><h4 id="Training-and-Decoding"><a href="#Training-and-Decoding" class="headerlink" title="Training and Decoding"></a>Training and Decoding</h4><h5 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h5><p>作者的方法采用一张二元交互图就可以将三元组全部抽取出, 因此Loss采用<strong>二分类交叉熵</strong>即可:</p><p>$$<br>\begin{aligned}<br>\mathcal{L}=-\frac{1}{(N+M)^2} \sum_i^{N+M} \sum_j^{N+M}\left(\mathbf{I}_{i, j}^\ast \log \mathbf{I}_{i, j}\right.<br>\left.+\left(1-\mathbf{I}_{i, j}^\ast\right) \log \left(1-\mathbf{I}_{i, j}\right)\right)<br>\end{aligned}<br>$$</p><p>$\mathbf{I}^\ast$ 为交互图矩阵中的Ground Truth.</p><h5 id="Decoding"><a href="#Decoding" class="headerlink" title="Decoding"></a>Decoding</h5><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unirel3.png" style="zoom:33%;" /><p>咱们直接拿交互图中的例子来说明解码流程:</p><ol><li>根据右侧的<strong>绿色框框</strong>(Subj - Rel)和下面的<strong>蓝色框框</strong>(Rel - Obj)分别解码出每种关系$r$ 对应的Subject $e_i$ 和Object $e_j$. 即抽取出$(e_{i,}r), (r, e_j)$.</li><li>根据上一步找到的所有实体$e_i, e_j$, 穷举出所有的实体对$(e_{p}, e_q)$, 结合左上角的<strong>红色框框</strong>(Ent - Ent), 查看是否有对应的$(e_{p,}e_q)$ 为1.</li><li>如果在红色框框里有$(e_{p,}e_q)$ 为1, 则构成三元组$(e_{i},r, e_j)$. 即利用Subj - Rel $(e_{i},r)$, Ent - Ent $(e_{i,}e_j)$, Rel - Obj $(r, e_j)$ 共同形成三元组$(e_{i,}r, e_j)$.</li></ol><p>例如, 在图中给出了一个既含有EPO又含有SEO的例子(在这里关系均使用语义Token代替):</p><ol><li>首先从右侧的绿色框框拿到<strong>Subj - Rel</strong>, 有<code>(Holmes, Live)</code>, <code>(London, Capital)</code>, <code>(UK, Contain)</code>.</li><li>从下面的蓝色框框拿到<strong>Rel - Obj</strong>, 有<code>(Capital, London)</code>, <code>(Live, London)</code>, <code>(Capital, UK)</code>, <code>(Live, UK)</code>.</li><li>然后<strong>穷举</strong>出所有的<strong>实体对</strong>, 并结合红色框框<strong>Ent - Ent</strong>进行检查, 有效的有<code>(Holmes, London)</code>, <code>(Holmes, UK)</code>, <code>(UK, London)</code>, <code>(London, UK)</code> 这四个实体对.</li><li>结合<strong>Subj - Rel</strong>, <strong>Rel - Obj</strong>, <strong>Ent - Ent</strong>, 可以解码出关系三元组. 比如, Subj - Rel有<code>(Holmes, Live)</code>, Rel - Obj有<code>(Live, London)</code>, 同时Ent - Ent有<code>(Holmes, London)</code>, 那么就可以解码出三元组<code>(Holmes, Live, London)</code>. 与之类似的, 可以共同解码出<code>(Holmes, Live, UK)</code>, <code>(UK, Contain, London)</code>, <code>(London, Capital, UK)</code> 这其他三个三元组.</li></ol><h3 id="Extended-to-Multi-token-Entity-Setting"><a href="#Extended-to-Multi-token-Entity-Setting" class="headerlink" title="Extended to Multi - token Entity Setting"></a>Extended to Multi - token Entity Setting</h3><p>如果上述方法去打标签, 会产生一个严重问题: 面对<strong>多Token实体</strong>要怎么办? 起初, 我认为该方法仅能在NYT和WebNLG的部分匹配数据集上生效.</p><p>后来我发现作者在附录中写到, UniRel实际上可以扩展到多Token实体. 具体的方法是:</p><ol><li>将实体识别从<strong>单Token识别</strong>扩展为<strong>边界识别</strong>. 即先将一张交互图变为<strong>两张</strong>, 来分别完成<code>(Subject Head, Relation, Object Head)</code>以及<code>(Subject Tail, Relation, Object Head)</code>的抽取.</li><li>添加<strong>第三张</strong>关于每个实体的头尾交互图, 完成<code>(Head, Relation, Tail)</code>的抽取.</li><li>对于每个关系, 将实体的<strong>头尾Token链接</strong>起来, 作为一个实体, 然后像之前一样解码.</li></ol><p>所以作者使用了<strong>三张</strong>交互图完成了单Token实体到多Token实体的转化.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文.</p><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><p>本论文所采用的实验数据集是NYT和WebNLG, 主要是二者的<strong>部分匹配</strong>版本:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unirel4.png" style="zoom:25%;" /><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>在NYT和WebNLG的部分匹配上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unirel5.png" style="zoom:25%;" /><p>先全局的看一下结果, UniRel打败了<a href="https://adaning.github.io/posts/4754.html">OneRel</a>成为了SOTA, 甚至WebNLG冲着95去了, NYT部分匹配也快能到94了.</p><p>这里面有两种额外设置:</p><ul><li><strong>unused</strong>: 用BERT<code>[unused*]</code> Token来代替每种关系的语义Embedding, 也就是重新训练一个随机初始化的关系Embedding.</li><li><strong>separate</strong>: 用一个共享的BERT分别将输入的<strong>句子</strong>和<strong>关系语义Token</strong>编码, 然后用不同的两个Transformer Layer获得对应的Q和K, 再将Q, K分别拼接后做点积预测.</li></ul><p>能够看到, 当把关系换到随机初始化的Embedding, 而不是它们的语义Token的Embedding后在WebNLG上效果骤降.</p><p>在原论文的附录中其实也有<strong>精准匹配</strong>的结果(多Token设置):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unirel6.png" style="zoom:25%;" /><p>总的来说也是SOTA, 但是可能对比没有部分匹配的结果提升明显, 并且它使用了三张交互图, 所以就没有放到正文中.</p><p>在NYT*上的复杂场景下UniRel结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unirel7.png" style="zoom:25%;" /><p>能够观察到比较明显的提升来自于识别<strong>Normal三元组</strong>(Normal三元组总体数量更多), 在更多三元组的句子中也有相较于别的模型比较大的提升.</p><blockquote><p>而且从交叉验证来看, 作者的算法是比较稳定的. 现在少有还在做交叉验证的算法, 做了比较严谨.</p></blockquote><h3 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h3><h4 id="Effect-of-the-Unified-Representation"><a href="#Effect-of-the-Unified-Representation" class="headerlink" title="Effect of the Unified Representation"></a>Effect of the Unified Representation</h4><p>为了证明作者的统一表示建模有效, 所以作者采用了<code>unused</code> 设置(详见主实验结果), 发现在WebNLG上效果下降的非常厉害. 所以作者怀疑是WebNLG的数据所致, WebNLG中有更多的关系, 但是却只有更少的数据.</p><p>为了验证这个猜想, 作者统计了不同数量样本的关系下预测三元组的情况, 如下所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unirel8.png" style="zoom: 40%;" /><p>F1明显的在关系所对应的样本数量增大后提升, 但UniRel在融入了一定的语义信息后, 即使是在&gt;1000条样本的设置下, 比<code>unused</code> 自由学习结果还是要好一些.</p><h4 id="Effect-of-the-Unified-Interaction"><a href="#Effect-of-the-Unified-Interaction" class="headerlink" title="Effect of the Unified Interaction"></a>Effect of the Unified Interaction</h4><p>为了证实作者建模的统一交互形式有效, 作者采用了<code>sparate</code> 设置. 在去掉了实体间交互和实体关系交互后, 效果有所下降.</p><p>作者分别测试了在两个数据集上的实体间交互和实体关系交互预测的准确率:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unirel9.png" style="zoom:40%;" /><p>无论是哪个数据集, 分离的方式都不如同时交互的方式, 说明把它们都放在一个表下交互是有效的.</p><h4 id="Computational-Efficiency"><a href="#Computational-Efficiency" class="headerlink" title="Computational Efficiency"></a>Computational Efficiency</h4><p>作者对比了UniRel与前人方法中的训练时间和推理时间:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unirel10.png" style="zoom: 25%;" /><p>在将关系表数量减少后, 训练速度和推理速度都有了明显的提升, 那么空间复杂度实际上也从表填充方法的$O(N \times M \times N)$ 优化到了$O((N + M)^2)$.</p><h4 id="Visualization"><a href="#Visualization" class="headerlink" title="Visualization"></a>Visualization</h4><p>作者的表填充设计非常有意思, 所以可以做一个句子和关系语义拼在一起的可视化:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unirel11.png" style="zoom:40%;" /><p>能看到图中的模型学的很好, 大多数置信度都非常高. 并且抽出了所有的六个三元组, 分别为<code>(Yunnan, country, China)</code>, <code>(China, administrative_divisions, Yunnan)</code>, <code>(Thailand, contains, Chiang Mai)</code>, <code>(Yunan, Contains, Jinghong)</code>, <code>(China, Contains, Jinghong)</code>, <code>(China, Contains, Yunnan)</code>.</p><blockquote><p>其实从图中也能看出一些表填充设计上的鲁棒性, <code>(China, Jinghong)</code> 之间的三元组成立是通过主对角线两侧有任意一个超过阈值即可. 即使主对角线下面标成0.34了, 但是上面是0.72挽救了这一问题, 这点作者没有在原文中提到.</p><p>在开源代码中, 分两次进行抽取, 分别抽取上三角和下三角, 所以在抽取上三角时, 0.72已经可以检测出<code>(China, Jinghong)</code> 存在关系, 不需要在意下三角是否抽取出.</p></blockquote><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>UniRel是一种表填充式的RTE方法, 通过将关系转化为<strong>语义Token</strong>, 把实体和关系变成了<strong>统一表示方法</strong>, 并且做<strong>完全同质建模</strong>, 效果达到了目前任务上真正的SOTA.</p><p>效率是非常高的, 从$O(N\times M \times N)$ 优化到了$O((N+M)^2)$, <strong>解决了表填充带来的关系表冗余的问题</strong>.</p><p>从实验上来看, 作者提出的算法还是比较<strong>稳定</strong>的, 并且具有一定的<strong>容错率</strong>.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RTE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OneEE: A One-Stage Framework for Fast Overlapping and Nested Event Extraction</title>
      <link href="/posts/20020.html"/>
      <url>/posts/20020.html</url>
      
        <content type="html"><![CDATA[<h1 id="OneEE-A-One-Stage-Framework-for-Fast-Overlapping-and-Nested-Event-Extraction"><a href="#OneEE-A-One-Stage-Framework-for-Fast-Overlapping-and-Nested-Event-Extraction" class="headerlink" title="OneEE: A One-Stage Framework for Fast Overlapping and Nested Event Extraction"></a>OneEE: A One-Stage Framework for Fast Overlapping and Nested Event Extraction</h1><p>本文是论文<a href="http://arxiv.org/abs/2209.02693" target="_blank" rel="noopener">OneEE: A One-Stage Framework for Fast Overlapping and Nested Event Extraction</a> 的阅读笔记和个人理解, 论文来自<strong>COLING 2022</strong>.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者认为, 在事件抽取任务中, 实际上包含下述三种类型的事件:</p><ul><li><p><strong>Flat Event</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/oneee1.png" style="zoom: 67%;" /></li></ul><p>最为一般的事件, 不涉及到重叠或嵌套问题.</p><ul><li><p><strong>Overlapped Event</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/oneee2.png" style="zoom:67%;" /><p><strong>重叠事件</strong>被定义为拥有<strong>相同的触发词或至少拥有一个扮演不同事件角色的相同事件论元</strong>的<strong>事件集</strong>.</p><p>例如, <code>acquired</code> 同为<code>Investment</code> 和<code>Share Transfer</code> 这两种不同类型的事件的触发词, <code>Guangzhou Securities</code> 在上述两个事件中扮演不同的事件角色.</p><blockquote><p>可能上述表述不够直观, 在CasEE论文中, 将重叠事件区分为三种详细情况:</p><ol><li><strong>触发词重叠</strong>: 两个不同类型的事件同时共享一个触发词.</li><li><strong>多事件论元重叠</strong>: 某个论元在多个事件中扮演不同的角色.</li><li><strong>单事件论元重叠</strong>: 某个论元在同一个事件中扮演不同的角色.</li></ol><p>准确的来说, 只要满足上述三种类型其中之一的都可以算作是重叠事件, 满足两条或者三者都满足的也可以是重叠事件.</p><p>按照现有的EE论文来看, 相同的论元在不同事件中扮演相同的事件角色是不算重叠的, 至于为什么还不是很清楚.</p></blockquote></li><li><p><strong>Nested Event</strong>:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/oneee3.png" alt=""></p><p><strong>嵌套事件</strong>被定义为<strong>某一事件的论元是另一事件的触发词</strong>的两个事件.<br>例如, 在事件<code>Positive Regulation</code>中, 其<code>Theme</code> 角色的论元<code>expression</code> 是另一事件<code>Gene Expression</code> 的触发词.</p></li></ul><blockquote><p>上述描述来自我自己总结, 可能有些不严谨, 作者在原论文中仅仅是给出了三个例子, 并未对这三种类型的事件下严格定义.</p></blockquote><p>在前人的工作中, 常常仅处理Flat Event而忽略了<strong>Overlapped</strong>和<strong>Nested</strong>两种类型的事件. 即使有处理后两种事件的模型也常基于Pipeline而存在<strong>误差的错误传播</strong>问题. 作者希望将上述三种类型的事件纳入到一个简单而有效的框架下标注, 更具体的是建模为<strong>单词间的关系分类问题</strong>.</p><blockquote><p>熟悉NER的小伙伴可能会想到AAAI 2022的一篇文章, 它们确实有相当大的关联.</p></blockquote><h2 id="OneEE"><a href="#OneEE" class="headerlink" title="OneEE"></a>OneEE</h2><h3 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h3><p>对于给定的由$N$ 个Token或单词组成的句子$X=\set{x_{1,}x_{2,}\dots, x_{n}}$ 和预定义好的事件类型$e \in \mathcal{E}$, 事件抽取的任务目标出Token对$(x_{i,}x_j)$ 之间的Span关系$\mathcal{S}$ 和论元关系$\mathcal{R}$.</p><p>上述事件类型$\mathcal{E}$, Span关系类型$\mathcal{S}$, 论元关系类型$\mathcal{R}$ 的含义如下:</p><ul><li>$\mathcal{S}$: <strong>Span关系</strong>代表起始Token$x_i$ 和结束Token$x_j$ 之间存在的Span关系, <code>S-T</code>, <code>S-A</code> 分别代表触发词的Span和事件论元的Span.</li><li>$\mathcal{R}$: <strong>角色关系</strong>意味着触发词$x_i$ 所对应的论元$x_j$ 在事件中扮演的<strong>事件角色</strong>.</li><li>$\text{NONE}$: 该Word Pair之间不存在任何关系.</li></ul><p>结合下述两个例子可以比较好的理解这种标注方式:</p><ul><li><p>Event: Investment</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/oneee4.png" style="zoom:67%;" /><p>在事件类型<code>Investment</code>中, 论元<code>Citic Securities</code> 中<code>Citic</code> 和<code>Securities</code> 之间的Span关系为<code>S-A</code>, 即论元的Span. 同理, 论元<code>Guangzhou Securities</code> 中<code>Guangzhou</code> 和<code>Securities</code> 的Span关系也是<code>S-A</code>. 而该类型对应的触发词<code>acquired</code> 的Span关系为<code>S-T</code>, 代表触发词的Span.</p><p>在该类型事件中, 触发词<code>acquired</code> 和论元<code>Citic Securities</code> 中每个Token的角色关系为<code>R-S</code>, 代表<code>Citic Securities</code> 是该事件中的Subject, <code>acquired</code> 和<code>Guangzhou Securities</code> 中的每个Token的角色关系为<code>R-O</code>, 代表<code>Guangzhou Securities</code> 是该事件中的Object.</p></li><li><p>Event: Transfer Share</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/oneee5.png" style="zoom:67%;" /><p>还是同样的例子, 在事件类型<code>Transfer Share</code>中, <code>Citic Securities</code>和<code>Guangzhou Securities</code> 的Span关系被以同样方式标出. 在角色关系中, 论元<code>Guangzhou Securities</code> 的每个Token和触发词<code>acquired</code> 之间的关系为<code>R-T</code>, 代表<code>Guangzhou Securities</code> 是该事件中的Target. 在该例子中, <code>100%</code> 也是该事件的论元, 扮演该事件中的Proportion.</p></li></ul><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>模型概览图如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/oneee6.png" style="zoom: 50%;" /><h4 id="Encoder-Layer"><a href="#Encoder-Layer" class="headerlink" title="Encoder Layer"></a>Encoder Layer</h4><p>想必编码层不需多说, 直接用BERT获取特征:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/oneee7.png" style="zoom:50%;" /><p>用BERT获得给定的句子$X=\set{x_{1,}x_{2,}\dots, x_{n}}$ 的特征, 然后用MaxPooling将Word的多个连续Token表示转为单词表示$\boldsymbol{H}=\set{\boldsymbol{h}_{1,}\boldsymbol{h}_{2,}\dots,\boldsymbol{h}_{N}} \in \mathbb{R}^{N\times d_h}$.</p><h4 id="Adaptive-Event-Fusion-Layer"><a href="#Adaptive-Event-Fusion-Layer" class="headerlink" title="Adaptive Event Fusion Layer"></a>Adaptive Event Fusion Layer</h4><p>由于作者设计的框架目标是为了预测出<strong>目标事件</strong>$e_t$ 所限定下的Word Pair之间的关系, 所以生成<strong>Event - Aware</strong>的表格表示是非常重要的. 因此, 作者设计了一个事件信息感知的自适应融合层, 将信息整合到词表示中.</p><p>该层引入门控和注意力机制, 将全局事件信息和, 如下图所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/oneee8.png" style="zoom:50%;" /><h5 id="Attention-Mechanism"><a href="#Attention-Mechanism" class="headerlink" title="Attention Mechanism"></a>Attention Mechanism</h5><p>作者将注意力机制用于抽取, 首先介绍了一下注意力机制:</p><p>$$<br>\operatorname{Attention}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V})=\operatorname{softmax}\left(\frac{\boldsymbol{Q} \boldsymbol{K}^{\top}}{\sqrt{d_h}}\right) \boldsymbol{V}<br>$$</p><p>其中$\sqrt{d_h}$ 为缩放因子, $\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}$ 为Query, Key, Value向量.</p><h5 id="Gate-Fusion-Mechanism"><a href="#Gate-Fusion-Mechanism" class="headerlink" title="Gate Fusion Mechanism"></a>Gate Fusion Mechanism</h5><p><strong>门控机制</strong>可以滤去信息流中夹杂着的噪声, 作者使用门控实现两个信息流的动态融合:</p><p>$$<br>\begin{aligned}<br>\operatorname{Gate}(\boldsymbol{p}, \boldsymbol{q}) &amp;=\boldsymbol{g} \odot \boldsymbol{p}+(1-\boldsymbol{g}) \odot \boldsymbol{q} \\<br>\boldsymbol{g} &amp;=\sigma\left(\boldsymbol{W}_g[\boldsymbol{p} ; \boldsymbol{q}]+\boldsymbol{b}_g\right)<br>\end{aligned}<br>$$</p><p>其中$\boldsymbol{p}, \boldsymbol{q}$ 为输入向量, $\boldsymbol{g}$ 为一个全连接层和Sigmoid得到的信息通过概率, $\sigma(\cdot)$ 为Sigmoid激活函数, $\odot, [;]$ 分别为逐元素点乘和拼接操作.$\boldsymbol{W}_{g,}\boldsymbol{b}_g$ 为可训练参数.</p><p>接着, 作者需要获取每个词所需要的全局事件表示.</p><p>对于给定的$M$ 个随机初始化事件类型Embedding$\boldsymbol{E}=\set{\boldsymbol{e}_{1,}\boldsymbol{e}_{2,}\dots, \boldsymbol{e}_{M}}\in \mathbb{R}^{M\times d_h}$, 以原来词表示$\boldsymbol{H}$ 为Query, 以事件类型Embedding $\boldsymbol{E}$ 当做Key和Value做<strong>软查询</strong>:</p><p>$$<br>\boldsymbol{E}^g=\operatorname{Attention}\left(\boldsymbol{W}_q \boldsymbol{H}, \boldsymbol{W}_k \boldsymbol{E}, \boldsymbol{W}_v \boldsymbol{E}\right)<br>$$</p><p>其中$\boldsymbol{W}_{q,}\boldsymbol{W}_{k,}\boldsymbol{W}_v$ 均为可训练参数. 这样对于每个词都可以获得与之对应的全局事件表示.</p><blockquote><p>从操作上来看, 该操作<strong>将原表示拆分成若干种事件Embedding的组合</strong>.</p></blockquote><p>为了将全局事件信息整合到词表示中, 还需要进一步的将原表示通过门控机制滤去不需要的信息, 判断事件感知信息和原表示语义的<strong>流通量</strong>, 最后进一步和目标事件$\boldsymbol{e}_t$ 相结合:</p><p>$$<br>\begin{aligned}<br>\boldsymbol{H}^g &amp;=\operatorname{Gate}\left(\boldsymbol{H}, \boldsymbol{E}^g\right) \\<br>\boldsymbol{V}^t &amp;=\operatorname{Gate}\left(\boldsymbol{H}^g, \boldsymbol{e}_t\right)<br>\end{aligned}<br>$$</p><p>其中$\boldsymbol{e} \in \boldsymbol{E}$ 是目标事件类型的Embedding, $\boldsymbol{V}^t=\set{\boldsymbol{v}_{1},\boldsymbol{v}_{2},\dots,\boldsymbol{v}_N}$ 为最终获得的事件感知的词表示.</p><blockquote><p>如果设计门控机制的时候, 不附带句子本身的语义信息$\boldsymbol{H}$, 单纯使用$\boldsymbol{E}^g$ 作为表格特征, 则会严重限制模型的表达能力.</p></blockquote><h4 id="Joint-Prediction-Layer"><a href="#Joint-Prediction-Layer" class="headerlink" title="Joint Prediction Layer"></a>Joint Prediction Layer</h4><p>在Adaptive Event Fusion Layer中, 获取了事件感知的词表示$\boldsymbol{V}^t$, 现将其用于Word Pair$(w_{i}, w_j)$ 的二维表构建, 再按照我们已经讲过的标注方式在二维表上完成Span关系和角色关系的标注:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/oneee9.png" style="zoom:50%;" /><h4 id="Distance-Aware-Score"><a href="#Distance-Aware-Score" class="headerlink" title="Distance - Aware Score"></a>Distance - Aware Score</h4><p>为了将<strong>相对位置信息</strong>和词对表示整合到一起, 作者在这里采用相对位置感知的打分函数. 沿着苏神<strong>Roformer</strong>的乘性位置编码思路, 作者将相对位置信息加入到打分过程中:</p><p>$$<br>\begin{aligned}<br>\operatorname{Score}\left(\boldsymbol{p}_i, \boldsymbol{p}_j\right) &amp;=\left(\boldsymbol{R}_i \boldsymbol{p}_i\right)^{\top}\left(\boldsymbol{R}_j \boldsymbol{p}_j\right) \\<br>&amp;=\boldsymbol{p}_i^{\top} \boldsymbol{R}_{j-i} \boldsymbol{p}_j<br>\end{aligned}<br>$$</p><p>其中$\boldsymbol{R}_{i,}\boldsymbol{R}_{j}$ 为$\boldsymbol{p}_{i,}\boldsymbol{p}_j$ 的相对位置Embedding, 并且$\boldsymbol{R}_{j-i}=\boldsymbol{R}_{i}^{\top}\boldsymbol{R}_j$.</p><p>因此, 可以得到目标事件类型为$t$ 的Word Paird$(x_{i,}x_j)$ 之间的Span得分$c_{ij}^s$ 和角色得分$c_{ij}^r$:</p><p>$$<br>\begin{aligned}<br>&amp;c_{i j}^s=\operatorname{Score}\left(\boldsymbol{W}_{s 1} \boldsymbol{v}_i^t, \boldsymbol{W}_{s 2} \boldsymbol{v}_j^t\right), \\<br>&amp;c_{i j}^r=\operatorname{Score}\left(\boldsymbol{W}_{r 1} \boldsymbol{v}_i^t, \boldsymbol{W}_{r 2} \boldsymbol{v}_j^t\right),<br>\end{aligned}<br>$$</p><p>其中$\boldsymbol{W}_{s 1}, \boldsymbol{W}_{s 2}, \boldsymbol{W}_{r 1}, \boldsymbol{W}_{r 2}$ 为可训练参数, $\boldsymbol{v}_{i}^{t}, \boldsymbol{v}_j^t$ 是由Adaptive Event Fusion Layer中得到的$\boldsymbol{V}^t$ 的分量.</p><h3 id="Training-Details"><a href="#Training-Details" class="headerlink" title="Training Details"></a>Training Details</h3><p>由于OneEE采用的是<strong>二分类多标签</strong>的标注方式, 所以会存在<strong>类别不平衡</strong>的问题, 必须对该问题进行处理. 常规的方法是用Focal Loss处理.</p><p>作者没有使用Focal Loss, 而是使用了一种<strong>改进版</strong>的二分类多标签Loss, 该Loss理论上能够在不经过Focal Loss或者精调阈值的情况下取得不错的效果:</p><p>$$<br>\mathcal{L}^{\star}=\log \left(e^\delta+\sum_{(i, j) \in \Omega^{\star}} e^{-c_{i j}^{\star}}\right)+\log \left(e^\delta+\sum_{(i, j) \notin \Omega^{\star}} e^{c_{i j}^{\star}}\right)<br>$$</p><p>其中, $\Omega^\star$ 为Word Pair之间的关系$\star$, $\delta$ 设置为0.</p><blockquote><p>虽然作者在原论文中引用的是<strong>Circle Loss</strong>, 但作者在这里实际上是直接采用苏神设计的改进版的二分类多标签损失, 因为苏神发表博客的时候是2020年.</p><p>关于这个Loss, 我觉得这里直接放苏神文章的链接比较合适, 推荐大家直接阅读苏神的原文:</p><ul><li>Circle Loss: <a href="https://arxiv.org/abs/2002.10857" target="_blank" rel="noopener">[2002.10857] Circle Loss: A Unified Perspective of Pair Similarity Optimization</a></li><li><a href="https://kexue.fm/archives/7359" target="_blank" rel="noopener">将“softmax+交叉熵”推广到多标签分类问题 - 科学空间|Scientific Spaces</a></li><li><a href="https://kexue.fm/archives/9064" target="_blank" rel="noopener">多标签“Softmax+交叉熵”的软标签版本 - 科学空间|Scientific Spaces</a></li><li><a href="https://kexue.fm/archives/9158" target="_blank" rel="noopener">不成功的尝试：将多标签交叉熵推广到“n个m分类”上去 - 科学空间|Scientific Spaces</a></li></ul><p>后续两篇文章是苏神就多标签问题进一步做的其他尝试, 有余力可以阅读一下.</p></blockquote><p>最后将标注<strong>Span关系表</strong>和<strong>角色关系表</strong>的损失相加:</p><p>$$<br>\mathcal{L}=\sum_{t \in \mathcal{E}^{\prime}}\left(\sum_{s \in \mathcal{S}} \mathcal{L}^s+\sum_{r \in \mathcal{R}} \mathcal{L}^r\right)<br>$$</p><p>其中$\mathcal{S}^\prime$ 为从$\mathcal{S}$ 中采样的采样策略.</p><blockquote><p>采样策略在论文的附录中.<br>作者认为, 在训练阶段引入所有的事件类型Embedding进来会导致较高的计算资源消耗, 所以作者常使用一个采样出来的事件类型子集$\mathcal{E}^{\prime}$ 代替原来的事件类型集$\mathcal{E}$, $\mathcal{E}^\prime$ 由一个正事件类型和$K-1$ 个负事件类型组成.</p></blockquote><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p>在推理时, 作者采用的解码策略如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/oneee10.png" style="zoom:50%;" /><p>作者将其总结为四个步骤:</p><ol><li>找到触发词和论元的起始位置和结束位置.</li><li>将对应的起始位置和结束位置整合成触发词或论元的Span.</li><li>根据关系$\text{R}-\star$ 匹配论元和触发词的事件角色.</li><li>将事件类型赋予该事件.</li></ol><p>作者在附录中还特别给出了一个<strong>Nested EE</strong>例子:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/oneee11.png" style="zoom:80%;" /><p>在该例子中, <code>stimulation</code> 作为了同一个类型的事件<code>Positive Regulation</code> 的论元和触发词, 按照本文开头给出的定义, 它是一种嵌套事件.</p><p>该事件解码示意图如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/oneee12.png" style="zoom:80%;" /><p>首先用Span关系抽取出整个句子中所包含的所有触发词<code>induced</code>, <code>stimulation</code>, 和论元<code>stimulation</code>, <code>synthe</code>. 由于作者所采用的标注策略是一种<strong>多标签分类</strong>策略, 所以<code>stimulation</code> 的<code>S-A, S-T</code> 两种Span关系能够同时被抽取出来.</p><p>接着按照角色关系找出两个触发词所对应的论元的角色, 最后就可以解码出两个同类型的嵌套事件.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文.</p><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><p>据作者统计, FewFC中将近22%的句子都有Overlapped Event, 而Genia11, Genia13有将近18%的句子有Nested Event, 具体细节如下所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/oneee13.png" style="zoom:50%;" /><h3 id="Evaluation-Metrics"><a href="#Evaluation-Metrics" class="headerlink" title="Evaluation Metrics"></a>Evaluation Metrics</h3><p>目前的事件抽取任务还比较困难, 因此还是采用的不同子任务的抽取评估指标, 主要是下述四个子任务:</p><ul><li><strong>Trigger Identification(TI)</strong>: 触发词Span抽取正确.</li><li><strong>Trigger Classification(TC)</strong>: 触发词抽取正确, 且触发词所对应的事件分类正确.</li><li><strong>Argument Indentification(AI)</strong>: 论元Span抽取正确且对应的事件类型正确.</li><li><strong>Argument Classification(AC)</strong>: 论元抽取正确且其对应的事件角色分类正确.</li></ul><p>实验将对它们的P, R, F1进行比较.</p><h3 id="All-EE"><a href="#All-EE" class="headerlink" title="All EE"></a>All EE</h3><p>在FewFC这种<strong>重叠事件抽取</strong>数据集上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/oneee14.png" style="zoom:50%;" /><p>在<strong>嵌套事件抽取</strong>数据集Genia11和Genia13表现如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/oneee15.png" style="zoom:50%;" /><p>无论是什么数据集, OneEE在EE的各个子任务上相较于之前的模型都具有显著优势.</p><h3 id="Overlapped-and-Nested-EE"><a href="#Overlapped-and-Nested-EE" class="headerlink" title="Overlapped and Nested EE"></a>Overlapped and Nested EE</h3><p>为验证OneEE对重叠和嵌套事件的处理能力, 作者统计了FewFC中至少含有1个Overlapped Event的句子和Genia11中至少含有1个Nested Event的句子的实验结果, 如下所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/oneee16.png" style="zoom:50%;" /><p>(a), (b)图分别代表FewFC中的触发词重叠, 论元重叠. (c), (d)分别代表Genia11中的触发词嵌套和论元嵌套.</p><p>从结果上来看, OneEE对这几种重叠和嵌套都有比CasEE更好的处理性能.</p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><h4 id="Effects-of-the-Modules-in-the-Fusion-Layer"><a href="#Effects-of-the-Modules-in-the-Fusion-Layer" class="headerlink" title="Effects of the Modules in the Fusion Layer"></a>Effects of the Modules in the Fusion Layer</h4><p>在FewFC上进行的消融实验效果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/oneee17.png" style="zoom:50%;" /><p>从中可以看出, 影响比较大的是去掉位置Embedding, 由此可以看出位置信息的重要性. 其次比较重要的是去掉Fusion Layer, 说明事件的信息其实也比较有用.</p><blockquote><p><code>w/o Gate</code> 代表作者去掉门控机制, 而更改为将两个向量加在一起, 从结果来看影响并不是很大, 感觉有可能模型能够自己学出来一个过滤掉无用信息的机制?</p></blockquote><h4 id="Effect-of-the-Distance-aware-Tag-Prediction"><a href="#Effect-of-the-Distance-aware-Tag-Prediction" class="headerlink" title="Effect of the Distance - aware Tag Prediction"></a>Effect of the Distance - aware Tag Prediction</h4><p>为了进一步探究位置信息对OneEE的影响, 作者按照距离将FewFC的测试集分为六组, 相对位置对OneEE的影响到底有多大:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/oneee18.png" style="zoom:50%;" /><p>随着触发词和论元之间的相对距离增大, 带位置编码的OneEE比去掉位置编码的OneEE标注AC Recall要高. 同时, 随着距离增大到50以上的时候, 二者均有显著衰减.</p><h3 id="Parameter-Number-amp-Efficiency-Comparisons"><a href="#Parameter-Number-amp-Efficiency-Comparisons" class="headerlink" title="Parameter Number &amp; Efficiency Comparisons"></a>Parameter Number &amp; Efficiency Comparisons</h3><p>作者对比了不同模型的抽取阶段数, 参数数量, 推理速度, 如下所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/oneee19.png" style="zoom:50%;" /><p>直观上来讲, OneEE的参数数量其实就比BERT多了一点点, 同时速度也比CasEE稍快一些.</p><h3 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h3><h4 id="Analysis-of-4-Role-Label-Strategies"><a href="#Analysis-of-4-Role-Label-Strategies" class="headerlink" title="Analysis of 4 Role Label Strategies"></a>Analysis of 4 Role Label Strategies</h4><p>作者探索了四种不同的角色关系标注策略, 如下所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/oneee20.png" style="zoom:50%;" /><p>它们分别为:</p><ul><li>TH-AH(Trigger Head-Argument Head): 标注触发词起始位置和论元的起始位置.</li><li>TW-AH(Trigger Word-Argument Head): 标注触发词的所有单词和论元的起始位置.</li><li>TH-AW(Trigger Head-Argument Word): 标注触发词的起始位置和对应的论元的所有单词.</li><li>TW-AW(Trigger Word-Argument Word): 标注触发词的所有单词和论元的所有单词.</li></ul><p>上述四种不同的标注策略在FewFC和Genia11上的对比结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/oneee21.png" style="zoom:50%;" /><p><code>TW-AW</code> 相较于其他标注策略要好很多, 作者猜测原因是因为它比其他策略更<strong>稠密</strong>.</p><blockquote><p>让我比较意外的是在重叠场景下<code>TW-AH</code>和<code>TH-AW</code>有很大差距.</p></blockquote><h4 id="Analysis-of-Event-Number"><a href="#Analysis-of-Event-Number" class="headerlink" title="Analysis of Event Number"></a>Analysis of Event Number</h4><p>对于句子中所包含的不同数量的事件, 对比结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/oneee22.png" style="zoom:50%;" /><p>随着句子中事件数量的增多, OneEE的综合表现与CasEE逐渐持平, 但对于事件数量为1或2的句子, 对论元的抽取和角色分类效果是比CasEE有肉眼可见的优势的.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>OneEE采用了一套基于<strong>表填充</strong>式的<strong>单阶段</strong>标注策略, 将事件抽取下的Flat Event, Overlapped Event, Nested Event<strong>统一</strong>为<strong>词对关系判断问题</strong>, 整合到同一个框架下处理, 同时还注意将<strong>事件类型作为感知</strong>, 融入到表填充过程中, 达到了目前事件抽取领域下的最好结果. </p><p>论文中的实验对该方法的探究是比较完备的, 从多个方面对该问题进行了较为精准的剖析.</p><blockquote><p>事实上, OneEE与AAAI 2022的另一篇论文<a href="https://adaning.github.io/posts/37376.html">W2NER</a>上的作者署名重复度很高, 它们所采用的方法也是类似的, 都是将信息抽取问题转化为词对之间的关系判断问题, 将嵌套, 重叠类似的难题放到统一框架下处理.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> EE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>W2NER: Unified Named Entity Recognition as Word - Word Relation Classification</title>
      <link href="/posts/37376.html"/>
      <url>/posts/37376.html</url>
      
        <content type="html"><![CDATA[<h1 id="W2NER-Unified-Named-Entity-Recognition-as-Word-Word-Relation-Classification"><a href="#W2NER-Unified-Named-Entity-Recognition-as-Word-Word-Relation-Classification" class="headerlink" title="W2NER: Unified Named Entity Recognition as Word - Word Relation Classification"></a>W2NER: Unified Named Entity Recognition as Word - Word Relation Classification</h1><p>本文是论文<a href="http://arxiv.org/abs/2112.10070" target="_blank" rel="noopener">Unified Named Entity Recognition as Word-Word Relation Classification</a> 的阅读笔记和个人理解, 论文来自<strong>AAAI 2022</strong>.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>在之前, NER可以被独立的分为<strong>Flat</strong>, <strong>Overlapped</strong>, <strong>Discontinuous</strong> 三大类. 最近有些工作尝试将上述三类NER归整到一个统一的NER框架当中, 当前主要有<strong>基于Span</strong>的和<strong>Seq2Seq</strong>两大类, 这两类模型要么对<strong>边界识别能力不足</strong>, 要么受到<strong>曝光偏差</strong>影响.</p><blockquote><p>当然, Nested可以视为是一种Overlapped的特殊情况.</p></blockquote><p>作者尝试提出一个<strong>统一</strong>的NER框架W2NER, 将NER转变成一个<strong>单词间</strong>的<strong>关系分类</strong>问题:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/w2ner1.png" style="zoom: 80%;" /><p>在上例中, <code>aching in legs</code> 和<code>aching in shoulders</code> 分别是连续实体和不连续实体, 依照Token之间的关系判断可以将它们分离出来.</p><h2 id="W2NER"><a href="#W2NER" class="headerlink" title="W2NER"></a>W2NER</h2><h3 id="NER-as-Word-Word-Relation-Classification"><a href="#NER-as-Word-Word-Relation-Classification" class="headerlink" title="NER as Word-Word Relation Classification"></a>NER as Word-Word Relation Classification</h3><p>无论是对于Flat, Overlapped, Discontinuous这三类NER中的哪一类, 都可以被抽象为从对于给定的$N$ 个Token或单词$X=\set{x_{1,}x_{2,}\dots, x_N}$ 中抽取出Token Pair$(x_{i,}x_j)$ 之间的关系$\mathcal{R}$, 其中$\mathcal{R}$ 可以是$\text{None}$, $\text{Next-Neighboring-Word}$ (NNW), 或者$\text{Tail-Head-Word-}\star$ (THW-*).</p><p>结合下图来说明这三种Token Pair之间的关系的含义:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/w2ner2.png" style="zoom:67%;" /><p>对于每行Token $x_i$ 和每列Token $x_j$, 即Token Pair$(x_{i,}x_j)$, 其之间的关系代表着:</p><ul><li>$\text{None}$: 该Token Pair之间不存在任何关系.</li><li>$\text{Next-Neighboring-Word}$: $(x_{i,}x_j)$ 位于一个实体提及中, 且在该实体提及中, 行Token $x_i$ 是列Token $x_j$ 的<strong>前一个</strong>Token.</li><li>$\text{Tail-Head-Word-}\star$: $(x_{i,}x_j)$ 位于一个实体提及中, 且在该实体提及中, 行Token $x_i$ 是该实体提及的<strong>结束Token</strong>, 列Token $x_j$ 为该实体提及的<strong>起始Token</strong>. $\star$ 代表该实体提及的实体类型.</li></ul><blockquote><p>从标签设计上来说, NNW指明了<strong>Token之间的连续性</strong>. THW指明了<strong>实体边界</strong>和<strong>实体类型</strong>. 熟悉信息抽取这块的小伙伴可能会觉得这个THW有些似曾相识. 在文末Summary我会指出它的来源.</p></blockquote><p>在上图例子中, 实体<code>aching in legs</code>, <code>aching in shoulders</code> 分别是一个连续实体和不连续实体, 它们共享了<code>aching in</code>.</p><p>通过提到的NNW关系, 可以建立<code>(aching -&gt; in)</code>, <code>(in -&gt; legs)</code>, <code>(in -&gt; shoulders)</code> 之间的关联. 然后再通过THW关系, 定位实体的边界和它们对应的类型<code>(legs -&gt; aching, Symptom)</code>, <code>(shoulders -&gt; aching, Symptom)</code>. 我们根据THW反向往回找, 就能解码出对应的整个实体提及了.</p><h3 id="Unified-NER-Framework"><a href="#Unified-NER-Framework" class="headerlink" title="Unified NER Framework"></a>Unified NER Framework</h3><p>整个W2NER的概览图如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/w2ner3.png" style="zoom:67%;" /><p>大致可以分为Encoder Layer, Convolution Layer, Co - Predictor Layer和最后的Decoding.</p><h4 id="Encoder-Layer"><a href="#Encoder-Layer" class="headerlink" title="Encoder Layer"></a>Encoder Layer</h4><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/w2ner4.png" style="zoom: 50%;" /><p>对于给定的句子$X = \set{x_{1,}x_{2,}\dots, x_N}$, 首先使用BERT获得每个Token或者Word $x_i$ 的表示. 每个Word可能由多个Token组成, 使用最大池化获得每个Word的表示. 接着用一个双向LSTM完成编码. 记最终编码表示$\mathbf{H}=\left\{\mathbf{h}_1, \mathbf{h}_2, \ldots, \mathbf{h}_N\right\} \in \mathbb{R}^{N \times d_h}$, 其中$d_h$ 为Word表示的维度.</p><h4 id="Convolution-Layer"><a href="#Convolution-Layer" class="headerlink" title="Convolution Layer"></a>Convolution Layer</h4><p>由于W2NER会构成一个2D的<strong>表格</strong>, 采用<strong>Conv2D</strong>来聚合表格信息就比较合适:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/w2ner5.png" style="zoom:67%;" /><h5 id="Conditional-Layer-Normalization"><a href="#Conditional-Layer-Normalization" class="headerlink" title="Conditional Layer Normalization"></a>Conditional Layer Normalization</h5><p>表格可以被看做是一个三维矩阵$\mathbf{V} \in \mathbb{R}^{N \times N \times d_h}$, $\mathbf{V}_{ij}$ 代表Word Pair $(x_{i,}x_j)$ 的表示. 由于输入的句子是有向的, 而作者设计的Word Pair间关系也是<strong>有向</strong>的, 作者希望这种<strong>有向条件关系</strong>能够被表示出来. 例如, $(x_{i,}x_j)$ 之间的关系是由 $x_i$ 指向 $x_j$ 时, 应该有$\mathbf{h_j}= f(x_{j} \mid x_{i})$.</p><p>作者使用<strong>Conditional Layer Normalization</strong>(<strong>CLN</strong>)来建模这种隐含关系:</p><p>$$<br>\mathbf{V}_{i j}=\operatorname{CLN}\left(\mathbf{h}_i, \mathbf{h}_j\right)=\gamma_{i j} \odot\left(\frac{\mathbf{h}_j-\mu}{\sigma}\right)+\lambda_{i j}<br>$$</p><p>$\mathbf{h}_i$ 将用于生成缩放系数$\gamma_{ij}$ 和平移系数$\lambda_{ij}$:</p><p>$$<br>\begin{aligned}<br>\gamma_{ij} = \mathbf{W}_{\alpha}\mathbf{h}_{i}+ \mathbf{b}_{\alpha}\\<br>\lambda_{ij} = \mathbf{W}_{\beta}\mathbf{h}_{i}+ \mathbf{b}_{\beta}<br>\end{aligned}<br>$$</p><p>$\mu, \sigma$ 分别是$\mathbf{h}_j$ 中跨元素的均值和标准差:</p><p>$$<br>\mu=\frac{1}{d_h} \sum_{k=1}^{d_h} h_{j k}, \quad \sigma=\sqrt{\frac{1}{d_h} \sum_{k=1}^{d_h}\left(h_{j k}-\mu\right)^2}<br>$$</p><p>$h_{jk}$ 代表$\mathbf{h}_j$ 的第$k$ 维.</p><h5 id="BERT-Style-Grid-Representation-Build-Up"><a href="#BERT-Style-Grid-Representation-Build-Up" class="headerlink" title="BERT-Style Grid Representation Build - Up"></a>BERT-Style Grid Representation Build - Up</h5><p>虽然BERT里面包含了Word Embedding, Positional Embedding, Segment Embedding, 但作者觉得还不够, 因此仿照BERT构建一种新的表格表示, 由三部分组成:</p><ul><li><strong>CLN</strong>得到的Word Pair表示$\mathbf{V} \in \mathbb{R}^{N \times N \times d_h}$.</li><li>Word Pair之间的<strong>相对位置信息</strong>记作$\mathbf{E}^d \in \mathbb{R}^{N \times N \times d_{E_d}}$.</li><li>上下三角的<strong>区域</strong>Embedding$\mathbf{E}^t \in \mathbb{R}^{N \times N \times d_{E_t}}$.</li></ul><p>把上面三种表示拼接到一起, 再用一个MLP整合, 即<strong>Position - Region Aware Representation</strong> $\mathbf{C} \in \mathbb{R}^{N \times N \times d_c}$:</p><p>$$<br>\mathbf{C}=\operatorname{MLP}_1\left(\left[\mathbf{V} ; \mathbf{E}^d ; \mathbf{E}^t\right]\right)<br>$$</p><h5 id="Multi-Granularity-Dilated-Convolution"><a href="#Multi-Granularity-Dilated-Convolution" class="headerlink" title="Multi - Granularity Dilated Convolution"></a>Multi - Granularity Dilated Convolution</h5><p>用2D卷积在2D表格上做聚合是比较符合我们直觉的.</p><p>前人实验表明, 空洞卷积可以有更大的<strong>感受野</strong>, 在NLP上表现良好, 在此使用<strong>空洞卷积</strong>来捕获二维表格上的信息:</p><p>$$<br>\mathbf{Q}^l=\sigma\left(\operatorname{DConv}_l(\mathbf{C})\right)<br>$$</p><p>$l$ 为空洞卷积的膨胀系数.</p><p>多粒度主要体现在不同的膨胀系数, 作者将三种不同膨胀系数的空洞卷积抽出的特征拼接到一起:</p><p>$$<br>\mathbf{Q}= \left[\mathbf{Q}^{1}, \mathbf{Q}^{2}, \mathbf{Q}^{3}\right] \in \mathbb{R}^{N \times N \times 3d_c}<br>$$</p><blockquote><p>作者在代码里写的与论文中似乎不同, 代码中使用的空洞卷积是<strong>顺序</strong>的, 即$\mathbf{Q}^1=\sigma\left(\operatorname{DConv}_1(\mathbf{C})\right)$, $\mathbf{Q}^2=\sigma\left(\operatorname{DConv}_2(\mathbf{Q}^1)\right)$, $\mathbf{Q}^3=\sigma\left(\operatorname{DConv}_3(\mathbf{Q}^2)\right)$, 而不是像文中叙述的分别对$\mathbf{C}$ 做空洞卷积激活再拼接, 不过这不是很重要.</p></blockquote><h4 id="Co-Predictor-Layer"><a href="#Co-Predictor-Layer" class="headerlink" title="Co - Predictor Layer"></a>Co - Predictor Layer</h4><p>Co - Predictor由双仿射和MLP两个组件同时对前面Encoder Layer的特征$\mathbf{H} \in \mathbb{R}^{N \times d_h}$ 和Convolution Layer特征$\mathbf{Q} \in \mathbb{R}^{N \times N \times 3d_c}$ 做聚合, 得到Logits:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/w2ner6.png" style="zoom:67%;" /><h5 id="Biaffine-Predictor"><a href="#Biaffine-Predictor" class="headerlink" title="Biaffine Predictor"></a>Biaffine Predictor</h5><p>Biaffine Predictor的作用对象是Encoder所抽取出的特征, 用两个MLP分别获得Subject和Object对应的表示, 然后再用双仿射获取Logits:</p><p>$$<br>\begin{aligned}<br>\mathbf{s}_i &amp;=\operatorname{MLP}_2\left(\mathbf{h}_i\right) \\<br>\mathbf{o}_j &amp;=\operatorname{MLP}_3\left(\mathbf{h}_j\right) \\<br>\mathbf{y}_{i j}^{\prime} &amp;=\mathbf{s}_i^{\top} \mathbf{U} \mathbf{o}_j+\mathbf{W}\left[\mathbf{s}_i ; \mathbf{o}_j\right]+\mathbf{b}<br>\end{aligned}<br>$$</p><p>这些都是NER里面比较常规的操作了, 不过多赘述.</p><h5 id="MLP-Predictor"><a href="#MLP-Predictor" class="headerlink" title="MLP Predictor"></a>MLP Predictor</h5><p>MLP的作用对象是前面空洞卷积获得的特征$\mathbf{Q}$, 用于聚合Convolution Layer获得的每个格子中的信息:</p><p>$$<br>\mathbf{y}_{i j}^{\prime \prime}=\operatorname{MLP}\left(\mathbf{Q}_{i j}\right)<br>$$</p><p>最后简单的将两个Logits相加到一起即可(也可以视为是同等权重求和):</p><p>$$<br>\mathbf{y}_{i j}=\operatorname{Softmax}\left(\mathbf{y}_{i j}^{\prime}+\mathbf{y}_{i j}^{\prime \prime}\right)<br>$$</p><p>双仿射没对卷积得到的特征操作, 而是以Encoder Layer的$\mathbf{H}$ 为输入, 也可以是看成一种从Encoder Layer拉出来的<strong>残差连接</strong>吧.</p><h4 id="Decoding"><a href="#Decoding" class="headerlink" title="Decoding"></a>Decoding</h4><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/w2ner7.png" style="zoom:67%;" /><p>对于Word Pair之间的关系解码, 可以抽象为一个<strong>有向图</strong>的路径查找, 即利用NNW(蓝)找到有向图中的<strong>实体提及路径</strong>, THW(红)来提供<strong>辅助信息</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/w2ner8.png" style="zoom: 67%;" /><ul><li>(a): 两个Flat实体. 对于简单的完全Flat的情况, 其实两条路径<code>A -&gt; B</code>, <code>D -&gt; E</code>就可以标注出来, THW在此时仅标注它们的实体类型.</li><li>(b): 两个重叠实体(该情况也是嵌套), 此时单纯用NNW没法解码出两个嵌套实体, 但THW在使得其可以解码出两个嵌套实体<code>ABC</code>, <code>BC</code>.</li><li>(c): 含有重叠的一个Flat实体和一个不连续实体. NNW可以找到<code>A -&gt; B -&gt; C</code> 和<code>A -&gt; B -&gt; D</code>这两条边, 在THW的辅助下解码出实体<code>ABC</code>和<code>ABD</code>.</li><li>(d): 最为复杂的情况, 两个部分重叠且不连续的实体. 同样是通过NNW无法单独解码出, 但在THW辅助下可解码出.</li></ul><blockquote><p>其实就是用NNW和THW结合成环, <strong>环即实体</strong>.</p></blockquote><h4 id="Learning"><a href="#Learning" class="headerlink" title="Learning"></a>Learning</h4><p>因为是填表式的Token Pair分类, 所以采用多分类交叉熵就好:</p><p>$$<br>\mathcal{L}=-\frac{1}{N^2} \sum_{i=1}^N \sum_{j=1}^N \sum_{r=1}^{|\mathcal{R}|} \hat{\mathbf{y}}_{i j}^r \log \mathbf{y}_{i j}^r<br>$$</p><p>其中$N$ 为句子中单词总数, $\hat{\mathbf{y}}_{i j}$ 为WordPair $(x_{i,}x_j)$ 之间的Golden Label, $\mathbf{y}_{i j}$ 则为模型预测概率分布, $r$ 代表预定义好的关系集合$\mathcal{R}$ 的关系.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的实验参数设置请参照原论文. 另外, 在阅读实验部分时, 要关注W2NER强大的任务形式统一能力.</p><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><p>根据现有的三大类NER采用了三大类不同的数据集:</p><ul><li><strong>Flat</strong>: CoNLL - 2003, OntoNotes 5.0(English), OntoNotes 4.0, MSRA, Weibo, Resume.</li><li><strong>Overlapped</strong>: ACE2004, ACE2005, GENIA.</li><li><strong>Discontinuous</strong>: 三个英文数据集CADEC, ShARe13, ShARe14, 两个中文数据集ACE2004, ACE2005.</li></ul><h3 id="Flat-NER"><a href="#Flat-NER" class="headerlink" title="Flat NER"></a>Flat NER</h3><p>在Flat NER上六个数据集结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/w2ner9.png" style="zoom: 60%;" /><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/w2ner10.png" style="zoom: 50%;" /><p>在Flat上结果不多说了, W2NER表现很好.</p><h3 id="Overlapped-NER"><a href="#Overlapped-NER" class="headerlink" title="Overlapped NER"></a>Overlapped NER</h3><p>在Overlapped NER三个数据集上表现如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/w2ner11.png" style="zoom:67%;" /><p>能够看到W2NER的Recall和Precision都维持在不错的水平.</p><h3 id="Discontinuous-NER"><a href="#Discontinuous-NER" class="headerlink" title="Discontinuous NER"></a>Discontinuous NER</h3><p>现有NER在Discontinuous NER上比较具有挑战性, 这部分应重点关注, 三个英文数据集上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/w2ner12.png" style="zoom:67%;" /><p>W2NER较其他方法拥有很好的性能, 要记得它在其他NER上表现仍然很猛.</p><p>两个中文数据集上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/w2ner13.png" style="zoom:67%;" /><p>Baseline两篇论文并没有在中文数据集上跑过, 这是作者自行使用官方源码得到的结果. 在两个数据集上领先Baseline半个点.</p><p>因为上述数据集也是含有Flat实体的, 所以作者做了ShARe14上重叠和不连续实体的预测性能:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/w2ner14.png" style="zoom:67%;" /><p>在重叠和不连续这两类比较难的问题下, W2NER相较于其他模型是有明显提升的, 而且差距不小. 其实在附录里还有其他两个数据集的对比, 提升也很明显.</p><h3 id="Model-Ablation-Studies"><a href="#Model-Ablation-Studies" class="headerlink" title="Model Ablation Studies"></a>Model Ablation Studies</h3><p>各组件消融实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/w2ner15.png" style="zoom:67%;" /><p>作者在文中的消融实验部分叙述非常简单, 能从结果中看到比较关键的有Region Embedding, 膨胀系数为2的空洞卷积, MLP, 以及NNW的标签设计. 其实相对位置编码和双仿射影响也不少.</p><p>空洞卷积的话好像<strong>组合</strong>起来性能提升比较多.</p><blockquote><p>MLP是挺猛的… 我倒是挺好奇<strong>CLN</strong>在Convolution Layer里面的作用, 可惜作者没有做实验.</p></blockquote><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>W2NER构建出一张Word Pair之间的<strong>二维表</strong>, 将现有的三大类NER任务转化成<strong>单词间</strong>的<strong>关系分类</strong>问题, 因此可以<strong>统一</strong>到同一个NER框架当中.</p><p>模型中, 引入了<strong>Conv2D</strong>来从<strong>二维表结构</strong>中获取信息, 并将Encoder Layer的信息和Convolution Layer的信息剥离开, 用Co -Predictor单独得到Logits.</p><p>从实验结果来看, W2NER展现出<strong>强大的NER任务形式统一能力</strong>, 并且性能良好, 以至于在实验部分作者也不需要过多的分析.</p><blockquote><p>这种基于Token Pair的分类方式很大程度受到<a href="https://adaning.github.io/posts/49694.html">TPLinker</a>的启发(也就是我在文中说的THW的相似性). 其实不光在NER, 在情感分析中, 以及其他信息抽取领域中已经逐渐成为主流.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NER </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>UniRE: A Unified Label Space for Entity Relation Extraction</title>
      <link href="/posts/8137.html"/>
      <url>/posts/8137.html</url>
      
        <content type="html"><![CDATA[<h1 id="UniRE-A-Unified-Label-Space-for-Entity-Relation-Extraction"><a href="#UniRE-A-Unified-Label-Space-for-Entity-Relation-Extraction" class="headerlink" title="UniRE: A Unified Label Space for Entity Relation Extraction"></a>UniRE: A Unified Label Space for Entity Relation Extraction</h1><p>本文是论文<a href="http://arxiv.org/abs/2107.04292" target="_blank" rel="noopener">UniRE: A Unified Label Space for Entity Relation Extraction</a> 的阅读笔记和个人理解, 论文来自<strong>ACL 2021</strong>.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者认为现有的ERE模型经常将标签空间分为实体检测和关系分类两个子空间, 这样做会<strong>缺失实体和关系的交互</strong>. 作者希望使用一个<strong>统一的标签空间</strong>来消除两个子空间之间的差别对待, 并通过<strong>填表法</strong>来在统一空间中将实体和关系同时抽取出来.</p><h2 id="UniRE"><a href="#UniRE" class="headerlink" title="UniRE"></a>UniRE</h2><h3 id="Task-Definition"><a href="#Task-Definition" class="headerlink" title="Task Definition"></a>Task Definition</h3><p>对于给定的句子$s=x_1, x_2, \ldots, x_{|s|}$, ERE的目标是抽取出所有的实体$\mathcal{E}$ 和实体之间存在的关系$\mathcal{R}$.</p><p>每个实体$e$ 对应着一种预定义好的实体类型$e.\text{type} \in \mathcal{Y}_e$, 例如<code>PER</code>, <code>GPE</code> 等, 由单个或多个<strong>连续Token</strong>构成.</p><p>每种关系是一个由两个实体$e_{1,}e_{2}$ 和预定义好的关系$l \in \mathcal{Y}_r$ 构成的三元组$(e_{1,}e_{2,} l)$,</p><p>作者将ERE视为一个表填充问题. 对于句子$s$, 作者尝试维护一个表$T^{|s| \times |s|}$, 在表$T$ 中的每个单元格$(i, j)$ 都应该被打上一个标签$y_{i, j} \in \mathcal{Y}$, $\mathcal{Y}=\mathcal{Y}_e \cup \mathcal{Y}_r \cup\{\perp\}$, $\perp$ 代表没有关系存在.</p><p>对于每个实体$e$, 它在表上对应的单元格$y_{i, j}\left(x_i \in e.\text{span}, x_{j \in} e.\text{span} \right)$ 都应该被填上它的实体类型$e.\text{type}$.</p><p>对于实体之间的每种关系$r=(e_{1,}e_{2,}l)$, 它们之间对应的单元格$y_{i, j}\left(x_i \in e.\text{span}, x_{j \in} e.\text{span} \right)$ 应该被填充上关系$l$. 其他的所有单元格应该被填上$\perp$.</p><p>上述标注策略如下所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unire1.png" style="zoom: 67%;" /><p>每个单元格都对应着一对Word. 作者注意到, 每个<strong>实体</strong>的标签都散落在<strong>对角线上</strong>, 并且是一个<strong>正方形</strong>, <strong>关系</strong>是散落在<strong>对角线两侧</strong>的<strong>长方形</strong>.</p><p>在例子中, <code>PER-SOC</code> 是无向关系, <code>PHYS</code> 和<code>ORG-AFF</code> 都是有向关系, 这意味着它们在表上的标签是<strong>反对称</strong>的. 同时, 由于表实质是Word Pair的优势, 存在<strong>关系重叠</strong>也可以轻松地解决(RTE之前的挑战主要就是解决重叠问题). 如例子中<code>David Perkins</code> 被<code>PHYS</code> 和<code>PER-SOC</code> 所共享(在RTE里叫Single Entity Overlap), 但是仍然可以通过<strong>同行的不同列</strong>将这两种关系标注出来.</p><p>在测试阶段, 对实体关系的解码就变成了一个<strong>寻找长方形</strong>问题.</p><blockquote><p>作者在这里简单的假设<strong>不存在重叠实体</strong>.</p></blockquote><h3 id="Biaffine-Model"><a href="#Biaffine-Model" class="headerlink" title="Biaffine Model"></a>Biaffine Model</h3><p>对于给定的输入句子$s$, 用BERT作为预训练语言模型PLM获得每个词的上下文表示$\mathbf{h}_i$, 那么句子中所有的Word表示为:</p><p>$$<br>\left\{\mathbf{h}_1, \ldots, \mathbf{h}_{|s|}\right\}=\operatorname{PLM}\left(\left\{\mathbf{x}_1, \ldots, \mathbf{x}_{|s|}\right\}\right)<br>$$</p><p>其中$\mathbf{x}_i$ 为每个Token$x_i$ 的输入表示(Embedding).</p><blockquote><p>$\mathbf{h_i}$ 为文中每个词WordPiece后的首个Token的表示, 代替了每个词的表示作为Entity的分割边界, 这样可以减少不必要的输入和计算.</p></blockquote><p>接着, 使用两个MLP对头尾角色(也就是表的行和列)做区分:</p><p>$$<br>\begin{aligned}<br>\mathbf{h}_i^{\text {head }}&amp;=\operatorname{MLP}_{\text {head }}\left(\mathbf{h}_i\right)<br>\\<br>\quad \mathbf{h}_i^{\text {tail }}&amp;=\operatorname{MLP}_{\text {tail }}\left(\mathbf{h}_i\right)<br>\end{aligned}<br>$$</p><p>其中$\mathbf{h}_i^{\text {head }} \in \mathbb{R}^{d,}\mathbf{h}_i^{\text {tail }} \in \mathbb{R}^d$ 分别为头尾的投影表示.</p><p>接着, 使用双仿射来计算表中每个单元格(Word Pair)的得分$\mathbf{g}_{i, j} \in \mathbb{R}^{|\mathcal{Y}|}$:</p><p>$$<br>\begin{aligned}<br>\mathbf{g}_{i, j} &amp;=\operatorname{Biaff}\left(\mathbf{h}_i^{\text {head }}, \mathbf{h}_j^{\text {tail }}\right) \\<br>\operatorname{Biaff}\left(\mathbf{h}_1, \mathbf{h}_{\mathbf{2}}\right) &amp;=\mathbf{h}_1^T \mathbf{U}_1 \mathbf{h}_2+\mathbf{U}_2\left(\mathbf{h}_1 \oplus \mathbf{h}_2\right)+\mathbf{b}<br>\end{aligned}<br>$$</p><p>其中$\mathbf{U}_{1}\in {|\mathcal{Y}| \times d \times d}, \mathbf{U}_{2}\in \mathbb{R}^{|\mathcal{Y} \times 2d|}, \mathbf{b} \in \mathbb{R}^{|\mathcal{Y}|}$ 为可训练参数, $\oplus$ 为拼接操作.</p><blockquote><p>双仿射被认为是<strong>最深层次</strong>的Token Pair间交互方式, 最早用在依存分析里面, 现在基本上是NER SOTA的标配了.<br>虽然双仿射同时包含了<strong>加性</strong>和<strong>乘性</strong>, 但在Pytorch里仍然可以用<code>torch.einsum</code> 很简洁的实现, 如下:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Biaffine</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>      <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_size<span class="token punctuation">,</span> out_size<span class="token punctuation">,</span> bias_x<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> bias_y<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>          super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>          self<span class="token punctuation">.</span>in_size <span class="token operator">=</span> in_size          self<span class="token punctuation">.</span>out_size <span class="token operator">=</span> out_size          self<span class="token punctuation">.</span>bias_x <span class="token operator">=</span> bias_x          self<span class="token punctuation">.</span>bias_y <span class="token operator">=</span> bias_y          weight <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>in_size <span class="token operator">+</span> int<span class="token punctuation">(</span>bias_x<span class="token punctuation">)</span><span class="token punctuation">,</span> out_size<span class="token punctuation">,</span> in_size <span class="token operator">+</span> int<span class="token punctuation">(</span>bias_y<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>          nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_normal_<span class="token punctuation">(</span>weight<span class="token punctuation">)</span>          self<span class="token punctuation">.</span>weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>weight<span class="token punctuation">)</span>      <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>          <span class="token keyword">if</span> self<span class="token punctuation">.</span>bias_x<span class="token punctuation">:</span>              x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>x<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>          <span class="token keyword">if</span> self<span class="token punctuation">.</span>bias_y<span class="token punctuation">:</span>              y <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>y<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>y<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>          hidden <span class="token operator">=</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bxi,ioj,byj->bxyo"</span><span class="token punctuation">,</span> x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> y<span class="token punctuation">)</span>          <span class="token keyword">return</span> hidden<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>关于该代码的推导可以参照这里:</p><ul><li><a href="https://zhuanlan.zhihu.com/p/464138965" target="_blank" rel="noopener">Biaffine for NER：Named Entity Recognition as Dependency Parsing - 知乎</a></li></ul></blockquote><h3 id="Table-Filling"><a href="#Table-Filling" class="headerlink" title="Table Filling"></a>Table Filling</h3><p>在有了表内单元格的打分$\mathbf{g}_{i, j}$ 后, 直接使用Softmax对单元格做多分类, 获得单元格内标签的概率$P\left(\mathbf{y}_{i, j} \mid s\right)$:</p><p>$$<br>P\left(\mathbf{y}_{i, j} \mid s\right)=\operatorname{Softmax}\left(\mathrm{dropout}\left(\mathbf{g}_{i, j}\right)\right)<br>$$</p><blockquote><p>作者提到, 这里在Logits之前使用Dropout的技巧为模型性能带来了一点提升. 这种Trick称为Logits Dropout. 默认设为0.2.</p></blockquote><p>然后使用多分类交叉熵优化:</p><p>$$<br>\mathcal{L}_{\text {entry }}=-\frac{1}{|s|^2} \sum_{i=1}^{|s|} \sum_{j=1}^{|s|} \log P\left(\mathbf{y}_{i, j}=y_{i, j} \mid s\right)<br>$$</p><p>其中$y_{i, j}$ 为表内的Golden Label.</p><h3 id="Constraints"><a href="#Constraints" class="headerlink" title="Constraints"></a>Constraints</h3><p>作者观察到, 表内的单元格并不是互相独立的, 上述分类器直接用上去就忽视了表内标签位置上的关联. 比如实体和无向关系在表内的标签总是正方形和长方形, 根据这个特性就令$\mathcal{P} \in \mathbb{R}^{|s| \times|s| \times|\mathcal{Y}|}$ 为表内所有单元格对应的标签概率的堆叠, 结合直观上的特性, 作者很直觉上的为模型加了两个约束Loss来帮助模型学习.</p><h4 id="Symmetry"><a href="#Symmetry" class="headerlink" title="Symmetry"></a>Symmetry</h4><p>表内的<strong>实体标签和对称关系标签总是对称</strong>的. 所以对它们来说, 表格上的对称位置应该具有<strong>相同的标签概率分布</strong>. 添加一个表内的对称Loss$\mathcal{L}_{\mathrm{sym}}$:</p><p>$$<br>\mathcal{L}_{\mathrm{sym}}=\frac{1}{|s|^{2}} \sum_{i=1}^{|s|} \sum_{j=1}^{|s|} \sum_{t \in \mathcal{Y}_{\mathrm{sym}}}\mid \mathcal{P}_{i, j, t}-\mathcal{P}_{j, i, t}\mid<br>$$</p><p>$\mathcal{Y}_\text{sym}$ 代表对称的标签, 也就是实体类型标签和对称关系类型的集合.</p><p>在数据集中包含的对称标签如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unire2.png" style="zoom:67%;" /><p>包含了所有的实体标签, 以及部分对称关系.</p><h4 id="Implication"><a href="#Implication" class="headerlink" title="Implication"></a>Implication</h4><p>因为<strong>关系是基于实体</strong>的, 所以作者认为<strong>关系存在的概率不应该比实体存在的概率更大</strong>.</p><p>更直观点, 因为第$i$ 行和第$i$ 列对应的是某个Word和其他Word之间存在的<strong>关系</strong>, 该Word对应的所有关系标签$l$ 的概率$\mathcal{P}_{i,:, l}, \mathcal{P}_{:, i, l}$ 都不应该大于它自身(位于主对角线上)实体标签$t$ 的概率$\mathcal{P}_{i, i, t}$, 因此添加下述辅助Loss:</p><p>$$<br>\mathcal{L}_{\mathrm{imp}}=\frac{1}{|s|} \sum_{i=1}^{|s|}\left[\max _{l \in \mathcal{Y}_r}\left\{\mathcal{P}_{i,:, l}, \mathcal{P}_{:, i, l}\right\}-\max _{t \in \mathcal{Y}_e}\left\{\mathcal{P}_{i, i, t}\right\}\right]_\ast<br>$$</p><p>其中$[u]_{\ast}= \max(u, 0)$ 为<strong>Hinge Loss</strong>, 由于概率之间的差值本身就比较小, 所以也就不用添加Margin了.</p><p>Hinge Loss可以最大化这两类概率之间的差值, 也就是让关系的概率更小些, 实体类型的概率更大些.</p><p>最后简单的将主Loss和两个约束Loss相加即可:</p><p>$$<br>\mathcal{L}=\mathcal{L}_{\text {entry }}+\mathcal{L}_{\text {sym }}+\mathcal{L}_{\mathrm{imp}}<br>$$</p><p>至此, 模型的大致运行过程可以由下图概括:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unire3.png" style="zoom:67%;" /><p>由PLM先抽取特征, 然后用双仿射得到一张将实体关系统一在同一空间下的表, 同时使用一个主Loss和两个辅助约束Loss对模型进行优化, 再由一种解码算法得到实体和关系.</p><h3 id="Decoding"><a href="#Decoding" class="headerlink" title="Decoding"></a>Decoding</h3><p>测试阶段需要根据$\mathcal{P} \in \mathbb{R}^{|s| \times|s| \times|\mathcal{Y}|}$ 进行解码, 作者希望这个解码算法能够做到又快又强.</p><p>作者使用了一种三步解码算法:</p><ol><li>解码Span以识别实体边界.</li><li>解码实体的实体类型.</li><li>解码实体对的关系类型.</li></ol><h4 id="Span-Decoding"><a href="#Span-Decoding" class="headerlink" title="Span Decoding"></a>Span Decoding</h4><p>作者观察到, 任何一个实体, 它所对应的<strong>行之间或列之间是完全一致的</strong>.</p><blockquote><p>因为同一个实体所对应的多个行或多个列<strong>仍然归属于该实体本身</strong>, 所以它们对应的关系和实体标签是一致的.</p></blockquote><p>因此, <strong>当相邻行或相邻列不同时, 一定意味着有实体边界的产生</strong>.</p><p>基于上述观察, 作者将$\mathcal{P} \in \mathbb{R}^{|s| \times|s| \times|\mathcal{Y}|}$ 从<strong>行视角</strong>展平为二维矩阵$\mathcal{P}^\text{row} \in \mathbb{R}^{|s| \times (|s| \times|\mathcal{Y}|)}$, 计算相邻行之间的欧氏距离. 与行相对应的, 从<strong>列视角</strong>展平为二维矩阵$\mathcal{P}^\text{col} \in \mathbb{R}^{(|s| \times |\mathcal{Y}| ) \times |s| }$, 计算相邻列之间的欧氏距离. 最后将二个距离求平均, 作为最终距离, 当最终距离大于一个阈值$\alpha$ 的时候, 则判定该位置为切割点.</p><p>结合作者给出的示意图很好理解找实体边界(切割点)的思想:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unire4.png" style="zoom:67%;" /><blockquote><p>这种对<strong>概率分布差求平均</strong>的解码方式很新颖啊, 也很取巧, 估计阈值$\alpha$ 的调整会对结果产生很大的影响(见后文实验).</p></blockquote><p>作者通过这种方式将解码时间复杂度缩减到了$\mathcal{O}(|s|)$.</p><blockquote><p>作者在脚注里偷偷写到, 在Inference阶段, 对于对称的标签$t \in \mathcal{Y}_{\text{sym}}$, 作者设置$\mathcal{P}_{i, j, t} = \mathcal{P}_{j, i, t} = (\mathcal{P}_{i, j, t} + \mathcal{P}_{j, i, t}) / 2$. 我认为这样做可以增大对称标签所对应的实体类型或关系类型的正确率, 对称关系或标签总是成对出现, 结合这种分割实体边界的分块思想, 更容易将块内实体或关系分类正确.</p></blockquote><h4 id="Entity-Type-Decoding"><a href="#Entity-Type-Decoding" class="headerlink" title="Entity Type Decoding"></a>Entity Type Decoding</h4><p>对于由Span Decoding给出的Span$(i, j)$, 实体类型$\hat{t}$ 由<strong>对角线</strong>上的对称的正方形中最大的实体类型概率得到:</p><p>$$<br>\hat{t}=\arg \max _{t \in \mathcal{Y}_e \cup\{\perp\}} \operatorname{Avg}\left(\mathcal{P}_{i: j, i: j, t}\right)<br>$$</p><p>若$\hat{t} \in \mathcal{Y}_e$, 则解码出实体, 若$\hat{t}=\perp$ 则认为Span$(i, j)$ 不是实体.</p><h4 id="Relation-Type-Decoding"><a href="#Relation-Type-Decoding" class="headerlink" title="Relation Type Decoding"></a>Relation Type Decoding</h4><p>类似于实体类型的得出, 在得到Span$(i,j)$ 存在的实体$e_1$ 和Span$(m, n)$ 存在的实体$e_2$ 后, 关系类型$\hat{l}$ 也由相应的实体构成的长方形得到:</p><p>$$<br>\hat{l}=\arg \max _{l \in \mathcal{Y}_r \cup\{\perp\}} \operatorname{Avg}\left(\mathcal{P}_{i: j, m: n, l}\right)<br>$$</p><p>若$\hat{t} \in \mathcal{Y}_r$, 则解码出关系三元组$(e1, e2, \hat{l})$, 若$\hat{t}=\perp$ 则认为$e_{1,}e_2$ 间没有关系.</p><p>上述三步解码可以归结为下图:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unire5.png" alt=""></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的实验设置请参照原论文.</p><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><p>数据集是NER上常用的三个数据及ACE04 / 05和SciERC, 统计信息如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unire6.png" style="zoom:75%;" /><h3 id="Performance-Comparison"><a href="#Performance-Comparison" class="headerlink" title="Performance Comparison"></a>Performance Comparison</h3><p>在这里关系预测使用的是<strong>严格评估</strong>, 也就是实体的边界, 类型, 以及它们之间的关系都正确时才算正确. 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unire7.png" style="zoom:67%;" /><p>Wang et al.和Zhong and Chen分别对应着20年ERE方法<a href="https://adaning.github.io/posts/37252.html">TSE</a>和<a href="https://adaning.github.io/posts/22256.html">PURE</a>. UniRE在ACE05上的表现并没有PURE好, 但在更困难的SciERC上效果略好于PURE. 在ACE04上的表现比Baseline好得多.</p><blockquote><p>由于关系预测使用的是<strong>严格标准</strong>, 所以关系预测任务难度要大得多, 整个ERE任务性能好不好应该看关系预测效果, 而不是实体抽取效果.</p></blockquote><p>当使用BERT - base做PLM的时候, 在所有数据集上UniRE都是略差于PURE的, 有些小尴尬. 我认为这可能证明了在困难任务上, Joint Model需要更大的PLM底层的知识作为驱动.</p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>在ACE05和SciERC上的消融实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unire8.png" style="zoom:67%;" /><p>在没有辅助Loss约束的情况下, 其实UniRE已经可以作为一个比较好的Baseline了.</p><p>影响如下:</p><ul><li>Symmetry Loss: 在ACE05上实体识别稍微掉了一点, 但是在SciERC上有明显涨点. 它对实体分类有效, 在两个数据集上似乎对关系分类也很有帮助的.</li><li>Implication Loss: 在ACE05上移除会有害于关系预测, 发挥了不小作用, SciERC上则是稍稍一点副作用.</li><li>Logit Dropout: 在两个数据集上, 使用Logit Dropout带来了两个点的提升.</li><li>Cross - Sentence Context: 像PURE里面一样, 也使用一个跨句的窗口来捕捉长距离依赖. 使用更大的上下文有益于任务.</li><li>Hard Decoding: 直接使用概率分布硬解码, 而不使用作者提出的解码算法. 对于正方形 / 长方形, 对应的实体类型 / 关系类型均取最高频次者. 如果直接硬解码, 效果会非常差, 证明了作者的解码方法有效.</li></ul><h3 id="Inference-Speed"><a href="#Inference-Speed" class="headerlink" title="Inference Speed"></a>Inference Speed</h3><p>作者比较了UniRE与PURE在两个数据集上的性能和推理速度, 以及参数:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unire9.png" style="zoom:75%;" /><p>PURE因为采用了两个独立的Encoder所以参数量是UniRE的两倍. 从结果中可以看出, UniRE比PURE的近似模型推理速度快不少. 性能上不相上下.</p><h3 id="Impact-of-Different-Threshold-α"><a href="#Impact-of-Different-Threshold-α" class="headerlink" title="Impact of Different Threshold α"></a>Impact of Different Threshold α</h3><p>在前文中提到过, 基于阈值的解码方法会因阈值大小而产生很大影响. 关于阈值的选取, 作者分析了ACE05验证集上的行距离分布, 如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unire10.png" style="zoom:75%;" /><p>作者发现不是实体边界的地方几乎都集中在0附近, 也就是归于同一实体的不同行得到的概率分布是十分相似的, 而位于实体边界的行之间的欧氏距离位于1.4到1.5左右较多.  这大致可以说明作者的直觉是正确的.</p><p>光有直观的感觉还不太够, 还要具体的实验数据来论证, 在ACE05验证集上Span F1, Entity F1, Relation F1随阈值的变化如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unire11.png" style="zoom:75%;" /><p>能够看到, 在阈值设定为1.4之前模型一直都有很稳定的表现, 而且表现得不错, 在将阈值设置为1.5时, Span F1和Entity F1直接有一个断崖式的下降. 也就是说当阈值大于1.5时边界将变得不再那么容易区分, 很多可以区分的实体都集中在1.4到1.5之间. Relation F1却下降的没有那么厉害, 作者认为是由于关系比较稀疏, 许多实体之间不存在关系, 所以关系受到的影响较小.</p><h3 id="Context-Window-and-Logit-Dropout-Rate"><a href="#Context-Window-and-Logit-Dropout-Rate" class="headerlink" title="Context Window and Logit Dropout Rate"></a>Context Window and Logit Dropout Rate</h3><p>作者探究了不同的上下文滑窗大小和Logits Dropout Rate对性能的影响:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unire12.png" style="zoom:67%;" /><p>Logits Dropout对UniRE有一定帮助, 在Dropout概率为0.2时比较好.</p><h3 id="Error-Analysis"><a href="#Error-Analysis" class="headerlink" title="Error Analysis"></a>Error Analysis</h3><p>作者对预测结果做了错误分析, 将其分为五类错误:</p><ul><li>Span Splitting Error(SSE): Span划分错误.</li><li>Entity not Found(ENF): 实体没预测出来.</li><li>Entity Type Error(ETE): 实体预测出来了, 但是实体类型找错了.</li><li>Relation not Found(RNF): 关系没有预测出来.</li><li>Relation Type Error(RTE): 关系预测出来了, 但是关系类型找错了.</li></ul><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unire13.png" style="zoom:75%;" /><p>能从图中看到, SSE占得比例其实比较小, 说明作者的Span解码算法是很有效的. 实体和关系没找到的比例远远比类型预测错误要多, 作者认为是因为表中的类别不平衡问题仍然存在(毕竟表内大多的地方的标签都为$\perp$).</p><p>在文章最后, 作者给出了两个关于解码算法<strong>鲁棒性</strong>的例子:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unire14.png" style="zoom:75%;" /><p>即使双仿射得出的模型预测Intermediate Table是有一部分错误的, 作者所提出的解码算法仍然能够抽取出正确的实体和关系.</p><p>第一个例子没太懂是怎么消除掉的, 第二个例子中, Intermediate Table的橘色部分虽然列视角第三列和第四列差异足够大, 但在行视角中四三行和第四行并没有差别, 所以没有被切割开, 因此解码后和Golden Label一致.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>非常扎实而且有意思的一篇论文.</p><p>UniRE用一种<strong>统一标签</strong>, 尝试将<strong>ERE</strong>任务在<strong>一张实体关系统一表</strong>上用<strong>填表法</strong>解决, 理论上消除了实体空间和关系空间的偏差.</p><p>我认为无论是从<strong>表标签设计</strong>, <strong>解码设计</strong>, 还是最后文中作者对模型中的阈值选取的分析等, 都是很有深度的.</p><p>尤为亮眼的是Span解码的方式设计, 使用了一种非常新颖的, <strong>基于概率分布差再求平均的方式判断实体边界</strong>, 因为将实体和关系杂糅在一张表中, 所以本身UniRE的表内解码是非常容易出错的. 甚至可以说这个解码算法是驱动UniRE能做Work的源头.<br>作者在文末还展示出解码算法的鲁棒性打消读者的疑虑, 可以说是考虑的面面俱到了.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ERE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DirectRel: Relational Triple Extraction - One Step is Enough</title>
      <link href="/posts/61261.html"/>
      <url>/posts/61261.html</url>
      
        <content type="html"><![CDATA[<h1 id="Relational-Triple-Extraction-One-Step-is-Enough"><a href="#Relational-Triple-Extraction-One-Step-is-Enough" class="headerlink" title="Relational Triple Extraction: One Step is Enough"></a>Relational Triple Extraction: One Step is Enough</h1><p>本文是论文<a href="https://arxiv.org/abs/2205.05270" target="_blank" rel="noopener">Relational Triple Extraction: One Step is Enough</a> 的阅读笔记和个人理解, 论文来自<strong>IJCAI 2022</strong>.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者认为当前的RTE方法大多包含两步, 首先抽取出头尾实体的边界位置, 然后再把它们转化成三元组. 作者认为大多数的方法仍然存在<strong>误差累计</strong>的问题, 比如头 / 尾实体边界的识别错误会影响最终生成的三元组. </p><p>因此, 作者希望提出一个非常简单的单步抽取模型来解决误差累计. 具体的, 作者希望从句子中生成<strong>候选实体</strong>, 并从<strong>二部图链接</strong>的视角来描述头实体到尾实体的链接过程, 这样逻辑上只有一步, 如下图所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/directrel1.png" style="zoom: 50%;" /><p>从图中可以看出, 作者通过穷举Span的方式构造了大量的候选实体, 并在它们之间做二部图链接, 以形成三元组.</p><blockquote><p>其实就是用穷举Span把实体边界抽取绕开了, 直接做实体之间的链接.</p></blockquote><h2 id="DirectRel"><a href="#DirectRel" class="headerlink" title="DirectRel"></a>DirectRel</h2><p>DirectRel的模型设计是十分简单的:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/directrel2.png" style="zoom:50%;" /><p>从图上来看, 先生成实体表示, 然后把实体表示分别投影到Subject和Object的空间, 接着对两个空间内的实体表示做二部图匹配, 从而构造出三元组. 图中展示了一种EPO的情况, 即使头尾实体相同, 也是可以抽取出来的.</p><h3 id="Task-Definition"><a href="#Task-Definition" class="headerlink" title="Task Definition"></a>Task Definition</h3><p>对于给定的$L$ 个Token的句子$\mathcal{S}=\set{w_1, w_2, \dots, w_l}$ 以及$K$ 个已经定义好的关系$\mathcal{R}=\set{r_{1}, r_{2}, \cdots, r_K}$, RTE的目标为识别出句子$\mathcal{S}$ 中的所有$N$ 个三元组$\mathcal{T}=\left\{\left(h, r, t\right)|h,t\in \hat{\mathcal{E}}, r \in \mathcal{R}\right\}$, $\hat{\mathcal{E}}$ 为三元组中的头尾实体, 而不是句子中出现的所有命名实体.</p><h3 id="Candidate-Entities-Generation"><a href="#Candidate-Entities-Generation" class="headerlink" title="Candidate Entities Generation"></a>Candidate Entities Generation</h3><p>在预处理时, 作者<strong>穷举</strong>出所有长度小于$C (C&lt;L)$ 的连续的Token Span作为候选实体. 例如, $C=2$ 时, 句子”Beijing is the capital of China”的候选实体为$\mathcal{E}$={“Beijing”, “Beijing is”, “is”, “is the”, “the”, “the Capital”, “Capital”, “Capital of”, “of”, “of China”, “China”}.</p><p>如果句子有$L$ 个Token, 那么候选实体数量为$|\mathcal{E}|$:</p><p>$$<br>|\mathcal{E}|=L \times C+\frac{C}{2}-\frac{C^2}{2}<br>$$</p><p>作者注意到, 这样做会不可避免的引入过多的<strong>负样本</strong>候选实体, 并带来非常大的<strong>计算开销</strong>. 因此, 作者只从生成的所有候选实体$\mathcal{E}$ 中<strong>随机采样</strong>出$n_{neg}$ 个负样本实体, 和三元组Ground Truth中的正样本实体构成新的子集$\overline{\mathcal{E}}$ 作为训练中的候选实体集.</p><p>作者在实验中设置负样本候选实体个数为100.</p><blockquote><p>采样仍然不能解决推理过程中的计算开销问题, 只能加速训练过程并解决正负样本不均衡的问题.</p></blockquote><h3 id="Bipartite-Graph-Linking"><a href="#Bipartite-Graph-Linking" class="headerlink" title="Bipartite Graph Linking"></a>Bipartite Graph Linking</h3><p>对于给定的句子以及其候选实体$\overline{\mathcal{E}}$ , 使用BERT来抽取句子中Token的特征$\boldsymbol{h}_{i}\in \mathbb{R}^d$:</p><p>$$<br>\left[\boldsymbol{h}_1, \boldsymbol{h}_2, \ldots, \boldsymbol{h}_L\right]=B E R T\left(\left[\boldsymbol{x}_1, \boldsymbol{x}_2, \ldots, \boldsymbol{x}_L\right]\right)<br>$$</p><p>其中$\boldsymbol{x}_i$ 为输入句子中第$i$ 个Token的表示.</p><p>为了将候选实体表示出来, 作者简单的采用候选实体的起始Token和结束Token的<strong>平均</strong>来作为候选实体表示$\boldsymbol{e}_i$:</p><p>$$<br>\boldsymbol{e}_i=\frac{\boldsymbol{h}^{\text {start }}+\boldsymbol{h}^{e n d}}{2}<br>$$</p><p>接着, 将所有实体表示$\boldsymbol{E}$ 投影到头尾实体的语义空间里, 可以由两个线性层拿到:</p><p>$$<br>\begin{aligned}<br>\boldsymbol{E}_{\text {head }} &amp;= \boldsymbol{W}^{T}_{h}\boldsymbol{E}+ \boldsymbol{b}_{h}\\<br>\boldsymbol{E}_{\text {tail }} &amp;= \boldsymbol{W}^{T}_{t}\boldsymbol{E}+ \boldsymbol{b}_{t}<br>\end{aligned}<br>$$</p><p>其中, $\boldsymbol{W}_h, \boldsymbol{W}_{t} \in \mathbb{R}^{d_{e}\times d}, \boldsymbol{b}_{h,}\boldsymbol{b}_t$ 为可训练参数.</p><p>然后直接就做Subject和Object的<strong>二部图链接</strong>, 预测出头实体和尾实体在关系$k$ 下链接存在的概率:</p><p>$$<br>\boldsymbol{P}^k=\sigma\left(\boldsymbol{E}_{\text {head }}^T \boldsymbol{U}_k \boldsymbol{E}_{\text {tail }}\right)<br>$$</p><p>其中, $\sigma$ 为Sigmoid激活函数, $\boldsymbol{U}_{k} \in \mathbb{R} ^ {d_{e}\times d_e}$ 为关系$k$ 特化的矩阵. 当三元组$\boldsymbol{P}^k_{ij}$ 成立的概率大于阈值$\theta$ 时, 则构成一个三元组.</p><blockquote><p>这样引入双线性的缺点就是, 每种关系都会带来一个大小为$\boldsymbol{U}_{k} \in \mathbb{R} ^ {d_{e}\times d_e}$ 的矩阵, 在WebNLG中关系比较多, 双线性矩阵就会非常多.</p></blockquote><h3 id="Objective-Function"><a href="#Objective-Function" class="headerlink" title="Objective Function"></a>Objective Function</h3><p>因为作者用的是二部图匹配, 所以损失函数用的是<strong>二分类交叉熵</strong>:</p><p>$$<br>\mathcal{L}=-\frac{1}{|\overline{\mathcal{E}}| \times K \times|\overline{\mathcal{E}}|} \times<br>\sum_{i=1}^{|\overline{\mathcal{E}}|} \sum_{k=1}^K \sum_{j=1}^{|\overline{\mathcal{E}}|}\left(y_t \log \left(\boldsymbol{P}_{i j}^k\right)+\left(1-y_t\right) \log \left(1-\boldsymbol{P}_{i j}^k\right)\right)<br>$$</p><p>其中, $|\overline{\mathcal{E}}|$ 为训练中使用的实体数量, $K$ 为预定义好的关系数量, $y_t$ 为三元组$(e_{i,}r_{k,}e_{j})$ 的Golden Label.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的实验设置请参照原论文.</p><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><p>仍然是选用NYT, WebNLG的精准匹配版本以及其部分匹配版本NYT*, WebNLG*作为数据集, 统计信息如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/directrel3.png" style="zoom: 50%;" /><p>作者额外添加了一个实体长度的统计, WebNLG里面的实体长度似乎有些很长.</p><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><p>在上述数据集上, DirectRel实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/directrel4.png" style="zoom:50%;" /><p>DirectRel在Baseline里面达到了SOTA, 在NYT上领先不是很多.</p><h3 id="Detailed-Results-on-Complex-Scenarios"><a href="#Detailed-Results-on-Complex-Scenarios" class="headerlink" title="Detailed Results on Complex Scenarios"></a>Detailed Results on Complex Scenarios</h3><p>DirectRel在不同类型三元组的表现如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/directrel5.png" style="zoom: 50%;" /><p>DirectRel似乎对SEO的处理要略差一点, 说明DirectRel处理一对多的三元组能力差一点. 它其他的都还不错.</p><h3 id="Results-on-Different-Sub-tasks"><a href="#Results-on-Different-Sub-tasks" class="headerlink" title="Results on Different Sub - tasks"></a>Results on Different Sub - tasks</h3><p>不同子任务上, DirectRel结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/directrel6.png" style="zoom:50%;" /><p>这里主要是为了证明单阶段抽取效果, 和PRGC比了一下, DirectRel是全面领先的(其实和其他Baseline比也一样).</p><h3 id="Parameter-Analysis"><a href="#Parameter-Analysis" class="headerlink" title="Parameter Analysis"></a>Parameter Analysis</h3><p>为了证明随机采样候选实体的好处, 作者做了采样数量关于每个Batch训练时间, GPU显存占用, 性能的变化曲线(从左至右):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/directrel7.png" style="zoom:50%;" /><p>随着采样数量增大, 训练时间边长, 显存占用变大, 但性能也是随之增大的, 到100左右不再增长.</p><p>训练速度在3090上看起来还是挺快的, 粗略的算了一下在WebNLG上跑完也就需要三个多小时.</p><h3 id="Error-Analysis"><a href="#Error-Analysis" class="headerlink" title="Error Analysis"></a>Error Analysis</h3><p>作者分析了测试集中DirectRel的实体识别错误类型, 分为Span分割错误, 实体未找到, 和实体角色错误:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/directrel8.png" style="zoom:50%;" /><p>可以看到, 最大一部分错误来源是实体角色错误, 也就是说头尾实体可能搞反了.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>事实上DirectRel和<a href="https://adaning.github.io/posts/4754.html">OneRel</a>出自同一个组, 这两篇文章的风格如出一辙, 基于简单的出发点, 提出简单的模型, 单阶段抽取, 然后从<strong>效率</strong>和<strong>性能</strong>两个方面论述模型的优点. 我推荐把<a href="https://adaning.github.io/posts/4754.html">OneRel</a>也顺手看一看.<br>无论是OneRel还是DirectRel, 它们的本质仍然是一种<strong>填表</strong>的方法, 所以还是<strong>单阶段了, 但是没有很单阶段</strong>.<br>模型本身非常简单, 论文写作技巧上还是非常值得学习的.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RTE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OneRel: Joint Entity and Relation Extraction with One Module in One Step</title>
      <link href="/posts/4754.html"/>
      <url>/posts/4754.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>TPLinker: <a href="https://adaning.github.io/posts/49694.html">TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking</a>.</li></ul></blockquote><h1 id="OneRel-Joint-Entity-and-Relation-Extraction-with-One-Module-in-One-Step"><a href="#OneRel-Joint-Entity-and-Relation-Extraction-with-One-Module-in-One-Step" class="headerlink" title="OneRel: Joint Entity and Relation Extraction with One Module in One Step"></a>OneRel: Joint Entity and Relation Extraction with One Module in One Step</h1><p>本文是论文<a href="https://arxiv.org/abs/2203.05412" target="_blank" rel="noopener">OneRel:Joint Entity and Relation Extraction with One Module in One Step</a>的阅读笔记和个人理解, 论文来自<strong>AAAI 2022</strong>.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>现有的RTE方法经常将其分解为几个步骤, 使得其比较好抽取. 然而, 这种分解常常忽视了三元组是互相关联的整体, 当前方法常存在<strong>误差累计</strong>和<strong>信息冗余</strong>的问题. 例如:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/onerel1.png" style="zoom: 70%;" /><p>它们分别对应着:</p><ul><li><strong>多模多步</strong>: <a href="https://adaning.github.io/posts/27105.html">CasRel</a>, <a href="https://adaning.github.io/posts/53442.html">PRGC</a>等.</li><li><strong>多模单步</strong>: <a href="https://adaning.github.io/posts/49694.html">TPLinker</a>, <a href="https://adaning.github.io/posts/50175.html">SPN</a>, UniRE等.</li></ul><p>在本文中, 作者希望将三元组抽取视为一种<strong>细粒度三元组分类</strong>任务, 用单模单步的方式解决上述问题.</p><h2 id="OneRel"><a href="#OneRel" class="headerlink" title="OneRel"></a>OneRel</h2><h3 id="Task-Definition"><a href="#Task-Definition" class="headerlink" title="Task Definition"></a>Task Definition</h3><p>对于给定的$L$ 个Token的句子$\mathcal{S}=\set{w_1, w_2, \dots, w_l}$ 以及$K$ 个已经定义好的关系$\mathcal{R}=\set{r_1, r_2, \cdots, r_K}$, RTE的目标为识别出句子$\mathcal{S}$ 中的所有$N$ 个三元组$\mathcal{T}=\left\{\left(h_{i}, r_{i}, t_{i}\right)\right\}_{i=1}^{N}$, $h_{i}, t_i$ 分别为由连续Tokens构成的Span而组成的头实体和尾实体.</p><h3 id="Relation-Specific-Horns-Tagging"><a href="#Relation-Specific-Horns-Tagging" class="headerlink" title="Relation Specific Horns Tagging"></a>Relation Specific Horns Tagging</h3><p>作者设计了一个分类器, 来对$(w_{i,}r_{k,}w_j)$ 所有可能的组合来做分类, 也就是维护一张$\boldsymbol{M}^{L \times K \times L}$ 的<strong>三维表</strong>存储分类结果.</p><h4 id="Tagging"><a href="#Tagging" class="headerlink" title="Tagging"></a>Tagging</h4><p>作者设计标签时, 以<strong>BIE</strong>(Begin, Inside, End)为出发点.</p><p>如果将头实体和尾实体的BIE组合则需要9种标签, 实际上不需要这么多.</p><p>作者使用3种有效标签, 来标注头尾实体并建立二者的关联:</p><ul><li><strong>HB - TB</strong>: 标注头实体的头和尾实体的头.</li><li><strong>HB - TE</strong>: 标注头实体的头和尾实体的尾.</li><li><strong>HE - TE</strong>: 标注头实体的尾和尾实体的尾.</li><li><strong>NULL</strong>: 啥也不是.</li></ul><p>例如:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/onerel2.png" style="zoom: 80%;" /><p>由于它构成三元组的形状只需要三个支点, 因此也被作者称为关系特化的<strong>山羊角标注法</strong>(Rel - Spec Horns Tagging).</p><p>作者认为这种标注策略有三好:</p><ol><li>用3种标签而不是9种缩小了搜索空间.</li><li>稀疏的表$\boldsymbol{M}$ 有大量的负样本.</li><li>$\boldsymbol{M}$ 的稀疏性保证了解码速度.</li></ol><h4 id="Decoding"><a href="#Decoding" class="headerlink" title="Decoding"></a>Decoding</h4><p>解码的判定逻辑就是按照顺序往下找:</p><ol><li>找HB - TB.</li><li>如果同一行的下一个标签是HB -TE, 就向下找.</li><li>找到HE - TE, 形成一对三元组.</li></ol><h3 id="Scoring-based-Classifier"><a href="#Scoring-based-Classifier" class="headerlink" title="Scoring - based Classifier"></a>Scoring - based Classifier</h3><p>首先Encoder的特征抽取部分大家都是老生常谈了, 每个Token$d$ 维的Token Embedding$\boldsymbol{e}_{i}$ 可以由BERT得到:</p><p>$$<br>\left\{\boldsymbol{e}_{1}, \boldsymbol{e}_{2}, \ldots, \boldsymbol{e}_{L}\right\}=B E R T\left(\left\{\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \ldots, \boldsymbol{x}_{L}\right\}\right)<br>$$</p><p>$\boldsymbol{x}_{i}$ 为每个Token的输入表示.</p><p>接着, 作者按照作者的原思路, 继续穷举所有句子中可能的$\left(\boldsymbol{e}_{i}, \boldsymbol{r}_{k}, \boldsymbol{e}_{j}\right)$ 组合($\boldsymbol{r}_{k}$ 为随机初始化的关系Embedding), 用神经网络弄一个三元组打分分类器, 判定$\left(\boldsymbol{e}_{i}, \boldsymbol{r}_{k}, \boldsymbol{e}_{j}\right)$ 是否成立就好. 但是作者认为这样有两个缺陷:</p><ol><li>简单的分类器不能充分使得实体和关系间的交互, 且难以建模三元组的结构化信息.</li><li>做$L \times K \times L$ 次分类很耗时.</li></ol><p>所以作者提出了另一种思路, 受启发于<strong>KGE</strong>里的HOLE的打分函数:</p><p>$$<br>f_{r}(h, t)=\boldsymbol{r}^{T}(\boldsymbol{h} \star \boldsymbol{t})<br>$$</p><p>其中$\boldsymbol{h}, \boldsymbol{t}$ 分别为头尾实体的表示, $\star$ 为循环卷积操作. HOLE挖掘了两实体间的关联. 在作者的模型中, 将$\star$ 重定义为拼接后非线性投影:</p><p>$$<br>\boldsymbol{h} \star \boldsymbol{t}=\phi\left(\boldsymbol{W}[\boldsymbol{h} ; \boldsymbol{t}]^{T}+\boldsymbol{b}\right)<br>$$</p><p>其中$\boldsymbol{W} \in \mathbb{R}^{d_{e} \times 2d}$, $\boldsymbol{b}$ 为可训练参数, $d_e$ 代表头尾实体对表示的维度, $[;]$ 为拼接操作, $\phi(\cdot)$ 为ReLU激活函数. 作者认为这样有三好:</p><ol><li>新设计的打分函数可以和Encoder无缝衔接.</li><li>从实体特征到实体对特征维度可以由$\boldsymbol{W}$ 改变.</li><li>非对称性, 即$[\boldsymbol{h} ; \boldsymbol{t}] \neq[\boldsymbol{t} ; \boldsymbol{h}]$.</li></ol><p>然后用所有关系表示$\boldsymbol{R} \in \mathbb{R}^{d_{e}\times 4K}$ 同时计算所有Token对$(w_i, w_j)$ 所对应的$(w_{i,}r_{k,}w_j)_{k=1}^K$, $4$ 为分类标签, 所以作者的方法最后归结为:</p><p>$$<br>\boldsymbol{v}_{\left(w_{i}, r_{k}, w_{j}\right)_{k=1}^{K}}=\boldsymbol{R}^{T} \phi\left(\operatorname{drop}\left(\boldsymbol{W}\left[\boldsymbol{e}_{i} ; \boldsymbol{e}_{j}\right]^{T}+\boldsymbol{b}\right)\right)<br>$$</p><p>其中$\boldsymbol{v}$ 为得分向量, $\operatorname{drop}(\cdot)$ 为Dropout.</p><blockquote><p>尽管计算方式不怎么稀奇, 但是从KGE和三元组分类这个角度出发来解释倒是有点新颖.</p></blockquote><p>作者认为, 由于把表分类矩阵设为$\boldsymbol{R} \in \mathbb{R}^{d_{e}\times 4K}$, 一次性可以处理所有关系, 所以处理步数实际上为$L \times 1 \times L$, 要强于TPLinker.</p><blockquote><p>这个地方我也不是太确定, 对于作者的说法, 我寻思这不还是$L \times K \times L$ 的填表分类么? </p><p>TPLinker使用$2|R|+1$的表来标注(EH-ET只使用了一张), 而OneRel使用$|R|$ 的表来标注, 因为<strong>OneRel在标签设计上规避了不同类型标签重叠的可能性</strong>. 我认为这种说法要更具有说服力.</p></blockquote><p>最后把得分向量做个Softmax:</p><p>$$<br>P\left(\mathrm{y}_{\left(w_{i}, r_{k}, w_{j}\right)} \mid \mathcal{S}\right)=\operatorname{Softmax}\left(\boldsymbol{v}_{\left(w_{i}, r_{k}, w_{j}\right)}\right)<br>$$</p><p>训练损失函数为多分类交叉熵:</p><p>$$<br>\mathcal{L}_{\text {triple }}=-\frac{1}{L \times K \times L} \times  \sum_{i=1}^{L} \sum_{k=1}^{K} \sum_{j=1}^{L} \log P\left(\mathrm{y}_{\left(w_{i}, r_{k}, w_{j}\right)}=g_{\left(w_{i}, r_{k}, w_{j}\right)} \mid \mathcal{S}\right)<br>$$</p><p>其中$g_{(w_{i,}r_{k}, w_{j})}$ 为Golden Label.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>仍然是NYT, NYT*, WebNLG, WebNLG* 四个数据集, 统计信息如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/onerel3.png" style="zoom: 50%;" /><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><p>OneRel在四个数据集上的部分匹配和精准匹配结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/onerel4.png" style="zoom:50%;" /><p>OneRel - 指的是将分类器替换成$\boldsymbol{W}[\boldsymbol{e_i};\boldsymbol{r_{k}};\boldsymbol{e_j}]+\boldsymbol{b}$, 也就是使用简单的线性分类器. OneRel完全体达到了现在的SOTA, 看起来OneRel - 和完全体还是有一定差距.</p><h3 id="Detailed-Results-on-Complex-Scenarios"><a href="#Detailed-Results-on-Complex-Scenarios" class="headerlink" title="Detailed Results on Complex Scenarios"></a>Detailed Results on Complex Scenarios</h3><p>OneRel在不同类型三元组的表现如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/onerel5.png" style="zoom:50%;" /><p>OneRel在大多数情况下表现良好.</p><h3 id="Results-on-Different-Sub-tasks"><a href="#Results-on-Different-Sub-tasks" class="headerlink" title="Results on Different Sub - tasks"></a>Results on Different Sub - tasks</h3><p>按照预测三元组的不同部分区分, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/onerel6.png" style="zoom:50%;" /><p>OneRel在关系预测上比之前的模型都要强挺多, 实体预测也不弱, 在NYT*上的实体预测能力和SPN相当.</p><h3 id="Model-Efficiency"><a href="#Model-Efficiency" class="headerlink" title="Model Efficiency"></a>Model Efficiency</h3><p>论文在前面说了OneRel的速度比TPLinker快, 在这里作者做了个实验证明下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/onerel7.png" style="zoom:50%;" /><p>WebNLG*上差距挺明显的, NYT*上差距并不是特别大.</p><blockquote><p>我猜这里<strong>实现差异</strong>因素占了一大部分, 当时TPLinker跑关系是用的循环然后再运算的, 而OneRel直接做的<strong>并行</strong>张量运算. NYT*的关系数量比WebNLG*少得多, 所以二者相差也不大. 我更倾向于OneRel的效率高原因是因为用的表少.</p></blockquote><h3 id="Topology-Structure-of-Relations"><a href="#Topology-Structure-of-Relations" class="headerlink" title="Topology Structure of Relations"></a>Topology Structure of Relations</h3><p>既然往KGE靠边了, 使用了关系Embedding, 自然而然就能想到对它做可视化来证明模型有效. 下面是NYT上24中关系在T - SNE下的降维可视化图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/onerel8.png" style="zoom: 90%;" /><p>从图上来看确实语义相似的Embedding靠的比较近. 不过这种可视化图也只能从直觉上显示性能, 并不是绝对的.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>总体来说, OneRel其实没有什么亮点, 实质上属于一种<strong>填表式</strong>RTE模型, 文章中所称的单模单步有点牵强. 这回OneRel了, 但是没有很OneRel.</p><p>一般来说, Table - Filling based model的Decoder部分会设计的比较简单, 真正发挥作用的是如何建立表标签.</p><p>OneRel相较于TPLinker来说, 相差并不大. 不用建立多张表, 使用了更精炼的标注策略, 当然也付出了相应的代价.</p><blockquote><p>事实上, <strong>需要在每个token后多添加一个<code>[unused1]</code>来避免单个token的实体抽取问题</strong>. 这点作者在文章中未提起过.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RTE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch学习: Pytorch Lightning</title>
      <link href="/posts/34610.html"/>
      <url>/posts/34610.html</url>
      
        <content type="html"><![CDATA[<h1 id="Pytorch学习-Pytorch-Lightning"><a href="#Pytorch学习-Pytorch-Lightning" class="headerlink" title="Pytorch学习: Pytorch Lightning"></a>Pytorch学习: Pytorch Lightning</h1><p>Pytorch Lightning是在Pytorch基础上封装的框架, 号称”Pytorch里的Keras”, 如官网所述, 它具有灵活, 解耦, 易于复现, 自动化, 扩展性好等优点(实际上大多也是Keras的优点哈哈哈). 知乎上对Pytorch Lightning的议论比较多, 有些人认为Pytorch Lightning纯属过度封装, 但它事实上确实能解决一些Pytorch自身不好解决的问题. 最主要的其实是保证了<strong>代码复用</strong>, 节省时间.<br>和Huggingface出品的Trainer相比, 我感觉在大多数任务上, Pytorch Lightning要更加灵活一些.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>下面是一个官方给出的VAE在MNIST上的例子, 大概建立一下Pytorch Lightning的初印象:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> random_split<span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> MNIST<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> transforms<span class="token keyword">import</span> pytorch_lightning <span class="token keyword">as</span> pl<span class="token keyword">class</span> <span class="token class-name">LitAutoEncoder</span><span class="token punctuation">(</span>pl<span class="token punctuation">.</span>LightningModule<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>          nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">28</span> <span class="token operator">*</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>          nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>          nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>          nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>          nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>          nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">28</span> <span class="token operator">*</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        embedding <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> embedding    <span class="token keyword">def</span> <span class="token function">configure_optimizers</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> optimizer    <span class="token keyword">def</span> <span class="token function">training_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> train_batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>        x<span class="token punctuation">,</span> y <span class="token operator">=</span> train_batch        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        z <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>x<span class="token punctuation">)</span>            x_hat <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>z<span class="token punctuation">)</span>        loss <span class="token operator">=</span> F<span class="token punctuation">.</span>mse_loss<span class="token punctuation">(</span>x_hat<span class="token punctuation">,</span> x<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token string">'train_loss'</span><span class="token punctuation">,</span> loss<span class="token punctuation">)</span>        <span class="token keyword">return</span> loss    <span class="token keyword">def</span> <span class="token function">validation_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> val_batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>        x<span class="token punctuation">,</span> y <span class="token operator">=</span> val_batch        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        z <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x_hat <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>z<span class="token punctuation">)</span>        loss <span class="token operator">=</span> F<span class="token punctuation">.</span>mse_loss<span class="token punctuation">(</span>x_hat<span class="token punctuation">,</span> x<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> loss<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># data</span>dataset <span class="token operator">=</span> MNIST<span class="token punctuation">(</span><span class="token string">''</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>mnist_train<span class="token punctuation">,</span> mnist_val <span class="token operator">=</span> random_split<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">55000</span><span class="token punctuation">,</span> <span class="token number">5000</span><span class="token punctuation">]</span><span class="token punctuation">)</span>train_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>mnist_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">)</span>val_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>mnist_val<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># model</span>model <span class="token operator">=</span> LitAutoEncoder<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># training</span>trainer <span class="token operator">=</span> pl<span class="token punctuation">.</span>Trainer<span class="token punctuation">(</span>gpus<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> num_nodes<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> precision<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> limit_train_batches<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span>trainer<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>model<span class="token punctuation">,</span> train_loader<span class="token punctuation">,</span> val_loader<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Pytorch Lightning最重要的两个API便是<code>LightningModule</code>和<code>Trainer</code>. <code>pl.LightningModule</code>和<code>nn.Module</code>有点像对吧? 都有<code>forward()</code>. 没错, “A <code>LightningModule</code> is still just a <code>torch.nn.Module</code>“.<br>从代码里可以看出, 在<code>pl.LightningModule</code>下重写了<code>training_step</code>, <code>validation_step</code>, 完成模型训练和验证的<strong>内部流程</strong>即可, 整个训练的逻辑已经被它封装好了, 无需重写.<br>同时, DataLoader使用的是Pytorch自己的DataLoader, 二者兼容. Pytorch Lightning有对DataLoader在逻辑上的进一步封装, 方便组织数据的加载逻辑. 但是我自己用的不是很习惯, 本文中就不提及了, 感兴趣的去<a href="https://pytorch-lightning.readthedocs.io/en/latest/data/datamodule.html" target="_blank" rel="noopener">这里</a>查阅.<br>通过<code>trainer.fit</code>就开始了模型的训练, 和Keras很像.<br>事实上, 整个<code>pl.LightningModule</code>只是组织了下列6种行为的逻辑:</p><ul><li><strong>Computations</strong> (init).</li><li><strong>Train Loop</strong> (training_step) </li><li><strong>Validation Loop</strong> (validation_step)</li><li><strong>Test Loop</strong> (test_step)</li><li><strong>Prediction Loop</strong> (predict_step)</li><li><strong>Optimizers and LR Schedulers</strong> (configure_optimizers)<br>记住, <strong>它并没有做进一步抽象</strong>, 只是简单的<strong>把逻辑组织在一起</strong>.</li></ul><h2 id="Initialization-amp-Forward"><a href="#Initialization-amp-Forward" class="headerlink" title="Initialization &amp; Forward"></a>Initialization &amp; Forward</h2><p><code>pl.LightningModule</code><strong>继承</strong>于<code>nn.Module</code>, 也就是说你call它的时候会默认调用它的<code>forward()</code>.</p><p>但是, <code>forward</code>的具体行为在Training和Validation甚至是Prediction的时候可能是不同的, 所以只能写模型自身的逻辑, 不要把Loss的计算也写进去, 也不要把<code>logits.argmax</code>写进去.<br>一般来说, <code>pl.LightningModule</code>的初始化和<code>forward</code>是这样写的:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">TaskModel</span><span class="token punctuation">(</span>pl<span class="token punctuation">.</span>LightningModule<span class="token punctuation">)</span><span class="token punctuation">:</span>      <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model<span class="token punctuation">)</span><span class="token punctuation">:</span>          super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>          self<span class="token punctuation">.</span>model <span class="token operator">=</span> model      <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">**</span>inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>          <span class="token keyword">return</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>没错, 仅仅是将模型在<code>pl.LightningModule</code>初始化时作为参数传进来, 然后添加一个Hook… 就像<a href="https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#child-modules" target="_blank" rel="noopener">这样</a>. 强烈建议把模型本身和训练逻辑解耦, 将来改起来方便很多.</p><h2 id="Training-amp-Validation"><a href="#Training-amp-Validation" class="headerlink" title="Training &amp; Validation"></a>Training &amp; Validation</h2><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p><code>pl.LightningModule</code>组织的训练逻辑伪代码如下:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">fit_loop</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    on_train_epoch_start<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> batch <span class="token keyword">in</span> train_dataloader<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        on_train_batch_start<span class="token punctuation">(</span><span class="token punctuation">)</span>        on_before_batch_transfer<span class="token punctuation">(</span><span class="token punctuation">)</span>        transfer_batch_to_device<span class="token punctuation">(</span><span class="token punctuation">)</span>        on_after_batch_transfer<span class="token punctuation">(</span><span class="token punctuation">)</span>        training_step<span class="token punctuation">(</span><span class="token punctuation">)</span>        on_before_zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        optimizer_zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        on_before_backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        on_after_backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        on_before_optimizer_step<span class="token punctuation">(</span><span class="token punctuation">)</span>        configure_gradient_clipping<span class="token punctuation">(</span><span class="token punctuation">)</span>        optimizer_step<span class="token punctuation">(</span><span class="token punctuation">)</span>        on_train_batch_end<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> should_check_val<span class="token punctuation">:</span>            val_loop<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># end training epoch</span>    training_epoch_end<span class="token punctuation">(</span><span class="token punctuation">)</span>    on_train_epoch_end<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>似乎很多对吧, 事实上我们只需要关注下面两个函数:</p><ol><li><code>training_step(self, batch, batch_idx)</code>.</li><li><code>training_epoch_end(self, training_step_outputs)</code>.</li></ol><p>其他的函数在项目规模不大的时候不会用到, 例如<code>on_train_epoch_end</code>, <code>on_train_batch_end</code>, 看起来比较美好, 但是实际上有些鸡肋, 因为合适的逻辑已经在<code>training_step</code>和<code>training_epoch_end</code>里搞定了, 而且它们不存在耦合问题.<br>我们使用Pytorch Lightning的目的就是为了快速搭建一套能跑的流程, 如果真的用到了再去查文档就好.</p><h4 id="training-step"><a href="#training-step" class="headerlink" title="training_step"></a>training_step</h4><p>batch就是<code>DataLoader</code>里返回的batch, 一般来说<code>training_step</code>里就是把batch解包, 然后计算loss. 例如:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">training_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>    x<span class="token punctuation">,</span> y <span class="token operator">=</span> batch    y_hat <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    loss <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span>    <span class="token keyword">return</span> loss<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>返回值可以是loss, 也可以是一个<strong>字典</strong>, 如果你想在每个训练epoch结束的时候再计算点别的什么东西, 可以这样写:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">training_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>    x<span class="token punctuation">,</span> y <span class="token operator">=</span> batch    y_hat <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    loss <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span>    preds <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>    <span class="token keyword">return</span> <span class="token punctuation">{</span>        <span class="token string">"loss"</span><span class="token punctuation">:</span> loss<span class="token punctuation">,</span>         <span class="token string">"other_stuff"</span><span class="token punctuation">:</span> preds<span class="token punctuation">,</span>    <span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这样在<code>training_epoch_end</code>中可以取到<code>other_stuff</code>. 但是一定要保证里面有个<code>loss</code>, 这样才能保证整个batch正常工作.</p><h4 id="training-epoch-end"><a href="#training-epoch-end" class="headerlink" title="training_epoch_end"></a>training_epoch_end</h4><p>在每个epoch训练结束时调用<code>training_epoch_end</code>, 其参数<code>training_step_outputs</code>实际上是<strong>每个step返回的字典的一个列表</strong>.<br>例如:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">training_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>    x<span class="token punctuation">,</span> y <span class="token operator">=</span> batch    y_hat <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    loss <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span>    preds <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>    <span class="token keyword">return</span> <span class="token punctuation">{</span><span class="token string">"loss"</span><span class="token punctuation">:</span> loss<span class="token punctuation">,</span> <span class="token string">"other_stuff"</span><span class="token punctuation">:</span> preds<span class="token punctuation">}</span><span class="token keyword">def</span> <span class="token function">training_epoch_end</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> training_step_outputs<span class="token punctuation">)</span><span class="token punctuation">:</span>    all_preds <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>training_step_outputs<span class="token punctuation">)</span>    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>training_epoch_end</code>无返回值限制.<br>例子中的<code>preds</code>应该也是一个Tensor, 我们也可以在每个step结束时返回其他类型的值.</p><h4 id="log"><a href="#log" class="headerlink" title="log"></a>log</h4><p>在训练时一般都要把loss记录下来, 使用<code>self.log()</code>就可以把标量记录下来, 在其他地方也都可以随时使用. 例如:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">training_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>    x<span class="token punctuation">,</span> y <span class="token operator">=</span> batch    y_hat <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    loss <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># logs metrics for each training_step,</span>    <span class="token comment" spellcheck="true"># and the average across the epoch, to the progress bar and logger</span>    self<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token string">"train_loss"</span><span class="token punctuation">,</span> loss<span class="token punctuation">,</span> on_step<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> on_epoch<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> prog_bar<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> logger<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> loss<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>on_step</code>即一个step记录一次, 如果也同时<code>on_epoch</code>, 它会将整个epoch的loss加起来求个平均, 在上述代码里同时记录了<code>train_loss_step</code>和<code>train_loss_epoch</code>.<br>记录的值可以在<strong>Tensorboard</strong>里看到, 非常方便.</p><ul><li>如果有多个要记录的值, 可以把它们都放进一个字典里, 然后使用<code>self.log_dict(dict)</code>一并记录下来. </li><li>如果要记录的内容是图像, 语音等其他类型, 则需要调用<code>logger</code>来存储, 从<a href="https://pytorch-lightning.readthedocs.io/en/latest/visualize/experiment_managers.html" target="_blank" rel="noopener">这里</a>获取更多信息.</li></ul><h3 id="Validataion"><a href="#Validataion" class="headerlink" title="Validataion"></a>Validataion</h3><p>验证和被包含在训练逻辑中, 但流程几乎是一样的, 只是少了梯度优化的参与.<br><code>pl.LightningModule</code>组织的验证逻辑伪代码如下:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">val_loop</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    on_validation_model_eval<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># calls `model.eval()`</span>    torch<span class="token punctuation">.</span>set_grad_enabled<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span>    on_validation_start<span class="token punctuation">(</span><span class="token punctuation">)</span>    on_validation_epoch_start<span class="token punctuation">(</span><span class="token punctuation">)</span>    val_outs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> batch_idx<span class="token punctuation">,</span> batch <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>val_dataloader<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        on_validation_batch_start<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span>        batch <span class="token operator">=</span> on_before_batch_transfer<span class="token punctuation">(</span>batch<span class="token punctuation">)</span>        batch <span class="token operator">=</span> transfer_batch_to_device<span class="token punctuation">(</span>batch<span class="token punctuation">)</span>        batch <span class="token operator">=</span> on_after_batch_transfer<span class="token punctuation">(</span>batch<span class="token punctuation">)</span>        out <span class="token operator">=</span> validation_step<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span>        on_validation_batch_end<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span>        val_outs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>out<span class="token punctuation">)</span>    validation_epoch_end<span class="token punctuation">(</span>val_outs<span class="token punctuation">)</span>    on_validation_epoch_end<span class="token punctuation">(</span><span class="token punctuation">)</span>    on_validation_end<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># set up for train</span>    on_validation_model_train<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># calls `model.train()`</span>    torch<span class="token punctuation">.</span>set_grad_enabled<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>与训练不同的是, 在验证开始前, <code>pl.LightningModule</code>会自动为我们启用<code>model.eval()</code>, 还会禁用梯度. 可以不必重复声明<code>torch.no_grad</code>, 如果不放心的话可以再包上一层.<br>我们同样只需要关注与训练过程相似的两个函数:</p><ol><li><code>validation_step(self, batch, batch_idx)</code>.</li><li><code>validation_epoch_end(self, validation_step_outputs)</code>.</li></ol><h4 id="validation-step"><a href="#validation-step" class="headerlink" title="validation_step"></a>validation_step</h4><p>与训练中的<code>training_step</code>相同. 直接贴出一个例子:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LitModel</span><span class="token punctuation">(</span>pl<span class="token punctuation">.</span>LightningModule<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">validation_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>        x<span class="token punctuation">,</span> y <span class="token operator">=</span> batch        y_hat <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        loss <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token string">"val_loss"</span><span class="token punctuation">,</span> loss<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="validation-epoch-end"><a href="#validation-epoch-end" class="headerlink" title="validation_epoch_end"></a>validation_epoch_end</h4><p>与训练中的<code>training_epoch_end</code>相同, 这里拿到的<code>validation_step_outputs</code>也是每个<code>validation_step</code>的返回值的一个字典的列表. 例如:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">validation_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>    x<span class="token punctuation">,</span> y <span class="token operator">=</span> batch    y_hat <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    loss <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span>    pred <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>    <span class="token keyword">return</span> pred<span class="token keyword">def</span> <span class="token function">validation_epoch_end</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> validation_step_outputs<span class="token punctuation">)</span><span class="token punctuation">:</span>    all_preds <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>validation_step_outputs<span class="token punctuation">)</span>    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>validation_epoch_end</code>无返回值限制.</p><h2 id="Optimizer-amp-LR-Scheduler"><a href="#Optimizer-amp-LR-Scheduler" class="headerlink" title="Optimizer &amp; LR Scheduler"></a>Optimizer &amp; LR Scheduler</h2><p>在文章最开始的例子中, 我们重写了<code>configure_optimizers()</code>来为模型准备优化器. 大多数时候我们只需要一个optimizer和scheduler:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">configure_optimizers</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># or</span><span class="token keyword">def</span> <span class="token function">configure_optimizers</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    optimizer <span class="token operator">=</span> Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span>    scheduler <span class="token operator">=</span> get_linear_schedule_with_warmup<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> self<span class="token punctuation">.</span>total_step<span class="token punctuation">)</span>    <span class="token keyword">return</span> <span class="token punctuation">[</span>optimizer<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>scheduler<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果只有optimizer, 直接返回即可, 如果还有scheduler, 则需要把optimizer和scheduler分别套上一个list返回.<br>同时, 在<code>pl.LightningModule</code>内部使用<code>self.parameters()</code>可以获得所有的模型参数, 因为它继承了<code>nn.Module</code>.<br>再复杂一点, 也可以通过返回字典来控制optimizer和scheduler执行的间隔(<code>interval / frequency</code>):</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># example with step-based learning rate schedulers</span><span class="token keyword">def</span> <span class="token function">configure_optimizers</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    gen_opt <span class="token operator">=</span> Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>model_gen<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>    dis_opt <span class="token operator">=</span> Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>model_disc<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.02</span><span class="token punctuation">)</span>    gen_sched <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'scheduler'</span><span class="token punctuation">:</span> ExponentialLR<span class="token punctuation">(</span>gen_opt<span class="token punctuation">,</span> <span class="token number">0.99</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                 <span class="token string">'interval'</span><span class="token punctuation">:</span> <span class="token string">'step'</span><span class="token punctuation">}</span>  <span class="token comment" spellcheck="true"># called after each training step</span>    dis_sched <span class="token operator">=</span> CosineAnnealing<span class="token punctuation">(</span>discriminator_opt<span class="token punctuation">,</span> T_max<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># called every epoch</span>    <span class="token keyword">return</span> <span class="token punctuation">[</span>gen_opt<span class="token punctuation">,</span> dis_opt<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>gen_sched<span class="token punctuation">,</span> dis_sched<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># example with optimizer frequencies</span><span class="token comment" spellcheck="true"># see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1</span><span class="token comment" spellcheck="true"># https://arxiv.org/abs/1704.00028</span><span class="token keyword">def</span> <span class="token function">configure_optimizers</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    gen_opt <span class="token operator">=</span> Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>model_gen<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>    dis_opt <span class="token operator">=</span> Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>model_disc<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.02</span><span class="token punctuation">)</span>    n_critic <span class="token operator">=</span> <span class="token number">5</span>    <span class="token keyword">return</span> <span class="token punctuation">(</span>        <span class="token punctuation">{</span><span class="token string">'optimizer'</span><span class="token punctuation">:</span> dis_opt<span class="token punctuation">,</span> <span class="token string">'frequency'</span><span class="token punctuation">:</span> n_critic<span class="token punctuation">}</span><span class="token punctuation">,</span>        <span class="token punctuation">{</span><span class="token string">'optimizer'</span><span class="token punctuation">:</span> gen_opt<span class="token punctuation">,</span> <span class="token string">'frequency'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">}</span>    <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>注意, <code>transformers</code> 里面的Warm up LRScheduler往往是根据Step完成学习率调节的!!! 而Pytorch Lightning默认是每个Epoch调整一次学习率</strong> 这点非常重要!!!</p><p>所以在使用<code>transformers</code> 的Scheduler时, <strong>必须</strong>把Scheduler的执行间隔<code>interval</code> 设置为<code>step</code>, 放入字典中返回:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AdamW<span class="token punctuation">,</span> get_linear_schedule_with_warmup<span class="token keyword">def</span> <span class="token function">configure_optimizers</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    optimizer <span class="token operator">=</span> Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span>    total_steps <span class="token operator">=</span> self<span class="token punctuation">.</span>trainer<span class="token punctuation">.</span>estimated_stepping_batches    warmup_steps <span class="token operator">=</span> warmup_ratio <span class="token operator">*</span> total_steps    scheduler <span class="token operator">=</span> get_linear_schedule_with_warmup<span class="token punctuation">(</span>        optimizer<span class="token punctuation">,</span>         num_warmup_steps<span class="token operator">=</span>warmup_steps<span class="token punctuation">,</span>          num_training_steps<span class="token operator">=</span>total_steps<span class="token punctuation">,</span>    <span class="token punctuation">)</span>    scheduler <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">"scheduler"</span><span class="token punctuation">:</span> scheduler<span class="token punctuation">,</span> <span class="token string">"interval"</span><span class="token punctuation">:</span> <span class="token string">"step"</span><span class="token punctuation">}</span>    <span class="token keyword">return</span> <span class="token punctuation">[</span>optimizer<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>scheduler<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>示例中的<code>total_steps</code>, 可以直接通过Trainer的<code>estimated_stepping_batches</code> 属性拿到, 不用手动计算.</p><blockquote><p>此外, 一些特殊情况会用到多个优化器或者多个Scheduler, 首先参考<a href="https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#configure-optimizers" target="_blank" rel="noopener">这里</a> , 并在<code>training_step</code>中使用<code>optimizer_idx</code>来控制loss和optimizer的关联, 参考<a href="https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#training-step" target="_blank" rel="noopener">这里</a>.</p></blockquote><h2 id="Test-amp-Predict"><a href="#Test-amp-Predict" class="headerlink" title="Test &amp; Predict"></a>Test &amp; Predict</h2><p>PL应该是为了满足客制化而将Test和Predict区分开. 在我们跑实验而没有部署时, Test和Predict行为并没有什么区别, 但测试和真正Inference的时候的Predict还是不一样的, Predict没有标签.<br>和验证时相同, model.eval()和<code>torch.no_grad()</code>会自动在测试和预测时自动配上.<br>当Trainer调用<code>trainer.test()</code>时, 会调用<code>test_step()</code>, 它与<code>training_step</code>, <code>validation_step</code>类似, 一般重写<code>test_step</code>时只是一层对<code>validation_step</code>的封装.<br>在测试结束时, 我比较推荐在<code>test_step</code>返回batch级的预测结果, <code>test_epoch_end</code>一并<strong>保存实验结果</strong>, 这样封装一层比较有意义.<br>Predict仅有<code>predict_step</code>, 而没有<code>predict_epoch_end</code>.</p><h2 id="Trainer"><a href="#Trainer" class="headerlink" title="Trainer"></a>Trainer</h2><p><code>pl.LightningModule</code><strong>组织</strong>了逻辑, 而<code>pl.Trainer</code><strong>驱动</strong>了流程.<br>其拟合阶段伪代码如下:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">fit</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> global_rank <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># prepare data is called on GLOBAL_ZERO only</span>        prepare_data<span class="token punctuation">(</span><span class="token punctuation">)</span>    configure_callbacks<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">with</span> parallel<span class="token punctuation">(</span>devices<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># devices can be GPUs, TPUs, ...</span>        train_on_device<span class="token punctuation">(</span>model<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">train_on_device</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># called PER DEVICE</span>    on_fit_start<span class="token punctuation">(</span><span class="token punctuation">)</span>    setup<span class="token punctuation">(</span><span class="token string">"fit"</span><span class="token punctuation">)</span>    configure_optimizers<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># the sanity check runs here</span>    on_train_start<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> epochs<span class="token punctuation">:</span>        fit_loop<span class="token punctuation">(</span><span class="token punctuation">)</span>    on_train_end<span class="token punctuation">(</span><span class="token punctuation">)</span>    on_fit_end<span class="token punctuation">(</span><span class="token punctuation">)</span>    teardown<span class="token punctuation">(</span><span class="token string">"fit"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>一般我们这样使用Trainer来完成包含测试在内的整个流程:</p><pre class="line-numbers language-python"><code class="language-python">model <span class="token operator">=</span> MyLightningModule<span class="token punctuation">(</span><span class="token punctuation">)</span>trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span><span class="token punctuation">)</span>trainer<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>model<span class="token punctuation">,</span> train_dataloader<span class="token punctuation">,</span> val_dataloader<span class="token punctuation">)</span>trainer<span class="token punctuation">.</span>test<span class="token punctuation">(</span>model<span class="token punctuation">,</span> test_dataloader<span class="token punctuation">,</span> ckpt<span class="token operator">=</span><span class="token string">"best"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>对Trainer的简单用法说明如下:</p><ul><li>使用<code>trainer.stage_name()</code>可以让模型执行相应的阶段(fit, validate, test, predict).<br>如果不主动调用<code>trainer.test()</code>, 则不会执行测试阶段.</li><li><code>trainer.validate()</code>, <code>trainer.predict()</code>, 可以分别让模型执行验证和预测阶段, 前者被包含在模型的训练过程中, 无需重复调用.</li><li>虽然官网有写<code>trainer.test()</code>, <code>trainer.predict()</code>会自动加载最好的模型检查点后再测试和预测, 但我实测的时候没有加载, 默认是使用最后一个epoch测试和预测. 而在设置<code>ckpt_path=&quot;best&quot;</code>才会加载最好的模型, 否则是以最后一个epoch的模型进行测试的. 该参数在<code>trainer.fit()</code>中附加也可以让模型从该检查点开始训练.</li></ul><h3 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h3><p>定义Trainer时有很多参数很好用, 在这里推荐一些.</p><ul><li><code>max_epochs</code>: <strong>最大Epoch</strong>, 肯定要设置.</li><li><code>default_root_dir</code>: 默认存储模型, 日志<strong>地址</strong>. 如果不设置, 每次跑实验时候都会多一个<code>version_x</code>文件夹, 看个人喜好和需求.</li><li><code>val_check_interval</code>: <strong>验证间隔</strong>, 计量单位是epoch, 如果有更高的验证频次需求, 也可以设置为小数, 即不到1个epoch验证一次.</li><li><code>gpus</code>: 使用的GPU数量. 在即将出现的2.0版本中会被<code>accelerator=gpu</code>, <code>device=x</code>取代.</li><li><code>precision</code>: 全精度 / 半精度训练.</li><li><code>accumulate_grad_batches</code>: <strong>梯度累加</strong>, 可以多个batch更新一次梯度, 以间接的近似大batch的效果. PS: 听说对比学习不能用.</li><li><code>gradient_clip_val</code>: <strong>梯度裁剪</strong>, 将梯度大小限制在该值内, 防止梯度过大崩掉.</li><li><code>num_sanity_val_steps</code>: 在执行训练前会先用几个batch的验证数据跑一下, 检查代码是否有问题, 设置为-1为全部, 0为不检测. 我一般设置为0.</li><li><code>callbacks</code>: <strong>回调函数</strong>, 接受值为回调函数的列表, 下小节会讲.</li></ul><h3 id="Callback"><a href="#Callback" class="headerlink" title="Callback"></a>Callback</h3><p>一般来说, <strong>早停</strong>和<strong>检查点</strong>是两个比较常用的Callback, 需要在Trainer定义时作为参数传入. 例如:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> pytorch_lightning <span class="token keyword">import</span> Trainer  <span class="token keyword">from</span> pytorch_lightning<span class="token punctuation">.</span>callbacks <span class="token keyword">import</span> EarlyStopping<span class="token punctuation">,</span> ModelCheckpoint  early_stopping <span class="token operator">=</span> EarlyStopping<span class="token punctuation">(</span><span class="token string">'val_loss'</span><span class="token punctuation">)</span>  checkpoint <span class="token operator">=</span> ModelCheckpoint<span class="token punctuation">(</span>      save_weights_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>      save_on_train_epoch_end<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>      monitor<span class="token operator">=</span><span class="token string">"valid_f1"</span><span class="token punctuation">,</span>      mode<span class="token operator">=</span><span class="token string">"max"</span><span class="token punctuation">,</span>      save_top_k<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>  <span class="token punctuation">)</span>trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>callbacks<span class="token operator">=</span><span class="token punctuation">[</span>early_stopping<span class="token punctuation">,</span> checkpoint<span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>仅当<code>ModelCheckpoint</code>的<code>save_on_train_epoch_end</code>设置为False时才会在验证时保存, 否则设置为True时是在训练时保存, 默认为None.</p></blockquote><p>还有一个<code>PrintTableMetricsCallback</code>, 不用带参数, 会在每个epoch结束时打印表格, 不过我基本不用.</p><h3 id="Trainer-in-Python-scripts"><a href="#Trainer-in-Python-scripts" class="headerlink" title="Trainer in Python scripts"></a>Trainer in Python scripts</h3><p>通常情况下, 使用<code>ArgumentParser</code>能更灵活的跑实验. 可以对Trainer手动添加参数:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> argparse <span class="token keyword">import</span> ArgumentParser<span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span>hparams<span class="token punctuation">)</span><span class="token punctuation">:</span>    model <span class="token operator">=</span> LightningModule<span class="token punctuation">(</span><span class="token punctuation">)</span>    trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>accelerator<span class="token operator">=</span>hparams<span class="token punctuation">.</span>accelerator<span class="token punctuation">,</span> devices<span class="token operator">=</span>hparams<span class="token punctuation">.</span>devices<span class="token punctuation">)</span>    trainer<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>model<span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    parser <span class="token operator">=</span> ArgumentParser<span class="token punctuation">(</span><span class="token punctuation">)</span>    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--accelerator"</span><span class="token punctuation">,</span> default<span class="token operator">=</span>None<span class="token punctuation">)</span>    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--devices"</span><span class="token punctuation">,</span> default<span class="token operator">=</span>None<span class="token punctuation">)</span>    args <span class="token operator">=</span> parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span>    main<span class="token punctuation">(</span>args<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果需要修改某些参数可以在命令行附带上:</p><pre class="line-numbers language-shell"><code class="language-shell">python main.py --accelerator 'gpu' --devices 2<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>但上面手动很麻烦, Trainer支持自动添加参数到里面:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> argparse <span class="token keyword">import</span> ArgumentParser<span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>    model <span class="token operator">=</span> LightningModule<span class="token punctuation">(</span><span class="token punctuation">)</span>    trainer <span class="token operator">=</span> Trainer<span class="token punctuation">.</span>from_argparse_args<span class="token punctuation">(</span>args<span class="token punctuation">)</span>    trainer<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>model<span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    parser <span class="token operator">=</span> ArgumentParser<span class="token punctuation">(</span><span class="token punctuation">)</span>    parser <span class="token operator">=</span> Trainer<span class="token punctuation">.</span>add_argparse_args<span class="token punctuation">(</span>parser<span class="token punctuation">)</span>    args <span class="token operator">=</span> parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span>    main<span class="token punctuation">(</span>args<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>也可以走混合路线, 同时定义别的超参和Trainer的参数到parser里.</p><blockquote><p>其实都不如<strong>Hydra</strong>来的优雅, 见文末Recommended推荐的模板.</p></blockquote><h2 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h2><p>这一节写一些我自己使用过程中用到的一些很有用的小技巧.</p><ol><li><p>在<code>pl.LightningModule</code>的构造函数里面, 使用<code>self.save_hyperparameters()</code>可以将<code>pl.LightningModule</code>中所有的传入参数记录到yaml文件里, 非常方便于实验记录.</p></li><li><p><code>pl.seedeverything()</code>, 彻底告别自己写随机种子设置函数.</p></li><li><p>有的时候想把模型的预测结果和模型本身的权重保存到同一个目录下, 但是我不想自己按照规则去写路径, 而是和Trainer的设置同步, 该怎么办呢? 在<code>pl.LightningModule</code>里会添加<code>Trainer</code>的Hook, 调用<code>self.trainer</code>就能够获得它身上的属性. 例如我想把模型预测结果保存到日志目录下, 应该这么写:</p><pre class="line-numbers language-python"><code class="language-python">pred_save_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>self<span class="token punctuation">.</span>trainer<span class="token punctuation">.</span>log_dir<span class="token punctuation">,</span> <span class="token string">"prediction.json"</span><span class="token punctuation">)</span>  your_save_function<span class="token punctuation">(</span>pred_save_path<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>其他需要的属性也是同理, 通过Hook可以轻松拿到Trainer身上的属性.</p></li><li><p>使用<code>pl.LightningModule.load_from_checkpoint(ckpt_path)</code>可以一条命令直接为TaskModel加载超参和模型权重.</p></li></ol><h2 id="Reference-amp-Recommended"><a href="#Reference-amp-Recommended" class="headerlink" title="Reference &amp; Recommended"></a>Reference &amp; Recommended</h2><ul><li>Pytorch Lightning官方文档: <a href="https://www.pytorchlightning.ai/tutorials" target="_blank" rel="noopener">PyTorch Lightning Tutorials</a>.</li><li>Pytorch Lightning API: <a href="https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html" target="_blank" rel="noopener">LightningModule — PyTorch Lightning 1.7.1 documentation</a>, <a href="https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html" target="_blank" rel="noopener">Trainer — PyTorch Lightning 1.7.1 documentation</a>.</li><li>Example of Transformer: <a href="https://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/text-transformers.html" target="_blank" rel="noopener">Finetune Transformers Models with PyTorch Lightning — PyTorch Lightning 1.8.0dev documentation</a>.</li><li>知乎的攻略帖: <a href="https://zhuanlan.zhihu.com/p/353985363" target="_blank" rel="noopener">Pytorch Lightning 完全攻略 - 知乎</a>.</li><li>一套传闻不错的PL模板(需要学习hydra): <a href="https://github.com/ashleve/lightning-hydra-template" target="_blank" rel="noopener">GitHub - ashleve/lightning-hydra-template: PyTorch Lightning + Hydra. A very user-friendly template for rapid and reproducible ML experimentation with best practices. ⚡🔥⚡</a>.</li><li>另一套简单很多的PL模板: <a href="https://github.com/KinWaiCheuk/pytorch_template" target="_blank" rel="noopener">GitHub - KinWaiCheuk/pytorch_template: Template that combines PyTorch Lightning and Hydra</a>.</li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RFBFN: A Relation - First Blank Filling Network for Joint Relational Triple Extraction</title>
      <link href="/posts/42381.html"/>
      <url>/posts/42381.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li><strong>SPN</strong>: <a href="https://adaning.github.io/posts/50175.html">SPN: Joint Entity and Relation Extraction with Set Prediction Networks</a>.</li></ul></blockquote><h1 id="RFBFN-A-Relation-First-Blank-Filling-Network-for-Joint-Relational-Triple-Extraction"><a href="#RFBFN-A-Relation-First-Blank-Filling-Network-for-Joint-Relational-Triple-Extraction" class="headerlink" title="RFBFN: A Relation - First Blank Filling Network for Joint Relational Triple Extraction"></a>RFBFN: A Relation - First Blank Filling Network for Joint Relational Triple Extraction</h1><p>本文是论文<a href="https://aclanthology.org/2022.acl-srw.2" target="_blank" rel="noopener">RFBFN: A Relation-First Blank Filling Network for Joint Relational Triple Extraction</a>的阅读笔记和个人理解, 论文来自<strong>ACL 2022</strong>.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>现有的RTE工作要么忽略了关系的<strong>语义信息</strong>, 要么抽取<strong>Subject</strong>和<strong>Object</strong>带有<strong>先后顺序</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rfbfn1.png" style="zoom:25%;" /><p>针对上述两点, 作者希望提出一种兼顾关系间语义信息, 且不带有Subject和Object的预测顺序的模型.</p><h2 id="RFBFN"><a href="#RFBFN" class="headerlink" title="RFBFN"></a>RFBFN</h2><h3 id="Task-Definition"><a href="#Task-Definition" class="headerlink" title="Task Definition"></a>Task Definition</h3><p>输入一个包含<code>[CLS]</code> Token  $x_{cls}$ 在内有$n$ 个Token的句子$X=(x_1, x_2, \dots, x_n)$, 任务目标为抽取出句子中所有可能的三元组$T(X) = (e_i, r_{ij}, e_j)$, $e_i, e_j$ 分别为代表Subject和Object的Token, $r_{ij} \in \mathcal{R}$ 为二者间的关系, $\mathcal{R}$ 为预先定义好的关系集.</p><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>按序分为三部分:</p><ol><li><strong>Span - Level Encoder</strong>: 从输入文本中抽取出<strong>Span表示</strong>.</li><li><strong>Relation Detection Module</strong>: 预测句子中的<strong>潜在关系</strong>, 并过滤掉不相关的关系.</li><li><strong>Blank Filling Module</strong>: 用很多指明关系的模板作为输入, 预测该关系下对应的<strong>实体对</strong>.</li></ol><p>将关系抽取任务建模为<strong>完形填空</strong>任务, 可以兼顾考虑关系的<strong>语义信息</strong>, <strong>同时</strong>抽取出<strong>Subject</strong>和<strong>Object</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rfbfn2.png" style="zoom: 28%;" /><p>模型概览图如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rfbfn3.png" style="zoom: 29%;" /><h3 id="Span-Level-Encoder"><a href="#Span-Level-Encoder" class="headerlink" title="Span - Level Encoder"></a>Span - Level Encoder</h3><p>Span - Level Encoder通过BERT来抽取句子中的Span表示. 令$S=(s_1, s_2, \dots, s_{n_s})$ 为输入句子$X$ 中的所有Span, 则对于Span $s_i \in S$, 其表示$\mathbf{h}_i^e$ 定义为:</p><p>$$<br>\mathbf{h}_{\mathrm{i}}^{\mathrm{e}}=\left[\mathbf{x}_{\mathrm{START}(\mathrm{i})}^{\mathrm{e}} ; \mathbf{x}_{\operatorname{END}(\mathrm{i})}^{\mathrm{e}} ; \phi\left(\mathbf{x}_{\mathrm{i}}\right)\right]<br>$$</p><p>其中$\mathbf{x}_{\text{START(i)}}^e, \mathbf{x}_{\text{END(i)}}^e$ 为上下文感知的边界Token, $\phi(\mathbf{x}_i)$ 为跟Span长度相关的Embedding. 记Encoder输出的所有Span表示为$\mathbf{H}^e \in \mathbb{R}^{n_s \times d}$, $d$ 为Embedding维度.</p><p>然后分别将$\mathbf{H}^e$ 通过两个不同的FFN获得Span中的关系表示$\mathbf{H}_{\mathrm{e}}^{\mathrm{rel}}$ 和实体表示$\mathbf{H}_{\mathrm{e}}^{\mathrm{ent}}$:</p><p>$$<br>\begin{aligned}<br>&amp;\mathbf{H}_{\mathrm{e}}^{\mathrm{rel}}=\mathbf{W}_{r e l} \mathbf{H}^{e}+\mathbf{b}_{r e l} \\<br>&amp;\mathbf{H}_{\mathrm{e}}^{\mathrm{ent}}=\mathbf{W}_{e n t} \mathbf{H}^{e}+\mathbf{b}_{e n t}<br>\end{aligned}<br>$$</p><p>其中, $\mathbf{W}_{rel}, \mathbf{W}_{ent} \in \mathbb{R} ^{d \times d}, \mathbf{b}_{rel}, \mathbf{b}_{ent} \in \mathbb{R}^d$ 均为可学习参数.</p><h3 id="Relation-Detection-Module"><a href="#Relation-Detection-Module" class="headerlink" title="Relation Detection Module"></a>Relation Detection Module</h3><p>与之前大多数对关系的处理方式不同, 之前大多同时考虑所有关系, 引入了相当多的冗余计算.</p><p>作者先抽取句子中关系的候选集, 然后在这些候选关系的基础上再做Subject和Object的预测.</p><p>与<a href="https://adaning.github.io/posts/50175.html">SPN</a>类似的, RFBFN使用<strong>Non - Autoregressive Decoder</strong>(<strong>NAD</strong>)并将关系作为二元分类问题抽取出来.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rfbfn4.png" style="zoom: 65%;" /><h4 id="Potential-Relation-Extractor"><a href="#Potential-Relation-Extractor" class="headerlink" title="Potential Relation Extractor"></a>Potential Relation Extractor</h4><p>具体的, NAD被输入多个可学习的Query Embedding $\mathbf{Q} \in \mathbb{R}^{\mathrm{n}_q \times d}$, 其中$n_q$ 为句子中最大可能的关系数量. NAD涉及到的信息流如节初图所示. </p><p>用先前计算好的关系侧Span表示$\mathbf{H}^{\text{rel}}_{\text{e}}$ 作为NAD的输入, NAD的输出为$\mathbf{H}^{\mathrm{r}} \in \mathbb{R}^{\mathrm{n_q} \times \mathrm d}$. 第$i$ 个输出$\mathbf{h}_{\mathrm{i}}^{\mathrm{r}}$ 所对应的关系类型概率$\mathbf{p}_{\mathrm{i}}^{\mathrm{r}}$ 由线性层后Softmax得到:</p><p>$$<br>\mathbf{p}_{\mathrm{i}}^{\mathrm{r}}=\operatorname{Softmax}\left(\mathbf{W}_{\mathrm{r}} \mathbf{h}_{\mathrm{i}}^{\mathrm{r}}+\mathbf{b}_{\mathrm{r}}\right)<br>$$</p><p>其中$\mathbf{W}_r \in \mathbb{R}^{|\mathcal{R}| \times d}, \mathbf{b}_r \in \mathbf{R}^{|\mathcal{R}|}$ 为可学习参数, $|\mathcal{R}|$ 为关系类型的总数. 因为存在Decoder输出的先后顺序问题, 和SPN一样的使用了<strong>二部图匹配</strong>损失.</p><h4 id="Candidate-Relation-Judgement"><a href="#Candidate-Relation-Judgement" class="headerlink" title="Candidate Relation Judgement"></a>Candidate Relation Judgement</h4><p>在预测完句子中潜在的关系子集后, 需要<strong>过滤掉无关关系</strong>以有效生成模板. </p><p>Candidate Relation Judgement以NAD的输出表征$\mathbf{H}^\mathrm{r}$ 和<code>[CLS]</code> 的Embedding作为输入, 以一个Mask向量$\mathbf{M}$ (其中的值都是布尔值)作为输出, 以确定该类型关系是否在该句子中成立:</p><p>$$<br>\mathbf{M}=\sigma\left(\mathbf{W}_{\mathrm{s}}\left[\mathbf{H}^{\mathrm{r}} ; \mathbf{x}_{\mathrm{cls}}^{\mathrm{e}}\right]+\mathbf{b}_{\mathrm{s}}\right)<br>$$</p><p>其中$\mathbf{W}_s, \mathbf{b}_s$ 为可学习参数, $\sigma$ 为Sigmoid激活函数. $\mathbf{M}$ 中的值越大, 该句子中包含该关系的置信度就越大. </p><blockquote><p>在Potential Relation Extractor中, 由于使用的是Softmax, 所以每个Query Embedding一定都有一个对应的关系, 但对于这个句子并不一定真的成立, Candidate Relation Judgement的作用就是判断每个关系是否在该句子中成立.</p></blockquote><h3 id="Blank-Filling-Module"><a href="#Blank-Filling-Module" class="headerlink" title="Blank Filling Module"></a>Blank Filling Module</h3><p>实体对抽取被作者建模为识别上下文中的Span并填入模板的空中. 作者为每个候选关系类型建立待填充的模板(缺失处用<code>[MASK]</code>表示), 当实体被识别时需要填入Subject和Object的对应槽位.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rfbfn5.png" style="zoom:67%;" /><h4 id="Relation-Template-Generation"><a href="#Relation-Template-Generation" class="headerlink" title="Relation Template Generation"></a>Relation Template Generation</h4><p>每种关系的模板由关系的<strong>语义信息</strong>和Subject, Object<strong>槽位</strong>组成.</p><p>例如关系<code>leaderName</code> 对应着模板<code>[MASK] is the leader of [MASK]</code>. 作者认为该关系模板蕴含着关系的语义信息, 对RTE很重要, 具体模板的构成形式如下:</p><p>$$<br>T_{r}=\left(m_{1}^{r}, t_{1}^{r}, t_{2}^{r}, \ldots, t_{n_{t}}^{r}, m_{2}^{r}\right)<br>$$</p><p>其中$m_1^r$ 代表Subject的空槽, $m_2^r$ 代表Object的空槽, $t_1^r, t_2^r, \dots, t_{n_t}^r$ 为关系$r$ 的Token. 每种关系模板被<strong>复制</strong>$k$ 次, 之间用<code>[SEP]</code> 拼接. 其中$k$ 比句子中预设好的三元组数量稍微大一点的数, 这样能抽取出同种关系下的多个实体对.</p><blockquote><p>将Relation Template复制多次和多个Query Embedding从形式上来说是一样的.</p></blockquote><h4 id="Entity-Pair-Extractor"><a href="#Entity-Pair-Extractor" class="headerlink" title="Entity Pair Extractor"></a>Entity Pair Extractor</h4><p>对于给定的关系模板和Span表示$\bar{\mathbf{H}}=\left[\mathbf{H}_{\text{e}}^{\text{ent}} ; \mathbf{x}^{\text{e}}_{\text{cls}}\right]$, Entity Pair Extractor的作用在于抽取模板中对应的实体对. 作者同样使用和关系抽取时一样的<strong>NAD</strong>作为特征抽取器. NAD涉及到的信息流如节初图所示.</p><p>在每层Transformer Decoder Layer中, 多头自注意力建模了<strong>空槽</strong>和<strong>语义</strong>关系的关联, 多头跨注意力建立了<strong>Span信息</strong>和<strong>模板</strong>之间的关联. </p><p>经过NAD处理后, 空槽表示为$\mathbf{H}_{\text{r}}^{\text{blk}} \in \mathbb{R} ^ {2\text{k} \times \text{d}}$, Span对每种关系$r$ 下的第$i$ 个槽位的表示由下式获得:</p><p>$$<br>\mathbf{h}_{\mathrm{i}, \mathrm{r}}^{\mathrm{b}}=\tanh \left(\mathbf{W}_{\mathrm{b}}^{1} \bar{\mathbf{H}}+\mathbf{W}_{\mathrm{b}}^{2} \mathbf{h}_{\mathrm{i}, \mathrm{r}}^{\mathrm{blk}}+\mathbf{b}_{\mathrm{b}}\right)<br>$$</p><p>其中$\mathbf{W}_\text{b}^1, \mathbf{W}_\text{b}^2 \in \mathbb{R} ^{\text{d} \times \text{d}}, \mathbf{b}_{\text b} \in \mathbb{R} ^ \text b$ 为可学习参数. 对于多余出来的不需要填入实体的槽位, 答案被设置为<code>[CLS]</code>.</p><p>最后用Softmax来获得句子中应该填入槽位的实体的概率:<br>$$<br>\mathbf{p}_{\mathrm{i}, \mathrm{r}}^{\mathrm{b}}=\operatorname{Softmax}\left(\mathbf{u}_{\mathrm{b}}^{\mathrm{T}} \cdot \mathbf{h}_{\mathrm{i}, \mathrm{r}}^{\mathrm{b}}\right)<br>$$<br>其中$\mathbf{u}_b \in \mathbb{R}^d$ 为可学习参数, 作者使用基于Span的方法抽取实体对, 所以实体可以在不使用指针网络或序列标注时被同时抽取.</p><blockquote><p>其实本质仍然是指针网络, 只不过从Token Level换成了Span Level.</p></blockquote><h3 id="Joint-Training"><a href="#Joint-Training" class="headerlink" title="Joint Training"></a>Joint Training</h3><p>作者的模型分为<strong>关系检测</strong>和<strong>实体对抽取</strong>两个任务. 作者以多任务学习联合训练该模型, 即<strong>共享Encoder</strong>.</p><p>预测实体对时, 作者根据实体在文本中的出现的顺序排序, 然后用交叉熵计算实体对抽取的损失:</p><p>$$<br>\mathcal{L}_{e n t}=-\sum_{r=1}^{n_{d}} \sum_{i=1}^{2 k} \log \mathbf{p}_{\mathrm{i}, \mathrm{r}}^{\mathrm{b}}\left(\mathrm{y}_{\mathrm{i}, \mathrm{r}}^{\mathrm{b}}\right)<br>$$</p><p>其中$\mathrm{y}^{\mathrm{b}}_{\mathrm{i,r}}$ 为实体Span的Ground Truth, $n_d$ 为检测到的关系数.</p><p>但是对于关系就不一样了, 同样由于使用NAD, 在句子中检测出的关系是不应该具有顺序特性的, 所以使用与SPN相同的<strong>二部图匹配损失</strong>, 穷举出所有预测出的关系顺序, 找到一种与Ground Truth相匹配的最小Cost顺序$\pi^\ast$:<br>$$<br>\pi^{\ast}=\underset{\pi \in \Pi\left(\mathrm{n}_{\mathrm{q}}\right)}{\operatorname{argmin}}\left(-\sum_{\mathrm{i}=1}^{\mathrm{I}_{\mathrm{q}}} \mathrm{I}\left(\mathrm{y}_{\mathrm{i}}^{\mathrm{r}}\right) \cdot \mathbf{p}_{\pi(\mathrm{i})}^{\mathrm{r}}\left(\mathrm{y}_{\mathrm{i}}^{\mathrm{r}}\right)\right)<br>$$</p><p>其中$\Pi\left(\mathrm{n}_{\mathrm{q}}\right)$ 为全排列策略空间, $\mathrm{y}_\mathrm{i}^\mathrm r$ 为关系的Ground Truth, $I(y_r^i)$ 为指示函数, 若$\mathrm{y_i^r} \neq \varnothing, \mathrm{I(y_i^r)}=1$, 否则为0. 关系侧的Loss定义为最优排序下的交叉熵:<br>$$<br>\mathcal{L}_{r e l}=-\sum_{\mathrm{i}=1}^{\mathrm{n}_{\mathrm{q}}} \log \mathbf{p}_{\pi^{*}(\mathrm{i})}^{\mathrm{r}}\left(\mathrm{y}_{\mathrm{i}}^{\mathrm{r}}\right)<br>$$</p><blockquote><p>二部图匹配损失详见<a href="https://adaning.github.io/posts/50175.html#toc-heading-8">SPN</a>.</p></blockquote><p>最后对两个任务的Loss做个加权:<br>$$<br>\mathcal{L}=\lambda \mathcal{L}_{e n t}+(1-\lambda) \mathcal{L}_{r e l}<br>$$</p><p>实验中作者将$\lambda$ 设为0.5, 即视两个任务同等重要, 且二者学习难度相同.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的实验参数请参照原论文.</p><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><p>作者选用两个常用数据集NYT和WebNLG的精准匹配和部分匹配版本做为实验数据集, 统计数据如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rfbfn6.png" style="zoom:33%;" /><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><p>在上述数据集上主要实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rfbfn7.png" style="zoom:40%;" /><p>看起来RFBFN的Precision和Recall都比其他模型更要均衡一些, 所以相对应的F1也偏高一点. </p><p>在不用预训练模型的时候比PRGC结果要好, 主要的提升来自于Recall, 我认为主要原因其一是RFBFN采用了Span Level的方式而非PRGC中Token Level的方式抽取实体, 可能会有一些提升, 再者是Fill in the Blanks的方式可能更契合BERT原来的预训练任务.</p><h3 id="Detailed-Results-on-Complex-Scenarios"><a href="#Detailed-Results-on-Complex-Scenarios" class="headerlink" title="Detailed Results on Complex Scenarios"></a>Detailed Results on Complex Scenarios</h3><p>在NYT, WebNLG部分匹配上按照不同类型区分的F1 Score如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rfbfn8.png" style="zoom:40%;" /><p>RFBFN几乎全面优于Baseline模型.</p><h3 id="Results-on-Different-Subtasks"><a href="#Results-on-Different-Subtasks" class="headerlink" title="Results on Different Subtasks"></a>Results on Different Subtasks</h3><p>在NYT, WebNLG部分匹配上各个子任务的实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rfbfn9.png" style="zoom:40%;" /><p>在NYT*上性能被实体识别拖了后腿, 但在WebNLG*上相反, 作者将其归因于后者的关系种类众多.</p><h3 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h3><h4 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h4><p>在WebNLG*消融实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rfbfn10.png" style="zoom: 40%;" /><p>它们分别对应着如下改动:</p><ul><li><strong>- Relation Detection Module</strong>: 直接移除Relation Detection Module, 假设句子中所有关系都成立, 喂给Entity Pair Extractor. 因为WebNLG* 关系太多了, 作者只选了正样本和30%的负样本.</li><li><strong>- Candidate Relation Judgement</strong>: 直接移除Candidate Relation Judgement, 即忽略Relation Detection Module里面抽取出的错误关系.</li><li><strong>- Relation Template Generation</strong>: 把Relation Template换成Relation Embedding.</li><li><strong>- NA Entity Pair Extractor</strong>: 替换NAD里面的Unmasked Self Attention为Casual Mask, 即按序生成Subject和Object. </li><li><strong>- Joint Training</strong>: 不再共享Encoder, 使用两个独立的Encoder做训练.</li></ul><p>影响最大的是Relation Detection Module和NA Entity Pair Extractor. Entity Pair Extractor更倾向于从正确的关系模板中抽取实体, 所以喂进去负样本时表现下降很严重(体现在Precision上), 让Non - Autoregressive变成Autoregressive去生成Subject和Object也显示出了不同时抽取带来的不一致性.</p><p>关于Relation Template Generation的影响, 作者特地做了个Case Study:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rfbfn11.png" style="zoom:33%;" /><p>使用Relation Embedding的模型不能正确理解关系的语义信息, 相反Relation Template可以.</p><h4 id="Visualization"><a href="#Visualization" class="headerlink" title="Visualization"></a>Visualization</h4><p>假设一句子中的三元组Ground Truth为<code>(Brom, club, Arnhem)</code> 和 <code>(Brom, club, Graafschap)</code>, 输入Entity Pair Extractor的关系类型为<code>club</code>, Entity Pair Extractor中的Attention Score Heatmap如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rfbfn12.png" style="zoom: 33%;" /><p>能够看到不同的槽位可以把Attention放到对应关系下的实体上.</p><blockquote><p>按理说最后两个<code>[MASK]</code>应该把注意力放到<code>[CLS]</code>上才对, 因为Label中设定多余的槽位答案为<code>[CLS]</code>.</p><p>当然也可能有一种解释, <code>[CLS]</code> 本身就是反映句子语义的, 所以在这里注意力分散是因为<code>[CLS]</code> 与句子本身的语义相似, 所以对无论是Attend到<code>[CLS]</code> 还是分散到其他地方都是合理的.</p></blockquote><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>我认为RFBFN算是<strong>SPN</strong>的一种改进, 是一个类似与PRGC的<strong>Pipeline</strong>模型, 先抽取<strong>关系</strong>, 然后根据关系构造出<strong>模板</strong>, 通过<strong>完形填空</strong>的方式抽取出<strong>实体</strong>. 它将SPN里的<strong>NADecoder Layer</strong>同时应用到了<strong>关系侧</strong>和<strong>实体侧</strong>. </p><p>由于使用NAD, 多次采用<strong>穷举</strong>的方法解决每个步骤的抽取问题: 穷举Span, 穷举Relation(指Query Embedding), 穷举Entity Pair(指复制模板多次).</p><p>虽然最后取得了优越的性能, 但说实话看着工程量和计算量都挺大的, 尤其是Span的引入占大头, 算是拿时间换性能吧.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RTE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SPN: Joint Entity and Relation Extraction with Set Prediction Networks</title>
      <link href="/posts/50175.html"/>
      <url>/posts/50175.html</url>
      
        <content type="html"><![CDATA[<h1 id="Joint-Entity-and-Relation-Extraction-with-Set-Prediction-Networks"><a href="#Joint-Entity-and-Relation-Extraction-with-Set-Prediction-Networks" class="headerlink" title="Joint Entity and Relation Extraction with Set Prediction Networks"></a>Joint Entity and Relation Extraction with Set Prediction Networks</h1><p>本文是论文<a href="https://arxiv.org/abs/2011.01675" target="_blank" rel="noopener">Joint Entity and Relation Extraction with Set Prediction Networks</a> 的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>现有的基于Seq2Seq的方法, 在训练阶段需要将<strong>三元组集合</strong>转化为<strong>Sequence</strong>输入到模型中, 这样存在预测<strong>三元组之间顺序</strong>的问题, 但实际上句子中抽取出的三元组是<strong>无序</strong>的. 作者希望将RTE问题转化直接的<strong>集合预测</strong>问题, 并使用<strong>非自回归Decoder</strong>来解决集合预测问题.</p><h2 id="SPN"><a href="#SPN" class="headerlink" title="SPN"></a>SPN</h2><h3 id="Task-Definition"><a href="#Task-Definition" class="headerlink" title="Task Definition"></a>Task Definition</h3><p>对与输入序列$X$, Relational Triple Extraction的任务目标是抽取所有目标三元组$Y=\set{(s_1, r_1, o_1), \dots, (s_n, r_n, o_n)}$, 其条件概率为:</p><p>$$<br>P(Y \mid X ; \theta)=p_{L}(n \mid X) \prod_{i=1}^{n} p\left(Y_{i} \mid X, Y_{j \neq i} ; \theta\right)<br>$$</p><p>其中$p_L(n|X)$ 建模了句子中目标三元组的数量, $p\left(Y_{i} \mid X, Y_{j \neq i} ; \theta\right)$ 代表目标三元组$Y_i$ 与句子中其他存在的三元组$Y_{j \neq i}$和输入序列$X$ 同时相关.</p><h3 id="Sentence-Encoder"><a href="#Sentence-Encoder" class="headerlink" title="Sentence Encoder"></a>Sentence Encoder</h3><p>Sentence Encoder直接用<strong>BERT</strong>, 提取到的特征为$\mathbf{H}_e \in \mathbb{R} ^{l \times d}$, $l$ 为包括<code>[CLS]</code>和<code>[SEP]</code>的序列长度.</p><h3 id="Non-Autoregressive-Decoder-for-Triple-Set-Generation"><a href="#Non-Autoregressive-Decoder-for-Triple-Set-Generation" class="headerlink" title="Non - Autoregressive Decoder for Triple Set Generation"></a>Non - Autoregressive Decoder for Triple Set Generation</h3><blockquote><p>首先来介绍下, <strong>Non - Autoregressive Decoder</strong>(<strong>NAD</strong>)最早使用在机器翻译中, 与三元组抽取问题出发点完全不同. 生成译文时如果是自回归的方式太<strong>慢</strong>了, 非自回归的生成方式可以<strong>一次性</strong>生成所有译文. 具体的, 在Decoder进行解码时并非一步一步的遵循自回归将输入反复喂入Decoder中得到单步生成内容, 而是使用多个Query向量同时喂入Decoder中以得到全部的生成内容:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spn1.png" style="zoom: 25%;" /><p>该图出自最早采用NAD的论文<a href="https://arxiv.org/abs/1711.02281" target="_blank" rel="noopener">Non-Autoregressive Neural Machine Translation</a>, 当时所使用的方法是将输入端内容复制多次喂入Decoder, 与现在使用的Query Embedding相似.</p></blockquote><h4 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h4><p>现在我们回归正题. 前人已有方法往往将三元组抽取建模为<strong>序列生成</strong>问题, 由于使用的是自回归Deocder, 模型生成三元组时明显存在三先后生成顺序的问题:<br>$$<br>P(Y \mid X ; \theta)=\prod_{i=1}^{n} p\left(Y_{i} \mid X, Y_{j&lt;i} ; \theta\right)<br>$$</p><p>而作者将三元组抽取视为一个用NAD完成的<strong>集合预测</strong>问题, 式子同Task Definition中给出的句子中所有目标三元组条件概率公式:<br>$$<br>P(Y \mid X ; \theta)=p_{L}(n \mid X) \prod_{i=1}^{n} p\left(Y_{i} \mid X, Y_{j \neq i} ; \theta\right)<br>$$<br>在解码前, Decoder需要知道句子中的目标三元组个数. 作者简单的设$p_L(n|X)$ 为一个常数. 对于每个句子, Decoder需要预测比句子Ground Truth中三元组数量$n$ 稍大一点的固定的集合数$m$, 以达到三元组全覆盖的效果.  具体的, 在词表中添加$m$ 个可学习的Query Embedding, 在Decoder解码时一次性输入即可.</p><h4 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h4><p>SPN的模型结构本身其实比较简单, 就是由BERT和NAD以及最后分类用的FFN组成:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spn2.png" style="zoom:33%;" /><p>将$m$ 个三元组Query Vector同时输入NAD, 同时获得所有三元组表示记为$\mathbf{H}_d \in \mathbb{R}^{m \times d}$, Decoder生成的三元组中的关系和实体由不同的FFN得到. </p><blockquote><p>由于使用的是NAD, 所有三元组是被一次性生成的, 所以Decoder可以做到双向解码, 同Encoder的双向上下文.</p></blockquote><p>每个Decoder的输出$\mathbf{h}_d \in \mathbb{R}^d$, 目标三元组对应的关系$r$ 的概率$\mathbf{p}^r$ 可以由下式得到:<br>$$<br>\mathbf{p}^{r}=\operatorname{softmax}\left(\mathbf{W}_{\mathbf{r}} \mathbf{h}_{\mathrm{d}}\right)<br>$$</p><p>其中, $\mathbf{W}_r$ 为可训练参数.</p><p>每个目标三元组中的实体由<strong>起始</strong>位置和<strong>结束</strong>位置分别确定, 也就是做$l$ 分类. Decoder每个输出$\mathbf{h}_d$ 和BERT下的句子输出 $\mathbf{H}_e$ 同时将Subject和Object的下标分别确定下来:<br>$$<br>\begin{aligned}<br>\mathbf{p}^{s-s t a r t} &amp;=\operatorname{softmax}\left(\mathbf{v}_{\mathbf{1}}^{\mathbf{T}} \tanh \left(\mathbf{W}_{\mathbf{1}} \mathbf{h}_{\mathrm{d}}+\mathbf{W}_{\mathbf{2}} \mathbf{H}_{\mathbf{e}}\right)\right) \\<br>\mathbf{p}^{s-e n d} &amp;=\operatorname{softmax}\left(\mathbf{v}_{\mathbf{2}}^{\mathbf{T}} \tanh \left(\mathbf{W}_{\mathbf{3}} \mathbf{h}_{\mathrm{d}}+\mathbf{W}_{\mathbf{4}} \mathbf{H}_{\mathbf{e}}\right)\right) \\<br>\mathbf{p}^{o-s t a r t} &amp;=\operatorname{softmax}\left(\mathbf{v}_{\mathbf{3}}^{\mathbf{T}} \tanh \left(\mathbf{W}_{\mathbf{5}} \mathbf{h}_{\mathrm{d}}+\mathbf{W}_{\mathbf{6}} \mathbf{H}_{\mathbf{e}}\right)\right) \\<br>\mathbf{p}^{o-e n d} &amp;=\operatorname{softmax}\left(\mathbf{v}_{\mathbf{4}}^{\mathbf{T}} \tanh \left(\mathbf{W}_{\mathbf{7}} \mathbf{h}_{\mathrm{d}}+\mathbf{W}_{\mathbf{8}} \mathbf{H}_{\mathbf{e}}\right)\right)<br>\end{aligned}<br>$$</p><p>其中, $\set{\mathbf{W}_i \in \mathbb{R}^{d \times d}}^8_{i=1}, \set{\mathbf{v}_i \in \mathbb{R}^{d \times d}}^4_{i=1}$ 为可学习参数, $t$ 为包含空关系在内的所有关系总数, $l$ 为句子长度.</p><h3 id="Bipartite-Matching-Loss"><a href="#Bipartite-Matching-Loss" class="headerlink" title="Bipartite Matching Loss"></a>Bipartite Matching Loss</h3><p>由于将联合抽取视为一个<strong>集合预测</strong>问题, 所以在优化时使用自回归时所采用的有序交叉熵就不太合适了, 因为自回归中的交叉熵是对<strong>每个时间步</strong>Decoder的输出分别应用上去的. 但是在三元组抽取任务中, 三元组本身是<strong>无序</strong>的, 模型生成的顺序和Ground Truth不一定完全匹配.</p><p>对此, 作者的解决思路非常简单, 既然原来交叉熵是有序的, 那我做<strong>全排列穷举</strong>出所有的顺序, 找到一个和Ground Truth相匹配的序列不就行了? 没错, 该过程可以抽象为一个<strong>二部图匹配</strong>问题, 需要找到模型预测结果和Ground Truth之间的一个最优匹配结果:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spn3.png" style="zoom:25%;" /><p>假定$\mathbf{Y}=\left\{\mathbf{Y}_{i}\right\}_{i=1}^{n}$ 为Ground Truth, $\hat{\mathbf{Y}}=\left\{\hat{\mathbf{Y}}_{i}\right\}_{i=1}^{m}$ 为模型预测到的三元组. 因为$m$ 比$n$ 要稍大些, 所以把多出来的那部分用空三元组填上.</p><p>更为具体的, 需要找到使得Cost<strong>最小</strong>的三元组排列顺序$\pi^\star$:</p><p>$$<br>\pi^{\star}=\underset{\pi \in \Pi(m)}{\arg \min } \sum_{i=1}^{m} \mathcal{C}_{m a t c h}\left(\mathbf{Y}_{i}, \hat{\mathbf{Y}}_{\pi(i)}\right)<br>$$</p><p>其中$\Pi(m)$ 代表长度为$m$ 的全排列空间, $\mathcal{C}_{m a t c h}\left(\mathbf{Y}_{i}, \hat{\mathbf{Y}}_{\pi(i)}\right)$ 为Ground Truth$\mathbf{Y}_i$ 和排列顺序为$\pi(i)$ 时的模型预测结果$\hat{\mathbf{Y}}_{\pi(i)}$ 之间的Cost.</p><p>接着定义二部图匹配的Cost, 每个三元组实际上由<strong>五元组</strong>确定, 即Ground Truth $\mathbf{Y}_i = \left(r_{i}, s_{i}^{\text {start }}, s_{i}^{e n d}, o_{i}^{\text {start }}, o_{i}^{e n d}\right)$, 模型预测结果 $\hat{\mathbf{Y}}_{i}=\left(\mathbf{p}_{i}^{r}, \mathbf{p}_{i}^{s-s t a r t}, \mathbf{p}_{i}^{s-e n d}, \mathbf{p}_{i}^{o-s t a r t}, \mathbf{p}_{i}^{o-e n d}\right)$, 定义$\mathcal{C}_{m a t c h}\left(\mathbf{Y}_{i}, \hat{\mathbf{Y}}_{\pi(i)}\right)$ 如下:</p><p>$$<br>\begin{aligned}<br>\mathcal{C}_{\text {match}}\left(\mathbf{Y}_{i}, \hat{\mathbf{Y}}_{\pi(i)}\right)<br>&amp;=-\mathbb{1}_{\left\{r_{i} \neq \varnothing\right\}}\left[\mathbf{p}_{\pi(i)}^{r}\left(r_{i}\right)\right.\\<br>&amp;+\mathbf{p}_{\pi(i)}^{s-start}\left(s_{i}^{start}\right) \\<br>&amp;+\mathbf{p}_{\pi(i)}^{s-e n d}\left(s_{i}^{end }\right) \\<br>&amp;+\mathbf{p}_{\pi(i)}^{o- start }\left(o_{i}^{start }\right) \\<br>&amp;+\mathbf{p}_{\pi(i)}^{o-e n d}\left(o_{i}^{end }\right)\left.\right]<br>\end{aligned}<br>$$</p><blockquote><p>在指示函数$\mathbb{1}$ 影响下,  $\mathbf{p}_{\pi(i)}^{r}(r_i)$ 就代表模型预测排列为$\pi(i)$ 时关系为$r_i$ 的概率, 对于实体位置的表示同理.</p></blockquote><p>上式即计算预测关系$r_i$ 不为空时, Ground Truth对应的关系类型, Subject, Object的起始位置和结束位置的概率的和. </p><p>如果Ground Truth所对应的预测概率比较大, 那么Cost就比较小, 这种排序就越有可能是对Ground Truth的最优排序.</p><p>作者在这里采用<strong>匈牙利算法</strong>来完成二部图匹配, 由此可以得到Ground Truth所对应的最优模型预测排序, 由此规避生成三元组顺序与Ground Truth不一致的问题.</p><blockquote><p>扩展阅读:</p><ul><li><a href="https://www.bilibili.com/video/BV16K4y1X7Ph" target="_blank" rel="noopener">14-4: 匈牙利算法 Hungarian Algorithm</a>.</li></ul><p>匈牙利算法的作用是找到二部图的最大或最小匹配(要求二部图的两个集合节点数相同), 在这里是找到$\mathbf{Y}$ 和$\hat{\mathbf{Y}}$ 之间Cost的<strong>最小匹配</strong>.</p></blockquote><p>最后按照最优的排列顺序$\pi^\star$ 去计算<strong>负对数似然</strong>, 最大化最优排序下对应的关系预测概率和实体起始结束位置概率:<br>$$<br>\begin{aligned}<br>\mathcal{L}(\mathbf{Y}, \hat{\mathbf{Y}}) &amp;=\sum_{i=1}^{m}\left\{-\log \mathbf{p}_{\pi^{\star}(i)}^{r}\left(r_{i}\right)\right.\\<br>&amp;+\mathbb{1}_{\left\{r_{i} \neq \varnothing\right\}}\left[-\log \mathbf{p}_{\pi^{\star}(i)}^{s-s t a r t}\left(s_{i}^{start }\right)\right.\\<br>&amp;-\log \mathbf{p}_{\pi^{\star}(i)}^{s-e n d}\left(s_{i}^{end}\right) \\<br>&amp;-\log \mathbf{p}_{\pi^{\star}(i)}^{o-s t a r t}\left(o_{i}^{start }\right) \\<br>&amp;\left.\left.-\log \mathbf{p}_{\pi^{\star}(i)}^{o-e n d}\left(o_{i}^{end}\right)\right]\right\}<br>\end{aligned}<br>$$</p><blockquote><p>没理解也不要紧, 原文附录中有一个完整的例子, 可以帮助大家理解二部图匹配损失, 可以自行查阅原论文.</p></blockquote><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文.</p><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><p>所采用的数据集是两个常用数据集NYT24和WebNLG, 具体信息如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spn4.png" style="zoom:33%;" /><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><p>在NYT24上, SPN的部分匹配和精准匹配结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spn5.png" style="zoom:33%;" /><p>当时最厉害的SOTA还是<a href="https://adaning.github.io/posts/27105.html">CasRel</a>, SPN超过了CasRel, 实际上也超过了同年的<a href="https://adaning.github.io/posts/49694.html">TPLinker</a>.</p><p>在WebNLG上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spn6.png" style="zoom:33%;" /><p>在WebNLG上SPN的Recall比CasRel要高得多, 个人认为这种收益来自于比Ground Truth多数量稍一点的Query Embedding, 且和SPN同时抽取实体和关系有关(CasRel是两阶段抽取).</p><p>分类看实体和关系的抽取结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spn7.png" style="zoom:33%;" /><p>CasRel除了在WebNLG的精度上稍稍比SPN好一点, 其余地方都不如SPN. </p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>SPN在NYT24上的消融实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spn8.png" style="zoom:33%;" /><p>不使用二部图匹配损失, 而是强套一个交叉熵上去, 严重的影响了Recall, Precision也有很大影响, NAD和二部图匹配损失才是最为重要的核心部分. 从Decoder层数来看的话, 单纯堆叠层出带来的增益没想象中那么小.</p><h3 id="Sentences-with-Different-Number-of-Triples"><a href="#Sentences-with-Different-Number-of-Triples" class="headerlink" title="Sentences with Different Number of Triples"></a>Sentences with Different Number of Triples</h3><p>将句子中的三元组个数进行分类, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spn9.png" style="zoom:33%;" /><p>SPN的各类型抽取效果都比CasRel好, 在NYT上的多三元组抽取比CasRel好挺多.</p><h3 id="Different-Overlapping-Patterns"><a href="#Different-Overlapping-Patterns" class="headerlink" title="Different Overlapping Patterns"></a>Different Overlapping Patterns</h3><p>按照不同的重叠类型区分句子, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spn10.png" style="zoom: 40%;" /><p>SPN本身并没有设计特别针对SEO和EPO的优化方法, 似乎对普通三元组的抽取能力提升更大些.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>SPN算是<strong>生成</strong>和<strong>Tagging</strong>混合的RTE模型, 不同于前人所使用的生成式方法, 将三元组抽取完全建模为自回归生成式任务, 而是将RTE建模为<strong>集合预测</strong>问题, 避免了三元组生成时存在的<strong>先后顺序</strong>问题, 同时因为使用了NAD, 获得了<strong>双向解码</strong>的能力, 使得每个三元组生成时都可以考虑到其他三元组的信息.</p><blockquote><p>也是很巧, NAD因为在机器翻译中<strong>难以保证序列</strong>的生成顺序而导致性能降低, 而在RTE问题中根本不需要考虑三元组之间的顺序问题, NAD用在这里非常合适.</p></blockquote><p>SPN性能不错, 相对于模型结构本身, 把三元组抽取建模成<strong>集合预测</strong>的思想倒是比较重要, 在<strong>NAD</strong>的选取和<strong>二部图匹配损失</strong>的设计上都有体现.</p><blockquote><p>另外, 有兴趣的小伙伴可以看看DETR: <a href="https://arxiv.org/abs/2005.12872" target="_blank" rel="noopener">End-to-End Object Detection with Transformers</a>这篇文章, SPN和DETR实际上是几乎一样的.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RTE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GRTE: A Novel Global Feature-Oriented Relational Triple Extraction Model based on Table Filling</title>
      <link href="/posts/54052.html"/>
      <url>/posts/54052.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>CasRel: 详见<a href="https://adaning.github.io/posts/27105.html">CasRel: A Novel Cascade Binary Tagging Framework for Relational Triple Extraction</a>.</li><li>TPLinker: 详见<a href="https://adaning.github.io/posts/49694.html">TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking</a>.</li></ul></blockquote><h1 id="A-Novel-Global-Feature-Oriented-Relational-Triple-Extraction-Model-based-on-Table-Filling"><a href="#A-Novel-Global-Feature-Oriented-Relational-Triple-Extraction-Model-based-on-Table-Filling" class="headerlink" title="A Novel Global Feature-Oriented Relational Triple Extraction Model based on Table Filling"></a>A Novel Global Feature-Oriented Relational Triple Extraction Model based on Table Filling</h1><p>本文是论文<a href="https://arxiv.org/abs/2109.06705" target="_blank" rel="noopener">A Novel Global Feature-Oriented Relational Triple Extraction Model based on Table Filling</a>的阅读笔记和个人理解, 论文来自<strong>EMNLP 2021</strong>.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>基于<strong>填表</strong>的RTE方法展示出优越的性能, 然而, 现有的方法并没有发挥表格的全部潜力, 它们大多聚焦于表格中的<strong>局部信息</strong>, 而非<strong>Relation</strong>和<strong>Token Pair</strong>的<strong>全局关联</strong>.</p><p>例如, 在句子<code>Edward Thomas and John are from New York City, USA.</code>中, 抽取出<code>(Edward Thomas, live_in, New York)</code> 有益于抽取出<code>(John, live_in, USA)</code>, 反之亦然. 因为它们的<code>(Subject, Object)</code> 是高度相同的, 它们的Subject均为Person, Object均为Location.</p><p>并且, 这两个三元组又有益于抽取出<code>(New York, located_in, USA)</code>, 因为<code>located_in</code> 要求Subject和Object均为Location, 且它的语义与<code>live_in</code> 相关, <code>live_in</code> 要求Object为Location, 所以有益.</p><p>为此, 作者希望提出一种充分挖掘表格中<strong>全局信息</strong>的<strong>填表式</strong>RTE方法.</p><h2 id="GRTE"><a href="#GRTE" class="headerlink" title="GRTE"></a>GRTE</h2><h3 id="Table-Filling-Strategy"><a href="#Table-Filling-Strategy" class="headerlink" title="Table Filling Strategy"></a>Table Filling Strategy</h3><p>对于句子$S = \set{w_1, w_2, \dots, w_n}$, 和每种关系$r$, 都维护一张大小为$n \times n$ 的二维表$table_r$, 填表法便是将每个表中的元素填上一个正确的标签. </p><p>作者预先定义好如下标签:<br>$$<br>L = \set{\text{“N/A”, “MMH”, “MMT”, “MSH”, “MST”, “SMH”, “SMT”, “SS”}}<br>$$</p><p>表的第$i$ 行第$j$列对应着Token Pair $(w_i, w_j)$ 之间的标签类别.</p><p>若$l \in \set{\text{“MMH”, “MMT”, “MSH”, “MST”, “SMH”, “SMT”}}$ 则意味着$(w_i, w_j)$ 是一对三元组对应的Subject和Object.</p><p>标签中的第一个字母代表Subject的实体类型是多Token还是单Token, $\text{M}$ 代表多Token实体, $\text{S}$ 代表单Token. 同理, 第二个标签代表Object的实体类型. 最后一个字母则代表$w_i, w_j$ 同是实体的头Token或尾Token, $\text{H}$ 代表头, $\text{T}$ 代表尾. 此外, $l=\text{“SS”}$ 代表$(w_i, w_j)$ 是一个实体对, 其两个实体都是单Token实体, $l=\text{“N/A”}$ 代表$(w_i, w_j)$ 间不存在关系.</p><blockquote><p>只要有其中一个是单Token实体, 另外一个是多Token实体, 则单Token即被看做是头也被看做是尾.</p></blockquote><p>还是本文开头的那个例子, 作者将标签套入其中, 详细说明:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/grte1.png" style="zoom:33%;" /><p>对于句子<code>Edward Thomas and John are from New York City, USA.</code>, 它的实体对可以被这样确定下来:</p><table><thead><tr><th>行</th><th>列</th><th>标签</th><th>行</th><th>列</th><th>标签</th><th>三元组</th><th>三元组类型</th></tr></thead><tbody><tr><td><code>Edward</code></td><td><code>New</code></td><td>MMH</td><td><code>Thomas</code></td><td><code>City</code></td><td>MMT</td><td><code>(Edward Thomas, New York City)</code></td><td>M - M</td></tr><tr><td><code>Edward</code></td><td><code>New</code></td><td>MMH</td><td><code>Thomas</code></td><td><code>York</code></td><td>MMT</td><td><code>(Edward Thomas, New York)</code></td><td>M - M</td></tr><tr><td><code>Edward</code></td><td><code>USA</code></td><td>MSH</td><td><code>Thomas</code></td><td><code>USA</code></td><td>MST</td><td><code>(Edward Thomas, USA)</code></td><td>M - S</td></tr><tr><td><code>John</code></td><td><code>New</code></td><td>SMH</td><td><code>John</code></td><td><code>City</code></td><td>SMT</td><td><code>(John, New York City)</code></td><td>S - M</td></tr><tr><td><code>John</code></td><td><code>New</code></td><td>SMH</td><td><code>John</code></td><td><code>York</code></td><td>SMT</td><td><code>(John, New York)</code></td><td>S - M</td></tr><tr><td><code>John</code></td><td><code>USA</code></td><td>SS</td><td>-</td><td>-</td><td>-</td><td><code>(John, USA)</code></td><td>S - S</td></tr></tbody></table><p>在这种填表策略下, 只需要对每个关系填一张表, 即需要填$n^2|R|$ 个元素, 而不是TPLinker中的两张表, 即$(2|R| + 1) \frac{n^2 +n} {2}$ 个元素, 因此需要填的元素数量是比TPLinker要少的, $(2|R| + 1) \frac{n^2 +n} {2} &gt; n^2|R|$.</p><blockquote><p>此外, 这种特殊的标签设置, 建模了<strong>Token Pair</strong>之间的全局关联, 因为模型除了必须同时知道Subject和Object是哪种类型的实体.</p></blockquote><h3 id="Model-Details"><a href="#Model-Details" class="headerlink" title="Model Details"></a>Model Details</h3><p>模型细节主要分为<strong>TFG</strong>, <strong>GFM</strong>, <strong>TG</strong>三个部分.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/grte2.png" style="zoom: 33%;" /><h4 id="Encoder-Module"><a href="#Encoder-Module" class="headerlink" title="Encoder Module"></a>Encoder Module</h4><p>用<strong>BERT</strong>作为预训练模型, 获得句子的表示$H \in \mathbb{R}^{n \times d_h}$, 然后用两个独立的FFN生成初始的Subject的特征$H_s^{(1)}$和Object的特征$H_o^{(1)}$:<br>$$<br>\begin{aligned}<br>&amp;H_{s}^{(1)}=W_{1} H+b_{1} \\<br>&amp;H_{o}^{(1)}=W_{2} H+b_{2}<br>\end{aligned}<br>$$<br>其中$W_1, W_2 \in \mathbb{R}^{d_h \times d_h}, b_1, b_2 \in \mathbb{R}^{d_h}$ 为可训练参数.</p><h4 id="Table-Feature-Generation-Module"><a href="#Table-Feature-Generation-Module" class="headerlink" title="Table Feature Generation Module"></a>Table Feature Generation Module</h4><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/grte3.png" style="zoom:33%;" /><p><strong>TFG Module</strong>的输入$H_s, H_j$ 来自于TFG和GFM之间的<strong>迭代</strong>过程, 故记第$t$ 轮的<strong>Subject</strong>和<strong>Object</strong>的特征分别为$H_s^{(t)}, H_j^{(t)}$. </p><p>TFG会根据每种关系$r$, 利用Subject和Object特征分别生成一张Token Pair$(w_i, w_j)$ 之间的二维表格特征$TF_r^{(t)}$.</p><p>在表$TF_r^{(t)}$ 中, 它的每个元素$TF_r^{(t)}(i, j)$ 的计算方式如下:<br>$$<br>T F_{r}^{(t)}(i, j)=W_{r} \operatorname{ReLU}\left(H_{s, i}^{(t)} \circ H_{o, j}^{(t)}\right)+b_{r}<br>$$<br>其中, $\circ$ 代表逐元素点乘, $W_r, b_r$ 为可训练参数.</p><blockquote><p>$TF^{(t)}_r \in \mathbb{R}^{n \times n \times |L|}, TF^{(t)} \in \mathbb{R}^{n \times n \times (|L| \times |R|)}$, 而且$TF_r^{(t)}$ 与$table_r$ 是同型的.</p></blockquote><h4 id="Global-Feature-Mining-Module"><a href="#Global-Feature-Mining-Module" class="headerlink" title="Global Feature Mining Module"></a>Global Feature Mining Module</h4><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/grte4.png" style="zoom: 20%;" /><p>GFM Module目的是<strong>同时考虑所有关系</strong>, <strong>挖掘表格全局特征</strong>, 分三步走.</p><p><strong>Step 1</strong>, <strong>整合表格特征</strong>. </p><p>我们把所有的关系$r$ 下的表格特征视为一个整体$TF^{(t)}$, 分别对所有关系下Subject / Object的特征做<strong>最大池化</strong>, 获得Subject / Object<strong>全局特征</strong>, 然后过一层线性层, 获得<strong>全局关系</strong>下的Subject / Object表特征$TF_{s/o}^{(t)}$:<br>$$<br>\begin{aligned}<br>&amp;T F_{s}^{(t)}=W_{s} \underset{s}{\operatorname{maxpool}}\left(T F^{(t)}\right)+b_{s} \\<br>&amp;T F_{o}^{(t)}=W_{o} \underset{o}{\operatorname{maxpool}}\left(T F^{(t)}\right)+b_{o}<br>\end{aligned}<br>$$<br>其中, $W_s, W_o \in \mathbb{R} ^{(|L| \times |R|) \times d_h}, b_s, b_o \in \mathbb{R}^{d_h}$ 为可训练参数.</p><blockquote><p>$TF^{(t)} \in \mathbb{R}^{n \times n \times (|L|\times |R|)}, \underset{s/o}{\operatorname{maxpool}}(TF^{(t)}) \in \mathbb{R}^{n \times (|L| \times |R|)}$, 所以经过线性层$W_{s/o}$ 后维度为$ TF_{s/o}^{(t)} \in \mathbb{R}^{n \times d_h}$, 大小又回到了$H$ 的大小.</p><p>此外, 将所有关系的表特征放在一起, 建立了<strong>Relation</strong>之间的全局关联.</p></blockquote><p><strong>Step 2</strong>, <strong>挖掘全局特征</strong>.</p><p>用多头自注意力重新分别获得Subject / Object视角下的表特征表示$\hat{T F}_{s/o}^{(t)}$, 然后用跨注意力以$\hat{T F}_{s / o}^{(t)}$ 为Query, 用Encoder输出的Token表示$H$ 为Key和Value, 获得Subject / Object视角下的更好的原Token表示$\hat{H}_{(s / o)}^{(t+1)}$, 最后经过一个线性层收尾:<br>$$<br>\begin{aligned}<br>\hat{T F}_{s / o}^{(t)} &amp;=\text {MultiHeadSelfAtt}\left(T F_{s / o}^{(t)}\right) \\<br>\hat{H}_{(s / o)}^{(t+1)} &amp;=\operatorname{MultiHeadAtt}\left(\hat{T F}_{s / o}^{(t)}, H, H\right) \\<br>H_{(s / o)}^{(t+1)} &amp;=\operatorname{ReLU}\left(\hat{H}_{(s / o)}^{(t+1)} W+b\right)<br>\end{aligned}<br>$$<br>其中$W, b$ 为可训练参数.</p><blockquote><p>两次注意力, 第一次是自注意力, 是对全局特征$T F_{s / o}^{(t)}$ 的内部重整, 挖掘Relation的全局关联. 第二次是跨注意力, 是利用全局特征对原始Token表示的重整, 挖掘Token Pair之间的全局关联.</p></blockquote><p><strong>Step 3</strong>, <strong>整合Subject / Object特征</strong>. </p><p>参考Transformer, 用LayerNorm和残差连接缓解梯度消失:<br>$$<br>H_{(s / o)}^{(t+1)}=\operatorname{LayerNorm}\left(H_{(s / o)}^{(t)}+H_{(s / o)}^{(t+1)}\right)<br>$$</p><blockquote><p>如果把第二步和第三步视为一个Block, 则Transformer Decoder Layer完全相同, 只需要反复用一个Decoder Layer迭代即可.</p></blockquote><p>GFM Module的最终输出$H_{(s / o)}^{(t+1)} $ 将会被重新送入TFG Module生成新的表特征$TF^{(t+1)}$, 进入下一轮迭代.</p><h4 id="Triple-Generation-Module"><a href="#Triple-Generation-Module" class="headerlink" title="Triple Generation Module"></a>Triple Generation Module</h4><p>TG Module的任务就是结合我们前面提到的Table Filling Strategy, 来解码出句子中所有的三元组.</p><p>由最后一次迭代时TFG Module生成的表格特征$TF^{(N)}$ 作为输入, 为每个关系$r$ 下的表的每个格子$T F_{r}^{(N)}(i, j)$ 打上标签, </p><p>$$<br>\begin{aligned}<br>\hat{\operatorname{table}}_{r}(i, j) &amp;=\operatorname{softmax}\left(T F_{r}^{(N)}(i, j)\right) \\<br>\operatorname{table}_{r}(i, j) &amp;=\underset{l \in L}{\operatorname{argmax}}\left(\hat{\operatorname{table}}_{r}(i, j)[l]\right)<br>\end{aligned}<br>$$</p><p>其中, $\hat{\operatorname{table}}_{r}(i, j) \in \mathbb{R}^{|L|}$, $\operatorname{table}_{r}(i, j)$ 是Token Pair $(w_i, w_j)$ 在关系$r$ 下的标签.</p><p>对于作者设计的标签, 伪代码如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/grte5.png" style="zoom:33%;" /><p>其实伪代码并不复杂. 对于每张关系表, 都使用启发式最近匹配算法, 有三种搜索方式:</p><ul><li><strong>Forward Search</strong>: 找到离标签尾为<strong>H</strong>的格子最近的标签尾为<strong>T</strong>且<strong>前缀与前者相同</strong>的格子, 解码作为一个三元组, 然后找下一个标签尾为<strong>H</strong>的格子.</li><li><strong>Reverse Search</strong>: 与前者相反, 找到离标签尾为<strong>T</strong>的格子最近的标签尾为<strong>H</strong>且<strong>前缀与前者相同</strong>的格子, 解码作为一个三元组, 然后找下一个标签尾为<strong>T</strong>的格子. 与前者刚好互为补充.</li><li><strong>Single Token Pair</strong>: 找到标为SS的格子, 两个实体Token均为<strong>单个Token</strong>, 按行列直接解码出一个三元组, 然后找下一个SS格子.</li></ul><blockquote><p>其实Reverse Search的情况只是用来处理嵌套实体的, 如果数据中没有过多的嵌套实体, 可以去掉Reverse Search.</p></blockquote><p>其实这三种方式分别就应着用来说明Table Filling Strategy例子图中的三种箭头, 分别对应着不同的解码方式:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/grte1.png" style="zoom:33%;" /><p>红箭头为Forward Search, 绿箭头为Reverse Search, 蓝箭头为Single Token Pair.</p><h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><p>表格的标签学习是一个多分类任务, 损失函数就用多分类交叉熵:<br>$$<br>\begin{aligned}<br>\mathcal{L} &amp;=\sum_{i=1}^{n} \sum_{j=1}^{n} \sum_{r=1}^{|R|}-\log p\left(y_{r,(i, j)}=t a b l e_{r}(i, j)\right) \\<br>&amp;=\sum_{i=1}^{n} \sum_{j=1}^{n} \sum_{r=1}^{|R|}-\log t a \hat{b} l e_{r}(i, j)\left[y_{r,(i, j)}\right]<br>\end{aligned}<br>$$<br>其中$y_{r, (i, j)} \in [1, |L|]$ 是$(w_i, w_j)$ 在关系$r$ 下的标签索引.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文.</p><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><p>数据集采用NYT24, WebNLG以及它们的部分匹配版本, 除此外还有NYT29, 它们的统计信息如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/grte6.png" style="zoom:33%;" /><blockquote><p>NYT29的规模比NYT24要大一些.</p></blockquote><h3 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h3><p>主试验和消融实验放在一起了.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/grte7.png" style="zoom:33%;" /><p>主试验结果上来看自然是达到了SOTA. 精度和召回都比较高. 在使用LSTM作为特征抽取器的情况下能够超过CasRel和TPLinker.</p><p>消融实验各个模型代表的是:</p><ul><li>GRTE w/o GFM: 去掉GFM模块, 即去掉挖掘表格特征的迭代过程. </li><li>GRTE GRU GIF: 用GRU代替GFM中的Transformer Decoder完成迭代过程.</li><li>GRTE w/o m-h: 不用多头注意力而单头注意力.</li><li>GRTE w/o shared: 在TFG和GFM中不共享参数, 即每轮迭代都使用不同的参数.</li></ul><p>第2, 3个实验体现了Transformer本身和多头注意力的效果, 而第1个实验体现了迭代过程的作用, 迭代过程还是起到了相当大的作用, 第4个实验体现了共享迭代过程参数的作用, 说明继续往上加参数容易过拟合, 共享参数就很好了.</p><p>GRTE在不同数据集和不同迭代次数的F1曲线如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/grte9.png" style="zoom:33%;" /><p>明显的能够看到, 对于不同数据集, 随着迭代次数上升到2, 3, 4时效果比迭代1次要好, 可以说明GRTE中迭代的必要性和有效性.</p><h3 id="Analyses-on-Different-Sentence-Types"><a href="#Analyses-on-Different-Sentence-Types" class="headerlink" title="Analyses on Different Sentence Types"></a>Analyses on Different Sentence Types</h3><p>按三元组类型和句子中包含的三元组个数分类, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/grte8.png" style="zoom:33%;" /><p>各类别下表现都比较好.</p><h3 id="Analyses-on-Computational-Efficiency"><a href="#Analyses-on-Computational-Efficiency" class="headerlink" title="Analyses on Computational Efficiency"></a>Analyses on Computational Efficiency</h3><p>关于迭代, 很容易被人质疑效率问题. 所以作者对比了NYT和WebNLG上的模型参数量和推理时间:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/grte10.png" style="zoom:33%;" /><p>看上去推理时间是和SPN, TPLinker相当的, 并且归功于<strong>共享</strong>迭代模块的参数, 使得总参数量不如SPN高.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>GRTE从三元组之间的<strong>相关性</strong>出发, 填补了前人忽视填表式方法中全局信息的空缺. 该方法基于<strong>全局特征</strong>, 通过独特的反复<strong>迭代</strong>方式不断<strong>提炼</strong>, 并从<strong>Relation</strong>的全局关联和<strong>Token Pair</strong>的全局关联两个方面挖掘表格中的全局特征, 也就是<code>Generate - Mine - Integrate</code> 过程. </p><p>标签的设定非常新颖, 建立了不同Token Pair之间的联系, 要求模型必须知道一对实体的信息才可以打标签, 并且对于每个关系只需要用<strong>一张表</strong>就能搞定了, 共享迭代模块的参数减少了参数效率方面的疑虑.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RTE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction</title>
      <link href="/posts/53442.html"/>
      <url>/posts/53442.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>CasRel: 详见<a href="https://adaning.github.io/posts/27105.html">CasRel: A Novel Cascade Binary Tagging Framework for Relational Triple Extraction</a>.</li><li>TPLinker: 详见<a href="https://adaning.github.io/posts/49694.html">TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking</a>.</li></ul></blockquote><h1 id="PRGC-Potential-Relation-and-Global-Correspondence-Based-Joint-Relational-Triple-Extraction"><a href="#PRGC-Potential-Relation-and-Global-Correspondence-Based-Joint-Relational-Triple-Extraction" class="headerlink" title="PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction"></a>PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction</h1><p>本文是论文<a href="https://arxiv.org/abs/2106.09895" target="_blank" rel="noopener">PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction </a> 的阅读笔记和个人理解, 论文来自<strong>ACL 2021</strong>.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>最近的RTE方法收到一些限制, 例如<strong>冗余关系预测</strong>, 基于<strong>边界</strong>的抽取<strong>缺少泛化能力</strong>, <strong>效率问题</strong>等.</p><p>作者希望通过一个关系检测模块, 来检测文本中的潜在关系, 以代替使用所有关系预测三元组中所对应的实体, 并兼容RTE中存在的EPO, SEO问题.</p><h2 id="PRGC"><a href="#PRGC" class="headerlink" title="PRGC"></a>PRGC</h2><p><strong>PRGC</strong>(<strong>P</strong>otential  <strong>R</strong>elation and <strong>G</strong>lobal <strong>C</strong>orrespondence)的模型特别简单, 简单到一张图就能说的清楚:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/prgc1.png" style="zoom: 25%;" /><h3 id="Problem-Definition"><a href="#Problem-Definition" class="headerlink" title="Problem Definition"></a>Problem Definition</h3><p>对于含有$n$ 个Token的句子$S=\set{x_1, x_2, \dots, x_n}$, 其目标是找出句子中含有的关系三元组$T(S) = \set{(s, r, o) \mid s, o \in E, r \in R}$, 其中$E, R$ 分别代表实体集合和关系集合.</p><p>作者将其拆分为三个<strong>子任务</strong>:</p><ul><li><strong>Relation Judgement</strong>: 检测句子$S$ 中的所有<strong>潜在关系</strong>$Y_r(S) = \set{r_1, r_2, \dots, r_m \mid r_i \in R}$, $m$ 为潜在关系数量.</li><li><strong>Entity Extraction</strong>: 抽取出句子$S$ 在潜在关系$r_i$ 下的<strong>实体</strong>, 用<strong>BIO</strong>标注, 即$Y_e(S, r_i \mid r_i \in R) = \set{t_1, t_2, \dots, t_n}$, 其中$t_j$ 为Tag.</li><li><strong>Subject - Object Alignment</strong>: 找到句子$S$ 中的<strong>头尾实体</strong>与<strong>关系类型无关</strong>的<strong>全局对应关系</strong>, 用矩阵$\mathbf{M}$ 来表示, $Y_s(S) = \mathbf{M} \in \mathbb{R}^{n\times n}$.</li></ul><p>作者对每个明确的子任务设计了相应的模块, 并将之前非常有代表性的模型CasRel和TPLinker作为对比:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/prgc2.png" style="zoom: 33%;" /><p>CasRel和TPLinker不特意检测文本中存在的关系, 而是对所有的关系下可能的头实体和尾实体做检测.</p><blockquote><p>从子任务的角度设计模型, 能看出PRGC属于Pipeline模型的一种.</p><p>作者指的Span - based应该并不是实际意义上的基于Span的处理, 而是Binary Tagging和Table  Filling.</p></blockquote><h3 id="PRGC-Encoder"><a href="#PRGC-Encoder" class="headerlink" title="PRGC Encoder"></a>PRGC Encoder</h3><p>直接用BERT作为Encoder:<br>$$<br>Y_{enc}(S)= \set{h_1, h_2, \dots, h_n \mid h_i \in \mathbb{R}^{d\times1}}<br>$$</p><h3 id="PRGC-Decoder"><a href="#PRGC-Decoder" class="headerlink" title="PRGC Decoder"></a>PRGC Decoder</h3><p>Decoder才是模型的主要部分.</p><h4 id="Potential-Relation-Prediction"><a href="#Potential-Relation-Prediction" class="headerlink" title="Potential Relation Prediction"></a>Potential Relation Prediction</h4><p>为解决冗余关系预测的问题, 作者先抽取出句子中的所有潜在关系. 对于含有$n$ 个Token的句子$S$ 的Embedding$\mathbf{h} \in \mathbb{R}^{n\times d}$, 每个潜在关系存在的概率$P_{rel}$ 可以由如下方式得到:</p><p>$$<br>\begin{aligned}<br>\mathbf{h}^{a v g} &amp;=\operatorname{Avgpool}(\mathbf{h}) \in \mathbb{R}^{d \times 1} \\<br>P_{r e l} &amp;=\sigma\left(\mathbf{W}_{r} \mathbf{h}^{a v g}+\mathbf{b}_{r}\right)<br>\end{aligned}<br>$$</p><p>其中$\mathbf{W}_r$ 可训练参数, $\sigma$ 为Sigmoid函数.</p><p>作者将Relation Judgement建模为一个多标签二分类任务, 当$P_{rel} &gt; \lambda_1$ 时视为关系存在, $rel$ 对应的输出标为1, 否则为0.</p><h4 id="Relation-Specific-Sequence-Tagging"><a href="#Relation-Specific-Sequence-Tagging" class="headerlink" title="Relation - Specific Sequence Tagging"></a>Relation - Specific Sequence Tagging</h4><p>在获取了潜在关系后, 作者将分别抽取潜在关系下的头实体和尾实体, 但此时的头实体和尾实体并<strong>没有成对</strong>, 因为它们还没有经过对齐. 在标注实体时采用<strong>BIO标签</strong>, 作者认为标注实体边界使得模型更倾向于记住实体的位置, 而限制了泛化能力.</p><p>每个Token所属类别概率计算方式如下:</p><p>$$<br>\begin{array}{r}<br>\mathbf{P}_{i, j}^{s u b}=\operatorname{Softmax}\left(\mathbf{W}_{s u b}\left(\mathbf{h}_{i}+\mathbf{u}_{j}\right)+\mathbf{b}_{s u b}\right) \\<br>\mathbf{P}_{i, j}^{o b j}=\operatorname{Softmax}\left(\mathbf{W}_{o b j}\left(\mathbf{h}_{i}+\mathbf{u}_{j}\right)+\mathbf{b}_{o b j}\right)<br>\end{array}<br>$$</p><p>其中 $\mathbf{u}_j \in \mathbb{R}^{d\times 1}$ 为第$j$ 个关系的Embedding, 所有的关系Embedding被存储在$\mathbf{U} \in \mathbb{R}^{d\times n_r}$ 中, $n_r$ 为关系数. $\mathbf{h}_i$ 为第$i$ 个Token的表示, $\mathbf{W}_{sub}, \mathbf{W}_{obj} \in \mathbb{R}^{d\times 3}$ 为可训练参数, $3$ 为$\set{\text{B, I, O}}$ 的大小.</p><blockquote><p>和先前的RTE模型标注方式不太一样, 使用的是BIO标签. 随着RTE领域的发展, 越来越多的模型不再使用Binary标签标注边界.</p></blockquote><h4 id="Global-Correspondence"><a href="#Global-Correspondence" class="headerlink" title="Global Correspondence"></a>Global Correspondence</h4><p>获取所有潜在关系下不成对的头实体和尾实体后, 接着用一个<strong>关系无关</strong>的矩阵$\mathbf{M}$ 来判断, 句子中的Token Pair之间潜在的头尾实体是否可以构成一个三元组. </p><p>矩阵$\mathbf{M}$ 中的元素代表<strong>起始位置</strong>Token Pair $i, j$ 所代表的实体之间可以构成<strong>一对</strong>头尾实体的概率$P_{i_{sub}, j_{obj}}$:</p><p>$$<br>P_{i_{s u b}, j_{o b j}}=\sigma\left(\mathbf{W}_{g}\left[\mathbf{h}_{i}^{s u b} ; \mathbf{h}_{j}^{o b j}\right]+\mathbf{b}_{g}\right)<br>$$</p><p>其中$\mathbf{h}_i^{sub}, \mathbf{h}_j^{obj} \in \mathbb{R}^{d \times 1}$ 为第$i, j$ 个Token的表示, 代表着一对潜在的头实体和尾实体. $\mathbf{W}_g \in \mathbb{R}^{2d \times 1}$ 为可训练参数, $\sigma$ 为Sigmoid函数.</p><p>仅当$P_{i_{sub}, j_{obj}} &gt; \lambda_2$ 时视为$i, j$ 可以构成一对头尾实体, 再结合之前抽取出的潜在关系, 可以联合解码出一个三元组.</p><h3 id="Training-Strategy"><a href="#Training-Strategy" class="headerlink" title="Training Strategy"></a>Training Strategy</h3><p>根据上述设计的三个子模块, 分别给出相应的损失函数:</p><p>$$<br>\begin{aligned}<br>\mathcal{L}_{r e l}&amp;=-\frac{1}{n_{r}} \sum_{i=1}^{n_{r}}\left(y_{i} \log P_{r e l}+\left(1-y_{i}\right) \log \left(1-P_{r e l}\right)\right) \\<br>\mathcal{L}_{s e q}&amp;=-\frac{1}{2 \times n \times n_{r}^{p o t}} \sum_{t \in\{s u b, o b j\}} \sum_{j=1}^{n_{r}^{p o t}} \sum_{i=1}^{n} \mathbf{y}_{i, j}^{t} \log \mathbf{P}_{i, j}^{t}<br>\\<br>\mathcal{L}_{g l o b a l}&amp;=-\frac{1}{n^{2}} \sum_{i=1}^{n} \sum_{j=1}^{n}\left(y_{i, j} \log P_{i_{s u b}, j_{o b j}}<br>+\left(1-y_{i, j}\right) \log \left(1-P_{i_{s u b}, j_{o b j}}\right)\right)<br>\end{aligned}<br>$$</p><p>其中, $n_r$ 为全关系数量, $n_r^{pot}$ 为潜在关系数量.</p><p>Potential Relation Prediction和Global Correspondence都可以看做是多标签二分类问题, 所以都用二分类交叉熵优化. 而Relation - Specific Sequence Tagging和NER大多数一致, 视为序列标注问题, 使用多分类交叉熵优化.</p><p>给这三个模块相对应的任务Loss简单加权求个和作为最终Loss:<br>$$<br>\mathcal{L}_{\text {total }}=\alpha \mathcal{L}_{r e l}+\beta \mathcal{L}_{s e q}+\gamma \mathcal{L}_{\text {global }}<br>$$</p><p>$\alpha, \beta, \gamma$ 为超参, 作者在这里简单的设它们三个均为1.</p><h2 id="Experiments-and-Analysis"><a href="#Experiments-and-Analysis" class="headerlink" title="Experiments and Analysis"></a>Experiments and Analysis</h2><p>详细的参数设置请参照原论文.</p><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><p>所选用的数据集是NYT, WebNLG和它们的部分匹配版本, 详细信息如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/prgc3.png" style="zoom:25%;" /><blockquote><p>作者在论文中还单列了一种存在的三元组情况, <strong>SOO</strong>(<strong>S</strong>ubject <strong>O</strong>bject <strong>O</strong>verlap), 从统计信息中可以看出这类三元组占比其实没有那么大. 也有某些论文把它称作<strong>HTO</strong>(<strong>H</strong>ead <strong>T</strong>ail <strong>O</strong>verlap), 都是一个意思.</p><p>例如, <code>[[Lebron] James] is a good basketball player.</code> 中存在的三元组为<code>(Lebron James, first name, Lebron)</code>, 头尾实体存在内部的嵌套关系. </p></blockquote><h3 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h3><h4 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h4><p>在各个数据集上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/prgc4.png" style="zoom:25%;" /><p>效果达到了SOTA. PRGC - Random指BERT的参数随机初始化.</p><h4 id="Detailed-Results-on-Complex-Scenarios"><a href="#Detailed-Results-on-Complex-Scenarios" class="headerlink" title="Detailed Results on Complex Scenarios"></a>Detailed Results on Complex Scenarios</h4><p>按三元组的种类分别统计F1结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/prgc5.png" style="zoom:25%;" /><p>作者认为NYT*上的SOO三元组数量太少了, 所以导致结果可信度比较低, WebNLG*的结果比较可靠.</p><p>按样本中三元组个数分类统计F1结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/prgc6.png" style="zoom:25%;" /><p>PRGC各数量三元组均有不错的抽取性能.</p><h3 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h3><h4 id="Model-Efficiency"><a href="#Model-Efficiency" class="headerlink" title="Model Efficiency"></a>Model Efficiency</h4><p>作者主要对比了CasRel, TPLinker和PRGC在复杂度, 计算量, 参数, 推理时间上的表现:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/prgc7.png" style="zoom:25%;" /><p>因为去掉了大量的冗余关系, 每条样本几乎只需要使用两三个关系, 所以大大减少了计算量和推理时间.</p><p>作者也对比了CasRel, TPLinker, PRGC在WebNLG* 验证集上的收敛速度:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/prgc8.png" style="zoom:25%;" /><p>PRGC收敛速度明显比TPLinker更快.</p><h4 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h4><p>作者探究了PRGC Decoder中各组件所带来的影响:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/prgc9.png" style="zoom:25%;" /><p>还有一个关于Rel - Spec Sequence Tagging的Case:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/prgc10.png" style="zoom: 33%;" /><p>根据实验结果观察, 有如下结论:</p><ul><li><p>Potential Relation Prediction: 去掉该组件后, 不再过滤冗余关系, 而是采用全关系预测. 关系数量比较少的NYT受影响不大, 而WebNLG下降的比较多. 由此说明该组件有良好的过滤冗余关系作用.</p><blockquote><p>其实相对后面两个组件来说这个组件影响并不够大, 可能比较强的点在加速上.</p></blockquote></li><li><p>Rel - Spec Sequence Tagging: 将BIO的标注方式替换为抽取实体Span的标注方式后, 精度骤减. 联合Case可以说明该模块使得模型更倾向于关注语义而非关注实体位置, 并且更关注关系.</p></li><li><p>Global Correspondence: 去掉该组件, 改用启发式最近匹配原则构成三元组, 对精度有很大影响. </p></li></ul><blockquote><p>Global Correspondence能保证前面两个组件精度低的情况下, 最后组成三元组时的精度, 就怕前面漏标(召回低).</p></blockquote><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>PRGC把任务拆解为三个子任务: 潜在关系预测, 关系特化的实体标注, 头尾实体对齐, 并按照三个子任务设计了相应的三个模块, 简单明了, 直接有效. 整体来说模型并不复杂, 论文包装的比较好, 给每个模块都找到了相应的解释.</p><p>但是因为每个模块的设计思路是按照三元组抽取来的, 所以存在Pipeline模型的通病, Exposure Bias. 如果关系抽错了, 那后面都会错, 如果实体漏标了, 也没法补救.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RTE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
      <link href="/posts/58173.html"/>
      <url>/posts/58173.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>Transformer: <a href="https://adaning.github.io/posts/6744.html">Transformer精讲</a>.</li><li>BERT, GPT: <a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>.</li></ul></blockquote><h1 id="Exploring-the-Limits-of-Transfer-Learning-with-a-Unified-Text-to-Text-Transformer"><a href="#Exploring-the-Limits-of-Transfer-Learning-with-a-Unified-Text-to-Text-Transformer" class="headerlink" title="Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"></a>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</h1><p>本文是论文<a href="https://arxiv.org/abs/1910.10683" target="_blank" rel="noopener">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a> 的阅读笔记和个人理解. T5的论文更像是一篇详细扎实的实验报告, 所以本文不再按照常规结构讲解, 所有详细的实验参数和训练过程请参照原论文.</p><p>所谓<strong>万物皆可生成</strong>, 任何任务都可以转化为Sequence to Sequence的形式完成, 只是不同的任务转化为生成后的任务难度有所差异. </p><p>T5是谷歌在2019年, 后BERT时代, 面向生成的Transformer的一次全面探索, 甚至是一次无穷尽的探索, 再一次展示了唯一真理”Money is All You Need”. </p><blockquote><p>其实T5在此基础上还有个小升级T5 1.1, 基本大差不差. 后来又有个mT5是T5的多语言版本, 重新构建了多国语言预训练数据集, 模型沿用的是T5 1.1, 感兴趣的可以自行了解, 咱们在这就只说说T5初始版本.</p></blockquote><h2 id="Text-to-Text-Framework"><a href="#Text-to-Text-Framework" class="headerlink" title="Text - to - Text Framework"></a>Text - to - Text Framework</h2><p><strong>T5</strong>全称为<strong>T</strong>ext-<strong>t</strong>o-<strong>T</strong>ext <strong>T</strong>ransfer <strong>T</strong>ransformer, 无论是何种任务(MT, QA, 文本分类等等), 全部使用文本作为输入, 并且用生成的文本作为输出. 这种方式最大的好处在于提供了预训练时和微调时的<strong>目标一致性</strong>. 所以谷歌才要探索这种基于生成式的预训练方式.</p><p>如何将所有任务转化成text to text形式? 图中列举了几种将不同形式任务统一到text - to - text框架下的例子:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t51.png" style="zoom: 33%;" /><blockquote><p>如果你能在其他NLP论文看到类似这样的图, 左边几个大框框右边几个小框框中间一个模型名字框框, 不出意外它一定是用T5或者T5的改进版做的预训练.</p></blockquote><ul><li><strong>机器翻译</strong>: 直接在训练数据前面加上<code>translate X to Y</code>, <code>X</code> 和<code>Y</code> 是不同种的语言.</li><li><strong>文本分类</strong>: 让模型输出一个单词, 代表不同种类的标签.</li><li><strong>情感分析</strong>:  加上任务前导名, 并在句子前加上<code>sentence:</code>.</li><li><strong>文本摘要</strong>: 在文本前面加上<code>summarize:</code>.</li><li><strong>语义相似度</strong>: 最特殊的一类任务. 加上任务前导名, 在两个句子前分别加上<code>sentence1:</code>, <code>sentence2:</code>.</li></ul><p>它们的输出都直接是任务中的<strong>标签文本</strong>, 注意是标签文本. 也就是说, 像STS - B这种<strong>语义相似度</strong>的任务, <strong>也是靠直接输出文本来完成的</strong>, 而非加个回归头输出数值. STS - B的相似度标签大多是在1 - 5之间的, 作者按0.2为一个增量步长把它改为了一个21分类任务, 如果有相近的值全部四舍五入到相应的类别里. </p><p>直觉上来讲这是比较离谱的, 但其实想想这样的统一也才符合T5的终极定位, 所有任务都用文本输入, 文本输出.</p><p>对于所有的分类问题(包括STS - B), 如果T5在分类任务中输出了不属于Label的单词, 直接记为错误.</p><p>不同文本的预处理方式实际上是个超参, 所以作者没有做探索, 因为方式实在是太多了.</p><p>在经过数据预处理后, 在这种任务大一统的情况下, 直接允许作者使用相同的模型架构, 损失函数, 超参等, 完成所有的NLP任务.</p><blockquote><p>详细的任务预处理例子请参照原论文附录D.</p></blockquote><h2 id="Differences-between-Transformer-and-T5"><a href="#Differences-between-Transformer-and-T5" class="headerlink" title="Differences between Transformer and T5"></a>Differences between Transformer and T5</h2><p>T5做的所有实验均是基于Transformer架构的, 和普通Transformer仅有一些不显著的不同: </p><ul><li><p><strong>Relative Positional Injection</strong>: T5中给模型注入的是<strong>相对位置信息</strong>, 但并没有采用在输入时直接加上位置编码引入位置信息的方式. Query和Key的绝对位置差值对应着不同的相对位置. T5令每个注意力头<strong>直接学习不同相对位置的Attention Score</strong>(就是在Softmax之前的logits), 在模型计算完Attention Score后直接加到上面. 每个注意力头都有自己的相对位置Embedding, 但所有层都共享同一套相对位置Embedding.</p><blockquote><p>注意! 既然该操作是直接加到Attention Score上的, 这意味着<strong>每次</strong>Attention都会完成一次注入. 这强化了T5对相对位置的感知. </p><p>其实, T5的相对位置其实不是一个值, 一个<strong>相对位置区间</strong>, 例如Query和Key的绝对位置差分别为<code>1, 2, 3, 4</code>, <code>1</code> 可能对应着最近的Attention Score, 而<code>2, 3</code> <strong>同时</strong>对应着中距离的Attention Score, <code>4</code> 对应着最远的Attention Score. 这点可以参考<a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py" target="_blank" rel="noopener">Hugging face的T5实现</a>中的<code>_relative_position_bucket</code>函数.</p></blockquote></li><li><p><strong>Activation Function</strong>: 从BERT使用的GeLU又退回了<strong>ReLU</strong>.</p></li><li><p><strong>No Bias Item</strong>: 从Attention到FFN, T5去掉了<strong>所有</strong>Linear的bias.</p></li><li><p><strong>Normalization</strong>: T5使用的是<strong>Pre Norm</strong>, 而非Post Norm. 此外, 与上一条相关, 把Layer Norm替换为了 <a href="https://arxiv.org/abs/1910.07467" target="_blank" rel="noopener">RMS Norm</a>, 即去掉了跟均值相关的那项:</p></li></ul><p>$$<br>\begin{aligned}<br>y_{i,j,k} = \frac{x_{i,j,k} - \mu_{i,j}}{\sqrt{\sigma_{i,j}^2 + \epsilon}}\times\gamma_k + \beta_k,\quad \mu_{i,j} = \frac{1}{d}&amp;\sum_{k=1}^d x_{i,j,k},\quad \sigma_{i,j}^2 = \frac{1}{d}\sum_{k=1}^d (x_{i,j,k}-\mu_{i,j})^2 \\<br>\Downarrow \\<br>y_{i,j,k} = \frac{x_{i,j,k}}{\sqrt{\sigma_{i,j}^2 + \epsilon}}\times\gamma_k,&amp;\quad \sigma_{i,j}^2 = \frac{1}{d}\sum_{k=1}^d x_{i,j,k}^2<br>\end{aligned}<br>$$</p><blockquote><p>除去上述几点外, 其实还有一个区别, 计算Attention Score的时候不用除以$\sqrt{d}$, 这么做的具体原因在<a href="https://kexue.fm/archives/8620" target="_blank" rel="noopener">浅谈Transformer的初始化、参数化与标准化</a>中提到, 强烈建议阅读. </p></blockquote><h2 id="C4-Colossal-Clean-Crawled-Corpus"><a href="#C4-Colossal-Clean-Crawled-Corpus" class="headerlink" title="C4: Colossal Clean Crawled Corpus"></a>C4: Colossal Clean Crawled Corpus</h2><p>不光有T5, 这篇文章还扔出来个<strong>C4</strong>(<strong>C</strong>olossal <strong>C</strong>lean <strong>C</strong>rawled <strong>C</strong>orpus), 这个名字起得挺搞的, 译为巨大号清洁爬下来的语料库. 它是爬虫从大量公开可用的<strong>HTML</strong>里爬下来的, 还没清洗时有20T大小. </p><p>爬下来的数据非常不干净, 它做了如下<strong>清洗</strong>工作:</p><ul><li>仅保留结尾有标点的句子.</li><li>去掉少于5个句子的页面, 保留至少3个单词的行.</li><li>去掉有脏话, 涩涩之类坏词的页面.</li><li>去掉包含Javascript的句子, 和网页警告的是否开启JS有关.</li><li>去掉包含<code>lorem ipsum</code> 占位符的页面, 和测试网页排版有关.</li><li>去掉所有包含花括号的页面, 和代码有关, 比如JS, Java, C都广泛使用花括号.</li><li>以三句为一个单位去重.</li></ul><p>清洗出的文本大约有750G, 作为预训练数据.</p><h2 id="Experimentsss…"><a href="#Experimentsss…" class="headerlink" title="Experimentsss…"></a>Experimentsss…</h2><p>T5的结构和训练细节是单纯考实验得来的, 每次做完一部分实验, 就确定T5的一部分细节. 待T5的训练细节固定后, 又对C4和训练策略做了探索.</p><h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p>不同体系的模型主要区别就在<strong>Attention Mask</strong>上:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t52.png" style="zoom:33%;" /><p>图中横轴为Attention的输入, 纵轴为输出. 黑色代表可以对该元素给予注意力, 白色代表不能对该元素给予注意力.</p><ul><li><strong>Fully - Visible</strong>: 输入对每个时间步的输出完全可见, 即Transformer Encoder的Attention Mask.</li><li><strong>Causal</strong>: 输出仅能依赖当前时间步及之前的输入, 即Transformer Decoder的Attention Mask.</li><li><strong>Casual with Prefix</strong>: 是前面二者的混合, 输入的一部分Fully - Visible, 一部分是Casual.</li></ul><p>不同类型的Attention Mask造就了Transformer各类不同结构的诞生:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t53.png" style="zoom:33%;" /><p>此处的x 和y代表输入序列和输出序列.</p><ul><li>Left: 标准的<strong>Encoder - Decoder</strong>结构, Encoder是全可见的, Decoder解码时可以看见全部的Encoder信息和已经解码的Decoder信息. Vallina Transformer就是这种结构, 后来这个流派又多了MASS, BART这样的模型.</li><li>Middle: 比较纯粹的<strong>语言模型</strong>, <strong>完全的自回归式生成</strong>. 当然这种模型也可以通过强行拼接不同的任务前导文本来执行不同的任务, 并入text to text框架中. 这个流派比较典型的是GPT.</li><li>Right: 就是上面Encoder和Decoder的<strong>共生体</strong>, 它的最大特点就是<strong>不做Encoder和Decoder的区分</strong>, 即共用一个Transformer. 而且它在解码的时候只能使用<strong>同层</strong>的Token表示, 换句话说, <strong>它的编码和解码过程是同步进行的</strong>, 这一流派的代表是UniLM.</li></ul><blockquote><p>一个非常有意思的看法是, XLNet这类的<strong>Permuted Language Model</strong>可以视为Prefix LM, 结合XLNet比较特殊的Attention Mask, 或许你会认同这种观点. 该观点来自于<a href="https://zhuanlan.zhihu.com/p/254821426" target="_blank" rel="noopener">张俊林</a>.</p></blockquote><p>下面直接看实验结果:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t54.png" style="zoom:33%;" /><ul><li><p>标准的Encoder - Decoder最好, 但是参数最多. 所以T5选择Encoder - Decoder作为架构.</p></li><li><p>共享Encoder和Decoder的参数似乎没有对结构有太多影响, 这点在ALBERT里也有说.</p></li><li><p>在非生成类任务上, 完全自回归的结构不太给力, 参数量和计算成本一个好处也没捞到, 大多数任务的性能还被人甩了好几条街.</p></li><li><p>Denoisiong作为训练目标确实提升了模型的能力, 几乎所有模型所有数据集表现都比LM要好.</p></li></ul><h4 id="Denoising-Object-for-Text-to-Text-Framework"><a href="#Denoising-Object-for-Text-to-Text-Framework" class="headerlink" title="Denoising Object for Text to Text Framework"></a>Denoising Object for Text to Text Framework</h4><p>这个Denoising是啥呢? 为了适配text to text框架, 受BERT中打Mask和Word Dropout启发, 作者提出了一种适用于text to text框架的Mask方法. 该方法从原文本中随机采样15%的Token变为<strong>不同的Mask</strong>, 不同被Mask掉的Token会被替换成不同类型的Mask. 模型将被要求生成Mask Token和与之对应被Mask掉的原内容, 以及最终的结束Token:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t55.png" style="zoom:33%;" /><p>在这个例子中, 原文本的<code>for inviting</code> 被替换为<code>&lt;X&gt;</code>, <code>last</code> 被替换为<code>&lt;Y&gt;</code>, 结束Token为<code>&lt;Z&gt;</code>. 从例子中来看, 多个连续Token如果同时被Mask, 只需要被替换为一个Mask Token, 因为生成类模型不需要多个Mask Token就能恢复出被遮掉的内容.</p><p>在下一节中, 该方法将会与其他训练方式对比.</p><h3 id="Unsupervised-Objectives"><a href="#Unsupervised-Objectives" class="headerlink" title="Unsupervised Objectives"></a>Unsupervised Objectives</h3><p>以<code>Thank you for inviting me to your party last week.</code> 为例, 作者列举了其他的无监督训练目标:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t56.png" style="zoom:33%;" /><blockquote><p>BERT - style的Inputs中的<code>apple</code> 是由<code>last</code> 随机替换得到的. </p></blockquote><h4 id="Disparate-High-Level-Approaches"><a href="#Disparate-High-Level-Approaches" class="headerlink" title="Disparate High - Level Approaches"></a>Disparate High - Level Approaches</h4><p>首先从Prefix LM, Denosing, Deshuffling三大类的角度来看它们的差距:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t57.png" style="zoom:33%;" /><p>BERT - style比其他两类都要好, Prefix LM在翻译上还算可以, 但理解类任务不够出色, Deshuffling可能因为完全颠倒语序语义损失太多了, 训练时全是乱序的句子, 下游任务全是正确的句子, 表现比较差.</p><h4 id="Simplifying-the-BERT-objective"><a href="#Simplifying-the-BERT-objective" class="headerlink" title="Simplifying the BERT objective"></a>Simplifying the BERT objective</h4><p>既然BERT - style是最好的, 那就把所有Denoising方法放在一起对比:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t58.png" style="zoom:33%;" /><p>总的来说保留Mask Token比较好, 和MASS - style对比, 连续的Mask只替换成一个Token就好, 不需要保留多个.</p><h4 id="Varying-the-Corruption-Rate"><a href="#Varying-the-Corruption-Rate" class="headerlink" title="Varying the Corruption Rate"></a>Varying the Corruption Rate</h4><p>很明显不同的<strong>打Mask几率</strong>有不同的效果:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t59.png" style="zoom:33%;" /><p>从实验上来看, 15%是一个比较好的值, 看来BERT开了一个好头. 比较大的打Mask率似乎对GLUE的影响比较大, SGLUE掉了一个点, 对其他任务影响好像都不是特别特别明显? 按理说给句子遮上一半会损失特别多的语义.</p><h4 id="Corrupting-spans"><a href="#Corrupting-spans" class="headerlink" title="Corrupting spans"></a>Corrupting spans</h4><p>出于对训练效率的考虑, 每次发生打Mask的动作时, 都会给要预测的<strong>目标序列</strong>多加一个<strong>引导预测</strong>的Mask Token, <strong>被Mask掉的Token不连续的越多</strong>, <strong>目标序列就越长</strong>. </p><p>对每个Token单独判断是否要Mask似乎不如<strong>直接Mask掉Span</strong>来得快, 这样可以<strong>保证目标序列里的Mask Token占比较少</strong>, 训练速度就比较快. 同时, 这个想法曾在SpanBERT上被验证为有益于BERT预训练, 所以作者做了直接Mask掉Span的实验. </p><p>作者所设计的方法是将给Token打Mask的比例和要Mask掉的Span个数设为超参, 以此来控制平均Mask掉的Span长度. </p><p>咱举个例子, 当前序列有500个Token, 所设置的被Mask掉的Token占比为15%, 要Mask掉25个Span, 则总共有500 x 15% = 75个Token会被Mask, Mask掉的Span的平均长度为75 / 25 = 3, 即<code>[Mask]</code>和被Mask掉的Token比例为1:3. 同时, 在固定被Mask掉的内容的Span长度的情况下, 它们全是不连续的, 目标序列长为75 + 25 = 100.</p><p>同样在前面所述的环境下, 采用独立同分布(长度为1)打Mask的方式, 仍是在500个Token中Mask掉15%的Token, 则目标序列长为75 + 75 = 150.</p><p>实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t510.png" style="zoom:33%;" /><p>在1, 2, 3, 5的设定下来看差异真的不是特别大, Mask掉长度为3的Span似乎比独立同分布的Mask掉Token在除了MT以外的任务要好一些. 但是Mask掉Span的训练速度比较快, 所以作者采用了Span长度为3的Mask方法.</p><h4 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h4><p>作者对无监督目标做出的搜索路径如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t511.png" style="zoom:33%;" /><p>作者选择的路径是 <code>BERT - style -&gt; Replace spans -&gt; 15% -&gt; 3</code>.</p><h3 id="Pre-training-Datasets"><a href="#Pre-training-Datasets" class="headerlink" title="Pre - training Datasets"></a>Pre - training Datasets</h3><h4 id="Unlabeled-datasets"><a href="#Unlabeled-datasets" class="headerlink" title="Unlabeled datasets"></a>Unlabeled datasets</h4><p>作者希望比较不同数据集给模型预训练带来的影响, 顺便评估一下C4.</p><p>不同预训练数据集结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t512.png" style="zoom:33%;" /><p>我粗略的介绍一下数据集, C4 unfiltered是指不适用启发式规则过滤的C4, 它的噪声比C4要大的多的多. RealNews和WebText的like版本指的是使用C4的启发式过滤规则过滤后的数据集.</p><p>结论如下:</p><ul><li>C4的<strong>启发式过滤</strong>还是挺有效的, 没过滤的C4明显效果拉了.</li><li>即使域内数据没有标签, <strong>特定领域数据集预训练对特定任务有效</strong>, 感兴趣的可以细搜这些数据集覆盖的区域和不同任务的测试数据来源.</li></ul><h4 id="Pre-training-Datasets-Size"><a href="#Pre-training-Datasets-Size" class="headerlink" title="Pre - training Datasets Size"></a>Pre - training Datasets Size</h4><p>创建C4的核心在于扩大数据规模, 并尽量保证样本的<strong>不重复性</strong>. 作者尝试探索预训练期间多次重复样本对下游任务的影响. 作者尝试缩小C4的规模, 以达到多次重复数据的目的. 实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t513.png" style="zoom:33%;" /><p>结果表明, 越多次重复, 下游任务表现越差.</p><blockquote><p>至于把C4的规模缩小会不会导致真正有用的样本被减少, 我个人觉得C4实在太大了, 所以截断C4可能不会导致其样本多样性的缺失… 而且每次Mask都是随机的, 所以模型也不会看到完全相同的样本.</p></blockquote><p>作者怀疑模型随着重复次数过多, 学会直接死记硬背预训练数据集, 导致下游任务学习的时候效果就变差了, 作者做出了各规模数据集的Training Loss:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t514.png" style="zoom:33%;" /><p>Training Loss随着数据集的缩小而减小, 这就意味着可能模型有<strong>记忆现象</strong>, 因为数据集越小, 重复的次数就越容易多, 但是做下游任务的时候就越不容易.</p><h3 id="Training-Strategy"><a href="#Training-Strategy" class="headerlink" title="Training Strategy"></a>Training Strategy</h3><h4 id="Fine-Tuning-Methods"><a href="#Fine-Tuning-Methods" class="headerlink" title="Fine - Tuning Methods"></a>Fine - Tuning Methods</h4><p>不同的训练策略会给模型在下游任务的表现带来不同影响.</p><p>作者在这里列举了两种高级一点的Fine Tuning方法:</p><ul><li><strong>Adapter</strong>: 在Fine Tuning的时候不调原来模型参数, 而是只调一些在原模型基础上加进去的<strong>即插即用插件</strong>, 以代替更新预训练模型的参数, 详情请见论文<a href="https://arxiv.org/abs/1902.00751" target="_blank" rel="noopener">Parameter-efficient transfer learning for NLP</a>.</li><li><strong>Gradual Unfreezing</strong>: 随时间<strong>逐步解冻</strong>模型的参数, 逐步使得模型所有参数都参与更新. 先只微调模型的最后一层, 训练一定次数后解冻倒数第二层, 直到整个模型都解封.</li></ul><p>结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t515.png" style="zoom:33%;" /><p>各种大小的Adapter似乎都不如Gradual Unfreezing, 根据作者所述, Adapter只在资源比较少的任务上设置比较小的d时效果比较好. 但总的来说还是<strong>微调全部参数</strong>效果最好.</p><h4 id="Multi-Task-Learning"><a href="#Multi-Task-Learning" class="headerlink" title="Multi - Task Learning"></a>Multi - Task Learning</h4><p>前面测试的都是预训练 + 微调的表现, 还没有测试过多任务下的表现. 在text to text下, T5的多任务训练比较简单, 只需要把不同任务的数据集混到一起. 而MTL比较关键的地方在于需要在每个任务之间做权衡, 不能让某个任务的数据过少或过多. 因此作者想了一些办法来控制每类任务的数据集大小, 详情请参照原论文.</p><p>最终结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t516.png" style="zoom:33%;" /><p>总的来说, 用直接多任务训练不如预训练 + 微调.</p><blockquote><p>此处的MTL应该指的是<strong>不做预训练</strong>, 把各类任务的数据混到一块训练, 所以和预训练必然差着很大的数据量级.</p></blockquote><h4 id="Combining-Multi-Task-Learning-with-Fine-Tuning"><a href="#Combining-Multi-Task-Learning-with-Fine-Tuning" class="headerlink" title="Combining Multi - Task Learning with Fine - Tuning"></a>Combining Multi - Task Learning with Fine - Tuning</h4><p>作者将多任务学习扩展到预训练上, 即同时对所有任务预训练, 然后有监督的对单任务微调. </p><p>把预训练 + 微调和MTL放到一起对比. 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t517.png" style="zoom:33%;" /><p>单任务学习的预训练 + 微调是最好的, <strong>多任务预训练 + 微调几乎和它一样</strong>. 但是多任务预训练可以在预训练期间就监视下游任务的性能, 所以作者最终为T5采纳了多任务预训练的策略.</p><p>对于翻译这种任务来说, 有监督比无监督要好, 除去翻译任务外无监督效果更好一些.</p><h3 id="Scaling"><a href="#Scaling" class="headerlink" title="Scaling"></a>Scaling</h3><p>大模型和更长的训练时间, 一般都会带来提升. 作者测试了不同规模的模型, 不同训练时间, 以及集成后的模型性能:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t518.png" style="zoom:33%;" /><p>结论显而易见, 大模型, 长时间都会让性能更好. 但是似乎增大batch size相较于提高训练步长没有那么大提升. <strong>模型分别预训练再微调集成比使用相同的预训练再微调集成效果好</strong>.</p><blockquote><p>当然这些讨论都是建立在像C4这样大规模数据集上的, 模型可能没有完全收敛. 如果数据不够多, 模型可能在训练一段时间就收敛了, 训练再长时间也没用.</p></blockquote><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>总的来说, T5给出了以下的主要结论:</p><ol><li>模型架构: 原生Transformer最好.</li><li>训练目标: 类似BERT的Denosing, 15%的Mask几率.</li><li>训练策略: 全参数微调最好, 多任务预训练后微调和无监督预训练后微调性能相当.</li><li>模型大小: 小模型给长训练时间不如大模型给短训练时间. </li></ol><p>最后再给大家看一眼T5论文的最后一页:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/t519.png" style="zoom: 33%;" /><p>确实看着让人头皮发麻, 这足以证明T5是在”Money is All You Need”, “全面探索”两个前提下的结果. 不得不说这种研究只有大公司能做得起. </p><p>我不太建议去读T5的原文, 因为实在是太长了, 但T5中涉及到的引文还是值得看看的, 因为这篇论文几乎把所有当时比较火的预训练模型做了个大串烧, BERT, GPT, MASS, BART, UniLM, ALBERT, 甚至还有SpanBERT, 扩展的话XLNet也算… 这些文章我也都做过笔记, 感兴趣的可以去看下.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introduction: Deep Reinforcement Learning</title>
      <link href="/posts/49404.html"/>
      <url>/posts/49404.html</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>2022.04.14</strong>: AlphaGo施工完成.</p></blockquote><h1 id="Introduction-Deep-Reinforcement-Learning"><a href="#Introduction-Deep-Reinforcement-Learning" class="headerlink" title="Introduction: Deep Reinforcement Learning"></a>Introduction: Deep Reinforcement Learning</h1><p>本文介绍的是Deep Reinforcement Learning(深度强化学习)入门相关知识, 适合想要入门理解或者直接使用DRL的读者. 文中包含有大量的公式, 在学习DRL时是根本逃避不开的, 如果哪里卡壳了可以去Recommended中找找有没有相关内容.</p><p>大多数内容来自<a href="https://www.bilibili.com/video/BV12o4y197US" target="_blank" rel="noopener">【王树森】深度强化学习(DRL)</a>, 我觉得是非常棒的课程. 虽然贴的链接是B站的, 转载油管上的视频, 但是评论区里附有视频内容节点.</p><p>文中所涉及的所有图片出处在结尾都会提供.</p><h2 id="Terminology"><a href="#Terminology" class="headerlink" title="Terminology"></a>Terminology</h2><p>Reinforcement Learning(RL)旨在通过<strong>智能体</strong>与<strong>环境</strong>的不断<strong>交互</strong>, 教会智能体如何执行任务, 比如让人工智障自己学会玩超级玛丽.</p><p>RL相关的术语很多, 下面先在超级玛丽这一游戏场景下介绍RL里面的相关概念.</p><ul><li><strong>Agent</strong>(智能体): 强化学习的主体. 即玛丽奥.</li><li><strong>State</strong>(状态): 环境的状态, 记为$s \in \mathcal{S}$, 可以理解成超级玛丽的游戏画面.</li><li><strong>Action</strong>(动作): 智能体基于当前环境状态可以做出的行为. 记为$a \in \mathcal{A}$. 例如玛丽奥可以向上, 左右跑动.</li></ul><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl1.png" style="zoom:50%;" /><ul><li><p><strong>Policy</strong>(策略): Agent应该根据状态做出什么决策, 其数学描述是个<strong>概率密度函数</strong>(在这里说的不严谨, 因为例子是一个离散的):<br>$$<br>\pi(a \mid s)=\mathbb{P}(A=a \mid S=s)<br>$$<br>根据给出的状态$s$, 都能根据$\pi(a\mid s)$给出一个做出动作$a$ 的概率. 例如在当前超级玛丽画面下, 策略函数可以判断出玛丽奥应该向上走, 还是向左向右走.</p><blockquote><p>为什么要让策略$\pi$ 是随机的? 我认为有两个原因:</p><ol><li>因为在与环境的交互中, 如果Agent一直保持恒定的策略运作, 很大概率会被不断变化的环境所击垮. 在后面也可以发现, 在强化学习中, Agent和环境始终保持<strong>博弈</strong>, 所以才要让Agent的策略变化多端.</li><li>鼓励Agent做一些随机动作, 不断<strong>探索</strong>做出不同动作对环境的影响. 尤其是在Agent训练早期, 随机性应该设置的更大一些.</li></ol></blockquote></li></ul><ul><li><p><strong>Reward</strong>(奖励): Agent做出动作后, 给予Agent的奖励, 记为$R$. 其定义方法不唯一. 例如打赢超级玛丽可以定义非常多的奖励, 而吃金币只能获得少量奖励, 被打死要给予负的奖励.</p></li><li><p><strong>State Transition</strong>(状态转移): 环境因Agent做出了某个动作后从旧状态变为新状态的过程. 可以是确定的也可以是, 通常认为是随机的, 状态转移的随机性来自于环境. 例如玛丽奥的敌人可能向左走也可能向右走. 同样可以用<strong>概率密度函数</strong>来表示:<br>$$<br>p(s^\prime\mid s, a) = \mathbb{P}(S^\prime=s \mid S=s, A=a)<br>$$<br>状态转移函数仅有环境自己可知.</p></li><li><p><strong>Episode</strong>: RL里的epoch不叫epoch, 与之类似的概念叫 <strong>episodes</strong>, 指智能体从游戏开始到通关或者结束的过程, 也就是玛丽奥直至通关或死亡的过程称为一个Episode. 因为RL对样本数量要求非常高, 所以让Agent玩一个游戏可能需要成千上万把.</p></li></ul><h3 id="Randomness-in-Reinforcement-Learning"><a href="#Randomness-in-Reinforcement-Learning" class="headerlink" title="Randomness in Reinforcement Learning"></a>Randomness in Reinforcement Learning</h3><p>在RL中, 存在两种<strong>随机性</strong>, </p><ol><li><p><strong>Actions</strong> have randomness: Policy函数根据State选择动作的时候存在随机性, 动作是根据Policy Function抽样得到的. 它是<strong>Agent</strong>的随机性.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl2.png" style="zoom:50%;" /></li><li><p><strong>State transitions</strong> have randomness: 状态转移存在随机性, 系统的状态转移也是根据抽样得来的. 它是<strong>Environment</strong>的随机性.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl3.png" style="zoom: 33%;" /></li></ol><blockquote><p>本节虽然简短, 但对后续理解Return和Value Function的概念至关重要.</p></blockquote><h3 id="Agent-Environment-Interaction"><a href="#Agent-Environment-Interaction" class="headerlink" title="Agent - Environment Interaction"></a>Agent - Environment Interaction</h3><p>下面描述一下Agent和Environment的交互过程:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl4.png" style="zoom:50%;" /><p>Agent总是和环境循环的按照下面的流程交互:</p><ol><li>观察环境, 得到状态$s_1$.</li><li>Agent做出动作$a_1$.</li><li>环境状态转移, 得到状态$s_2$, 同时给予Agent奖励$r_1$.</li><li>Agent做出动作$a_2$.</li><li>……</li></ol><p>它对应着这样一幅图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl5.png" style="zoom:50%;" /><p>由此我们可以得到一组关于<code>(state, action, reaward)</code>的Trajectory(迹):<br>$$<br>s_1, a_1,r_1,\ s_2, a_2, r_2,\ \cdots,\ s_T, a_T, r_T.<br>$$<br>$T$ 为结束时刻.</p><h3 id="Rewards-and-Returns"><a href="#Rewards-and-Returns" class="headerlink" title="Rewards and Returns"></a>Rewards and Returns</h3><h4 id="Return"><a href="#Return" class="headerlink" title="Return"></a>Return</h4><p><strong>Return</strong>(回报)指的是<strong>当前时刻开始到这个episode结束</strong>时<strong>奖励的和</strong>, 它的另一个名字是<strong>Cumulative Future Reward</strong>(未来累计奖励), 所以<strong>它和当前时刻的奖励$R_t$ 不是同一个概念</strong>:<br>$$<br>U_t= R_t+ R_{t+1} + R_{t+2} + R_{t+3} + \cdots<br>$$<br>在RL中认为, 对于Agent来说, $R_t$ 比$R_{t+1}$ 要更重要, 即未来的奖励没有现在的奖励重要, 因为未来的不确定性更大, 同样都是奖励, 不如立刻得到奖励 ,而非. 这就引出了另一个概念, <strong>Discounted Return</strong>(折扣奖励, 也叫折扣未来累计奖励):<br>$$<br>U_t= R_t+ \gamma R_{t+1} + \gamma^2 R_{t+2} + \gamma^3 R_{t+3} + \cdots<br>$$<br>$\gamma \in [0, 1]$, 叫<strong>折扣率</strong>, 是个超参. 如果未来和现在的奖励权重相同, 那就令$\gamma=1$, 如果根本不考虑未来的奖励, 那就令$\gamma=0$.</p><blockquote><p>一般情况下, RL中说Return指的都是Discounted Return.</p></blockquote><p>为什么要定义Return呢? 因为<strong>RL的目标是最大化Return</strong>, <strong>而非最大化Reward</strong>. 下棋时我们应该赢得整个比赛(Return), 而不是吃掉对面一个棋子(Reward).</p><h4 id="Randomness-in-Returns"><a href="#Randomness-in-Returns" class="headerlink" title="Randomness in Returns"></a>Randomness in Returns</h4><p>在任意时刻$t$, Return $U_t$ 是<strong>随机</strong>的. 因为Return由未来的Reward构成, 而未来时刻$i \geq t$ 的Reward $R_i $ 由Action $A_i$ 和State $S_i$决定, 但Action和State是随机的, 所以Return也是随机的. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl6.png" style="zoom:50%;" /><blockquote><p>观测值不具有随机性(小写字母), 未知的随机变量有随机性(大写字母).</p></blockquote><p>同时注意到, <strong>Return依赖于未来</strong>. 根据$U_t$ 的定义, 对于已观测到的$s_t$, $U_t$ 由<strong>随机变量</strong>$A_t, A_{t+1}, A_{t+2}, \cdots$ 以及$S_{t+1}, S_{t+2}, \cdots$ 决定.</p><h3 id="Value-Functions"><a href="#Value-Functions" class="headerlink" title="Value Functions"></a>Value Functions</h3><h4 id="Action-Value-Function"><a href="#Action-Value-Function" class="headerlink" title="Action - Value Function"></a>Action - Value Function</h4><p>前面说过Return依赖于未来, 在当前时刻$t$, 未来的Return是随机变量, 然而我们根本不可能预知未来, 不可能在$t$ 时刻直接知道$U_t$ 是什么, 因为我们不知道未来的Reward是多少. 我们在抛硬币之前并不知道抛出去的硬币是正面还是反面.</p><p>那我们如何评估当前形式? 因为$U_t$ 是个<strong>随机变量</strong>, 那我们就把所有的<strong>随机性全都积掉</strong>(也就是求<strong>期望</strong>). 虽然在抛硬币之前不知道会抛出正面还是反面, 但应该知道正反面各有一半的概率, 并且当正面记作1, 反面记作0时可得到期望0.5.</p><p>$U_t$ 取决于$A_t, A_{t+1}, A_{t+2}, \cdots$ 和$S_{t+1}, S_{t+2}, \cdots$, 从$t+1$ 时刻起的所有动作和状态都具有随机性, 如果把$U_t$ 视为$s_t$ 和$a_t$ 的函数, 求条件期望可以去掉随机, 这个函数被称为<strong>Action - Value Function</strong>(动作价值函数):<br>$$<br>Q_{\pi}\left(s_{t}, a_{t}\right)=\mathbb{E}\left[U_{t} \mid S_{t}=s_{t}, A_{t}=a_{t}\right]<br>$$</p><ul><li>动作的概率密度函数是策略函数 $\mathbb{P}(A=a \mid S=s) = \pi(a \mid s)$.</li><li>状态的概率密度函数是状态转移函数 $\mathbb{P}(S^\prime=s \mid S=s, A=a) = p(s^\prime\mid s, a)$.</li><li>动作价值函数与$\pi$ 有关. 因为在不同策略函数$\pi$ 下, 条件期望不同.</li></ul><blockquote><p>它更准确地说应该叫<strong>动作状态价值函数</strong>, 因为它依赖$s_t, a_t$ 两个变量. 但大家习惯的称之为动作价值函数.</p></blockquote><p>除了已观测到的$s_t$ 和$a_t$, 其余的未来随机变量$A_{t+1}, A_{t+2}, \cdots$ 和$S_{t+1}, S_{t+2}, \cdots$ 都被积掉了. </p><p>动作价值函数直观的告诉我们, 如果用策略函数$\pi$, 在$s_t$ 下做$a_t$ 是好还是不好, 就可以根据当前局势对动作打分.</p><p>因为动作价值函数跟$\pi$ 有关, 想要把$\pi$ 去掉只需要令Agent找到使得$Q_\pi$ 最大化的那种策略, 即<strong>Optimal Action - Value Function</strong>(最优动作价值函数):<br>$$<br>Q^{\star}\left(s_{t}, a_{t}\right)=\max _{\pi} Q_{\pi}\left(s_{t}, a_{t}\right)<br>$$<br>得到了最优动作价值函数$Q^\star$ 后, Agent直接根据$Q^\star (s_t, A)$ 的值就能直接做出决策. </p><h4 id="State-Value-Function"><a href="#State-Value-Function" class="headerlink" title="State - Value Function"></a>State - Value Function</h4><p>如果Agent想知道当前状态 / 局势$s_t$ 对自己好不好, 就得在动作价值函数的基础上把$A$ 也给积掉, 或者说无论Agent做出什么动作, 环境状态对Agent来说如何?</p><p>这就引出了<strong>State - Value Function</strong>(状态价值函数)的概念. 假如$A$ 是离散的,<br>$$<br>V_{\pi}\left(s_{t}\right) =\mathbb{E}_A\left[Q_{\pi}\left(s_{t}, A\right)\right]<br>=\sum_{a \in \mathcal{A}} \pi\left(a \mid s_{t}\right) \cdot Q_{\pi}\left(s_{t}, a\right)<br>$$<br>如果$A$ 是连续的就求积分:<br>$$<br>V_{\pi}\left(s_{t}\right) =\mathbb{E}_A\left[Q_{\pi}\left(s_{t}, A\right)\right]<br>=\int_{a \in \mathcal{A}} \pi\left(a \mid s_{t}\right) \cdot Q_{\pi}\left(s_{t}, a\right)\mathrm{d}a<br>$$</p><blockquote><p>$V_\pi$ 还能评价$\pi$ 的好坏, 如果$\pi$ 越好, 去掉状态的不确定性(对$S$ 求期望)后的值$\mathbb{E}_S\left[V_{\pi}\left(S\right)\right]$ 就越大.</p></blockquote><h2 id="Value-Based-Reinforcement-Learning"><a href="#Value-Based-Reinforcement-Learning" class="headerlink" title="Value - Based Reinforcement Learning"></a>Value - Based Reinforcement Learning</h2><blockquote><p>请理解并回顾价值函数相关概念.</p></blockquote><p>假设$Q^\star$ 已知, Agent直接变身预言家, 直接根据$Q^\star$ 判断在$s$ 下什么才是最优的动作, 即$a^{\star}=\underset{a}{\operatorname{argmax}} Q^{\star}(s, a) $, 然而$Q^\star$ 未知, Agent并不能变身预言家. Value - Based RL思想就是通过某种方法近似出$Q^\star$.</p><h3 id="Deep-Q-Network"><a href="#Deep-Q-Network" class="headerlink" title="Deep Q - Network"></a>Deep Q - Network</h3><p>Agent不能变身预言家, 除非… 除非Agent玩了无数把超级玛丽, 对超级玛丽, 那它也和先知没什么区别了, 瞅一眼屏幕就能告诉你要选什么Action. </p><p>行啊, 那我就玩呗. <strong>Deep Q - Network</strong>(DQN) 正是基于这个想法, 用神经网络$Q(s, a;\mathbf{w})$来<strong>近似</strong>$Q^\star(s, a)$, 即:<br>$$<br>Q(s, a, \mathbf{w}) \approx Q^\star (s, a)<br>$$</p><blockquote><p>请注意, DQN近似的是最优动作价值函数$Q^\star(s, a)$, 而不是动作价值函数$Q_\pi(s, a)$.</p></blockquote><p>举个例子, 让神经网络玩无数次超级玛丽, 用CNN来捕获屏幕上的State Feature, 然后喂给全连接层给出当前状态特征下不同动作的打分(离散Action):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl7.png" style="zoom:50%;" /><p>向上的$Q$ 最大, 所以让玛丽奥往上跳更好.</p><p>所以DQN的RL流程很简单嘛:</p><ol><li>观察环境状态$s_t$, 比如用CNN来捕获特征.</li><li>用DQN获得Agent各个动作的打分, 让Agent做出打分最大的动作$a_t$.</li><li>环境根据$a_t$ 发生状态转移, 新状态为$s_{t+1}$, 同时给予Agent奖励$r_t$.</li><li>……</li></ol><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl8.png" style="zoom: 33%;" /><h3 id="Temporal-Difference-TD-Learning"><a href="#Temporal-Difference-TD-Learning" class="headerlink" title="Temporal Difference (TD) Learning"></a>Temporal Difference (TD) Learning</h3><p><strong>Temporal Differnece</strong>(时间差分)算法是训练DQN最常用的方法.</p><h4 id="Scenario"><a href="#Scenario" class="headerlink" title="Scenario"></a>Scenario</h4><p>设想一个场景, 我们需要从NYC开车到Atlanta. 模型$Q(\mathbf{w})$给出了一个预估花费时间为1000mins, 我们走完全程后发现实际花费时间为860mins:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl9.png" style="zoom: 33%;" /><p>如何更新我们的模型? 很简单对吧, 只需要用均方误差作为Loss, 然后用梯度下降就行了:</p><ul><li>$q=Q(\mathbf{w}), q=1000, y=860$.</li><li>Loss: $L = \frac{1}{2} (q-y)^2$.</li><li>Gradient: $\frac{\partial L}{\partial \mathbf{w}} = \frac{\partial{L}}{\partial{q}} \cdot \frac{\partial{q}}{\partial{\mathbf{w}}} = (q-y)\cdot \frac{\partial Q(\mathbf{w})}{\partial\mathbf{w}}$.</li><li>Gradient Descent: $\mathbf{w}_{t+1} = \mathbf{w}_t - \alpha \cdot \frac{\partial{L}}{\partial\mathbf{w}}|_{\mathbf{w}=\mathbf{w_t}}$.</li></ul><p>但是这样太憨了, 我们从NYC到Atlanta的预测可能不那么靠谱, 因为这段测算距离太远了. 如果说我在旅途中发现NYC到DC用了300mins, 而此时模型测算从DC到Atlanta所需时间为600mins, 所以我根据新的模型估计, 此时需要的总共时间可能是900mins:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl10.png" style="zoom:50%;" /><p>虽然900mins不一定是最终花费时间, 但是肯定比直接从NYC开始测算的1000mins靠谱得多. 900mins称为<strong>TD Target</strong>. 我们假设TD Target给出的900mins就是Ground Truth, 然后依然按照前面的方式更新模型参数:</p><ul><li>$q=Q(\mathbf{w})=1000, y=300+600=900$.</li><li>Loss: $L = \frac{1}{2}{\left(Q\left(\mathbf{w}\right)-y\right)}^2$.</li><li>Gradient: $\frac{\partial L}{\partial\mathbf{w}} = \frac{\partial{L}}{\partial{q}} \cdot \frac{\partial{q}}{\partial{\mathbf{w}}} = \left(Q\left(\mathbf{w}\right)-y\right)\cdot \frac{\partial Q(\mathbf{w})}{\partial\mathbf{w}}=(1000-900)\cdot\frac{\partial Q(\mathbf{w})}{\partial\mathbf{w}}$.</li><li>Gradient Descent: $\mathbf{w}_{t+1} = \mathbf{w}_t - \alpha \cdot \frac{\partial{L}}{\partial\mathbf{w}}|_{\mathbf{w}=\mathbf{w_t}}$.</li></ul><p>$Q(\mathbf{w}) -y = 1000-900=100$ 也称为<strong>TD Error</strong> $\delta$.</p><p>模型原来估计NYC到Atlanta用掉1000mins, DC到Atlanta用掉600mins, 按照模型估计, NYC到DC需要400mins, 但是到DC的时候发现实际只花了300mins, 比400mins要快:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl11.png" style="zoom:50%;" /><p>因此从NYC到DC这段旅途中, <strong>模型估计</strong>的400mins和<strong>真实观测时间</strong>300mins的<strong>差</strong>就是TD Error, 与我们前面的计算值相同. 理想状态下我们肯定希望<strong>TD Error为0</strong>, 也就是模型估计和真实观测时间相同. TD算法的目的最小化TD Error, 来减小损失.</p><blockquote><p>从DC到Atlanta的模型预测实际上是跟模型参数相关的$\mathbf{w}$ 的函数, 但TD算法在求解时常常忽略这一点, 把它当做常数.</p></blockquote><p>为什么TD算法比模型原来一次测算靠谱呢? 因为TD算法中所利用的信息不单单是模型的预测, 包含了一定的事实依据. 当我们的位置越接近Atlanta, 可以利用的事实越多, 估计就越准确. 换个角度来说, 观测到的事实消除了模型预测时附加进去的未知性, 既定事实已经没有未知干扰, 所以TD更准.</p><h4 id="TD-Learning-for-DQN"><a href="#TD-Learning-for-DQN" class="headerlink" title="TD Learning for DQN"></a>TD Learning for DQN</h4><p>把TD算法应用在DQN上是相同的道理, 我们在旅途中途就可以更新模型参数, 是因为它有这样一个关系存在:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl12.png" style="zoom:50%;" /><p>模型的<strong>完整预测</strong>可以被拆分为已经观测到的<strong>事实</strong>和模型<strong>剩余预测</strong>.</p><p>非常巧的是, 在深度强化学习中也有一个类似的式子存在:<br>$$<br>Q\left(s_{t}, a_{t} ; \mathbf{w}\right) \approx r_{t}+\gamma \cdot Q\left(s_{t+1}, a_{t+1} ; \mathbf{w}\right)<br>$$<br>回想一下我们之前讲过的Discounted Return $U_t$ 的定义, 不难明白为什么也存在类似的关系.<br>$$<br>\begin{aligned}<br>U_t&amp;= R_t+ \gamma R_{t+1} + \gamma^2 R_{t+2} + \gamma^3 R_{t+3} + \cdots \\<br> &amp;=  R_t+  \gamma \cdot \left(R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots \right) \\<br> &amp; = R_t + \gamma \cdot U_{t+1}<br>\end{aligned}<br>$$<br>对上式两边同求个期望:<br>$$<br>\mathbb{E}\left[U_t\right] = \mathbb{E} \left[R_t + \gamma \cdot U_{t+1}\right]<br>$$<br>在DQN里面, 用神经网络来近似动作价值函数$Q$, 所以$Q(s_t, a_t; \mathbf{w})$ 是$U_t$ 的估计$\mathbb{E}[U_t]$, $Q(S_{t+1}, A_{t+1}, ; \mathbf{w})$ 是$U_{t+1}$ 的估计$\mathbb{E}[U_{t+1}]$. 因此就有:<br>$$<br>Q(s_t, a_t; \mathbf{w}) \approx \mathbb{E}\left[R_t + \gamma \cdot Q\left(S_{t+1}, A_{t+1}, ; \mathbf{w}\right)\right]<br>$$<br>所以我们这样训练DQN:</p><ul><li><p>Prediction: $Q(s_t, a_t; \mathbf{w}_t)$.</p></li><li><p>TD Target:</p><p>在$t+1$ 时刻, 可以观测到奖励$r_t$ 和新的状态$s_{t+1}$, 新的动作$a_{t+1}$ 可以由$Q$ 得到, 故TD Target $y_t$:</p></li></ul><p>$$<br>\begin{aligned}<br>y_t&amp;=  r_t + \gamma \cdot Q(s_{t+1}, a_{t+1} ; \mathbf{w}_t) \\<br>&amp;= r_t + \gamma \cdot \underset{a}{\max}Q(s_{t+1}, a; \mathbf{w}_t)<br>\end{aligned}<br>$$</p><ul><li>Loss: $L_t = \frac{1}{2}{[Q(s_t, a_t; \mathbf{w}) - y_t]}^2$.</li><li>Gradient Descent: $\mathbf{w}_{t+1} = \mathbf{w}_t - \alpha \cdot \frac{\partial{L_t} }{\partial\mathbf{w}} |_{\mathbf{w}=\mathbf{w}_t} $.</li></ul><blockquote><p>实际上这样并不好训练, 因为我们目前所说的DQN兼顾了输出左侧$Q(s_t, a_t; \mathbf{w})$ 和右侧$Q(s_{t+1}, a_{t+1} ; \mathbf{w}_t)$ 的任务, 反向传播的时候会把两个更新结果放在一起. 如果网络$Q(s, a;\mathbf{w})$ 被更新, 那么右侧的值也会发生变化, 导致训练时候不太稳定. 所以后面会有针对DQN更高阶的技巧.</p></blockquote><h2 id="Policy-Based-Reinforcement-Learning"><a href="#Policy-Based-Reinforcement-Learning" class="headerlink" title="Policy - Based Reinforcement Learning"></a>Policy - Based Reinforcement Learning</h2><blockquote><p>请理解并回顾策略函数, 价值函数, 回报相关概念.</p></blockquote><p>有了好的策略$\pi(a \mid s)$, 就能用$\pi$ 根据环境$s$ 自动控制Agent. 问题是如何得到好的策略$\pi^\star$.</p><h3 id="Policy-Network"><a href="#Policy-Network" class="headerlink" title="Policy Network"></a>Policy Network</h3><p>如果状态$S$ 和动作$A$ 的数量比较少, 那策略$\pi(a \mid s)$ 直接画一张表, 穷举出所有的可能性不就好了. 但是超级玛丽这样的游戏有无数状态, 没法用一张表搞定, 所以我们就得用一种方法来近似.</p><blockquote><div style="text-align: center;"> "神经网络时代的哲学: 难算的我们都用神经网络来拟合!" <div style="float: right"><br>----<a href="https://spaces.ac.cn/archives/5253" target="_blank" rel="noopener">苏剑林</a></div></div></blockquote><p>我们用神经网络来近似策略函数$\pi(a|s)$, 这种神经网络称为<strong>Policy Network</strong>(策略网络), 记$\pi(a \mid s, \boldsymbol{\theta})$. 其中$\boldsymbol{\theta}$ 是网络可训练参数, 因为Policy Function给出的是选择各个Action的概率, 只要满足$\sum_{a\in \mathcal{A}}\pi(a \mid s; \boldsymbol{\theta})=1$ 即可.</p><p>拿超级玛丽来说, 可以把网络设计成这样:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl13.png" style="zoom: 50%;" /><p>在超级玛丽中DQN和Policy Network区别不大对吗(仅仅在这个例子中)? 除了最后多加了一层Softmax. </p><h3 id="State-Value-Function-Approximation"><a href="#State-Value-Function-Approximation" class="headerlink" title="State - Value Function Approximation"></a>State - Value Function Approximation</h3><h4 id="Policy-Based-Reinforcement-Learning-1"><a href="#Policy-Based-Reinforcement-Learning-1" class="headerlink" title="Policy - Based Reinforcement Learning"></a>Policy - Based Reinforcement Learning</h4><p>因为我们用的是策略网络$\pi\left(a \mid s_{t};\boldsymbol{\theta}\right)$, 而状态价值函数的计算是由策略函数推导来的, 所以状态价值函数也需要做相应的近似:<br>$$<br>\displaylines{<br>V_{\pi}\left(s_{t}\right) =\mathbb{E}_A\left[Q_{\pi}\left(s_{t}, A\right)\right]<br>=\sum_{a \in \mathcal{A}} \pi\left(a \mid s_{t}\right) \cdot Q_{\pi}\left(s_{t}, a\right) \\<br>\Downarrow \\<br>V\left(s_{t};\boldsymbol{\theta}\right)<br>=\sum_{a \in \mathcal{A}} \pi\left(a \mid s_{t};\boldsymbol{\theta}\right) \cdot Q_{\pi}\left(s_{t}, a\right)<br>}<br>$$<br>状态价值函数现在只和状态$s_t$, 策略网络模型参数$\boldsymbol{\theta}$ 有关, $V_\pi$ 中的$\pi$ 被整合到了$\boldsymbol{\theta}$ 当中. </p><p><strong>策略网络$\pi(a\mid s;\boldsymbol{\theta})$ 越好</strong>, <strong>$V(s;\boldsymbol{\theta})$ 的值就越大</strong>. <strong>通过改进模型参数最大化$V$ 就能使得模型参数$\boldsymbol{\theta}$ 越来越好</strong>, 这是策略学习最核心的思想. 基于这个想法, 可以写出策略学习的目标函数:<br>$$<br>J(\boldsymbol{\theta})=\mathbb{E}_{S}[V(S ; \boldsymbol{\theta})]<br>$$<br>求期望后只和$\boldsymbol{\theta}$ 有关, 与状态$S$ 无关. 我们可以放心的优化模型参数了.</p><blockquote><p>实际求的时候不可能真正去求期望, 一般是随机采样得到.</p></blockquote><p>如果观测到状态$s$, 用<strong>Policy Gradient ascent</strong>(策略梯度上升)来优化$\boldsymbol{\theta}$:<br>$$<br>\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}+\beta \cdot \frac{\partial V(s ; \boldsymbol{\theta})}{\partial\boldsymbol{\theta}}<br>$$<br>$\beta$ 为学习率. 但这里的梯度指的是<strong>策略梯度</strong>.</p><h3 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h3><blockquote><p>本小节中, 将涉及到如何推导和近似求解策略梯度. 可以选择性阅读.</p></blockquote><p>不严谨的推导Policy Gradient:<br>$$<br>\begin{aligned}<br>\frac{\partial V(s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} &amp;=\frac{\partial \sum_{a} \pi(a \mid s ; \boldsymbol{\theta}) \cdot Q_{\pi}(s, a)}{\partial \boldsymbol{\theta}} \\<br>&amp;=\sum_{a} \frac{\partial \pi(a \mid s ; \boldsymbol{\theta}) \cdot Q_{\pi}(s, a)}{\partial \boldsymbol{\theta}} \\<br>&amp;=\sum_{a} \frac{\partial \pi(a \mid s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot Q_{\pi}(s, a)<br>\end{aligned}<br>$$<br>第一步到第二步是把求导拿到里面, 和的导数变成导数的和. 为了方便推导和理解, 第二步到第三步做了一个不正确的简化, 假设$Q_\pi(s, a)$ 和$\boldsymbol{\theta}$ 无关, 所以$Q_\pi$ 视为常数拿出来. </p><blockquote><p>实际上这种假设对最终结果影响不大, 只是前面相差一个系数, 在梯度上升时会被学习率$\beta$ 吸收.</p></blockquote><p>如果动作都是离散的, 推导到这里已经能够算出策略梯度了. 但实际应用时不会用这个公式来计算策略梯度, 而且这种形式没法处理连续的动作.</p><p>书接上文, 继续推导:<br>$$<br>\begin{aligned}<br>\frac{\partial V(s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} &amp;=\sum_{a} \frac{\partial \pi(a \mid s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot Q_{\pi}(s, a) \\<br>&amp;=\sum_{a} \pi(a \mid s ; \boldsymbol{\theta}) \cdot \frac{\partial \log \pi(a \mid s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot Q_{\pi}(s, a) \\<br>&amp;= \mathbb{E}_A\left[ \frac{\partial{\log \pi(A \mid s ; \boldsymbol{\theta})}}{\partial \boldsymbol{\theta}} \cdot Q_{\pi}(s, A)\right]<br>\end{aligned}<br>$$<br>第一步到第二步正推不好推, 但逆着比较好验证, 首先是链式法则有:<br>$$<br>\frac{\partial \log \left[\pi(\boldsymbol{\theta})\right]}{\partial \boldsymbol{\theta}}=\frac{1}{\pi(\boldsymbol{\theta})} \cdot \frac{\partial \pi(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}<br>$$<br>然后两边同乘$\pi(\boldsymbol{\theta})$:<br>$$<br>\pi(\boldsymbol{\theta}) \cdot \frac{\partial \log \left[\pi(\boldsymbol{\theta})\right]}{\partial \boldsymbol{\theta}}=<br>\cancel{\pi(\boldsymbol{\theta})} \cdot \frac{1}{\cancel{\pi(\boldsymbol{\theta})}} \cdot \frac{\partial \pi(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = \frac{\partial \pi(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}<br>$$<br>第二步到第三步比较好理解, 形式上就是把动作看成随机变量求期望.</p><h3 id="Calculate-Policy-Gradient"><a href="#Calculate-Policy-Gradient" class="headerlink" title="Calculate Policy Gradient"></a>Calculate Policy Gradient</h3><h4 id="Policy-Gradient-for-Discrete-Actions"><a href="#Policy-Gradient-for-Discrete-Actions" class="headerlink" title="Policy Gradient for Discrete Actions"></a>Policy Gradient for Discrete Actions</h4><p>先来看看第一种形式的策略梯度要如何计算. 如果Action是<strong>离散</strong>的, 设动作空间$\mathcal{A}=\{\text{“left”}, \text{“right”}, \text{“up”} \}$, 第一种形式:<br>$$<br>\frac{\partial V(s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} =<br>\sum_{a} \frac{\partial \pi(a \mid s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot Q_{\pi}(s, a)<br>$$</p><ol><li>对于每个动作$a \in \mathcal{A}$, 令$\mathbf{f}(a, \boldsymbol{\theta})=\frac{\partial \pi(a \mid s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot Q_{\pi}(s, a)$. </li><li>策略梯度$\frac{\partial V(s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = \mathbf{f}(\text{“left”}, \boldsymbol{\theta}) + \mathbf{f}(\text{“right”}, \boldsymbol{\theta}) + \mathbf{f}(\text{“up”}, \boldsymbol{\theta})  $.</li></ol><h4 id="Policy-Gradient-for-Continuous-Actions"><a href="#Policy-Gradient-for-Continuous-Actions" class="headerlink" title="Policy Gradient for Continuous Actions"></a>Policy Gradient for Continuous Actions</h4><p>上面那种形式对于<strong>连续</strong>的Action就比较麻烦了, 假设$\mathcal{A} = [0, 1]$, 第二种形式:<br>$$<br>\frac{\partial V(s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = \mathbb{E}_{A \sim  \pi(\cdot|s;\boldsymbol{\theta})}\left[ \frac{\partial{\log \pi(A \mid s ; \boldsymbol{\theta})}}{\partial \boldsymbol{\theta}} \cdot Q_{\pi}(s, A)\right]<br>$$<br>定积分没法求解这个期望, 因为$\pi$ 是个神经网络, 所以我们只能用<strong>蒙特卡洛近似</strong>:</p><ol><li>根据$\pi(\cdot \mid s;\boldsymbol{\theta})$ 抽样得到一个随机动作$\hat{a}$, </li><li>令$\mathbf{g}(\hat{a}, \boldsymbol{\theta}) = \frac{\partial{\log \pi(A \mid s ; \boldsymbol{\theta})}}{\partial \boldsymbol{\theta}} \cdot Q_{\pi}(s, A)$, 直接算$\mathbf{g}(\hat{a}, \boldsymbol{\theta})$ 就行了.</li><li>根据$\mathbf{g}(\hat{a}, \boldsymbol{\theta})$ 的定义, 直接有$\mathbb{E}_{A}[\mathbf{g}(A, \boldsymbol{\theta})]=\frac{\boldsymbol{\partial} V(s ; \boldsymbol{\theta})}{\boldsymbol{\partial} \boldsymbol{\theta}}$, 因为$\hat{a}$ 是从$\pi(\cdot\mid s;\boldsymbol{\theta})$ 里抽出来的, 所以$\mathbf{g}(\hat{a}, \boldsymbol{\theta})$ 就是策略梯度$\frac{\boldsymbol{\partial} V(s ; \boldsymbol{\theta})}{\boldsymbol{\partial} \boldsymbol{\theta}}$ 的<strong>无偏估计</strong>. 故$\mathbf{g}(\hat{a}, \boldsymbol{\theta})$ 就是$\frac{\boldsymbol{\partial} V(s ; \boldsymbol{\theta})}{\boldsymbol{\partial} \boldsymbol{\theta}}$ 的近似.</li></ol><blockquote><p>这种方法对离散动作也适用.</p></blockquote><h3 id="Update-Policy-Network-using-Policy-Gradient"><a href="#Update-Policy-Network-using-Policy-Gradient" class="headerlink" title="Update Policy Network using Policy Gradient"></a>Update Policy Network using Policy Gradient</h3><p>我们已经推导出了策略梯度, 和如何用策略梯度更新策略网络, 现在来梳理一下:</p><ol><li>观察状态$s_t$.</li><li>用蒙特卡洛从$\pi(\cdot \mid s;\boldsymbol{\theta})$ 中抽样出随机动作$a_t$.</li><li>计算价值函数的值$q_t\approx Q_\pi(s_t, a_t)$.</li><li>对策略网络$\pi$ 求导, 计算$\mathbf{d}_{\boldsymbol{\theta}, t}=\left.\frac{\partial \log \pi\left(a_{t} \mid s_{t}, \boldsymbol{\theta}\right)}{\partial \boldsymbol{\theta}}\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}_{t}}$.</li><li>近似计算策略梯度$\mathbf{g}\left(a_{t}, \boldsymbol{\theta}_{t}\right)=q_{t} \cdot \mathbf{d}_{\theta, t}$.</li><li>用策略梯度更新策略网络: $\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_{t}+\beta \cdot \mathbf{g}\left(a_{t}, \boldsymbol{\theta}_{t}\right)$.</li></ol><p>等会… 是不是还有个问题, $q_t$ 怎么求来的? 我们只能用以下两种方法来近似$q_t$:</p><ol><li><strong>Reinforce</strong>(蒙特卡洛策略梯度): <ul><li>在一个Episode中, 记录下迹, 就能获得所有时刻$t$ 时的回报$u_t=\sum^T_{k=t} \gamma^{k-t} r_k$​, </li><li>由于$Q_\pi(s_t, a_t) = \mathbb{E}[U_t]$, 所以我们用观测值$u_t$ 来近似$Q_\pi(s_t, a_t)$, 即$q_t = u_t$.</li></ul></li><li><strong>Actor - Critic</strong>:<ul><li>贯彻神经网络近似一切的传统, 用一个新的神经网络来近似$Q_\pi$, 这样就有俩神经网络, 一个是Actor, 另一个是Critic, 这也就是Actor - Critic Method.</li></ul></li></ol><h2 id="Actor-Critic-Method"><a href="#Actor-Critic-Method" class="headerlink" title="Actor - Critic Method"></a>Actor - Critic Method</h2><p>前面说过, 由于策略梯度$\frac{\partial V(s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}}$求解需要用到动作价值函数$Q_\pi$, 在策略学习中用Reinforce方法可以求得.</p><p>如果使用神经网络来近似动作价值函数$Q_\pi$, 这就催生了<strong>Actor - Critic Method</strong>, 把价值学习和策略学习结合到一起:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl14.png" style="zoom:33%;" /><h3 id="State-Value-Function-Approximation-1"><a href="#State-Value-Function-Approximation-1" class="headerlink" title="State - Value Function Approximation"></a>State - Value Function Approximation</h3><p>在价值学习时, 我们把策略函数$ \pi\left(a \mid s\right) $  用策略网络$ \pi\left(a \mid s;\boldsymbol{\theta}\right) $ 来近似, 我们只需要类似的把动作价值函数$Q_\pi{s, a}$ 用<strong>Value Network</strong>(价值网络)$q(s, a; \mathbf{w})$ 来表达即可:<br>$$<br>V_{\pi}\left(s\right) =\sum_{a \in \mathcal{A}} \pi\left(a \mid s\right) \cdot Q_{\pi}\left(s, a\right) \approx \sum_{a \in \mathcal{A}} \pi\left(a \mid s;\boldsymbol{\theta}\right) \cdot q(s, a;\mathbf{w})<br>$$<br>$\boldsymbol{\theta}, \mathbf{w}$ 分别为Policy Network, Value Network的参数.</p><h3 id="Policy-Network-Actor"><a href="#Policy-Network-Actor" class="headerlink" title="Policy Network (Actor)"></a>Policy Network (Actor)</h3><p>Policy Network近似了<strong>策略函数</strong>$\pi(a\mid s)$, 继续沿用我们之前讲过的结构吧:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl15.png" style="zoom:50%;" /><p>Policy Network的输出是在状态$s$ 下选择各个动作$a$ 的概率, $\sum_{a\in \mathcal{A}}\pi(a \mid s; \boldsymbol{\theta})=1$.</p><h3 id="Value-Network-Critic"><a href="#Value-Network-Critic" class="headerlink" title="Value Network (Critic)"></a>Value Network (Critic)</h3><p>Value Network近似了<strong>动作价值函数</strong>$Q_\pi(s, a)$, 根据$s$ 对所有的$a$ 打分.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl16.png" style="zoom:50%;" /><blockquote><p>虽然Value Network和DQN有相同的网络结构, 但Value Network近似的是动作价值函数$Q_\pi(s, a)$, 而不是最优动作价值函数$Q^\star(s, a)$, 因为在Actor - Critic Method中, <strong>Agent的动作由Policy Network决定</strong>, <strong>而不是由Value Network自身决定</strong>.</p></blockquote><p>Value Network的输出是在状态$s$ 下各动作$a$ 的打分.</p><h3 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor Critic"></a>Actor Critic</h3><p>Policy Network像<strong>演员</strong>一样, 根据策略做出相应的动作, 而Value Network像<strong>评委</strong>一样, 评价演员的动作好坏:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl17.png" style="zoom: 33%;" /><p> 随着网络的训练, 演员的动作越来越好, 裁判的打分越来越精准.</p><blockquote><p>如果只是从网络结构的角度来看, GAN与它几乎是相同的! Actor对应着GAN的<strong>Generator</strong>, Critic对应着GAN的<strong>Discriminator</strong>. 和GAN类似的, 在训练后, <strong>Value Network会被丢弃</strong>.</p><p>但是突然想起GAN是一个十分难训练的网络… 估计这个问题也会在Actor - Critic上也存在. 如果从时间线上来看, Actor - Critic比GAN要早! 所以到底是谁比较像谁呢?</p></blockquote><h3 id="Train-the-Neural-Network"><a href="#Train-the-Neural-Network" class="headerlink" title="Train the Neural Network"></a>Train the Neural Network</h3><blockquote><p>请回顾TD算法和策略梯度.</p></blockquote><p>怎么才能训练这两个网络呢? 我们来观察一下Actor - Critic 的状态价值函数:<br>$$<br>V(s; \boldsymbol{\theta}, \mathbf{w}) = \sum_{a \in \mathcal{A}} \pi\left(a \mid s;\boldsymbol{\theta}\right) \cdot q(s, a;\mathbf{w})<br>$$</p><ul><li>更新Policy Network $\pi\left(a \mid s;\boldsymbol{\theta}\right)$ 是为了让状态价值$V(s; \boldsymbol{\theta}, \mathbf{w})$ 更大, 令演员得到的平均分更高. 所以Policy Network学习时, 依赖的价值是有监督的由Value Network提供的.</li><li>更新Value Network $q(s, a;\mathbf{w})$ 是为了对演员动作的打分更为精准, 从而更好估计Return. 所以Value Network学习时, 依赖的奖励是有监督的由环境提供的.</li></ul><p>Actor观察状态$s$ 后, 做出动作$a$, Critic根据$s$ 和$a$, 给出打分$q$. 然后环境状态转移, 同时给予奖励$r$. Critic从环境奖励中吸取经验改进自己, 演员也根据评委的意见改进自己, 然后如此循环:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl18.png" style="zoom:50%;" /><ul><li><p>为了让评委越来越好, 评委根据$s, a, r$ 来改进自己的打分$q$. 可以让评委比较相邻两次的打分$q_t, q_{t+1}, r_t$, 使用TD算法更新参数.</p></li><li><p>为了让演员越来越好, 演员根据$q$ 来改进自己在不同$s$ 下选择$a$ 的能力. 如果裁判很辣鸡, $q$ 就会不准确, 演员为了迎合评委的口味, 自身也会跑偏.</p></li></ul><p>架子搭好了, 我们把具体的计算流程往里面填充:</p><ol><li>观察状态$s_t$, 并从$\pi\left(\cdot \mid s_t;\boldsymbol{\theta}_t\right)$ 中随机抽样得到动作$a_t$.</li><li>令Agent做出动作$a_t$, 同时得到新的环境状态$s_{t+1}$ 和奖励$r_t$.</li><li>从$\pi\left(\cdot \mid s_{t+1};\boldsymbol{\theta}_t\right)$ 随机采样出<strong>假设要做出的动作</strong>$\tilde{a}_{t+1}$, 但<strong>并不执行</strong>$\tilde{a}_{t+1}$.</li><li>为了使用TD算法, 计算两次Value Network的输出$q_t = q(s_t, a_t;\mathbf{w}_t), q_{t+1}=q(s_{t+1}, \tilde{a}_{t+1}; \mathbf{w}_t)$.</li><li>计算TD Error, $\delta_t=q_t - (r_t + \gamma\cdot q_{t+1})$, $r_t + \gamma\cdot q_{t+1}$ 即为TD Target.</li><li>计算Value Network的导数$\mathbf{d}_{w, t}=\left.\frac{\partial q\left(s_{t}, a_{t} ; \mathbf{w}\right)}{\partial \mathbf{w}}\right|_{\mathbf{w}=\mathbf{w}_{t}}$</li><li>用TD算法梯度下降更新Value Network, $\mathbf{w}_{t+1} = \mathbf{w}_t - \alpha \cdot \delta \cdot \mathbf{d}_{w, t}$.</li><li>计算Policy Network的导数$\mathbf{d}_{\theta, t}=\left.\frac{\partial \log \pi\left(a_{t} \mid s_{t}, \boldsymbol{\theta}\right)}{\partial \boldsymbol{\theta}}\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}_{t}}$.</li><li>用策略梯度上升来更新Policy Network, $\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_t + \beta \cdot q_t \cdot \mathbf{d}_{\theta, t}$.</li></ol><blockquote><p>第九步中很多书和论文都写成$\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_t + \beta \cdot \delta_t \cdot \mathbf{d}_{\theta, t}$, 这么写称为Policy Gradient with Baseline, 一般使用Baseline效果更好. 二者期望完全相等, 虽不影响期望, 但是好的Baseline可以降低方差, 加快收敛. </p><p>事实上, 只要接近$q_t$ 的数都能设定为Baseline, 只要它不是动作$a_t$ 的函数.</p></blockquote><h2 id="AlphaGo"><a href="#AlphaGo" class="headerlink" title="AlphaGo!"></a>AlphaGo!</h2><blockquote><p>注: 本节中讲解的细节和AlphaGo原论文并不一致, 做了相应的简化.</p></blockquote><p>AlphaGO在2016年击败了世界围棋冠军李世石, 这在当时是一件非常轰动的事情, 各大媒体更是把AI吹上天了.</p><h3 id="Go-Game"><a href="#Go-Game" class="headerlink" title="Go Game"></a>Go Game</h3><p>围棋棋盘上共有$19 \times 19=361$ 个点, 我们先把RL中的相关概念抽象出来:</p><ul><li>State: 按照<strong>黑白二子</strong>在棋盘上的<strong>位置</strong>来区分, 它的状态可以是$19 \times 19 \times 2$ 的张量, 其值全部为0或1, 用来表示黑 / 白子是否在该位置有子. 实际上AlphaGo使用了更大的状态张量$19 \times 19 \times 48$来存储其他信息.</li><li>Action: 状态空间$\mathcal{A}$ 自然就为棋盘上的361个位置, 即$\mathcal{A} = \set{1, 2, 3 \dots, 361}$.</li></ul><p>整场游戏可能选择的策略是一个Action Sequence, 非常非常大.</p><h3 id="High-Level-Ideas"><a href="#High-Level-Ideas" class="headerlink" title="High Level Ideas"></a>High Level Ideas</h3><p>从比较高的视角来看, AlphaGo通过三步来训练自己:</p><ol><li>用Behavior Cloning来初始化训练Policy Network. 其实它是<strong>监督学习</strong>, 从人类的对战中学习到.</li><li>用Reinforcement Learning的策略梯度进一步训练Policy Network, 让两个Policy Network互相对战.</li><li>训练完Policy Network后再训练Value Network.</li></ol><p>在对战李世石时使用的实际上不是Policy Network来做动作, 而是借助Policy Network和Value Network的<strong>指导</strong>做<strong>Monte Carlo Tree Search</strong>(MCTS, 蒙特卡洛树搜索) 来做出最终决策的.</p><h3 id="Policy-Network-1"><a href="#Policy-Network-1" class="headerlink" title="Policy Network"></a>Policy Network</h3><h4 id="State"><a href="#State" class="headerlink" title="State"></a>State</h4><p>AlphaGo有多个版本, 采用的State不一样. </p><p>最初版本的AlphaGo(后文称之为AlphaGo初号机)用$19 \times 19 \times 48$ 的张量表示棋盘状态.</p><p>最新版本的AlphaGo Zero用$19 \times 19 \times 17$ 的张量作为状态, 这个更好解释:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl19.png" style="zoom: 33%" /><ul><li><p>$19 \times 19$ 描述的棋盘的所有位置, 共有17个$19 \times 19$的矩阵.</p></li><li><p>把当前黑子的位置, 和之前7步黑子的位置, 用8个$19 \times\ 19$ 的矩阵来表示.</p></li><li><p>把当前白子的位置, 和之前7步白子的位置, 用8个$19 \times\ 19$ 的矩阵来表示.</p></li><li><p>最后一个矩阵, 也就是图中最下面那个矩阵, 是黑白子指示矩阵, 如果该黑子走了则矩阵全是1, 白子走全是0.</p></li></ul><p>具体为什么是把之前7步的位置信息整合在一起, 应该是理解为实验超参.</p><h4 id="Policy-Network-Architecture"><a href="#Policy-Network-Architecture" class="headerlink" title="Policy Network Architecture"></a>Policy Network Architecture</h4><p>AlphaGo Zero的策略网络结构是这样的:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl20.png" style="zoom: 33%;" /><p>就是CNN先提取特征, 然后直接加一个全连接获取动作概率.</p><p>AlphaGo初号机的策略网络结构就比较简单了:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl21.png" style="zoom:33%;" /><p>直接就只使用了CNN.</p><h4 id="Behavior-Cloning"><a href="#Behavior-Cloning" class="headerlink" title="Behavior Cloning"></a>Behavior Cloning</h4><p>AlphaGo初号机使用了Behavior Cloning.</p><p>因为Policy Network开始随机初始化, 网络参数不太好, 探索环境需要花很久很久的时间, 所以给策略网络一个比较好的初始引导是比较重要的, 这样能给网络一个比较好的<strong>初始解</strong>. 如果把人类游戏录像给AlphaGo看, 它就能够在初期模仿人类的游戏行为.</p><p>刚巧, 有一个人类围棋的数据集KGS, 拥有16w条围棋游戏记录, 作为训练数据, 让AlphaGo学习.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl22.png" style="zoom:33%;" /><p><strong>Behavior Cloning不是强化学习</strong>, 而是一种<strong>模仿学习</strong>(Imitation Learning), 这种方法与奖励无关. 下面来在场景下来介绍一下它的执行流程:</p><ol><li>观察State $s_t$.</li><li>用策略网络针对$s_t$ 做个决策: $\mathbf{p}_{t}=\left[\pi\left(1 \mid s_{t}, \boldsymbol{\theta}\right), \cdots, \pi\left(361 \mid s_{t}, \boldsymbol{\theta}\right)\right] \in(0,1)^{361}$.</li><li>假设下一步人类玩家做出的动作$a_t^\star=281$, 同时将其编码为One hot标签$y_t \in \set{0, 1}^{361}$.</li><li>对$y_t, \mathbf{p}_t$ 用CrossEntropy作为Loss, 梯度下降更新策略网络参数.</li></ol><p>其实就是把每个时间步的人类围棋行为当做多分类问题, 用梯度下降训练了Policy Network, 对吧? </p><p>Behavior Cloning后AlphaGo已经能碾压围棋小白了, 为啥呢? 大概率是因为$s_t$ 在AlphaGo看过的棋局里面出现过. </p><p>那为啥还在后面要用RL来强化AlphaGo呢? 因为$s_t$ 可能没有出现在AlphaGo看过的棋局里面. 如果没出现过, 策略网络可能直接就傻了, 做出的$a_t$ 不会太好, 所谓<strong>一步错步步皆错</strong>, 正是因为某个$a_t$ 导致策略网络进入了没见过的棋局, 后面做出的错误都会慢慢累加, 导致下棋下的越来越差, 最终输掉棋局, 尤其是在围棋庞大的棋局局势规模下, 该缺点尤为明显.</p><blockquote><p>事实上, AlphaGo Zero没有使用Behavior Cloning, 能打爆AlphaGo初号机. 以现在的目光来看可能在围棋游戏中模仿人类的思路去下棋对Agent是有害的, 可能不利于养成它自己的思路. 当然也仅限于围棋中, 有些<strong>零容错场景</strong>Behavior Cloning依然强大, 比如无人驾驶, 手术机器人等. </p></blockquote><h4 id="Train-Policy-Network-Using-Policy-Gradient"><a href="#Train-Policy-Network-Using-Policy-Gradient" class="headerlink" title="Train Policy Network Using Policy Gradient"></a>Train Policy Network Using Policy Gradient</h4><p>如果用RL来训练策略网络, 必须要有Agent和Environment:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl23.png" style="zoom:33%;" /><p>这里的Player作为Agent, 它的参数是每一局过后都会更新的. 而Opponent作为Environment, 它的参数不需要更新, 只需要从过去的Policy Network参数里面随便选一个就好.</p><h4 id="Reward"><a href="#Reward" class="headerlink" title="Reward"></a>Reward</h4><p>用RL训练Agent还需要奖励. 加入$T$ 时刻游戏结束, 定义如下奖励$r$:</p><ul><li>$r_1 = r_2 = r_3 = \cdots = r_{T-1}=0$.</li><li>$r_T = + 1$ (Winner).</li><li>$r_T = -1$ (Loser).</li></ul><p>这里定义的Return 不使用折扣, 即$u_t = \sum_{i=t}^T r_i$. 那么Winner的Return就为$u_1 = u_2 = u_3 = \cdots = u_{T}=+1$, Loser的Return为$u_1 = u_2 = u_3 = \cdots = u_{T}=-1$.</p><p>策略梯度的推导在前面已经讲过了. 我们可以用前面讲过的Reinforce方法来用$u_t$ 近似$Q_\pi(s_t, a_t)$, 策略梯度为就可以近似为$\frac{\partial \log \pi(a_t \mid s_t, \boldsymbol{\theta})}{ \partial \boldsymbol{\theta}} \cdot u_t$, 直观来看就是说如果赢了每步棋下的都是对的, 输了每步都是错的. </p><p>我们来概括一下策略网络的训练:</p><ol><li>让两个策略网络相互博弈, Agent所使用的策略网络每一把都更新一次参数.</li><li>一局打完后会获得一个迹$s_1, a_1, s_2, a_2, \cdots, s_T, a_T$, 以及输赢所对应的$u_T$.</li><li>知道这一把的结果和回报后, 求出<strong>每一把</strong>策略梯度总和$\mathbf{g}_{\theta}=\sum_{t=1}^{T} \frac{\partial \log \pi\left(a_{t} \mid s_{t}, \boldsymbol{\theta}\right)}{\partial \boldsymbol{\theta}} \cdot u_{t}$.</li><li>策略梯度上升 $\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} + \beta \cdot \mathbf{g}_\theta$.</li></ol><p>这时候策略网络已经比较强了, 但还不够强, 会犯一些小错误, 所以才不使用策略网络做决策, 而是以它为依据, 辅助MCTS做决策.</p><h3 id="Value-Network"><a href="#Value-Network" class="headerlink" title="Value Network"></a>Value Network</h3><p>前面讲的价值网络是为了近似动作价值函数$Q_\pi(s, a)$, 而在AlphaGo中<strong>直接使用价值网络$v(s ; \mathbf{w})$ 近似价值函数</strong>$V_\pi(s)$. 在我们定义的回报$U_t$ 中, $V_\pi(s) = \mathbb{E}\left[U_t \mid S_t = s\right]$ 会给出$\left[-1, 1\right]$ 之间的值, 当$V_\pi$ 快逼近-1时说明快输了, +1说明快赢了. MCTS需要借助价值网络来做决策.</p><blockquote><p>在AlphaGo 初号机中, 策略网络和价值网络是两个分开的网络. 但在Alpha Zero中已经共享了策略网络和价值网络对状态的抽取过程, 即卷积部分是共享的. </p></blockquote><p>策略网络和价值网络是<strong>分开训练</strong>的, 因此它不能算做是Actor - Critic.</p><p>价值网络训练流程如下:</p><ol><li>仍然是让两个<strong>策略网络</strong>互相打, 还是赢了的回报为+1, 输了的为-1. <strong>不更新策略网络的参数</strong>.</li><li>我们希望观测到的回报和价值网络预测的价值相同, 所以使用<strong>MSE</strong>作为损失函数优化, 即$L=\sum_{t=1}^{T} \frac{1}{2}\left[v\left(s_{t} ; \mathbf{w}\right)-u_{t}\right]^{2}$.</li><li>用梯度下降优化价值网络参数, $\mathbf{w} \leftarrow \mathbf{w} - \alpha \cdot \frac{\partial L}{\partial \mathbf{w}}$.</li></ol><p>至此, AlphaGo的训练就结束了.</p><h3 id="Monte-Carlo-Tree-Search"><a href="#Monte-Carlo-Tree-Search" class="headerlink" title="Monte Carlo Tree Search"></a>Monte Carlo Tree Search</h3><p>用Behavior Cloning和Policy Gradient训练完Policy Network, 用Gradient Descent训练完Value Network后, 在真正Inference的时候用的不是前面的二者, 而是使用的<strong>蒙特卡洛树搜索</strong>(Monte Carlo Tree Search). </p><p>用人类的习惯去思考一下, 这样做确实是有一些道理的. 因为我们在下棋的时候, 经常思考, 我们这一步走了$a_t$, 对手会怎么走出下一步呢 ($s_{t+1}$)? 假如对手按$s_{t+1}$ 走了, 这种局势对我好不好, 那我下一步该怎么走呢($a_{t+1}$)? 这样思考可以确保局势在自己的掌控之中, 不落入对手的圈套. 计算机强大的计算能力导致它十分擅长做这种搜索, 曾经97年的深蓝就是这么做的, 当时深蓝能一次性的暴力穷举出之后的12步.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl24.png" style="zoom:33%;" /><p>搜索的大概想法是:</p><ul><li>根据策略网络给出的动作好坏<strong>筛选</strong>一些好的动作, 随机选择一个动作$a$.</li><li>让策略网络根据选择的$a$ 做<strong>自我博弈</strong>, 直到游戏结束, 看看输赢.</li><li>如此<strong>反复</strong>, 直到所有好的动作都被穷举完.</li><li>每个动作$a$ 都应该对应着一个打分, 每回选择<strong>最高分数</strong>的作为真正要执行的动作.</li></ul><p>MCTS以上述四个过程为原型执行.</p><h4 id="Step-1-Selection"><a href="#Step-1-Selection" class="headerlink" title="Step 1: Selection"></a>Step 1: Selection</h4><p>观测到$s_t$, 有很多位置都可以放棋子, 但是<strong>并不是每个位置都值得考虑</strong>, 否则那样搜索空间太大了, 可以根据动作的好坏程度来选择.</p><p>首先, 给所有有效动作$a$ 打分, 计算方式如下:<br>$$<br>\operatorname{score}(a) = Q(a) + \eta \cdot \frac{\pi(a \mid s_t; \boldsymbol{\theta})}{1 + N(a)}<br>$$<br>其中有:</p><ul><li>$Q(a)$: 它是一张搜索记录计算出来的动作价值表, 后面会解释.</li><li>$\pi(a \mid s_t; \boldsymbol{\theta})$: 策略网络根据$s_t$ 选择的动作$a$的概率.</li><li>$N(a)$: 对于$s_t$, $a$ 已经被探索过的次数, 这样可以减少$a$ 被过多探测的次数.</li><li>$\eta$: 超参数, 作为加权系数.</li></ul><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl25.png" style="zoom: 50%;" /><p>然后, 假如计算出三个动作的打分分为0.6, 0.3, 0.8, 则只会选择<strong>分数最大的动作</strong>作为<strong>假定</strong>要执行的动作$a_t$.</p><h4 id="Step-2-Expansion"><a href="#Step-2-Expansion" class="headerlink" title="Step 2: Expansion"></a>Step 2: Expansion</h4><p>根据假定做出的动作$a_t$, AlphaGo会揣测对手做出的动作$a^\prime_t$, 这对于AlphaGo来说会导致出现新的环境状态$s_{t+1}$. 这样的推测肯定也必须是有依据的, 采用<strong>换位思考</strong>的思路, AlphaGo会用自己的策略网络来抽样得出一个对手动作$a_t^\prime$:<br>$$<br>a_t^\prime \sim \pi(\cdot \mid s_t^\prime; \boldsymbol{\theta})<br>$$<br>$s_t^\prime$ 指的是对手视角的状态.</p><blockquote><p>这里<strong>对手相当于Agent的环境</strong>, 其状态转移函数$p(s_{t+1} \mid s_t , a_t)$ 当然是未知的, 但是我们可以换位思考, 使用$\pi$ 来代替它.</p></blockquote><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl26.png" style="zoom: 40%;" /><p>这样在抽样得出的对手动作$a_t^\prime$ 影响下, 我方观察到的状态为$s_{t+1}$.</p><h4 id="Step-3-Evaluation"><a href="#Step-3-Evaluation" class="headerlink" title="Step 3: Evaluation"></a>Step 3: Evaluation</h4><p>从$s_{t+1}$ 开始, 后面都是策略网络的<strong>自我博弈</strong>, 也就是<strong>Fast Rollout</strong>:</p><ul><li>Player: $a_k \sim \pi(\cdot \mid s_k; \boldsymbol{\theta})$.</li><li>Opponent: $a_k^\prime \sim \pi(\cdot \mid s_k^\prime; \boldsymbol{\theta})$.</li><li>当分出胜负时, 赢了则$r_T=+1$, 输了$r_T=-1$. 这种奖励可以一定程度评价$s_{t+1}$ 的好坏. 应该想办法令赢了增加状态$s_{t+1}$ 的评分, 反之降低.</li></ul><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl27.png" style="zoom:40%;" /><p>除了用$r_T$ 来判断状态好坏, AlphaGo还用价值网络$v$ 来评价$s_{t+1}$:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl28.png" style="zoom:40%;" /><p>把价值网络给出的$s_{t+1}$ 评分$v(s_{t+1}; \mathbf{w})$ 和$r_T$ 做个平均再加在一起, 作为$s_{t+1}$ 的分数, 并记录下来.</p><p>$s_{t+1}$ 越好, $V(s_{t+1})$ 就越高, 如果往回追溯, $a_t$ 动作就越好.</p><h4 id="Step-4-Backup"><a href="#Step-4-Backup" class="headerlink" title="Step 4: Backup"></a>Step 4: Backup</h4><p>由于各类随机性, Evaluation会反复模拟很多次, 所以每个动作$a_t$ 下都会有很多组不同状态$s_{t+1}$ 对应的$V(s_{t+1})$:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl29.png" style="zoom: 40%;" /><p>在我们Step 1中提到的动作价值$Q(a_t)$ 就可以被状态价值$V(s_{t+1})$ 所表示了:<br>$$<br>Q(a_t)= \operatorname{mean}(\text{recorded } V\text{‘s})<br>$$<br>初始时, 还没有被记录下来的$V$, 那么$Q$ 初始化时都为0.</p><p>回想一下我们在Step 1中写过的打分式:<br>$$<br>\operatorname{score}(a) = Q(a) + \eta \cdot \frac{\pi(a \mid s_t; \boldsymbol{\theta})}{1 + N(a)}<br>$$<br>这意味着两点:</p><ul><li>刚开始时$Q(a)$ 都为0, 所以基本上依赖后一项, 也就是<strong>策略网络</strong>给出的打分来判断动作$a$ 好坏.</li><li>随着模拟次数增加, 策略网络觉得越好的动作就越有可能被选中, 即$N(a)$ 变大, 第二项缩小, 后期会主要依赖<strong>动作价值</strong>$Q(a)$ 来评价动作好坏.</li></ul><h4 id="Decision-Making-after-MCTS"><a href="#Decision-Making-after-MCTS" class="headerlink" title="Decision Making after MCTS"></a>Decision Making after MCTS</h4><p>前面说过, 一个动作越好, 它的$Q(a), \pi(a)$, 都会越大, 它的$\operatorname{score}$ 也就高, 被选中的几率就越大, 即$N(a)$ 越大. </p><p>AlphaGo做决策很简单, 就是选择被选中次数最多的动作作为实际所选择的动作$a_t$:<br>$$<br>a_{t}=\underset{a}{\operatorname{argmax}} N(a)<br>$$</p><blockquote><p>既然每次选择的都是$N(a)$ 最大的动作, 它也与动作$a$ 本身的好坏有关, 要是策略网络能强大到和费那么老大劲的搜索一样就好了. 直觉告诉我们, MCTS做了非常多次采样模拟才得到的决策应该比策略网络一股脑给出的决策要好一些, 因此具有指导意义.</p><p>没错, AlphaGo Zero的想法是把所有$a$ 对应的$N(a)$ 都归一化, 视为概率分布, 最小化其与策略网络给出的动作概率分布的交叉熵, 这样MCTS在训练过程中也可以参与到策略网络的优化过程中.</p></blockquote><p>至此, AlphaGo的讲解结束. 这不禁让我想起这样一幅图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/drl30.jpg" style="zoom: 67%;" /><p><del>好的, 到这里你已经学会了AlphaGo的基本原理, 快手去写一份能复现AlphaGo的代码吧!!!</del></p><h2 id="Recommended"><a href="#Recommended" class="headerlink" title="Recommended"></a>Recommended</h2><ul><li><a href="https://www.bilibili.com/video/BV12o4y197US" target="_blank" rel="noopener">【王树森】深度强化学习(DRL)</a>: 推荐观看原视频, 和<a href="https://github.com/wangshusen/DRL" target="_blank" rel="noopener">Github配套笔记</a>, 这里面除了每节课的Slide还有一本书, 内容和视频基本一致.</li><li><a href="https://www.bilibili.com/video/BV1MW411w79n" target="_blank" rel="noopener">李宏毅深度强化学习(国语)课程(2018)</a>: 李宏毅老师的课, 不用多说.</li><li><a href="https://github.com/datawhalechina/easy-rl" target="_blank" rel="noopener">蘑菇书EasyRL</a>: 如果对DRL真的感兴趣, 可以看一下这本书, 搭配李宏毅老师的课使用.</li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo博客迁移</title>
      <link href="/posts/6682.html"/>
      <url>/posts/6682.html</url>
      
        <content type="html"><![CDATA[<h1 id="Hexo博客迁移"><a href="#Hexo博客迁移" class="headerlink" title="Hexo博客迁移"></a>Hexo博客迁移</h1><p>本文记录一次带主题的Hexo博客迁移过程, 从Win上迁移到MacOS.</p><h2 id="前置依赖"><a href="#前置依赖" class="headerlink" title="前置依赖"></a>前置依赖</h2><p>首先, 需要在MacOS上装Node.js和Git的环境, 网上有大把大把的教程, 这里就不再多说了, 自行搜索即可.</p><h2 id="必要文件"><a href="#必要文件" class="headerlink" title="必要文件"></a>必要文件</h2><p>把Windows上博客下的下列文件和文件夹暂存下来, 方便后续使用:</p><pre><code>_config.ymlpackage.jsonscaffoldssourcethemes</code></pre><h2 id="Hexo"><a href="#Hexo" class="headerlink" title="Hexo"></a>Hexo</h2><p>装完Node.js后需要安装Hexo:</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> -g hexo-cli<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>然后在你想迁移到的地方初始化Hexo, 例如我想把<code>blog</code>文件夹当做博客新位置:</p><pre class="line-numbers language-bash"><code class="language-bash">hexo init blog<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="迁移博客"><a href="#迁移博客" class="headerlink" title="迁移博客"></a>迁移博客</h2><p>切换到<code>blog</code>文件夹下, 把刚才Windows上暂存的文件全部都拿过来后, 安装相关博客依赖:</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>然后检查一下博客是否迁移成功:</p><pre class="line-numbers language-bash"><code class="language-bash">hexo g <span class="token operator">&amp;&amp;</span> hexo s<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>去浏览器看一下网址有没有问题就行, 后续使用流程和之前就没有任何区别了, 整个流程还是比较简单的.</p><h2 id="后处理"><a href="#后处理" class="headerlink" title="后处理"></a>后处理</h2><p>我迁移后出现了公式无法正常显示的问题, 才想起来之前Mathjax和Markdown语法是有冲突的, 我之前改过匹配规则, 参照<a href="https://adaning.github.io/posts/33457.html#MathJax和Markdown冲突">这里</a>重新做一遍即可.</p>]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Matery </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SDN: Synchronous Dual Network with Cross-Type Attention for Joint Entity and Relation Extraction</title>
      <link href="/posts/879.html"/>
      <url>/posts/879.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li><strong>LSTM</strong>: <a href="https://adaning.github.io/posts/60202.html">循环神经网络小结</a></li></ul></blockquote><h1 id="Synchronous-Dual-Network-with-Cross-Type-Attention-for-Joint-Entity-and-Relation-Extraction"><a href="#Synchronous-Dual-Network-with-Cross-Type-Attention-for-Joint-Entity-and-Relation-Extraction" class="headerlink" title="Synchronous Dual Network with Cross-Type Attention for Joint Entity and Relation Extraction"></a>Synchronous Dual Network with Cross-Type Attention for Joint Entity and Relation Extraction</h1><p>本文是论文<a href="https://aclanthology.org/2021.emnlp-main.219/" target="_blank" rel="noopener">Synchronous Dual Network with Cross-Type Attention for Joint Entity and Relation Extraction</a>的阅读笔记和个人理解, 论文来自<strong>EMNLP 2021</strong>.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>联合抽取因NER和RE之间的复杂交互而有挑战性, 现有的方法经把二者通过一个共享的网络来解决, 丢失了<strong>实体类型</strong>和<strong>关系类型</strong>的<strong>相互依赖</strong>.</p><p>因此, 作者从<strong>多任务学习</strong>角度, 设计了一种跨类型注意力的同步对偶网络, 来充分利用实体类型和关系类型之间的联系.</p><p>下图是一个实体关系抽取的例子, 需要根据给出的句子来抽取出实体以及其对应的实体类型, 并判断实体之间所存在的关系, 以此组成三元组:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/sdn1.jpg" style="zoom: 50%;" /><h2 id="Type-Attention-LSTM"><a href="#Type-Attention-LSTM" class="headerlink" title="Type - Attention LSTM"></a>Type - Attention LSTM</h2><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/sdn3.jpg" style="zoom: 50%;" /><p>作者设计的框架TA - LSTM(<strong>T</strong>ype <strong>A</strong>ttention <strong>LSTM</strong>)是基于LSTM的, 作者先介绍了标准LSTM. 在$t$ 时刻, 输入的Token的Embedding为$\mathbf{x}_t$, 基于细胞状态$\mathbf{c}_t$ 的隐态输出$\mathbf{h}_t^c$ 计算流程为:</p><p>$$<br>\begin{aligned}<br>\left[\begin{array}{c}<br>\mathbf{i}_{t} \\<br>\mathbf{o}_{t} \\<br>\mathbf{f}_{t} \\<br>\widetilde{\mathbf{c}}_{t}<br>\end{array}\right] &amp;=\left[\begin{array}{c}<br>\sigma \\<br>\sigma \\<br>\sigma \\<br>\tanh<br>\end{array}\right]\left(\mathbf{W}\left[\mathbf{h}_{t-1} ; \mathbf{x}_{t}\right]+\mathbf{b}\right) \\<br>\mathbf{c}_{t} &amp;=\mathbf{i}_{t} \odot \widetilde{\mathbf{c}}_{t}+\left(\mathbf{1}-\mathbf{i}_{t}\right) \odot \mathbf{c}_{t-1} \\<br>\mathbf{h}_{t}^{c} &amp;=\mathbf{o}_{t} \odot \tanh \left(\mathbf{c}_{t}\right)<br>\end{aligned}<br>$$</p><p>其中$\mathbf{W}, \mathbf{b}$ 为可学习参数, $\sigma$ 为Sigmoid激活函数.</p><p>上述式子由上到下分别为: $t$ 时刻的输入门$\mathbf{i}_t$, 输出门$\mathbf{o}_t$, 遗忘门$\mathbf{f}_t$, 初始细胞状态$\tilde{\mathbf{c}}_t$, 细胞状态$\mathbf{c}_t$, 隐态输出$\mathbf{h}_t^c$(也作为上下文表示).</p><h3 id="Type-Attention-Mechanism"><a href="#Type-Attention-Mechanism" class="headerlink" title="Type - Attention Mechanism"></a>Type - Attention Mechanism</h3><blockquote><p>这部分是TA - LSTM额外添加的内容.</p></blockquote><p>在Type - Attention机制中, 对于类型$k$, 以及给定的$t$ 时刻输入$\mathbf{x}_t$ 和$t-1$ 时刻的隐态$\mathbf{h}_{t-1}$. 该类型相关的Key - Value对可以由下式计算得来:<br>$$<br>\left[\begin{array}{l}<br>\mathbf{k}_{k}^{(t)} \\<br>\mathbf{v}_{k}^{(t)}<br>\end{array}\right]=\left[\begin{array}{c}<br>\sigma \\<br>\sigma<br>\end{array}\right]\left(\mathbf{W}_{k}\left[\mathbf{h}_{t-1} ; \mathbf{x}_{t}\right]+\mathbf{b}_{k}\right)<br>$$</p><p>$k \in \left[1, \dots, m \right]$ 可以是实体类型或关系类型, 其中$\mathbf{W}_k, \mathbf{b}_k$ 为可学习参数, $\sigma$ 为Sigmoid函数.  其实跟$\mathbf{i}_t, \mathbf{o}_t, \mathbf{f}_t$ 得来的方式差不多.</p><p>这样就可以根据类型数量$m$, 得到$m$ 个类别特化的Key - Value对$\mathbf{K}^{(t)}=\left[\mathbf{k}_{1}^{(t)}, \ldots, \mathbf{k}_{m}^{(t)}\right]$ 和$\mathbf{V}^{(t)}=\left[\mathbf{v}_{1}^{(t)}, \ldots, \mathbf{v}_{m}^{(t)}\right]$.</p><p>接着把上下文表示$\mathbf{h}_{t}^c$ 视为Query, 把Attention机制应用到这上面来:<br>$$<br>\begin{aligned}<br>\mathbf{h}_{t}^{l} &amp;=\operatorname{attention}\left(\mathbf{h}_{t}^{c}, \mathbf{K}^{(t)}, \mathbf{V}^{(t)}\right)=\boldsymbol{\alpha}^{(t)} \mathbf{V}^{(t)} \\<br>\boldsymbol{\alpha}^{(t)} &amp;=\operatorname{softmax}\left(\frac{\mathbf{h}_{t}^{c} \mathbf{K}^{(t)}{ }^{\top}}{\sqrt{d_{e}}}\right)<br>\end{aligned}<br>$$</p><p>$\sqrt{d_e}$ 为Hidden state维度. </p><p>最后, TA - LSTM的$t$ 时刻上下文表示$\mathbf{h}_t^c$ 和类别表示$\mathbf{h}_t^l$ 相加得到$t$ 时刻的最终表示$\mathbf{h}_t$: </p><p>$$<br>\mathbf{h}_{t}=\mathbf{h}_{t}^{c}+\mathbf{h}_{t}^{l}<br>$$</p><h2 id="Synchronous-Dual-Network-with-Cross-Type-Attention"><a href="#Synchronous-Dual-Network-with-Cross-Type-Attention" class="headerlink" title="Synchronous Dual Network with Cross - Type Attention"></a>Synchronous Dual Network with Cross - Type Attention</h2><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/sdn2.jpg" style="zoom: 50%;" /><h3 id="Task-Definition"><a href="#Task-Definition" class="headerlink" title="Task Definition"></a>Task Definition</h3><p>在这里重新给出<strong>形式化</strong>的实体关系抽取任务定义.</p><p>对于给定的包含$n$ 个单词的句子$\mathbf{s}=[w_1, \dots, w_n]$, RTE的任务目标是抽取出句子$\mathbf{s}$ 中的关系三元组$\mathcal{T}=\left\{\left(\mathbf{e}_i, r, \mathbf{e}_j \right) \mid \mathbf{e}_i, \mathbf{e}_j \in \mathcal{E}, r \in \mathcal{R}\right\}$. $\mathbf{e}_i, \mathbf{e}_j, r$ 分别代表关系三元组的Subject, Object, Relation. </p><p>Subject, Object规定在实体集$\mathcal{E}=\left\{\mathbf{e}_i\right\}^P_{i=1}$中, 关系从预定义好的关系集$\mathcal{R}=\{\mathcal{R}_1, \dots, \mathcal{R}_m\}$中选出, $m$ 为有效关系类型数.</p><h3 id="Synchronous-Dual-Learning"><a href="#Synchronous-Dual-Learning" class="headerlink" title="Synchronous Dual Learning"></a>Synchronous Dual Learning</h3><p>接下来作者将通过Entity Type Learning和Relation Type Learning来捕获实体类型增强的表示$\mathbf{h}_t^e$, 关系类型增强的表示$\mathbf{h}_t^r$, 以增强模型对Type的感知力.</p><h4 id="Entity-Type-Learning"><a href="#Entity-Type-Learning" class="headerlink" title="Entity Type Learning"></a>Entity Type Learning</h4><p>NER作为序列标注问题, 实体类型作为标签, 例如PER, LOC, ORG等:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/sdn5.jpg" style="zoom: 50%;" /><p>若有$p$ 种实体标签, 每个实体类型都对应一个<strong>ETC</strong>(Entity Type Cell), 就有$p$ 个ETCs. </p><p>为了让模型学会实体类型预测的感知, 所以引入了<strong>Entity Type Learning</strong>作为<strong>辅助任务</strong>.</p><blockquote><p>下面的公式有些琐碎, 与原论文保持同步, 但其实<strong>并不复杂</strong>, 请耐心看完.</p></blockquote><p>从上一节可以仅用LSTM直接得到$t$ 时刻的上下文表示$\mathbf{h}_t^c$​, 用双向平均来, 即$\bar{\mathbf{h}}_{t}^{c}=\left[\left(\overrightarrow{\bar{\mathbf{h}}_{t}^{c}}+\overleftarrow{\bar{\mathbf{h}}_{t}^{c}}\right) / 2\right]$.</p><p>并由Type Attention机制得到一组与每种关系一一对应的Key - Value对 $\bar{\mathbf{K}}^{(t)}=\left[\bar{\mathbf{k}}_{1}^{(t)}, \ldots, \bar{\mathbf{k}}_{p}^{(t)}\right]$, $\bar{\mathbf{V}}^{(t)}=\left[\bar{\mathbf{v}}_{1}^{(t)}, \ldots, \bar{\mathbf{v}}_{p}^{(t)}\right]$, 其中的每个实体类型$p$ 对应的Key和Value也是由单向LSTM生成后, 将双向平均而来, 即 $\bar{\mathbf{k}}_{l}^{(t)}=\left[\left(\overrightarrow{\bar{\mathbf{k}}_{l}^{(t)}}+\overleftarrow{\bar{\mathbf{k}}_{l}^{(t)}}\right) / 2\right]$, $\bar{\mathbf{v}}_l^{(t)} = \left[\left(\overrightarrow{\bar{\mathbf{v}}_{l}^{(t)}}+\overleftarrow{\bar{\mathbf{v}}_{l}^{(t)}}\right) / 2\right],(l \in[1, \ldots, p])$.</p><p>$t$ 时刻实体类型相关的表示由两个方向拼接而成, $\mathbf{h}_{t}^{e}=\left[\overrightarrow{\mathbf{h}_{t}^{e}} \oplus \overleftarrow{\mathbf{h}_{t}^{e}}\right]$, 整个序列的实体类型表示记为$\mathbf{H}^{(e)}=\left[\mathbf{h}_{1}^{e}, \ldots, \mathbf{h}_{n}^{e}\right]$. </p><p>根据上述过程, <strong>每个时间步$t$ 都能得到不同的Type Specific Key - Value对</strong>.</p><p>然后把上下文表示$\bar{\mathbf{h}}_{t}^{c}$ 和不同实体类型的Key$\bar{\mathbf{k}}_{l}^{(c)}$做缩放点积, 得到当前时刻Token最相似的实体类型$T_l^e$:<br>$$<br>p\left(T_{l}^{e} \mid w_{t}\right)=\operatorname{softmax}\left(\frac{\overline{\mathbf{h}}_{t}^{c} \overline{\mathbf{k}}_{l}^{(t) \top}}{\sqrt{d_{e}}}\right)<br>$$</p><p>然后用极大似然优化:</p><p>$$<br>\mathcal{L}_{E T}=-\sum_{t=1}^{n} \log \left(p\left(T_{t}^{e} \mid w_{t}\right)\right)<br>$$</p><h4 id="Relation-Type-Learning"><a href="#Relation-Type-Learning" class="headerlink" title="Relation Type Learning"></a>Relation Type Learning</h4><p>同样的, 跟Entity Type Learning相类似, Relation Type Learning也是用来强化模型对类型感知的<strong>辅助任务</strong>.</p><p>因为存在<strong>相同实体对存在多种关系</strong>的情况(<strong>EPO</strong>问题), 所以用<strong>多标签</strong>来标注, 即用0, 1标签来表明实体之间的关系, 例如:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/sdn6.jpg" style="zoom: 50%;" /><p>在这里, 作者区分了Subject和Object的<strong>位置关系</strong>, 并将其纳入标签中. 设$M$ 为关系数量, 加上<strong>没有关系</strong>的情况, 共含有$2\times M + 1$ 种标签.</p><p>下内容与Entity Type Learning完全类似, 这里只是将实体类型转换为$q$ 种关系类型, 不再多加赘述.</p><p>上下文表示为$\hat{\mathbf{h}}_{t}^{c}=\left[\left(\overrightarrow{\hat{\mathbf{h}}_{t}^{c}}+\overleftarrow{\hat{\mathbf{h}}_{t}^{c}}\right) / 2\right]$, Key - Value对 $\hat{\mathbf{K}}^{(t)}=\left[\hat{\mathbf{k}}_{1}^{(t)}, \ldots, \hat{\mathbf{k}}_{q}^{(t)}\right]$,  $\hat{\mathbf{V}}^{(t)}=\left[\hat{\mathbf{v}}_{1}^{(t)}, \ldots, \hat{\mathbf{v}}_{q}^{(t)}\right]$, 由双向后求和平均得到, $\hat{\mathbf{k}}_{l}^{(t)}=\left[\left(\overrightarrow{\hat{\mathbf{k}}_{l}^{(t)}}+\overleftarrow{\hat{\mathbf{k}}_{l}^{(t)}}\right) / 2\right]$,$\left[\left(\overrightarrow{\hat{\mathbf{v}}_{l}^{(t)}}+\overleftarrow{\hat{\mathbf{v}}_{l}^{(t)}}\right) / 2\right],(l \in[1, \ldots, q])$</p><p>最后也是将双向拼接得到关系表示,  $\mathbf{h}_{t}^{r}=\left[\overrightarrow{\mathbf{h}_{t}^{r}} \oplus \overleftarrow{\mathbf{h}_{t}^{r}}\right]$, 整个句子的关系表示记为$\mathbf{H}^{(e)}=\left[\mathbf{h}_{1}^{r}, \ldots, \mathbf{h}_{n}^{r}\right]$.</p><p>因为是多标签问题, 所以用的是Sigmoid得到$w_t$ 的关系类型$T_l^r$:</p><p>$$<br>p\left(T_{l}^{r} \mid w_{t}\right)=\operatorname{sigmoid}\left(\frac{\hat{\mathbf{h}}_{t}^{c} \hat{\mathbf{k}}_{l}^{(t) \top}}{\sqrt{d_{e}}}\right)<br>$$</p><p>也是用极大似然优化:</p><p>$$<br>\mathcal{L}_{R T}=-\sum_{t=1} \sum_{r=1}^{2}<br>\left\{\log \left(p\left(T_{t}^{r} \mid w_{t}\right)\right)^{\mathbb{I}\left\{\hat{T}_{t}^{r}=1\right\}}+\log \left(1-p\left(T_{t}^{r} \mid w_{t}\right)\right)^{\mathbb{I}\left\{\hat{T}_{t}^{r}=0\right\}}\right\}<br>$$</p><h3 id="Cross-Type-Attention-Mechanism"><a href="#Cross-Type-Attention-Mechanism" class="headerlink" title="Cross - Type Attention Mechanism"></a>Cross - Type Attention Mechanism</h3><p>其实在Entity Type Learning和Relation Type Learning中的Entity Type和Relation Type使用是<strong>独立</strong>的, 所以接下来作者需要让它们彼此产生<strong>交互</strong>.</p><p>对于Entity Type Learning中得到的实体类型增强的表示$\mathbf{h}_t^e$, 以及关系类型相关的Key - Value对$\hat{\mathbf{K}}^{(t)}, \hat{\mathbf{V}}^{(t)}$, 关系 - 实体表示$\mathbf{c}_t^e$ 可以由前面讲过的Type - Attention机制得到. </p><p>与之相似的, 对于Relation Type Learning中得到的关系类型增强表示$\mathbf{h}_{t}^{r}$, 以及实体类型相关的Key - Value对$\bar{\mathbf{K}}^{(t)}, \bar{\mathbf{V}}^{(t)}$, 实体 - 关系表示$\mathbf{c}_t^r$ 也可以由Type - Attention得到. 即:<br>$$<br>\begin{aligned}<br>\mathbf{c}_{t}^{e} &amp;=\operatorname{attention}\left(\mathbf{h}_{t}^{e}, \hat{\mathbf{K}}^{(t)}, \hat{\mathbf{V}}^{(t)}\right) \\<br>\mathbf{c}_{t}^{r} &amp;=\operatorname{attention}\left(\mathbf{h}_{t}^{r}, \bar{\mathbf{K}}^{(t)}, \bar{\mathbf{V}}^{(t)}\right)<br>\end{aligned}<br>$$</p><blockquote><p>注: 该Cross Attention形式绝非首次出现, 在<strong>多模态模型</strong><a href="https://arxiv.org/abs/1908.02265" target="_blank" rel="noopener">ViLBERT</a> 中早就已经有把两种跨模态信息互相作为Query的方法. 只不过这里是将两种模态换为两种包含相关性的任务而已, 这二者十分相似.</p></blockquote><p>然后仿照TA - LSTM unit的最后, 把两种表示相加作为新的实体类型增强表示和新的关系类型增强表示:<br>$$<br>\begin{aligned}<br>\tilde{\mathbf{h}}_{t}^{e} &amp;= \mathbf{c}_t^e + \mathbf{h}_t^e \\<br>\tilde{\mathbf{h}}_{t}^{r} &amp;= \mathbf{c}_t^r + \mathbf{h}_t^r<br>\end{aligned}<br>$$<br>这也就是处理NER和RE前的<strong>最终表示形式</strong>了.</p><h3 id="Joint-Entity-and-Relation-Extraction"><a href="#Joint-Entity-and-Relation-Extraction" class="headerlink" title="Joint Entity and Relation Extraction"></a>Joint Entity and Relation Extraction</h3><p>下面的内容才是针对实体关系联合抽取的模型设计, 这部分设计的非常简单, 因为本文主要侧重点在于前面.</p><p>首先把Entity Type Learning中得到的实体表示$\tilde{\mathbf{h}}_{t}^{e}$ 和Relation Type Learning关系表示$\tilde{\mathbf{h}}_{t}^{r}$ 拼接起来, 得到一个联合表示:</p><p>$$<br>\tilde{\mathbf{h}}_{t}=\tilde{\mathbf{h}}_{t}^{e} \oplus \tilde{\mathbf{h}}_{t}^{r}<br>$$</p><h4 id="Named-Entity-Recognition"><a href="#Named-Entity-Recognition" class="headerlink" title="Named Entity Recognition"></a>Named Entity Recognition</h4><p>在NER任务中, 使用BIESO标签, 用Softmax和一层线性层搞定:<br>$$<br>y_{t}=\operatorname{softmax}\left(\mathbf{W}_{e} \tilde{\mathbf{h}}_{t}+\mathbf{b}_{e}\right)<br>$$</p><p>用极大似然优化即可:<br>$$<br>\mathcal{L}_{E}=-\sum_{t=1}^{n} \log \left(y_{t}\right)<br>$$</p><h4 id="Relation-Extraction"><a href="#Relation-Extraction" class="headerlink" title="Relation Extraction"></a>Relation Extraction</h4><p>作者Follow了前人的做法, 因为RE是一个与<strong>实体对相关</strong>的多标签任务, 所以这里做了实体对穷举, 判断Token $i$ 和Token $j$ 之间的关系$r^\prime$:</p><p>$$<br>\begin{gathered}<br>\mathbf{m}=\phi\left(\mathbf{W}_{m}\left(\tilde{\mathbf{h}}_{i} \oplus \tilde{\mathbf{h}}_{j}\right)+\mathbf{b}_{m}\right) \\<br>y_{i, j}^{r^{\prime}}=\operatorname{sigmoid}\left(\mathbf{W}_{r^{\prime}} \mathbf{m}+\mathbf{b}_{r^{\prime}}\right)<br>\end{gathered}<br>$$</p><p>其中$\mathbf{W}_{m} , \mathbf{b}_{m}, \mathbf{W}_{r^{\prime}} , \mathbf{b}_{r^{\prime}}$ 为可学习参数, $\phi$ 为ReLU.</p><p>然后用二分类交叉熵做Loss:</p><p>$$<br>\mathcal{L}_{R}=-\sum_{r^{\prime}=1}^{M} \sum_{i, j=1}^{n}\{\log \left(y_{i, j}^{r^{\prime}}\right)^{\mathbb{I}\left\{\hat{y}_{i, j}^{r^{\prime}}=1\right\}}<br>\left.+\log \left(1-y_{i, j}^{r^{\prime}}\right)^{\mathbb{I}\left\{\hat{y}_{i, j}^{\prime}=0\right\}}\right\}<br>$$</p><p>其中$\hat{y}_{i,j}^{r\prime}$ 为关系的Golden Label.</p><p>至此, SDN的模型结构已经完全确定:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/sdn4.jpg" style="zoom: 50%;" /><p>其实就是用TA - LSTM和ETC提供的信息完成辅助任务Entity Type Prediction, 再由这部分信息和Cross Attention组合, 经过简单的变换处理NER. 关系侧则完全同理, 不再叙述.</p><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>Training的Loss一共是五个, 最后加上一个L2正则化Loss:</p><p>$$<br>\mathcal{L}=\lambda^{t 1} \mathcal{L}_{E T}+\lambda^{t 2} \mathcal{L}_{R T}+\lambda^{e} \mathcal{L}_{E}+\lambda^{r} \mathcal{L}_{R}+\frac{\lambda}{2}|\Theta|^{2}<br>$$</p><p>$\lambda$ 为各个任务的权重系数, $\Theta$ 为模型参数. </p><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p>在推断时, 需要判断三元组是否正确. </p><p>对于NER抽取出的实体集$\mathcal{E}$ 中的实体, 有Subject $\mathbf{e}_{i}=\left[w_{\xi_{i}}, \ldots, w_{\zeta_{i}}\right]$, Object $e_{j}=\left[w_{\xi_{j}}, \ldots, w_{\zeta_{j}}\right]$, 关系$r$ 下的概率为$p_r$ 为:<br>$$<br>p_{r}=\frac{1}{\left|\mathbf{e}_{i}\right|} \frac{1}{\left|\mathbf{e}_{j}\right|} \sum_{f=\xi_{i}}^{\zeta_{i}} \sum_{s=\xi_{j}}^{\zeta_{j}} y_{f, s}^{r}<br>$$<br>$\left|\mathbf{e}_{i}\right|, \left|\mathbf{e}_{j}\right|$ 为$\mathbf{e}_i, \mathbf{e}_j$ 的长度, 仅当$p_r &gt; \theta$ 时三元组成立, $\theta$ 为阈值.</p><blockquote><p>这种计算方式确实比较特殊, 是<strong>将实体内所有Token对逐一求和平均</strong>来确定两实体之间是否存在指定关系.</p></blockquote><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>作者使用了RTE上最常用的两个Benchmark NYT和WebNLG, 统计信息如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/sdn7.jpg" style="zoom: 50%;" /><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><p>作者将SDN分别与多任务类, Tagging类, 生成类放在一起对比, 结果如下(应该指的是<strong>精确匹配结果</strong>):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/sdn8.jpg" style="zoom: 50%;" /><p>SDN在F1上是最好的.</p><h3 id="Ablation-Experiments"><a href="#Ablation-Experiments" class="headerlink" title="Ablation Experiments"></a>Ablation Experiments</h3><p>文中消融实验如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/sdn9.jpg" style="zoom: 50%;" /><p>Entity Type Learning和Relation Type Learning对模型性能均有相当大提升, 而且是对NER和RE任务都有影响, 并且能观察到NER和RE任务之间的关联性很强.</p><h3 id="Analysis-of-Inference-Threshold"><a href="#Analysis-of-Inference-Threshold" class="headerlink" title="Analysis of Inference Threshold"></a>Analysis of Inference Threshold</h3><p>作者做了不同阈值$\theta$ 和模型结果之间变化图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/sdn10.jpg" style="zoom: 50%;" /><p>作者将阈值对性能的影响归因与WebNLG和NYT之间实体长度不一.</p><h3 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h3><p>作者做了Case Study, 如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/sdn11.jpg" style="zoom: 50%;" /><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>作者从<strong>实体类型</strong>和<strong>关系类型</strong>的假设入手, 提出了一种基于TA - LSTM和Cross Type Attention的同步<strong>对偶</strong>网络, 通过强调对实体类型的感知, 关系类型的感知, 以及<strong>跨类型注意力</strong>解决了实体关系抽取问题.</p><p>但其实从文章中看出, 能把这个想法做Work是一件非常不容易的事情, 引入了额外的两个辅助任务, 除去正则外4个Loss属实难顶. 文章中的符号描述比较混乱, 尤其是两个辅助任务那部分, 但其实不复杂, </p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RTE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PFN: A Partition Filter Network for Joint Entity and Relation Extraction</title>
      <link href="/posts/27457.html"/>
      <url>/posts/27457.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>RNN: 详见<a href="https://adaning.github.io/posts/60202.html">循环神经网络小结</a>.</li></ul></blockquote><h1 id="A-Partition-Filter-Network-for-Joint-Entity-and-Relation-Extraction"><a href="#A-Partition-Filter-Network-for-Joint-Entity-and-Relation-Extraction" class="headerlink" title="A Partition Filter Network for Joint Entity and Relation Extraction"></a>A Partition Filter Network for Joint Entity and Relation Extraction</h1><p>本文是论文<a href="https://arxiv.org/abs/2108.12202" target="_blank" rel="noopener">A Partition Filter Network for Joint Entity and Relation Extraction</a>的阅读笔记和个人理解, 论文来自<strong>EMNLP 2021</strong>. 图片全部出自原论文和<a href="https://docs.google.com/presentation/d/1CiHWBdwoQexY0JgSP_JxC-QFciZBmTGo" target="_blank" rel="noopener">PPT</a>. 本文为RTE问题中, 探讨NER和RE任务间关系的系列三部曲中的第三篇.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>在先前的联合抽取任务中, 人们少有考虑NER和RE任务间的关系.要么是以<strong>Pipeline</strong>的形式做NER和RE, 导致<strong>任务间的特征交互不平衡</strong>, 要么是<strong>并行</strong>的对NER和RE任务的特征分别编码, 导致特征构建很大程度上是<strong>相互独立</strong>的. </p><p>因此, 作者希望提出一种两路交互模型, 从<strong>多任务学习</strong>视角充分挖掘NER和RE两任务之间的关系, 保证二者之间信息的<strong>均衡传递</strong>, 在此基础上完成联合抽取任务. </p><h2 id="PFN"><a href="#PFN" class="headerlink" title="PFN"></a>PFN</h2><blockquote><p>PFN由LSTM改进而来, 有LSTM基础再理解会好一些.</p></blockquote><h3 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h3><p>首先联合抽取中的两个子任务NER, RE做出定义.</p><p>对于给定的长度为$L$ 的输入序列$s=\set{w_1, \dots, w_L}$:</p><ul><li>NER的任务目标是在给定实体起始Token$w_i$ 和结束Token$w_j$ 的情况下, 抽取出标有类别的实体$e \in \mathcal{E}$ , $\left\langle w_{i}, e, w_{j}\right\rangle \in S$.</li><li>RE的任务目标是在给定Subject的起始Token $w_i$ 和Object的起始Token$w_j$ 的情况下, 抽取出二者存在的关系$r \in \mathcal{R}$, 即$\left\langle w_{i}, r, w_{j}\right\rangle \in T$, $T$ 为三元组.</li></ul><p>从形式上观察, NER和RE这两个任务都可以仅由实体的Span完成, 它们从形式上是<strong>保持一致</strong>的. 只不过NER要求的Span同属于同一个实体, 而RE的Span一个来自于Subject, 另一个来自于Object.</p><h3 id="Partition-Filter-Encoder"><a href="#Partition-Filter-Encoder" class="headerlink" title="Partition Filter Encoder"></a>Partition Filter Encoder</h3><p>PFN的Ecndoer就是被改进过的<strong>LSTM</strong>, 作者在LSTM基础上增加了<strong>Partition</strong>和<strong>Filter</strong>机制, 使得两任务之间的信息能够均衡合理的传递.</p><h4 id="Partition"><a href="#Partition" class="headerlink" title="Partition"></a>Partition</h4><p>NER和RE之间存在很强的关联, 但每个任务也同样应该有自己独立的特征, 这部分独立特征是不该被另一个任务所影响的. 划分(Partition)机制的目标就是要学习到仅与本任务相关联的特征位置.</p><blockquote><p>并不是所有特征全部共享就好, 而是<strong>只共享该共享的</strong>. 这看起来是一句废话, 但在多任务学习(Multi Task Learning, MTL) 中很常见, 也很重要. 若共享了不该共享的部分, 对每个任务的完成可能都有害.</p></blockquote><p>先从LSTM的细胞状态入手, 当前时刻$t$ 的初始细胞状态$\tilde{c_t}$ 由当前时刻输入$x_t$  和上个时刻的记忆$h_{t-1}$ 共同决定:</p><p>$$<br>\tilde{c}_{t}=\tanh \left(\operatorname{Linear}\left(\left[x_{t} ; h_{t-1}\right]\right)\right)<br>$$</p><p>这点与LSTM的输入门得到$\tilde{C}_t$ 的方式保持一致:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lstm%E8%BE%93%E5%85%A5%E9%97%A8.png" style="zoom:50%;" /><p>接着, 作者希望使用<strong>门控</strong>机制, 使得两个任务能够找到独属于自己的那部分特征划分. $\text{cummax}$ 可以完成划分的这个动作.<br>$$<br>\operatorname{cummax}(\cdot)=\operatorname{cumsum}(\operatorname{softmax}(\cdot))<br>$$<br>其中, $\text { cumsum }$ 为作者自定义的函数:<br>$$<br>\begin{gathered}<br>\text { cumsum }\left(x_{1}, x_{2}, \ldots, x_{n-1}, x_{n}\right)=\left(x_{1}, x_{1}+x_{2}, \ldots,\right. \\<br>\left.x_{1}+x_{2}+\cdots+x_{n-1}, x_{1}+x_{2}+\cdots+x_{n-1}+x_{n}\right)<br>\end{gathered}<br>$$</p><p>每个位置上的返回值为输入的对应位置元素及之前元素的和, 亦或许可以写做:<br>$$<br>\begin{gathered}<br>\text { cumsum }\left(x_{1}, x_{2}, \ldots, x_{n-1}, x_{n}\right)=\left(\sum^1_{n=1}x_i, \sum^2_{n=1}x_i,\sum^{n-1}_{n=1}x_i,\cdots, \sum^n_{n=1}x_i\right)<br>\end{gathered}<br>$$<br>$\text{cummax}$ 的输出值可以近似成一个二进制向量$(0, \cdots, 0, 1, \cdots, 1)$.  举个栗子:<br>$$<br>\begin{aligned}<br>x &amp;= (0.1, 0.1, 0.6, 0.1, 0.1) \\<br>\text{cumsum}(x) &amp;= (0.1, 0.2, 0.8, 0.9, 1.0) \\<br>&amp;\approx (0, 0, 1, 1, 1)<br>\end{aligned}<br>$$<br>NER和RE这两个任务都各有一个对输入划分的门控剪刀, 把输入分为任务<strong>相关</strong>, 任务<strong>不相关</strong>两块. 这两把剪刀分别记为$\tilde{e}$ 和$\tilde{r}$:<br>$$<br>\begin{aligned}<br>&amp;\tilde{e}=\operatorname{cummax}\left(\operatorname{Linear}\left(\left[x_{t} ; h_{t-1}\right]\right)\right) \\<br>&amp;\tilde{r}=1-\operatorname{cummax}\left(\operatorname{Linear}\left(\left[x_{t} ; h_{t-1}\right]\right)\right)<br>\end{aligned}<br>$$</p><p>该设计使得NER和RE任务都有独属于自己的部分.</p><blockquote><p>注意, 这两个$\text{Linear}$ 的参数是不一样的. 对于同一组输入, $\tilde{e}$ 和$\tilde{r}$ 可能得到一块交叉的区域, 这也就是<strong>共享区域</strong>, 该区域是由两个门控共同决定的.</p></blockquote><p>两刀切三份, 接下来就把两把剪刀切出来的结果表示出来.</p><p>这里给出的是上个时刻$t-1$ 的划分$\rho_{c_{t-1}}$, 因为在LSTM中, 后面要借助$t-1$ 时刻表达当前时刻$t$ 对应的划分:</p><p>$$<br>\begin{aligned}<br>\rho_{s, c_{t-1}} &amp;=\tilde{e}_{c_{t-1}} \circ \tilde{r}_{c_{t-1}} \\<br>\rho_{e, c_{t-1}} &amp;=\tilde{e}_{c_{t-1}}-\rho_{s, c_{t-1}} \\<br>\rho_{r, c_{t-1}} &amp;=\tilde{r}_{c_{t-1}}-\rho_{s, c_{t-1}}<br>\end{aligned}<br>$$<br>$\tilde{e}, \tilde{r}$ 的结果做逐元素点乘, 就能获取共享部分, <strong>细胞状态</strong>被切成了三份: <strong>NER区</strong>, <strong>RE区</strong>, <strong>Share区</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/pfn1.jpg" style="zoom: 50%;" /><p>对于三个式子, PPT中给出了一个例子. 若$\tilde{e}=(0, 1, 1), \tilde{r} =(1, 1, 0)$, 则有:<br>$$<br>\begin{aligned}<br>\rho_{s, c_{t-1}} &amp;= (0,1,1) \circ (1,1,0)=(0,1,0)\\<br>\rho_{e, c_{t-1}} &amp;= (0,1,1)- (0,1,0)=(0,0,1)\\<br>\rho_{r, c_{t-1}} &amp;= (1,1,0)- (0,1,0)=(1,0,0)<br>\end{aligned}<br>$$<br>对于NER, RE, Share区, 每个区域划分到的细胞状态$\rho$ 都由上个时刻的对应区域的细胞状态$c_{t-1}$ 和当前时刻对应区域的初始细胞状态$\tilde{c_t}$ 共同决定:<br>$$<br>\begin{aligned}<br>\rho_{e} &amp;=\rho_{e, c_{t-1}} \circ c_{t-1}+\rho_{e, \tilde{c}_{t}} \circ \tilde{c}_{t} \\<br>\rho_{r} &amp;=\rho_{r, c_{t-1}} \circ c_{t-1}+\rho_{r, \tilde{c}_{t}} \circ \tilde{c}_{t} \\<br>\rho_{s} &amp;=\rho_{s, c_{t-1}} \circ c_{t-1}+\rho_{s, \tilde{c}_{t}} \circ \tilde{c}_{t}<br>\end{aligned}<br>$$</p><p>$\rho$ 并不是PFN Encoder最终的细胞状态, 上述过程仅是与LSTM中的细胞状态更新相似:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lstm%E7%BB%86%E8%83%9E%E7%8A%B6%E6%80%81.png" style="zoom: 50%;" /><blockquote><p>作者认为, 把细胞状态拆为三个区域, 再将它们合起来, 和不划分时不是等价的, 会损失一部分信息, 这个机制与LSTM的遗忘门类似:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lstm%E9%81%97%E5%BF%98%E9%97%A8.png" style="zoom:50%;" /><p>这种解释还是非常巧妙的, 不需要在模型中显式设计出遗忘门.</p></blockquote><h4 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h4><p>在上个阶段Partition对输入区域完成了划分, 在本阶段将对每个任务所需特征做<strong>表征重组</strong>.</p><p>首先把上阶段得到的三部分<strong>组装</strong>在一起, 对应区域得到新的记忆$\mu$.</p><p>NER可以使用独有的细胞状态$\rho_e$ 和共享细胞状态$\rho_s$, RE可以使用独有细胞状态$\rho_r$ 和共享细胞状态$\rho_s$, 对任务无关的信息直接被滤去:<br>$$<br>\begin{aligned}<br>\mu_{e}&amp;=\rho_{e}+\rho_{s} \\\<br>\mu_{r}&amp;=\rho_{r}+\rho_{s}\\<br>\mu_{s}&amp;=\rho_{s}<br>\end{aligned}<br>$$</p><p>然后给记忆$\mu$ 激活一下子:<br>$$<br>\begin{aligned}<br>&amp;h_{e}=\tanh \left(\mu_{e}\right) \\<br>&amp;h_{r}=\tanh \left(\mu_{r}\right) \\<br>&amp;h_{s}=\tanh \left(\mu_{s}\right)<br>\end{aligned}<br>$$</p><p>$h$ 将作为特定任务的表示, 在解码过程中使用.</p><p>最后就是PFN Encoder的更新细胞状态$c_t$, 并由$c_t$ 得到当前时刻隐态$h_t$, 完成输出过程:<br>$$<br>\begin{aligned}<br>c_{t} &amp;=\operatorname{Linear}\left(\left[\mu_{e, t} ; \mu_{r, t} ; \mu_{s, t}\right]\right) \\<br>h_{t} &amp;=\tanh \left(c_{t}\right)<br>\end{aligned}<br>$$</p><p>这与LSTM的输出门相似, 但又不太一样:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lstm%E8%BE%93%E5%87%BA%E9%97%A8.png" style="zoom: 50%;" /><p>至此, PFN Encoder信息流可以由下图概括:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/pfn3.jpg" style="zoom: 50%;" /><p>只是看输入和输出, 相较于LSTM仍然没有变化, 信息流为:</p><ol><li>上时刻$t-1$ 被划分好的三份的细胞状态(NER区, RE区, Share区)记为$\rho_{c_{t-1}}$, 与当前时刻划分好的三份初始细胞状态$\rho_{\tilde{c}_t}$, 生成$t$ 时刻划分好的新细胞状态$\rho$.</li><li>新细胞状态进一步过滤得到任务特化的细胞状态$\mu$.</li><li>激活后得到任务所需的表征$h_e, h_r, h_s$.</li><li>用类似输出门的方式得到当前时刻$t$ 的最终细胞状态$c_t$, 和输出隐态$h_t$.</li></ol><h3 id="Global-Representation"><a href="#Global-Representation" class="headerlink" title="Global Representation"></a>Global Representation</h3><p>因为作者使用的是单向编码器, 只有前向(Forward)而没有反向(Backward), 所以这里作者用提取的全局特征$h_{g}$ 来代替反向编码所获取的信息:</p><p>$$<br>\begin{aligned}<br>h_{g_{e}, t}=\tanh \left(\operatorname{Linear}\left[h_{e, t} ; h_{s, t}\right]\right) \\<br>h_{g_{r}, t}=\tanh \left(\operatorname{Linear}\left[h_{r, t} ; h_{s, t}\right]\right) \\<br>h_{g_{e}}=\operatorname{maxpool}\left(h_{g_{e}, 1}, \ldots, h_{g_{e}, L}\right) \\<br>h_{g_{r}}=\operatorname{maxpool}\left(h_{g_{r}, 1}, \ldots, h_{g_{r}, L}\right)<br>\end{aligned}<br>$$</p><p>最后用了一个最大池化, 只保留整个句子中最重要的信息, 作为全局表示.</p><h3 id="Task-Units"><a href="#Task-Units" class="headerlink" title="Task Units"></a>Task Units</h3><p>Task Unit也就是Decoder, 采用朴实无华的<strong>Table Filling</strong> Decoding策略. 因为前面观察过NER和RE任务的形式一致, 均可视为<strong>二分类</strong>问题, 仅依赖于两两Token就能完成实体类别和关系类别的判定, 所以两个任务的解码单元也可以设计得一样.</p><h4 id="NER-Unit"><a href="#NER-Unit" class="headerlink" title="NER Unit"></a>NER Unit</h4><p>每个实体类型$k$ 都有一张表, 在$k$ 表中第$i$ 行第$j$ 列被填上的值为, 类型为$k$ 的实体起始Token为$w_i$, 结束Token为$w_j$ 的概率$e_{ij}^k$, 由$h_i^e, h_j^e$, 以及全局表示$h_{g_e}$ 拼接后变换得到:<br>$$<br>\begin{aligned}<br>h_{i j}^{e}&amp;=\operatorname{ELU}\left(\operatorname{Linear}\left(\left[h_{i}^{e} ; h_{j}^{e} ; h_{g_{e}}\right]\right)\right)\\<br>e_{i j}^{k} &amp;=p\left(e=\left\langle w_{i}, k, w_{j}\right\rangle \mid e \in S\right) \\<br>&amp;=\sigma\left(\operatorname{Linear}\left(h_{i j}^{e}\right)\right), \forall k \in \mathcal{E}<br>\end{aligned}<br>$$</p><h4 id="RE-Unit"><a href="#RE-Unit" class="headerlink" title="RE Unit"></a>RE Unit</h4><p>RE单元可以由NER单元如法炮制. 每个关系类型$l$ 都有一张表, 在$l$ 表中第$i$ 行第$j$ 列被填上的值为, Token为$w_i$ 的Subject和起始Token为$w_j$ 间存在关系$l$ 的概率$r_{ij}^l$ , 同样由两Token间表示$h_i^r, h_j^r$, 以及全局表示$h_{g_r}$ 拼接, 后经过激活, 变换得到:<br>$$<br>\begin{aligned}<br>h_{i j}^{r} &amp;=\operatorname{ELU}\left(\operatorname{Linear}\left(\left[h_{i}^{r} ; h_{j}^{r} ; h_{g_{r}}\right]\right)\right) \\<br>r_{i j}^{l} &amp;=p\left(r=\left\langle w_{i}, l, w_{j}\right\rangle \mid r \in T\right) \\<br>&amp;=\sigma\left(\operatorname{Linear}\left(h_{i j}^{r}\right)\right), \forall l \in \mathcal{R}<br>\end{aligned}<br>$$</p><h3 id="Training-and-Inference"><a href="#Training-and-Inference" class="headerlink" title="Training and Inference"></a>Training and Inference</h3><p>因为作者假定填表任务为二分类问题, 所以使用二分类交叉熵(BCE Loss)优化:</p><p>$$<br>\begin{aligned}<br>L_{n e r} &amp;=\sum_{\hat{e}_{i j}^{k} \in S} \operatorname{BCELoss}\left(e_{i j}^{k}, \hat{e}_{i j}^{k}\right) \\<br>L_{r e} &amp;=\sum_{\hat{r}_{i j}^{l} \in T} \mathrm{BCELoss}\left(r_{i j}^{l}, \hat{r}_{i j}^{l}\right)<br>\end{aligned}<br>$$</p><p>$\hat{e}_{i j}^{k}, \hat{r}_{i j}^{l}$ 为真实标签. 在推理时, 各实体, 关系存在的概率仅当均大于阈值$\lambda$ 时, 三元组$(s_{i, j}^k, l, o_{m, n}^{k\prime})$ 成立:<br>$$<br>e_{i j}^{k} \geq \lambda_{e} ; e_{m n}^{k^{\prime}} \geq \lambda_{e} ; r_{i m}^{l} \geq \lambda_{r}<br>$$</p><p>文中取阈值均为0.5.</p><p>PFN整体的模型框架图如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/pfn2.jpg" style="zoom: 50%;" /><p>先由Partition Filter划分过滤得到任务特化的特征, 再提取整句全局特征, 最后结合二者通过两个简单的Task Unit二分类填表, 完成联合抽取.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><p>在联合抽取常用的两个数据集NYT和WebNLG上, 效果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/pfn4.jpg" style="zoom: 50%;" /><p>性能超过了之前最强的TPLinker.</p><p>在ADE, ACE05, ACE04, SciERC上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/pfn5.jpg" style="zoom: 50%;" /><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/pfn6.jpg" style="zoom: 50%;" /><p>PFN的性能也与强调任务间交互的<a href="https://adaning.github.io/posts/37252.html">Table - Sequence</a>和Pipeline设计模型<a href="https://adaning.github.io/posts/22256.html">PURE</a>(也是本系列前两篇文章介绍的模型)各有优劣, 整体上来说PFN要好. PURE在ACE05的NER上性能仍为最强.</p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>在SciERC上消融实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/pfn7.jpg" style="zoom: 50%;" /><h4 id="Number-of-Encoder-Layers"><a href="#Number-of-Encoder-Layers" class="headerlink" title="Number of Encoder Layers"></a>Number of Encoder Layers</h4><p>堆叠PFN层数不能带来性能提升. 个人认为多层PFN会令信息流混乱, 干扰了任务之间的平衡.</p><h4 id="Bidirection-Vs-Unidirection"><a href="#Bidirection-Vs-Unidirection" class="headerlink" title="Bidirection Vs Unidirection"></a>Bidirection Vs Unidirection</h4><p>无论是单向和还是双向, 使用全局信息都有一点点提升, 但是全局信息对单向增益更大.</p><h4 id="Encoding-Scheme"><a href="#Encoding-Scheme" class="headerlink" title="Encoding Scheme"></a>Encoding Scheme</h4><p>作者把PFN Encoder替换成两个LSTM, 观察不同编码方式的效果. Joint代表本文的做法.</p><p>在后两种实验中均把PFN Encoder替换成LSTM. Sequential代表Pipeline式的做法, 即单路交互, 先生成实体特征再生成关系特征. Parallel代表两个LSTM都分别编码, 不做交互.</p><p>从结果上来看, 平衡交互 &gt; 单路交互 &gt; 不做交互. 似乎平衡交互更有利于召回.</p><blockquote><p>其实有点不公平, 比较单独比较编码策略的时候还是都用LSTM比较好, 可以补一个只用LSTM的Joint结果.</p></blockquote><h4 id="Partition-Granularity"><a href="#Partition-Granularity" class="headerlink" title="Partition Granularity"></a>Partition Granularity</h4><p>作者探究了粗细粒度门控对性能的影响. 例如表示是300维的, 如果把300维劈成10份, 每份都共用30维相同的实体门和关系门, 称为Coarse. 如果300维不劈开, 用300维的实体门和关系门处理, 就称为Fine - grained. </p><p>结果表明细粒度更强, 其实也很好解释. 细粒度能更好的区分任务所需要的特征区间.</p><h4 id="Decoding-Strategy"><a href="#Decoding-Strategy" class="headerlink" title="Decoding Strategy"></a>Decoding Strategy</h4><p>作者把Pipeline的解码方式称为Selective Decoding, 因为关系模型只对Entity Golden Label解码, 建立在有效实体对之上的. 作者认为, 更好的解码策略是让关系模型把有效和无效实体对都考虑进去, 即Universal Decoding.</p><p>Selective是将NER Unit预测得出的有效实体对送给RE Unit解码的结果.</p><p>接下来的观点就很有意思了, 作者认为通用解码类似于<strong>对比学习</strong>, 因为其中包含了无效实体对作为<strong>负例</strong>, 所以通用解码效果更好.</p><h3 id="Effects-of-Relation-Signal-on-Entity-Recognition"><a href="#Effects-of-Relation-Signal-on-Entity-Recognition" class="headerlink" title="Effects of Relation Signal on Entity Recognition"></a>Effects of Relation Signal on Entity Recognition</h3><p>在先前的研究中, NER的实体信息对RE有帮助是大家公认的, 但RE是否对NER有益仍然有争议.</p><h4 id="Analysis-on-Entity-Prediction-of-Different-Types"><a href="#Analysis-on-Entity-Prediction-of-Different-Types" class="headerlink" title="Analysis on Entity Prediction of Different Types"></a>Analysis on Entity Prediction of Different Types</h4><p>因为在主实验结果中, PFN的表现没有很抢眼, 作者认为, 可能是ACE05中包含很多<strong>不属于任何三元组的实体</strong>. </p><p>因此作者将NER任务的结果分为<strong>三元组内</strong>的实体预测, <strong>三元组外</strong>的实体预测, 观察它们的差距:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/pfn8.jpg" style="zoom: 50%;" /><p>结果发现, 在三元组内的实体预测和在三元组外的实体预测性能有巨大差距, 在不借助关系信息时, 精度下降的很厉害. 与ACE05上NER SOTA Pipeline模型PURE对比, PFN的NER性能似乎与三元组外的实体百分比负相关, 即<strong>对于联合模型来说</strong>, <strong>三元组外的实体越多</strong>, <strong>NER性能越差</strong>. 因为在三元组内的实体和三元组外的实体预测推理时本质上是不同的, <strong>在三元组内部的实体预测可以借助关系信息</strong>, <strong>而三元组外实体预测却不能</strong>.</p><h4 id="Robustness-Test-on-Named-Entity-Recognition"><a href="#Robustness-Test-on-Named-Entity-Recognition" class="headerlink" title="Robustness Test on Named Entity Recognition"></a>Robustness Test on Named Entity Recognition</h4><p>在ACE05上的鲁棒性测试结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/pfn9.jpg" style="zoom: 50%;" /><p>相较于其他模型, PFN的鲁棒性非常好.</p><h4 id="Does-Relation-Signal-Helps-in-Predicting-Entities"><a href="#Does-Relation-Signal-Helps-in-Predicting-Entities" class="headerlink" title="Does Relation Signal Helps in Predicting Entities"></a>Does Relation Signal Helps in Predicting Entities</h4><p>与PURE中得出的结论相反, 作者认为关系信息对实体预测影响很大. </p><p>PURE中的实验是在ACE05上做的, 而忽略了ACE05上的三元组外实体的巨大影响.</p><blockquote><p>附录中还有CasRel, TPLinker, PFN三者在NYT和WebNLG上应对重叠三元组的详细表现, 结果表明PFN在处理重叠三元组的实力也很出色.</p></blockquote><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>本文从<strong>多任务学习</strong>视角, 以任务间<strong>不平衡的信息交互</strong>为出发点, 通过<strong>划分</strong>, <strong>过滤</strong>在最基础的<strong>特征区域</strong>上控制任务间的信息流通, 再结合整句的<strong>全局特征</strong>, 用两个任务特化的Task Unit用<strong>填表</strong>的方法解决联合抽取问题.</p><p>我个人认为, 这是一篇挺有想法的文章, 顺着论文的引文, 发现其灵感似乎也来自于<a href="https://openreview.net/forum?id=B1l6qiR5F7" target="_blank" rel="noopener">ON - LSTM</a>, 感兴趣的可以直接看<a href="https://spaces.ac.cn/archives/6621" target="_blank" rel="noopener">苏神博客</a>.</p><p>PFN进一步的说明了联合抽取中NER和RE任务之间是密不可分的, 并有效驳斥了PURE中的论点.</p><p>从论文本身来说, 是很优秀的一篇. 实验完备, 论据充分, 图也很好看.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ERE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PURE: A Frustratingly Easy Approach for Entity and Relation Extraction</title>
      <link href="/posts/22256.html"/>
      <url>/posts/22256.html</url>
      
        <content type="html"><![CDATA[<h1 id="A-Frustratingly-Easy-Approach-for-Entity-and-Relation-Extraction"><a href="#A-Frustratingly-Easy-Approach-for-Entity-and-Relation-Extraction" class="headerlink" title="A Frustratingly Easy Approach for Entity and Relation Extraction"></a>A Frustratingly Easy Approach for Entity and Relation Extraction</h1><p>本文是论文<a href="https://arxiv.org/abs/2010.12812" target="_blank" rel="noopener">A Frustratingly Easy Approach for Entity and Relation Extraction</a> 的阅读笔记和个人理解. 论文来自<strong>NAACL 2021</strong>. 本文为RTE问题中, 探讨NER和RE任务间关系的系列三部曲中的第二篇.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>最近的工作对NER和RE引入了联合训练, 即将NER和RE在多任务学习下一起解决. 并且联合模型经常使用同一个<strong>共享</strong>的Encoder解码, 解决问题的效果也都还不错, 这使得人们认为共享Encoder能够更好的解决这两个任务, 且认为联合模型能<strong>缓解错误误差传递</strong>的问题.</p><p>作者认为, <strong>Pipeline模型不一定真弱于联合学习模型</strong>. 因此希望构建一个简单的Pipeline模型来击败联合学习模型的SOTA, 来打破人们的固有观念.</p><h2 id="PURE"><a href="#PURE" class="headerlink" title="PURE"></a>PURE</h2><p><strong>PURE</strong>(the <strong>P</strong>rinceton <strong>U</strong>niversity <strong>R</strong>elation <strong>E</strong>xtraction system)简单的分为<strong>实体模型</strong>和<strong>关系模型</strong>两部分, 训练时完全分开训练. 因为是<strong>Pipeline</strong>, 所以关系模型使用实体的<strong>Golden Label</strong>训练.</p><p>用如下一张图可以简单概括:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/pure1.jpg" style="zoom: 50%;" /><ul><li>(a) <strong>实体模型</strong>: 根据Span的表示, 判断给定的Span的实体类型.</li><li>(b) <strong>关系模型</strong>: 根据Span对的表示, 判断给定的Span对的关系类型.</li><li>(c) <strong>批计算的关系模型</strong>: 关系模型的一种加速实现.</li></ul><h3 id="Problem-Definition"><a href="#Problem-Definition" class="headerlink" title="Problem Definition"></a>Problem Definition</h3><p>本文所采用的方法是基于Span的方法, 因此给出基于Span的视角下问题的定义和NER, RE的定义.</p><p>对于给输入句子$X$ 含有$n$ 个Token$x_1, x_2, \dots, x_n$. 令$S$ 为$X$ 中的长度为$L$ 的Span集合, 即$S=\{s_1, s_2, \dots, s_m\}$, $\mathrm{START}(i), \mathrm{END}(i)$ 分别代表$s_i$ 的起始和结束切片Token.</p><ul><li>NER: 对于预定义的实体类型集合$\mathcal{E}$, 基于Span的NER任务为判断每个Span $s_i \in S$ 的实体类型$y_e(s_i) \in \mathcal{E}$, 若Span不是实体, 则记为$y_e(s_i) = \epsilon$. 任务的最终输出为$Y_e=\set{(s_i, e): s_i \in S, e \in \mathcal{E}}$.</li><li>RE: 对于预定义的关系类型集合$\mathcal{R}$, RE任务为判断每个Span对$s_i\in S, s_j \in S$ 之间的关系类型$y_r(s_i, s_j) \in \mathcal{R}$, 若Span对之间不存在关系, 则记为$y_r(s_i, s_j) = \epsilon$. 任务最终输出为$Y_r = \set{(s_i, s_j, r): s_i, s_j \in S, r \in \mathcal{R}}$.</li></ul><h3 id="Entity-Model"><a href="#Entity-Model" class="headerlink" title="Entity Model"></a>Entity Model</h3><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/pure9.png" style="zoom: 60%;" /><p>实体模型简单的建模为基于<strong>Span</strong>的模型, 先用BERT获得输入Token $x_t$ 的上下文表示$\mathbf{x}_t$, 对于某个Span $s_i \in S$, 它的表示如下:</p><p>$$<br>\mathbf{h}_{e}\left(s_{i}\right)=\left[\mathbf{x}_{\mathrm{START}(i)} ; \mathbf{x}_{\mathrm{END}(i)} ; \phi\left(s_{i}\right)\right]<br>$$</p><p>其中, $\phi{(s_i)}$ 为Span长度的Embedding.</p><blockquote><p>Span的表示仅简单的使用了Span<strong>边界</strong>以及Span<strong>长度</strong>信息, 而非把Span内部所有信息全部囊括. 基于Span的模型经常只使用边界上的信息, 可能是假设边界蕴含的语义更强.</p></blockquote><p>在得到Span表示$\mathbf{h}_{e}\left(s_{i}\right)$ 后, 直接用一个FeedForward Network就能预测得到实体类型的概率分布:<br>$$<br>P_{e}\left(e \mid s_{i}\right)=\operatorname{softmax}\left(\mathbf{W}_{e} \mathrm{FFNN}\left(\mathbf{h}_{e}\left(s_{i}\right)\right)\right.<br>$$</p><h3 id="Relation-Model"><a href="#Relation-Model" class="headerlink" title="Relation Model"></a>Relation Model</h3><p>在实体模型得到了实体的Span $s_i, s_j$ 的表示$\mathbf{h}_e(s_i), \mathbf{h}_e(s_j)$ 后, 也直接接一个FeedForward Network就能预测Span间关系类型. </p><p>但作者认为, 实体模型得到的边界表示只能捕获每个<strong>单独实体</strong>周围的上下文信息, 没法感知到<strong>Span之间</strong>的依存信息, 而这种依存信息是对RE非常重要的.</p><p>所以, 作者在Span周围插入了独立的Text Marker指导模型获得Span间的依存关系:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/pure10.png" style="zoom: 60%;" /><p>具体来说, 对于给定的输入句子$X$ 和<strong>每个</strong>存在$e_i, e_j \in \mathcal{E} \cup \set{\epsilon}$ 的Subject - Object Span对$s_i, s_j$, 定义了四种类型的Text Marker, $\left\langle\mathrm{S}: e_{i}\right\rangle,\left\langle/ \mathrm{S}: e_{i}\right\rangle, \left\langle\mathrm{O}: e_{j}\right\rangle,\left\langle/ \mathrm{O}: e_{j}\right\rangle$, 分别插入到Subject的前后和Object的前后.</p><blockquote><p>这种Text Marker除了提供了Subject和Object的角色信息, 实体边界信息, 还提供了实体类型信息.</p></blockquote><p>即插入Text Marker后的输入序列$\widehat{X}$ 为:<br>$$<br>\begin{aligned}<br>&amp;\widehat{X}=\ldots\left\langle\operatorname{S}: e_{i}\right\rangle, x_{\operatorname{START}(i)}, \ldots, x_{\operatorname{END}(i)},\left\langle/ \mathrm{S}: e_{i}\right\rangle \\<br>&amp;\ldots\left\langle\mathrm{O}: e_{j}\right\rangle, x_{\mathrm{START}(j)}, \ldots, x_{\mathrm{END}(j)},\left\langle/ \mathrm{O}: e_{j}\right\rangle, \ldots<br>\end{aligned}<br>$$</p><blockquote><p>注意, 这种Type Marker对于每个Subject - Object Span对都是在最初输入文本$X$ 上重新插入的, <strong>每个不同的Span对都能生成一个不同的输入文本</strong>$\widehat{X}$. 这在关系判断阶段引入了巨大的计算量, 作者在后文对这一点进行了改进.</p></blockquote><p>接着使用不同于实体模型的第二个Encoder捕获关系模型下Subject和Object的表示:<br>$$<br>\mathbf{h}_{r}\left(s_{i},s_{j}\right)=\left[\widehat{\mathbf{x}}_{\widehat{\operatorname{START}}(i)} ; \widehat{\mathbf{x}}_{\widehat{\operatorname{START}}(j)}\right]<br>$$</p><p>$\widehat{\operatorname{START}}(i), \widehat{\operatorname{START}}(j)$ 为$\left\langle\mathrm{S}: e_{i}\right\rangle, \left\langle\mathrm{O}: e_{j}\right\rangle$ 的在$\widehat{X}$ 的下标. </p><blockquote><p>仅使用Text Marker的<strong>起始</strong>作为Subject和Object的表示, 在这里结束位置信息仅作为区分实体是否结束的标志.</p></blockquote><p>同样的, 在表示后面用一个FeedForward Network来预测关系类型:<br>$$<br>P_{r}\left(r \mid s_{i}, s_{j}\right)=\operatorname{softmax}\left(\mathbf{W}_{r} \mathbf{h}_{r}\left(s_{i}, s_{j}\right)\right)<br>$$</p><h3 id="Cross-sentence-context"><a href="#Cross-sentence-context" class="headerlink" title="Cross - sentence context"></a>Cross - sentence context</h3><p>跨句信息对预测实体类型和关系是有帮助的. </p><p>作者简单的使用固定大小为$W$ 的<strong>滑动窗口</strong>, 来获取更长的上下文. 假设输入句子有$n$ 个单词, 则滑动窗口可以分别获得左侧和右侧的$(W - n) / 2$ 个单词作为额外上下文.</p><h3 id="Training-amp-inference"><a href="#Training-amp-inference" class="headerlink" title="Training &amp; inference"></a>Training &amp; inference</h3><p>两个模型分别用交叉熵优化即可:</p><p>$$<br>\begin{aligned}<br>&amp;\mathcal{L}_{e}=-\sum_{s_{i} \in S} \log P_{e}\left(e_{i}^{\ast} \mid s_{i}\right) \\<br>&amp;\mathcal{L}_{r}=-\sum_{s_{i}, s_{j} \in S_{G}, s_{i} \neq s_{j}} \log P_{r}\left(r_{i, j}^{\ast} \mid s_{i}, s_{j}\right)<br>\end{aligned}<br>$$</p><p>$e_i^\ast$ 为$s_i$ 的真实实体类型, $r_{i, j}^\ast$ 为$s_i, s_j$ 间的真实关系类型.</p><h3 id="Differences-from-DYGIE"><a href="#Differences-from-DYGIE" class="headerlink" title="Differences from DYGIE++"></a>Differences from DYGIE++</h3><p>作者的方法与DYGIE++非常相似, 因此在这里强调了与它的不同:</p><ol><li>作者用两个独立的Encoder分别做NER和RE任务, 而非用MTL(Multi - task Learning)的角度去看待.并且关系模型所需的实体类型信息能完全产生于实体模型.</li><li>引入了Text Marker, 在关系模型中Span对的表示不同.</li><li>使用跨句信息, 而不是使用Beam Search和图网络.</li></ol><h3 id="Efficient-Batch-Computations"><a href="#Efficient-Batch-Computations" class="headerlink" title="Efficient Batch Computations"></a>Efficient Batch Computations</h3><p>在加入Type Marker时, 每个实体对都有不同的$\widehat{X}$, 即使输入的原句子$X$ 相同, 也会因为Type Marker的位置, 类型不同而必须重新计算<strong>整句表示</strong>, 这种计算开销实在是太大了, 所以作者提出了一种加速的<strong>近似模型</strong>.</p><p>首先, 把所有Text Marker全部添加到句子的尾部, 并令Text Marker与实体的起始位置, 结束位置的Position Embedding共享:<br>$$<br>\begin{aligned}<br>&amp;\mathrm{P}\left(\left\langle\mathrm{S}: e_{i}\right\rangle\right), \mathrm{P}\left(\left\langle/ \mathrm{S}: e_{i}\right\rangle\right):=\mathrm{P}\left(x_{\mathrm{START}(i)}\right), \mathrm{P}\left(x_{\mathrm{END}(i)}\right) \\<br>&amp;\mathrm{P}\left(\left\langle\mathrm{O}: e_{j}\right\rangle\right), \mathrm{P}\left(\left\langle/ \mathrm{O}: e_{j}\right\rangle\right):=\mathrm{P}\left(x_{\mathrm{START}(j)}\right), \mathrm{P}\left(x_{\mathrm{END}(j)}\right)<br>\end{aligned}<br>$$</p><p>$P(\cdot)$ 代表取Token的位置ID.</p><p>然后, 对Attention Layer添加约束, 强制Text Token只能Pay Attention to Text, 不能对Marker分配注意力权重, 而Marker Token可以对所有Token分配注意力. </p><blockquote><p>这样就能拿到一组<strong>纯净</strong>而<strong>共享</strong>的Text表示, Text表示没有Type Marker干预.</p></blockquote><p>给出一个例子:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/pure11.png" style="zoom: 60%;" /><p>图中Text Marker均位于句子尾部, 并且<code>mor</code> 和<code>[S:Md]</code>  共享相同的Position Embedding, <code>##pa</code> 和<code>[/S:Md]</code> 共享相同的Position Embedding. </p><p>在运行RE模型的时候, 因为<strong>Text Token表示不再与Marker Token相关</strong>, 所以不用重复计算Text的表示, 直接用Marker表示可以<strong>一次性计算句子中所有的实体对之间的关系</strong>, 把同一个句子中的所有实体对的关系预测完全压缩到单个句子中.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><p>实验中使用的数据集是ACE04, ACE05, SciERC, 统计数据如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/pure3.jpg" style="zoom: 40%;" /><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><p>在上述三个数据集上的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/pure2.jpg" style="zoom: 40%;" /><p>♣代表引入跨句, †代表引入额外数据. 结果中L = LSTM, L + E = LSTM + Elmo, Bb = BERT - base, Bl = BERT - large, SciB = SciBERT(规模与Bert - base相同), ALB = ALBERT - xxlarge - v1. Rel为实体边界准确且关系准确, Rel+为实体边界和实体类型, 关系均准确.</p><p>其中Wang and Lu是<a href="https://adaning.github.io/posts/37252.html">TSE</a> 的结果. ALBERT下的PURE比TSE性能要好一点. Wadden是DYGIE++的结果.</p><p>在单句情况下PURE达到了SOTA, 在跨句情况下更是比单句要高上一层.</p><h3 id="Batch-Computations-and-Speedup"><a href="#Batch-Computations-and-Speedup" class="headerlink" title="Batch Computations and Speedup"></a>Batch Computations and Speedup</h3><p>近似的关系模型效果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/pure4.jpg" style="zoom: 40%;" /><p>近似模型基本上掉了一个点左右, 但是耗时有明显下降.</p><h3 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h3><h4 id="Importance-of-Typed-Text-Markers"><a href="#Importance-of-Typed-Text-Markers" class="headerlink" title="Importance of Typed Text Markers"></a>Importance of Typed Text Markers</h4><p>不同类型的Type Markers也对关系模型性能有着影响, 在ACE05和SciERC上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/pure5.jpg" style="zoom: 40%;" /><p>gold为使用Golden Entity, e2e代表端到端, 也就是训练时直接使用实体模型的结果. </p><p>上表对应了6种输入关系模型前的Span表示方法:</p><ul><li><strong>TEXT</strong>: 直接用原Text的每个实体的的起始Token Embedding.</li><li><strong>TEXTETYPE</strong>: 在TEXT基础上拼接Entity Type Embedding.</li><li><strong>MARKERS</strong>: 使用不包含实体类型的边界Text Marker的Embedding.</li><li><strong>MARKERSETYPE</strong>: 在MARKERS基础上拼接实体类型Embedding.</li><li><strong>MARKERSELOSS</strong>: 使用不包含实体类型的边界Text Marker Embedding, 并引入一个辅助Loss, 用边界表示判断实体类型.</li><li><strong>TYPEDMARKERS</strong>: 正文中所使用的方法.</li></ul><p>能看到, 不同类型的Type Markers前前后后能差出四个点来, 增益非常大. </p><h4 id="Modeling-Entity-Relation-Interactions"><a href="#Modeling-Entity-Relation-Interactions" class="headerlink" title="Modeling Entity-Relation Interactions"></a>Modeling Entity-Relation Interactions</h4><p>作者探究了共享Encoder对性能的影响, 在ACE05上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/pure7.jpg" style="zoom: 40%;" /><p>当两个模型共享Encoder联合优化时, 性能都有下降, 解释为两个任务形式不同, 使用面向单一任务的单一Encoder比共享要专一. <strong>简单的共享Encoder对作者的方法无益</strong>.</p><blockquote><p>作者在论文脚注处写到, 某些方法的作者提到共享Encoder有提升. </p><p>论文中的用词很严谨, 一是”简单共享”, 二是”对我们模型没有好处”, 个人认为对于大多数模型共享是否有害还无法一锤定音.</p><p>但是再想想, 如果使用纯Pipeline不共享Encoder, 那就有了两个独立的Encoder, 相对于共享一个Encoder来说, 多了一倍预训练参数, 涨一点性能上去是理所应当的吧?</p></blockquote><h4 id="Mitigating-Error-Propagation"><a href="#Mitigating-Error-Propagation" class="headerlink" title="Mitigating Error Propagation"></a>Mitigating Error Propagation</h4><p>误差的错误传播在联合抽取中一直是没有被解决的问题, 正是因为这一问题, 才提出的联合抽取模型. </p><p>作者探究了关系模型采用实体模型预测出来的实体, 而不是Golden Entity时的F1:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/pure6.jpg" style="zoom: 40%;" /><p>10 - way jackkniﬁng就是十折交叉, 即为把数据划分成十份, 并且每个模型都用其中9份训练, 预测最后1份, 每个模型都对应着1份的预测结果, 把它们作为关系模型的输入, 然后训练关系模型. </p><p>结果表明使用实体模型的预测结果不如使用Golden Entity训练, 解释为实体模型的预测为关系模型在训练时带来了更多的噪声, 导致性能下降. Pipeline的曝光偏差影响或许并不像我们想的那样.</p><p>作者尝试召回更多实体, 把实体模型输出分数最高的40%作为关系模型的输入, 然后用Beam Search得到结果, 发现F1仍在下降. 结果并没有表明误差错误传播的问题存在, 同时召回更多实体在训练时也会引入更多的<strong>噪声</strong>, 因为关系模型还需要判断Span是不是实体.</p><h4 id="Effect-of-Cross-sentence-Context"><a href="#Effect-of-Cross-sentence-Context" class="headerlink" title="Effect of Cross - sentence Context"></a>Effect of Cross - sentence Context</h4><p>作者研究了滑动窗口大小$W$ 对模型性能的影响:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/pure8.jpg" style="zoom: 40%;" /><p>$W=100$ 后提升不大, 甚至对关系模型有负面影响.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>本篇论文主要指出了基于Pipeline的模型不一定要弱于已经存在的联合抽取模型. 属于”逆行者”类论文. 在印象中, Pipeline性能是要弱于joint模型的, 因为其误差错误传播问题比较大.</p><p>本文使用两个极其<strong>简单</strong>且<strong>完全没有交互</strong>的Encoder, 用基于<strong>Span</strong>的<strong>Pipeline</strong>模型达到了联合抽取模型的SOTA效果, 并指出关系模型中的效率问题, 给出了一个近似实现. 此外, 针对联合抽取任务中Pipeline模型存在的偏见做出了探究, 奇怪的是误差的错误传播在本模型中并没有得到体现. </p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ERE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Two are Better than One: Joint Entity and Relation Extraction with Table - Sequence Encoders</title>
      <link href="/posts/37252.html"/>
      <url>/posts/37252.html</url>
      
        <content type="html"><![CDATA[<h1 id="Two-are-Better-than-One-Joint-Entity-and-Relation-Extraction-with-Table-Sequence-Encoders"><a href="#Two-are-Better-than-One-Joint-Entity-and-Relation-Extraction-with-Table-Sequence-Encoders" class="headerlink" title="Two are Better than One: Joint Entity and Relation Extraction with Table - Sequence Encoders"></a>Two are Better than One: Joint Entity and Relation Extraction with Table - Sequence Encoders</h1><p>本文是论文<a href="https://arxiv.org/abs/2010.03851" target="_blank" rel="noopener">Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders</a>的阅读笔记和个人理解. 论文来自<strong>EMNLP 2020</strong>. 本文为RTE问题中, 探讨NER和RE任务间关系的系列三部曲中的第一篇.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>NER和RE是两个NLP里的基础任务, 最近<strong>联合学习</strong>的算法尝试把NER和RE<strong>同时解决</strong>, 很多算法都尝试用<strong>填表式</strong>一次性完成. 但是现有的算法大多采用的是单个Encoder, 把NER和RE放在同一个Encoder提供的空间捕获信息.</p><p>作者举了一个非常简单的例子:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse1.jpg" style="zoom: 50%;" /><p>上图中, NER和RE在一张表中被一次性解决:</p><ul><li>在主对角线中, 实体被BIO标注法标出, NER被完成.</li><li>在对角线两侧, 不同的颜色代表不同的关系, 而方向标明了该实体是Subject还是Object, RE被完成.</li></ul><p>作者认为, 这样的设计是不符合联合学习的设计思想的, 必然会<strong>限制</strong>联合算法的效果:</p><ul><li>存在<strong>特征困惑</strong>问题, 即<strong>One Feature for Two Task</strong>, NER和RE本来是两个不同的任务, 单个Encoder会在学习过程中产生困惑, 不能学习到同时解决NER和RE任务的特征.</li><li>没有充分使用<strong>表结构</strong>信息, 大多数的Joint Extraction论文确实构造了表结构, 但是却最后把Table Filling问题转化为一个Sequence Labeling问题去解决.</li></ul><p>基于上述问题, 作者尝试在模型底层构建两个单独学习特征的Encoder, 分别学习NER和RE的特征.</p><h2 id="TSE"><a href="#TSE" class="headerlink" title="TSE"></a>TSE</h2><p><strong>TSE</strong>(<strong>T</strong>able - <strong>S</strong>equence <strong>E</strong>ncoders)是我自己给模型起的名字. 相较于后续其他论文起的Table - Sequence, 我还是认为简写更好.</p><p>作者眼中的NER和RE任务形式为:</p><ul><li><strong>NER</strong>: NER被建模为一个序列标注问题, 其Label $\boldsymbol{y}^{\text {NER}}$ 为标准的BIO标注. </li><li><strong>RE</strong>: RE被建模为一个填表问题, 对于给出的句子$\boldsymbol{x}=\left[x_{i}\right]_{1 \leq i \leq N}$, 在标注表中标注出$\boldsymbol{y}^{\mathrm{RE}}=\left[y_{i, j}^{\mathrm{RE}}\right]_{1 \leq i, j \leq N}$. 实体$x_{i^{b}}, \ldots, x_{i^{e}}$ 与实体$x_{j^{b}}, . ., x_{j^{e}}$ 之间的关系定义为$y_{i, j}^{\mathrm{RE}}=\overrightarrow{r}$, 当两实体调换位置时, 记为$y_{j, i}^{\mathrm{RE}}=\overleftarrow{r}$. $\perp$ 代表不存在关系.</li></ul><p>TSE主要由三个部分组成: 基础表示Text Embedder, 表表示Table Encoder, 序列表示Sequence Encoder:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse2.jpg" style="zoom: 50%;" /><h3 id="Text-Embedder"><a href="#Text-Embedder" class="headerlink" title="Text Embedder"></a>Text Embedder</h3><p>句子中含有$N$ 个单词$\boldsymbol{x}=\left[x_{i}\right]_{1 \leq i \leq N}$, 句子中的Token表示分别由三类Embedding组成, 分别是<strong>Word Embedding</strong>, LSTM提取的<strong>Character Embedding</strong>, 以及BERT抽取的上下文相关的<strong>Contextualized Word Embedding</strong>. </p><p>句子级别的Token表示分别为$\boldsymbol{x}^w$, $\boldsymbol{x}^c$, $\boldsymbol{x}^l$. 句子表示$\boldsymbol{S}_0$如下:</p><p>$$<br>\boldsymbol{S}_{0}=\operatorname{Linear}\left(\left[\boldsymbol{x}^{c} ; \boldsymbol{x}^{w} ; \boldsymbol{x}^{\ell}\right]\right)<br>$$</p><p>即将三个Embedding拼接后直接变换, $\boldsymbol{S}_0 \in \mathbb{R}^{N \times H}$.</p><h3 id="Table-Encoder"><a href="#Table-Encoder" class="headerlink" title="Table Encoder"></a>Table Encoder</h3><p>Table Encoder专门学习<strong>表</strong>表示.</p><p>首先, 由Sequence表示构造表结构, 刚开始表是<strong>表内上下文无关</strong>的, 从第$l-1$ 层Sequence Encoder得到的Sequence表示中获取表第$i$ 行第$j$ 列所需的信息, 简单的用$\operatorname{Linear}$ 展开成一个表:</p><p>$$<br>X_{l, i, j}=\operatorname{ReLU}\left(\operatorname{Linear}\left(\left[S_{l-1, i} ; S_{l-1, j}\right]\right)\right)<br>$$</p><p>第$l$ 层Table Encoder有$\boldsymbol{X}_l \in \mathbb{R} ^{N \times N \times H}$.</p><p>接着, 采用<strong>多维RNN</strong>(MD - RNN)去迭代的融合<strong>表间</strong>信息, 也就是<strong>表内上下文相关</strong>的信息: </p><p>$$<br>T_{l, i, j}=\operatorname{GRU}\left(X_{l, i, j}, T_{l-1, i, j}, T_{l, i-1, j}, T_{l, i, j-1}\right)<br>$$</p><p>$\operatorname{GRU}$ 跨越了三个维度, 除了表的行$i$, 列$j$, 还有跨越不同Table Encoder层的$l$.</p><p>第$l$ 个Table Encoder的表中的第$i$ 行, 第$j$ 列的信息$T_{l, i, j}$ 由多个GRU捕获, 每个GRU可以捕获到不同信息:</p><p>$$<br>\begin{aligned}<br>&amp;T_{l, i, j}^{(a)}=\operatorname{GRU}^{(a)}\left(X_{l, i, j}, T_{l-1, i, j}^{(a)}, T_{l, i-1, j}^{(a)}, T_{l, i, j-1}^{(a)}\right) \\<br>&amp;T_{l, i, j}^{(c)}=\operatorname{GRU}^{(c)}\left(X_{l, i, j}, T_{l-1, i, j}^{(c)}, T_{l, i+1, j}^{(c)}, T_{l, i, j+1}^{(c)}\right) \\<br>&amp;T_{l, i, j}=\left[T_{l, i, j}^{(a)} ; T_{l, i, j}^{(c)}\right]<br>\end{aligned}<br>$$</p><p>在表结构中, RNN可以选择递归<strong>方向</strong>, 对于式子中出现的$a, b, c, d$ 分别代表作者所尝试的方向, 示意图如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse4.jpg" style="zoom: 50%;" /><p>作者尝试了a, c组合和a, b, c, d组合两种设置, 后来发现前后二者差别不大, 所以采用简单的前者.</p><blockquote><p>个人猜测应该是b, d<strong>信息冗余</strong>. 从a触发, 代表表行和列的顺向, 从c出发代表表行和列的逆向. 而b代表行的顺向, 列的逆向, d代表行的逆向, 列的顺向. </p><p>在我们所习惯的双向RNN中扩展到表结构上, 就是a, c的组合, 无需再重复添加b, d.</p></blockquote><h3 id="Sequence-Encoder"><a href="#Sequence-Encoder" class="headerlink" title="Sequence Encoder"></a>Sequence Encoder</h3><p>Sequence Encoder专门学习<strong>序列</strong>表示.</p><p>作者认为单纯的缩放点积注意力在本模型中不够好, 于是提出了Table - Guided Attention, 用表信息指导Sequence中的Attention.</p><p>对于给定的$\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}$, 一般的注意力可被下述式子概括:<br>$$<br>f\left(Q_{i}, K_{j}\right)=U \cdot g\left(Q_{i}, K_{j}\right)<br>$$</p><p>$f$ 能返回$Q_i$ 下$V_i$ 的权重. 其中$U$ 为可学习参数, $g$ 为计算$Q, K$ 间相关性的函数.</p><p>如下图所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse5.jpg" style="zoom: 50%;" /><p>在本模型中, 恰好$\boldsymbol{Q}= \boldsymbol{K} =\boldsymbol{V}=\boldsymbol{S}_{l-1}$, 而$\boldsymbol{T}_l$ 是由$\boldsymbol{S}_{l-1}$ 得到的, 可以视为$T_{l, i, j} = g(S_{l-1, i}, S_{l-1, j}) = g(Q_i, K_j)$. 因此有:</p><p>$$<br>f\left(Q_{i}, K_{j}\right)=U \cdot T_{l, i, j}<br>$$</p><p>作者认为这种Attention有三个好处:</p><ol><li>不用算$g(\cdot)$, 直接从Table Encoder里拿$\boldsymbol{T}_l$ 就行.</li><li>因为$\boldsymbol{T}_l$ 是表内上下文相关的, 所以更能捕捉词间不同相关性.</li><li>使得Table Encoder能参与到Sequence Encoder的学习过程中, 从而提升两个Encoder间的交互性.</li></ol><p>其余的内容和Self - Attention一样:<br>$$<br>\begin{aligned}<br>\tilde{\boldsymbol{S}}_{l} &amp;=\operatorname{LayerNorm}\left(\boldsymbol{S}_{l-1}+\operatorname{SelfAttn}\left(\boldsymbol{S}_{l-1}\right)\right) \\<br>\boldsymbol{S}_{l} &amp;=\operatorname{LayerNorm}\left(\tilde{\boldsymbol{S}}_{l}+\operatorname{FFNN}\left(\tilde{\boldsymbol{S}}_{l}\right)\right)<br>\end{aligned}<br>$$</p><p>我再回来看一眼Table Encoder和Sequence Encoder之间<strong>迭代提升</strong>的过程:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse2.jpg" style="zoom: 50%;" /><blockquote><p>注意, 在这个图左侧是有虚线的! 它的作用将在下一小节说明.</p></blockquote><h3 id="Exploit-Pre-trained-Attention-Weights"><a href="#Exploit-Pre-trained-Attention-Weights" class="headerlink" title="Exploit Pre - trained Attention Weights"></a>Exploit Pre - trained Attention Weights</h3><p>作者认为, BERT的潜力没有被充分发掘, 因为BERT能很好的把握住<strong>词和词</strong>之间的关系, 所以<strong>Attention Weights</strong>最好被<strong>显式</strong>的加进来, 预训练模型的潜力就被更进一步的挖掘了.</p><p>作者把所有层的所有头的Attention Weights全部堆叠起来, 记为$\boldsymbol{T}^{l} \in \mathbb{R}^{N \times N \times (L^\ell \times A^\ell)}$, $L^{\ell}$ 为Transformer堆叠的层数, $A^\ell$ 为注意力头数.</p><p>接着只要把前面Sequence Encoder公式替换即可:<br>$$<br>\displaylines{X_{l, i, j}=\operatorname{ReLU}\left(\operatorname{Linear}\left(\left[S_{l-1, i} ; S_{l-1, j}\right]\right)\right)<br> \\<br> \downarrow<br> \\<br>X_{l, i, j}=\operatorname{ReLU}\left(\operatorname{Linear}\left(\left[S_{l-1, i} ; S_{l-1, j} ; T_{i, j}^{\ell}\right]\right)\right)}<br>$$</p><p>在每层生成Table表示的时候, 就必须考虑第$i$ 个词和第$j$ 个词之间的Attention Score $T_{i, j}^\ell$.</p><p>其余部分完全不用变, 就可以直接把预训练的BERT拿过来用.</p><p>再来回顾一下模型的整个流程:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse3.jpg" style="zoom: 50%;" /><ol><li>由Sequence Encoding和预训练模型的Attention Score一起得到最初的Table Encoding.</li><li>Table Encoding内部用MD - RNN做表间和跨层的交互.</li><li>Table信息通过Table - Guided Attention反作用于Sequence Encoder的训练.</li></ol><p>总体来看设计还是挺精妙的.</p><h3 id="Training-and-Evaluation"><a href="#Training-and-Evaluation" class="headerlink" title="Training and Evaluation"></a>Training and Evaluation</h3><p>因为作者假定NER为序列标注问题, RE为填表问题, 所以直接拿Sequence的结果$\boldsymbol{S}_L$ 来标注NER, 填表$\boldsymbol{T}_L$ 的结果来标注RE: </p><p>$$<br>\begin{aligned}<br>P_{\theta}\left(\boldsymbol{Y}^{\mathrm{NER}}\right) &amp;=\operatorname{softmax}\left(\operatorname{Linear}\left(\boldsymbol{S}_{L}\right)\right) \\<br>P_{\theta}\left(\boldsymbol{Y}^{\mathrm{RE}}\right) &amp;=\operatorname{softmax}\left(\operatorname{Linear}\left(\boldsymbol{T}_{L}\right)\right)<br>\end{aligned}<br>$$</p><p>这二者都是分类问题, 直接用交叉熵优化:</p><p>$$<br>\begin{aligned}<br>\mathcal{L}^{\mathrm{NER}} &amp;=\sum_{i \in[1, N]}-\log P_{\theta}\left(Y_{i}^{\mathrm{NER}}=y_{i}^{\mathrm{NER}}\right) \\<br>\mathcal{L}^{\mathrm{RE}} &amp;=\sum_{i, j \in[1, N] ; i \neq j}-\log P_{\theta}\left(Y_{i, j}^{\mathrm{RE}}=y_{i, j}^{\mathrm{RE}}\right)<br>\end{aligned}<br>$$</p><p>最终目标就是联合优化二者, 即最小化$\mathcal{L}^{\text{NER}} + \mathcal{L}^{\text{RE}}$.</p><p>在推断时, 还是把NER和RE作为两个任务<strong>分别处理</strong>.</p><p>对于NER任务, 简单的有:<br>$$<br>\underset{e}{\operatorname{argmax}} P_{\theta}\left(Y_{i}^{\mathrm{NER}}=e\right)<br>$$<br>对于RE任务, 需要区分开关系所对应的Subject和Object. 对于NER抽取到的两实体的Span$\left(i^{b}, i^{e}\right), \left(j^{b}, j^{e}\right)$, 关系由下式得来:<br>$$<br>\underset{\vec{r}}{\operatorname{argmax}} \sum_{i \in\left[i^{b}, i^{e}\right], j \in\left[j^{b}, j^{e}\right]} P_{\theta}\left(Y_{i, j}^{\mathrm{RE}}=\overrightarrow{r}\right)+P_{\theta}\left(Y_{j, i}^{\mathrm{RE}}=\overleftarrow{r}\right)<br>$$</p><p>$\perp$ 代表不存在关系, 它是不存在方向的.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文. BTW, 附录里面还有作者很多小想法和实验结果, 建议阅读.</p><p>作者采用了ACE04, ACE05, CoNLL04,  ADE这几个数据集.</p><p>下文中, 作者所采用的指标为F1 Score, NER仅当实体类型和边界均匹配时才算正确, RE当实体边界和关系类型匹配时才算正确.</p><h3 id="Comparison-with-Other-Models"><a href="#Comparison-with-Other-Models" class="headerlink" title="Comparison with Other Models"></a>Comparison with Other Models</h3><p>作者将本文方法与诸多模型做对比, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse7.jpg" style="zoom: 67%;" /><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse8.jpg" style="zoom: 67%;" /><p>RE + 代表实体边界, 以及实体类型, 关系类型, 三者均匹配时才算正确, 比传统的RE任务要更加严格, 还包含了<strong>实体类型</strong>.</p><p>模型在上述数据集上均达到SOTA.</p><h3 id="Comparison-of-Pre-trained-Models"><a href="#Comparison-of-Pre-trained-Models" class="headerlink" title="Comparison of Pre - trained Models"></a>Comparison of Pre - trained Models</h3><p>作者通过比较不同预训练之间的差距, 来间接凸显自己方法的有效性. 在ACE05上的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse9.jpg" style="zoom: 67%;" /><p>$+\boldsymbol{x}^\ell$ 代表使用了上下文相关的Word Embedding, $+\boldsymbol{T}^\ell$ 代表使用了Attention Weights.</p><p>即使是使用了不是真正上下文相关的ELMo, 性能依然能够与一些最好的模型媲美. 引入预训训练后, 性能进一步上涨, 在引入Attention Weights后涨幅比较大. </p><p>这证明了利用Attention Weight的重要性, 对NER和RE都有增益.</p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><h4 id="Bidirectional-Interaction"><a href="#Bidirectional-Interaction" class="headerlink" title="Bidirectional Interaction"></a>Bidirectional Interaction</h4><p>作者研究了NER和RE这两个任务之间的交互对性能的影响:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse10.jpg" style="zoom: 67%;" /><p>RE(gold) 代表在做RE时候直接使用Golden Entity.</p><p>结论如下:</p><ul><li>不使用NER Loss或者不使用RE Loss, 都会降低性能, NER和RE彼此间确实存在关联, 且有相互促进作用.</li><li>不使用Table Encoder或不使用Sequence Encoder都会对性能产生负面影响, 如果切断两个Encoder之间的交互, 效果会更差.</li><li>如果直接把Table Representation的主对角线用来做NER任务, 也就是把NER和RE统一在一个空间中解决, 效果也不错, 但达不到作者的设置. 作者认为这是因为NER和RE仍然存在潜在差异, 不能直接用RE任务的Feature来做NER. 在该基础上, 继续去掉Sequence Encoder, 效果会变差, 这表明了Sequence信息对NER的作用.</li></ul><h4 id="Encoding-Layers"><a href="#Encoding-Layers" class="headerlink" title="Encoding Layers"></a>Encoding Layers</h4><p>作者探究了Encoder层数对性能的影响, ACE05上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse11.jpg" style="zoom: 67%;" /><p>堆到3层的时候性能就没有明显提升了, 如果每层的参数共享, 层数越多越好.</p><h4 id="Settings-of-MD-RNN"><a href="#Settings-of-MD-RNN" class="headerlink" title="Settings of MD - RNN"></a>Settings of MD - RNN</h4><p>作者还对不同MD - RNN设定做了探究, 在ACE05上部分结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse12.jpg" style="zoom: 67%;" /><p>就如在前文中提到的, 四方向的效果并不比双向好太多. 且融入跨层信息后对RE提升比较大.</p><h3 id="Attention-Visualization"><a href="#Attention-Visualization" class="headerlink" title="Attention Visualization"></a>Attention Visualization</h3><p>作者还模型做了Attention可视化:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse13.jpg" style="zoom: 67%;" /><p>相较于ALBERT下的Attention, 本文提出的Table - Guided Attention要更像人类的Ground Truth一些, 可视化一定程度上说明了改进的有效性.</p><h3 id="Probing-Intermediate-States"><a href="#Probing-Intermediate-States" class="headerlink" title="Probing Intermediate States"></a>Probing Intermediate States</h3><p>在ACE05中作者挑了一个Case, 然后把训练好的Linear层放在每层Encoder后面检测结果:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tse6.jpg" style="zoom: 50%;" /><p>从下到上层数依次变深, 能看到模型逐渐对结果做<strong>修正</strong>, 作者认为更多地允许层间交互有助于捕捉困难实体和关系.</p><blockquote><p>Probing在这里起作用的原因是作者引入了跨Encoder层信息.</p></blockquote><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>在当时, 填表类的联合抽取模型已经有几年没有被提起, 此篇论文算是重新将填表式的方法抬上了桌面.</p><p>作者注意到了联合模型中存在的任务<strong>特征学习冲突</strong>问题, 并设计了一种<strong>两个独立Encoder</strong>的<strong>迭代</strong>式<strong>交互提升</strong>模型. 实验做的非常全面, 还用到了Probing等手段, 最后得到了”<strong>Two are better than One</strong>“的结论.</p><blockquote><p>非常巧合的是, 本文同期与另一篇论述Pipeline模型不一定弱于Joint模型的文章<a href="https://arxiv.org/abs/2010.12812" target="_blank" rel="noopener">A Frustratingly Easy Approach for Entity and Relation Extraction</a>得到了相似的结论, 这篇文章打算下次更新, 算作三部曲中的第二部.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ERE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking</title>
      <link href="/posts/49694.html"/>
      <url>/posts/49694.html</url>
      
        <content type="html"><![CDATA[<h1 id="TPLinker-Single-stage-Joint-Extraction-of-Entities-and-Relations-Through-Token-Pair-Linking"><a href="#TPLinker-Single-stage-Joint-Extraction-of-Entities-and-Relations-Through-Token-Pair-Linking" class="headerlink" title="TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking"></a>TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking</h1><p>本文是论文<a href="https://arxiv.org/abs/2010.13415" target="_blank" rel="noopener">TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking</a>的阅读笔记和个人理解, 论文来自<strong>COLING 2020</strong>.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>虽然人们注意到在现有的实体关系抽取工作中, 含有<strong>重叠三元组</strong>的问题(感觉是特指<a href="https://adaning.github.io/posts/27105.html">CasRel</a>), 例如:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tplinker1.jpg" style="zoom: 50%;" /><ul><li><strong>EPO</strong>(Entity Pair Overlap): 实体对重叠是指<strong>实体对之间具有多种关系</strong>, 这些三元组共享相同的实体.</li><li><strong>SEO</strong>(Single Entity Overlap): 单实体重叠是指有<strong>单个实体被多个三元组共享</strong>.</li></ul><p>但现有<strong>基于分解</strong>的方法仍然受到<strong>曝光偏差</strong>(<strong>Exposure Bias</strong>)的困扰, 即Subject和Object的抽取存在<strong>先后顺序</strong>上的影响, 如果Subject抽取错误, 则会导致<strong>错误累积</strong>.</p><p>所以作者希望<strong>一步到位</strong>(One Stage)同时抽取Subject和Object, 来规避曝光偏差问题.</p><h2 id="TPLinker"><a href="#TPLinker" class="headerlink" title="TPLinker"></a>TPLinker</h2><h3 id="HandShaking-Tagging-Scheme"><a href="#HandShaking-Tagging-Scheme" class="headerlink" title="HandShaking Tagging Scheme"></a>HandShaking Tagging Scheme</h3><h4 id="Tagging"><a href="#Tagging" class="headerlink" title="Tagging"></a>Tagging</h4><p>TPLinker将联合抽取建模为<strong>Token Pair之间的链接问题</strong>(<strong>T</strong>oken <strong>P</strong>air <strong>Link</strong>ing Problem).</p><p>对于给定的句子中所有的Token Pair, 作者将其之间的链接考虑为:</p><ul><li><p><strong>EH-to-ET</strong>: 两Token分别为<strong>同实体</strong>的<strong>起止</strong>位置(Entity Head to Entity Tail).</p></li><li><p><strong>SH-to-OH</strong>: 两Token分别为Subject和Object的<strong>起始</strong>位置(Subject Head to Object Head).</p></li><li><p><strong>ST-to-OT</strong>: 两Token分别为Subject和Object的<strong>结束</strong>位置(Subject Tail to Object Tail).</p></li></ul><blockquote><p>简单的记<strong>头</strong>为实体的<strong>起始</strong>位置, <strong>尾</strong>为实体的<strong>结束</strong>位置.</p></blockquote><p>对于上述三种链接的情况, 作者给出了一个例子:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tplinker2.jpg" style="zoom: 55%;" /><p>我们从图中与上述三类链接做对应:</p><ul><li><strong>EH-to-ET</strong>: 图中的<strong>紫色</strong>Tag, (<code>New</code>, <code>City</code>) 就分别是<code>New York City</code>的头尾, (<code>De</code>, <code>Blasio</code>) 分别是<code>De Blasio</code> 的头尾.</li><li><strong>SH-to-OH</strong>: 图中的<strong>红色</strong>Tag, (<code>New</code>, <code>De</code>) 分别是<strong>给定关系</strong>下<code>New York City</code> 和<code>De Blasio</code> 的头. </li><li><strong>ST-to-OT</strong>: 图中的<strong>蓝色</strong>Tag, (<code>City</code>, <code>Blasio</code>) 分别是<strong>给定关系</strong>下<code>New York City</code> 和<code>De Blasio</code> 的尾. </li></ul><p>并且, 在关系<code>major</code>中<code>New York City</code> 和<code>De Blasio</code> 分别为Subject和Object.</p><p>该建模能够解决<strong>SEO</strong>问题, 例如嵌套实体<code>New York</code> 和<code>New York City</code> 在关系<code>major</code> 中都共享Object <code>De Blasio</code>(图中未标注), 我们观察量两嵌套Subject和Object的不同Link Type包含的Token Pair:</p><table><thead><tr><th>Subject</th><th>EH-to-ET</th><th>SH-to-OH</th><th>ST-to-OT</th></tr></thead><tbody><tr><td><code>New York</code></td><td>(<code>New</code>, <code>York</code>)</td><td>(<code>New</code>, <code>De</code>)</td><td>(<code>York</code>, <code>Blasio</code>)</td></tr><tr><td><code>New York City</code></td><td>(<code>New</code>, <code>City</code>)</td><td>(<code>New</code>, <code>De</code>)</td><td>(<code>City</code>, <code>Blasio</code>)</td></tr></tbody></table><p>尽管两嵌套实体有相同的SH-to-OH, 但其他两种Link的Token Pair并不相同, 自然能区分出共享Object的两个三元组.</p><p>除此外, 作者从中有更深的思考:</p><ol><li>若按图中方式建模, 矩阵会非常<strong>稀疏</strong>. 并且因为实体的尾永远都在头的前面. 过于稀疏会导致<strong>负样本</strong>过多, 不易于优化.</li><li>在<strong>EH-to-ET</strong>这类Link Type中, 矩阵的下三角永远都是<strong>0</strong>, 浪费了大量空间. </li><li>有趣的是, <strong>Object可能出现在Subject前面</strong>, 所以对于<strong>SH-to-OH</strong>和<strong>ST-to-OT</strong>这两种链接类型, 直接把下三角丢掉是错误的.</li></ol><p>基于上述问题, 作者巧妙的将下三角的值全部映射到上三角的对应位置, 并把原来的下三角的1转而记为与上三角做区分的2, 在解码阶段做区分就可以了, 如下图所示(注意关系不再是上图的<code>major</code>, 变成了<code>born in</code>, 与刚才的Subject和Object的位置刚好相反, 也就是上文所述第三点的场景):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tplinker3.jpg" style="zoom: 55%;" /><p>这一映射是我觉得全文最绝的一点. </p><p>在下三角时, (<code>De</code>, <code>New</code>) 之间的Link Type为SH-to-OH, 但映射到上三角后, Token Pair内<strong>完全颠倒</strong>了, 变成了(<code>New</code>, <code>De</code>). </p><p>不要紧, 同时交换Token Pair的两Token位置和Link Type的指向, 交换前后是<strong>等价</strong>的. 也就是说, 若想保持原来<strong>SH-to-OH</strong>原含义不变, 映射后也必须颠倒, 即<strong>OH-to-SH</strong>, 也就是<strong>反向链接</strong>, 即图中的<strong>箭头回指</strong>. ST-to-OT也同理, (<code>Blasio</code>, <code>York</code>) 映射到上三角后变为(<code>York</code>, <code>Blasio</code>), ST-to-OT变为OT-to-ST. </p><p>前文提到SEO能处理了, 那<strong>EPO</strong>呢? 直接参照<a href="https://adaning.github.io/posts/27105.html">CasRel</a>把<strong>层叠标注</strong>的思想引进过来就行了, 即对每个不同的关系都创建出一张表来分别标注, 也就是<strong>关系特化的标注</strong>, 这样就能处理同一实体对存在多关系的EPO问题. </p><blockquote><p>应该注意到, 当遇到单个Token实体时, SH-OH和ST-OT会落在<strong>同一位置</strong>, 因此必须使用两张表来区分它们.</p></blockquote><p>在TPLinker中, EH-to-ET没有必要做关系特化, 因为这种链接类型处理的是Token Pair是否归属于同一实体的问题, 不涉及到跨实体之间的关系. 而SH-to-OH, ST-to-OT是不同实体之间的链接, 涉及到关系, 需要做关系特化.</p><h4 id="Decoding"><a href="#Decoding" class="headerlink" title="Decoding"></a>Decoding</h4><p>解码过程可以直接拿图来说. 将Token Pair之间的表结构<strong>展平</strong>,每张表都视为一个序列, 这样方便我们观察不同Relation特化的表的标注情况:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tplinker4.jpg" style="zoom: 45%;" /><p>EH-to-ET分别有三个Token Pair, (<code>New</code>, <code>York</code>), (<code>New</code>, <code>City</code>), (<code>De</code>, <code>Blasio</code>), 分别对应实体<code>New York</code>, <code>New York City</code>, <code>De Blasio</code>, 这是区分是否为同实体的Tag, 必须要与其他两种Link Type结合使用.</p><p>在关系<code>major</code>中, SH-to-OH标为1的Token Pair为(<code>New</code>, <code>De</code>), 且ST-to-OT标为1的Token Pair为(<code>City</code>, <code>Blasio</code>), 二者均为同向且顺向, 故抽取出三元组(<code>New York City</code>, <code>major</code>, <code>De Blasio</code>). </p><p>别忘了2代表反向链接. 在关系<code>born in</code>中, ST-to-OT标为2的Token Pair为(<code>York</code>, <code>Blasio</code>), 结合SH-to-OH标为2的Token Pair(<code>De</code>, <code>New</code>), 二者均为同向且逆向, 故抽取出三元组(<code>De Blasio</code>, <code>born in</code>, <code>New York</code>). </p><p>在该图中, EH-to-ET需要一张表, SH-to-OH, ST-to-OT做了关系特化, 分别每个关系需要一张表, 共计$2|R| + 1$ 张表. 但是由于做了下三角舍弃的优化, 若句长为$n$, 每张表只有$\frac{n^2+n}{2}$ 个元素.</p><p>TPLinker的解码流程伪代码如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tplinker5.jpg" style="zoom: 50%;" /><p>可以概括为:</p><ol><li>根据EH-toET的表, 检测所有存在的实体, 存入字典$D$.</li><li>对每一种关系, 根据ST-to-OT的表记录Subject和Object的尾位置存入集合$E$. 将Tag为1的顺序记下来, 及Tag为2的逆序记下来. </li><li>对每一种关系, 根据SH-to-OH的表获得Subject和Object的头位置(Tag为1, 2, 同上), 直接去$D$ 中查询对应的实体对, 查询到后去$E$ 中做匹配, 如果命中, 则抽取出三元组. </li></ol><h3 id="Token-Pair-Representation"><a href="#Token-Pair-Representation" class="headerlink" title="Token Pair Representation"></a>Token Pair Representation</h3><p>对于长为$n$ 的句子$[w_1, w_2, \cdots, w_n]$, 首先将每个Token都用Encoder(应该是BERT之类)映射成上下文相关的低维表示$\mathbf{h_i}$, 然后按如下方式生成$(w_i, w_j)$ 之间的Token Pair表示$\mathbf{h}_{i, j}$:</p><p>$$<br>\mathbf{h}_{i, j}=\tanh \left(\mathbf{W}_{h} \cdot\left[\mathbf{h}_{\mathbf{i}} ; \mathbf{h}_{\mathbf{j}}\right]+\mathbf{b}_{h}\right), j \geq i<br>$$</p><p>其中$\mathbf{W}_h, \mathbf{b}_h$ 为可学习参数.</p><h3 id="Handshaking-Tagger"><a href="#Handshaking-Tagger" class="headerlink" title="Handshaking Tagger"></a>Handshaking Tagger</h3><p>我们只需要很简单的对Token Pair之间的表示做Softmax就好:</p><p>$$<br>\begin{aligned}<br>&amp;P\left(y_{i, j}\right)=\operatorname{Softmax}\left(\mathbf{W}_{o} \cdot \mathbf{h}_{i, j}+\mathbf{b}_{o}\right) \\<br>&amp;\operatorname{link}\left(w_{i}, w_{j}\right)=\arg \max _{l} P\left(y_{i, j}=l\right)<br>\end{aligned}<br>$$</p><p>记Token Pair $(w_i, w_j)$ 之间的链接标签为$l$, $P(y_{i, j}=l)$ 为预测值为标签的概率.</p><p>这样就可以把前面图中的所有Token Pair之间的打上Tag.</p><h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><p>损失函数为NLL Loss:</p><p>$$<br>L_{l i n k}=-\frac{1}{N} \sum_{i=1, j \geq i}^{N} \sum_{\ast \in\{E, H, T\}} \log P\left(y_{i, j}^{\ast}=\hat{l}^{\ast}\right)<br>$$</p><p>$N$ 为句子长, $\hat{l}$ 为正确Tag, $E, H, T$ 分别代表EH-to-ET, SH-toOH, ST-to-OT. </p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文.</p><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>作者所采用的数据集为常用数据集NYT和WebNLG, 统计信息如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tplinker6.jpg" style="zoom:67%;" /><blockquote><p>下文中, NYT*, WebNLG*, 均代表部分匹配下的结果, 否则为精准匹配结果. </p></blockquote><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><p>作者将TPLinker与Tagging类, 生成类的模型, 以及其他类的模型放在一起做了对比, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tplinker7.jpg" style="zoom:67%;" /><p>主要观察CasRel和TPLinker之间的性能差距. TPLinker在LSTM下与CasRel取得了相近的性能, BERT抽取出的特征对TPLinker的增益比CasRel要大得多.</p><h3 id="Analysis-on-Different-Sentence-Types"><a href="#Analysis-on-Different-Sentence-Types" class="headerlink" title="Analysis on Different Sentence Types"></a>Analysis on Different Sentence Types</h3><p>作者对三元组不同类型, 每条样本中三元组的个数分类, 观察TPLinker的能力, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tplinker8.jpg" style="zoom:67%;" /><p>从中观察到, TPLinker不太受句子中三元组数量增多的影响, 同时处理SEO和EPO问题的效果也都很不错.</p><h3 id="Analysis-on-Computational-Efficiency"><a href="#Analysis-on-Computational-Efficiency" class="headerlink" title="Analysis on Computational Efficiency"></a>Analysis on Computational Efficiency</h3><p>最后, TPLinker的解码相对于其他模型来说是比较复杂的, 或许从直观上来看, 这可能成为TPLinker的弱势, 所以作者还做了TPLinker和CasRel的复杂度比较:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tplinker9.jpg" style="zoom:67%;" /><p>TPLinker的推理时间其实并没有像想象中的那么长, 也算是打消了直观上的疑虑.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>本质上, TPLinker巧妙的将联合抽取问题建模为<strong>Token Pair之间的关系抽取问题</strong>, 使得联合抽取<strong>一步到位</strong>, 不但规避了<strong>归纳偏置</strong>问题, 还能同时处理<strong>EPO</strong>, <strong>SEO</strong>问题. 效果也超越了之前SOTA的CasRel. </p><p>方法很简练, <strong>没有花里胡哨</strong>. 本篇论文有颇多值得思考的地方, 除了对抽取任务本身的考量, 能看得出来作者的工程思维很强, 在比较容易被质疑的算法复杂度部分提出了一种<strong>减少空间开销</strong>的实现, 并专门做了<strong>时间开销</strong>的比较, 这是值得学习的地方.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RTE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CasRel: A Novel Cascade Binary Tagging Framework for Relational Triple Extraction</title>
      <link href="/posts/27105.html"/>
      <url>/posts/27105.html</url>
      
        <content type="html"><![CDATA[<h1 id="A-Novel-Cascade-Binary-Tagging-Framework-for-Relational-Triple-Extraction"><a href="#A-Novel-Cascade-Binary-Tagging-Framework-for-Relational-Triple-Extraction" class="headerlink" title="A Novel Cascade Binary Tagging Framework for Relational Triple Extraction"></a>A Novel Cascade Binary Tagging Framework for Relational Triple Extraction</h1><p>本文是论文<a href="https://arxiv.org/abs/1909.03227" target="_blank" rel="noopener">A Novel Cascade Binary Tagging Framework for Relational Triple Extraction</a>的阅读笔记和个人理解, 论文来自<strong>ACL 2020</strong>.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>在面向三元组抽取设计的模型中, 少有模型能够解决多重<strong>关系三元组重叠</strong>问题, 这类三元组往往是<strong>共享实体</strong>的. 例如:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/casrel1.jpg" style="zoom: 60%;" /><p>作者认为有两种三元组重叠现象发生:</p><ul><li><strong>EPO</strong>(Entity Pair Overlap): 实体对重叠是指<strong>实体对之间具有多种关系</strong>, 这些三元组共享相同的实体.</li><li><strong>SEO</strong>(Single Entity Overlap): 单实体重叠是指有<strong>单个实体被多个三元组共享</strong>, 如例子中的<code>Washington</code>.</li></ul><p>基于重叠问题, 作者提出<strong>将关系视为句间实体的映射</strong>, 以此解决重叠问题.</p><h2 id="CasRel"><a href="#CasRel" class="headerlink" title="CasRel"></a>CasRel</h2><p>CasRel与先前的模型不同, 它希望将目标函数指定在<strong>Triple Level</strong>, 而非像先前的做法将三元组抽取的过程拆开, 并分别设立目标函数.</p><p>在数据集$D$ 中已标注的句子$x_i$ 中, 对于给定的三元组$T_j = \set{(s,r,o)}$, 作者希望最大化三元组在给定句子下的概率:</p><p>$$<br>\begin{aligned}<br>&amp; \prod_{j=1}^{|D|}\left[\prod_{(s, r, o) \in T_{j}} p\left((s, r, o) \mid x_{j}\right)\right] \\<br>=&amp; \prod_{j=1}^{|D|}\left[\prod_{s \in T_{j}} p\left(s \mid x_{j}\right) \prod_{(r, o) \in T_{j} \mid s} p\left((r, o) \mid s, x_{j}\right)\right] \\<br>=&amp; \prod_{j=1}^{|D|}\left[\prod_{s \in T_{j}} p\left(s \mid x_{j}\right) \prod_{r \in T_{j} \mid s} p_{r}\left(o \mid s, x_{j}\right) \prod_{r \in R \backslash T_{j} \mid s} p_{r}\left(o_{\varnothing} \mid s, x_{j}\right)\right]<br>\end{aligned}<br>$$</p><p>第一步到第二步是条件概率, 第二步到第三步将$r$ 分为$s$ 应该有的关系 $r \in T_{j} \mid s$ 和$s$ 一定不会存在的关系$r \in R \backslash T_{j} \mid s$ 来考虑. </p><p>所有与$s$ 可能发生的关系, 都应该在句子中找到与之对应的$o$ , 如果$s$ 不可能存在$r$, 那么必然找不到对应的$o$, 因此就要有$o_{\varnothing}$ 存在, 即”Null” Object.</p><p>这样, 如果三元组存在, 就应该可以通过Subject在Relation下找到相应的Object, 也就是所谓的”<strong>映射关系</strong>“.</p><p>作者认为这样有三点好处:</p><ol><li>Triple Level的目标函数建立与最终<strong>评估方式</strong>是相符的.</li><li>Triple Level的建模直接<strong>规避</strong>了实体如何被<strong>共享</strong>的问题.</li><li>第三步的拆解启发了模型的<strong>结构设计</strong>. 通过最大化$p\left(s \mid x_{j}\right)$训练Subject Tagger, 再在已知Subject的条件下最大化给定关系$r$ 的$p_{r}\left(o \mid s, x_{j}\right)$ 来训练Object Tagger, 关系便自然成为实体之间的映射函数.</li></ol><h3 id="BERT-Encoder"><a href="#BERT-Encoder" class="headerlink" title="BERT Encoder"></a>BERT Encoder</h3><p>对于句子$x_j$, BERT抽取它的特征. 令$\operatorname{Trans}(\mathbf{x})$ 为Transfomer Encoder Bolck, 将$N$ 层堆叠, 得到特征$\mathbf{h}_\alpha$: </p><p>$$<br>\begin{aligned}<br>\mathbf{h}_{0} &amp;=\mathbf{S} \mathbf{W}_{s}+\mathbf{W}_{p} \\<br>\mathbf{h}_{\alpha} &amp;=\operatorname{Trans}\left(\mathbf{h}_{\alpha-1}\right), \alpha \in[1, N]<br>\end{aligned}<br>$$</p><p>$\mathbf{S}$ 为句子的One Hot向量, $\mathbf{W}_s$ 为Subword的Embedding矩阵, $\mathbf{W}_p$ 为位置编码矩阵.</p><p>根据任务, 作者在这里只使用<strong>单句</strong>, 而非像BERT原文使用句子对.</p><h3 id="Cascade-Decoder"><a href="#Cascade-Decoder" class="headerlink" title="Cascade Decoder"></a>Cascade Decoder</h3><p>作者希望分两步抽取三元组:</p><ol><li>首先从句子中抽取<strong>Subject</strong>.</li><li>然后在所有可能的<strong>关系</strong>$r$ 的条件下, 看是否有与Subject相对应的<strong>Object</strong>.</li></ol><p>因此作者也设计了两相对应的模块.</p><h4 id="Subject-Tagger"><a href="#Subject-Tagger" class="headerlink" title="Subject Tagger"></a>Subject Tagger</h4><p>简单的使用两个二分类器分别识别Subject的起始和结束位置, 对每个Token的对应位置都是两个二分类问题, 这个位置是不是起始位置? 是不是结束位置?</p><p>描述如下:</p><p>$$<br>\begin{array}{r}<br>p_{i}^{\text {start_s }}=\sigma\left(\mathbf{W}_{\text {start }} \mathbf{x}_{i}+\mathbf{b}_{\text {start }}\right) \\<br>p_{i}^{\text {end_s }}=\sigma\left(\mathbf{W}_{\text {end }} \mathbf{x}_{i}+\mathbf{b}_{\text {end }}\right)<br>\end{array}<br>$$</p><p>$p_{i}^{\text {start_s }}, p_{i}^{\text {end_s }}$ 分别为输入句子的第$i$ 个Token是否为Subject起始位置的概率和结束位置的概率.</p><p>其中$\mathbf{W}_{\text{start}}, \mathbf{W}_{\text{end}}$ 为权重矩阵, $\mathbf{b}_{\text {start }}, \mathbf{b}_{\text {end }}$ 为偏置, $\sigma$ 为Sigmoid函数.</p><p>得出的概率超过先设定好的<strong>阈值</strong>时, 便标记为1, 否则为0.</p><p>对于Subject $s$, 我们最大化如下概率:</p><p>$$<br>p_{\theta}(s \mid \mathbf{x})<br>=\prod_{t \in\{\text { start_s }, \text { end_s }\}} \prod_{i=1}^{L}\left(p_{i}^{t}\right)^{\mathbf{I}\left\{y_{i}^{t}=1\right\}}\left(1-p_{i}^{t}\right)^{\mathbf{I}\left\{y_{i}^{t}=0\right\}}<br>$$</p><p>$L$ 为句长, $\mathbf{I}\set{z} = 1$ 表示如果$z$ 为真则为1, 否则为0. $y_i^{\text {start_s }}, y_i^{\text {end_s }}$ 分别为Subject的起始结束位置二值化的Tag. $\theta$ 就是前面提到的Subject Tagger的参数.</p><p>由于一定会检测到多个Subject的Start和End, 作者遵循<strong>最近匹配原则</strong>, 即将最近的Start和End做匹配, 在区间内的Token标识为一个实体.</p><p>Subject Tagger如下图所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/casrel7.jpg" style="zoom: 40%;" /><p>句子中的Token被BERT编码后, Subject Tagger能从中预测每个Token所对应的位置是否为Subject的起始位置, 结束位置. 在获取所有的起始结束位置后, 遵循最近匹配原则, 将最近的Start和最近的End匹配到一起, 区间内被视为一个Subject. </p><p>如图中以<code>Jackle</code>为起始, <code>Brown</code>为结束, <code>Jackle R. Brown</code>就被标识为一个Subject. <code>Washington</code>的起始和结束时同一个位置.</p><h4 id="Relation-specific-Object-Taggers"><a href="#Relation-specific-Object-Taggers" class="headerlink" title="Relation - specific Object Taggers"></a>Relation - specific Object Taggers</h4><p>关系特化的Object Tagger是一种比Subject Tagger更高阶的模块, 因为它是建立在Subject Tagger抽取出的Subject的基础之上的, 它的结构与Subject Tagger基本相同, 只是对关系做了特化处理, 即<strong>对每个关系都做给定Subject的Object抽取</strong>.</p><p>具体形式如下:<br>$$<br>\begin{array}{r}<br>p_{i}^{\text {start_o }}=\sigma\left(\mathbf{W}_{\text {start }}^{r}\left(\mathbf{x}_{i}+\mathbf{v}_{\text {sub }}^{k}\right)+\mathbf{b}_{\text {start }}^{r}\right) \\<br>p_{i}^{\text {end_o }}=\sigma\left(\mathbf{W}_{\text {end }}^{r}\left(\mathbf{x}_{i}+\mathbf{v}_{s u b}^{k}\right)+\mathbf{b}_{\text {end }}^{r}\right)<br>\end{array}<br>$$</p><p>$p_{i}^{\text {start_o }}, p_{i}^{\text {end_o }}$ 分别为输入句子的第$i$ 个Token是否为Object起始位置的概率和结束位置的概率. 这里作者简单的取了第$k$ 个Subject所有Token的<strong>平均</strong>$\mathbf{v}^k_{sub}$ 作为Subject传递的信息.</p><p>其中$\mathbf{W}^r_{\text{start}}, \mathbf{W}^r_{\text{end}}$ 为关系$r$ 对应的权重参数, $\mathbf{b}^r_{\text {start }}, \mathbf{b}^r_{\text {end }}$ 为关系$r$ 对应的偏置.</p><p>接着对如下概率做极大似然:<br>$$<br>p_{\phi_{r}}(o \mid s, \mathbf{x})<br>=\prod_{t \in\{\text { start_o, end_o }\}} \prod_{i=1}^{L}\left(p_{i}^{t}\right)^{\mathbf{I}\left\{y_{i}^{t}=1\right\}}\left(1-p_{i}^{t}\right) ^\mathbf{\mathbf { I } \{ y _ { i } ^ { t } = 0 \}}<br>$$</p><p>$y_i^{\text {start_o }}, y_i^{\text {end_o }}$ 分别为Object的起始结束位置二值化的Tag. $\phi_r$ 为关系特化的Object Tagger的参数.</p><p>对于”Null” Object $o_{\varnothing}$, 所有的$y_i^{start\_o_{\varnothing}}, y_i^{end\_o_{\varnothing}}$ 全部为0.</p><p>Relation - specific Object Taggers如下图所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/casrel8.jpg" style="zoom: 40%;" /><p>CasRel会在不同的Subject之间迭代. 从Subject Tagger获取了指定的实体后, 简单的将第$k$ 个实体所在区域的Token取平均得到$\mathbf{v}^k_{sub}$, 与BERT抽取出的$\mathbf{h}_N$ 相加, 以此判断句子中所有位置是否有可能含有给定关系$r$ 的Object, 并用起始位置和结束位置来标识Object.</p><p>在图中, 假设此时$k=1$, 所选定的Subject为<code>Jackle R. Brown</code> 其可能的关系<code>Birth_place</code>中, 探测到的Object是 <code>Washington</code> 和<code>United States Of America</code>. 故抽取出三元组<code>(Jackle R. Brown, Birth_place, Washington)</code> 及<code>(Jackle R. Brown, Birth_place, United States Of America)</code>.</p><p>当$k=2$ 时, 绿色的Subject <code>Washington</code> 可能的关系<code>Capital_of</code>中, 探测到的Object为 <code>United States Of America</code>, 图中绿色的位置将会从0变为1, 以此抽取到三元组<code>(Washington, Capital_of, United States Of America)</code>.</p><h3 id="Data-Log-likelihood-Objective"><a href="#Data-Log-likelihood-Objective" class="headerlink" title="Data Log-likelihood Objective"></a>Data Log-likelihood Objective</h3><p>根据最前面的推导, 目标函数$J(\Theta)$ 为:<br>$$<br>J(\Theta) = \sum_{j=1}^{|D|}\left[\sum_{s \in T_{j}} \log p_{\theta}\left(s \mid \mathbf{x}_{j}\right)+\sum_{r \in T_{j} \mid s} \log p_{\phi_{r}}\left(o \mid s, \mathbf{x}_{j}\right)+\sum_{r \in R \backslash T_{j} \mid s} \log p_{\phi_{r}}\left(o_{\varnothing} \mid s, \mathbf{x}_{j}\right)\right]<br>$$</p><p>使用极大似然优化即可.</p><p>总览图如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/casrel2.jpg" style="zoom: 50%;" /><p>因为对于每种关系都分别检测相应的Object, 感觉就像是一堆指针堆叠起来, 所以也被称为<strong>层叠式指针标注</strong>.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的实验参数请参照原论文.</p><blockquote><p>注: 接下来的实验中BERT是<strong>没有经过预训练</strong>的, 这展示了CasRel强大的潜力.</p></blockquote><p>作者在NYL和WebNLG这两个数据集上做了实验, 数据集的详细信息如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/casrel3.jpg" style="zoom: 50%;" /><p>其中NYT规模较大, WebNLG规模较小. 它们都包含有大量的EPO和SEO三元组.</p><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><p>在这两个数据集上的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/casrel4.jpg" style="zoom: 50%;" /><p>无论是Tagging类的方法, 还是基于图的方法, 基于Copy生成式的方法, 都比CasRel差一大截. 尤其是在重叠三元组很多的WebNLG上, CasRel展示出强大的性能.</p><h3 id="Detail-Results"><a href="#Detail-Results" class="headerlink" title="Detail Results"></a>Detail Results</h3><p>面对不同类型的三元组, 各个Baseline在NYT, WebNLG的F1结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/casrel5.jpg" style="zoom: 50%;" /><p>左, 中, 右分别为Normal Class, EPO Class, SEO Class的F1. CasRel在面对重叠三元组时, 比Baseline领先的更多. 这证明了CasRel能较好的解决SEO和EPO问题.</p><p>作者还测试了句子中三元组个数不同时, 模型预测它们时的F1, 结果如下: </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/casrel6.jpg" style="zoom: 50%;" /><p>即使是面对句子中5个以上的三元组, 相较于Baseline, CasRel仍然能保证不受过大的影响, 这表明它有更强的鲁棒性和抽取多三元组的能力.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>在本文中, 作者将目光聚焦到<strong>三元组重叠</strong>问题上, 并提出<strong>将关系视为句间实体的映射</strong>的思想, 并通过<strong>层叠式指针标注</strong>的方法, 巧妙的解决了三元组重叠问题.</p><p>这是一篇非常有开创性意义的文章, 模型效果相较于之前的Baseline有巨大涨幅.</p><p>更为细致的考量, 可以在苏神的文章<a href="https://kexue.fm/archives/6671" target="_blank" rel="noopener">基于DGCNN和概率图的轻量级信息抽取模型</a>中看到, CasRel仅仅只是把文中的DGCNN换成了BERT, 阅读起来应该不会有问题.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RTE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Unified MRC Framework for Named Entity Recognition</title>
      <link href="/posts/53990.html"/>
      <url>/posts/53990.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>BERT: 详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>.</li></ul></blockquote><h1 id="A-Unified-MRC-Framework-for-Named-Entity-Recognition"><a href="#A-Unified-MRC-Framework-for-Named-Entity-Recognition" class="headerlink" title="A Unified MRC Framework for Named Entity Recognition"></a>A Unified MRC Framework for Named Entity Recognition</h1><p>本文是论文<a href="https://arxiv.org/abs/1910.11476" target="_blank" rel="noopener">A Unified MRC Framework for Named Entity Recognition</a>的阅读笔记和个人理解, 论文来自<strong>ACL 2020</strong>.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者认为, 基于单标签设计的模型不适合在处理Flat NER的同时, 处理<strong>嵌套NER</strong>任务, 因为一个Token仅可被分配给一个实体, 但在嵌套NER问题中, 一个Token往往隶属于多个实体.</p><p>下面有一个嵌套NER的例子:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mrc4ner1.jpg" style="zoom:40%;" /><p>这个句子中, <code>PEBP2</code> 为蛋白质, 但<code>PEBP2 site</code>是一种DNA, 实体间存在嵌套关系.</p><p>作者尝试将NER任务迁移到MRC的框架下, 同时解决<strong>Flat NER</strong>和<strong>Nested NER</strong>问题, 因为在MRC中, <strong>抽取两嵌套实体, 仅仅只是回答两个不同类别的问题</strong>.</p><h2 id="Unified-MRC-Framework-for-NER"><a href="#Unified-MRC-Framework-for-NER" class="headerlink" title="Unified MRC Framework for NER"></a>Unified MRC Framework for NER</h2><h3 id="Task-Formulation"><a href="#Task-Formulation" class="headerlink" title="Task Formulation"></a>Task Formulation</h3><h4 id="NER-Task-Formulation"><a href="#NER-Task-Formulation" class="headerlink" title="NER Task Formulation"></a>NER Task Formulation</h4><p>对于给定的长度为$n$ 的输入序列$X=\left\{x_{1}, x_{2}, \ldots, x_{n}\right\}$, 对于每个在序列中的实体, 都有一个预先定义好的标签$y \in Y$, $Y$ 为预先定义好的实体所有可能的类型.</p><h4 id="Dataset-Constraction"><a href="#Dataset-Constraction" class="headerlink" title="Dataset Constraction"></a>Dataset Constraction</h4><p>为了将NER任务迁移到MRC框架下, 作者将数据集视为$(\text{QUESTION}, \text{ANSWER}, \text{CONTEXT})$ 的三元组形式. 对于每个实体的类型$y \in Y$, 都需要一个与之对应的长度为$m$ 的自然语言问题$q_y=\set{q_1, q_2, \dots, q_m}$, 实体被标注为$x_{\text{start}, \text{end}} = \set{x_{\text{start}, }x_{\text{start+1}, }\dots, x_{\text{end}-1, }, x_{\text{end}}}$, 且同一实体的$\text{start}$ 必在$\text{end}$ 前. </p><p>$X$ 和$Y$ 在NER任务中是已存在的, 还缺少问题$\text{QUESTION}$, 需要根据标签$y$ 进一步生成问题$q_y$, 来凑齐$(q_y, x_{\text{start,end}}, X)$ 的形式.</p><h3 id="Query-Generation"><a href="#Query-Generation" class="headerlink" title="Query Generation"></a>Query Generation</h3><p>问题生成是一个非常重要的注入先验知识的过程, 作者尝试了多种生成问题的方式, 最终选择”Annotation guideline notes”, 作者认为这种方法能<strong>不产生歧义</strong>问题. 这种方法直接使用数据集构建者发配给标注者的<strong>标注提示</strong>作为问题, 更符合NLP的处理风格.</p><p>其方法例子如下表所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mrc4ner2.jpg" style="zoom:33%;" /><blockquote><p>感觉这玩意就是一种Prompt啊… 给模型对应类型的实体信息提示, 以获取相应类型的实体位置.</p></blockquote><h3 id="Model-Details"><a href="#Model-Details" class="headerlink" title="Model Details"></a>Model Details</h3><h4 id="Model-Backbone"><a href="#Model-Backbone" class="headerlink" title="Model Backbone"></a>Model Backbone</h4><p>现在需要把前面所说的$(q_y, x_{\text{start,end}}, X)$ 整合进MRC框架下. 作者以BERT作为模型的Backbone, 把问题$q_y$ 和句子$X$ 一并输入进去, 格式为$\left\{[\mathrm{CLS}], q_{1}, q_{2}, \ldots, q_{m},[\mathrm{SEP}], x_{1}, x_{2}, \ldots, x_{n}\right\}$. </p><p>最后可以获得BERT抽取出的句子$X$ 所对应的上下文表示$E \in \mathbb{R}^{n \times d}$. </p><blockquote><p>作者在这里直接舍弃了问题的表示, 或许在某些问题中, 问题起到增强的作用.</p></blockquote><h4 id="Span-Selection"><a href="#Span-Selection" class="headerlink" title="Span Selection"></a>Span Selection</h4><p>在MRC任务中, 问题的答案一般是基于<strong>抽取式</strong>的, 也就是直接基于问题和原文内容, 直接在原文中指出问题所在的<strong>位置</strong>. </p><p>往往在MRC中有两类抽取答案的策略:</p><ul><li>用两个<strong>$n$ 元分类器</strong>, 分别获取问题的$\text{start}, \text{end}$ 位置.</li><li>用两个<strong>二元分类器</strong>, 判断每个Token所对应的位置是否是$\text{start}$, 是否是$\text{end}$.</li></ul><p>作者在这里选择第二种, 因为第一种只能抽取出一组$\text{start}, \text{end}$, 而第二种可以抽取出多个$\text{start}, \text{end}$, 这样有更大希望命中与问题$q_y$ 对应的实体.</p><blockquote><p>这两种方法生成Label的时候都可能会有稀疏问题, 可以用Focal Loss等方式缓解.</p></blockquote><h5 id="Start-and-End-Index-Prediction"><a href="#Start-and-End-Index-Prediction" class="headerlink" title="Start and End Index Prediction"></a>Start and End Index Prediction</h5><p>对于从BERT获取来的表示$E \in \mathbb{R} ^{n \times d}$, 首先预测$n$ 个token分别是否为$\text{start}$ 的概率:</p><p>$$<br>P_{\text {start }}=\operatorname{softmax}_{\text {each row }}\left(E \cdot T_{\text {start }}\right) \in \mathbb{R}^{n \times 2}<br>$$</p><p>其中$P_{\text{start}}$ 是是否为$\text{start}$ 的概率分布, $T_{\text{start}} \in \mathbb{R} ^ {d\times 2}$ 是权重矩阵. End Index的获取方式同理, $T_{\text{end}} \in \mathbb{R} ^ {d\times 2}$ 也是权重矩阵, $P_{\text{end}}$ 为是否为$\text{end}$ 的概率分布.</p><h5 id="Start-End-Matching"><a href="#Start-End-Matching" class="headerlink" title="Start - End Matching"></a>Start - End Matching</h5><p>一般的NER任务中, 一句话都会包含<strong>多个实体</strong>. 经过了上一步的Index Prediction, 我们应该获得了多个$\text{start}$ 和多个$\text{end}$ 的概率分布. </p><p>如果只是简单的遵循<strong>最近匹配</strong>$\text{start}$ 和$\text{end}$ 的原则, 是没有办法解决<strong>实体嵌套</strong>问题的. 因此, 作者考虑一个模型来专门做$\text{start}$ 和$\text{end}$ 的匹配.</p><p>先根据我们前面求得的概率分布$P_{\text{start}}$ 和$P_{\text{end}}$ 来确定$\text{start}$ 和$\text{end}$ 的位置:<br>$$<br>\begin{aligned}<br>&amp;\hat{I}_{\text {start }}=\left\{i \mid \operatorname{argmax}\left(P_{\text {start }}^{(i)}\right)=1, i=1, \cdots, n\right\} \\<br>&amp;\hat{I}_{\text {end }}=\left\{j \mid \operatorname{argmax}\left(P_{\text {end }}^{(j)}\right)=1, j=1, \cdots, n\right\}<br>\end{aligned}<br>$$</p><p>其中${}^{(i)}$ 代表矩阵的第$i$ 行.</p><p>对于任意的$i_{\text{start}} \in \hat{I}_{\text{start}}$ 和$i_{\text{end}} \in \hat{I}_{\text{end}}$, 都直接用一个神经网络接一个Sigmoid做二分类判断是否匹配:<br>$$<br>P_{i_{\text {start }}, j_{\text {end }}}=\operatorname{sigmoid}\left(m \cdot \operatorname{concat}\left(E_{i_{\text {start }}}, E_{j_{\text {end }}}\right)\right)<br>$$</p><p>$m \in \mathbb{R} ^{1 \times 2d}$ 为可学习权重.</p><h4 id="Train-and-Test"><a href="#Train-and-Test" class="headerlink" title="Train and Test"></a>Train and Test</h4><p>在训练时, 首先要计算选择起点和终点的损失:</p><p>$$<br>\begin{aligned}<br>\mathcal{L}_{\text {start }} &amp;=\mathrm{CE}\left(P_{\text {start}}, Y_{\text {start }}\right) \\<br>\mathcal{L}_{\text {end }} &amp;=\mathrm{CE}\left(P_{\text {end}}, Y_{\text {end }}\right)<br>\end{aligned}<br>$$</p><p>然后是匹配模型的Span匹配损失, 也是用交叉熵来衡量:</p><p>$$<br>\mathcal{L}_{\text {span }}=\operatorname{CE}\left(P_{\text {start}, \text { end }}, Y_{\text {start, end }}\right)<br>$$</p><p>最后联合优化这些损失:</p><p>$$<br>\mathcal{L}=\alpha \mathcal{L}_{\text {start }}+\beta \mathcal{L}_{\text {end }}+\gamma \mathcal{L}_{\text {span }}<br>$$<br>$\alpha, \beta, \gamma \in [0, 1]$, 均为超参, 来控制三个损失对任务总目标的贡献. </p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文.</p><blockquote><p>原文在实验部分的写作采用的是QA的方式, 这种方式很利于集中读者的注意力.</p></blockquote><h3 id="Nested-NER"><a href="#Nested-NER" class="headerlink" title="Nested NER"></a>Nested NER</h3><p>对于嵌套NER任务, 实验在英文数据集ACE 2004, ACE 2005, GENIA, KBP2017上进行, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mrc4ner3.jpg" style="zoom: 67%;" /><p>性能涨幅较大.</p><h3 id="Flat-NER"><a href="#Flat-NER" class="headerlink" title="Flat NER"></a>Flat NER</h3><p>对于Flat NER任务, 在英文数据集CoNLL 2003, OntoNotes 5.0, 中文数据集MSRA, OntoNotes 4.0 上进行, 实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mrc4ner4.jpg" style="zoom: 67%;" /><p>在Flat NER任务上也有一些提升, 结合嵌套NER任务, 此框架确实能很好的处理这两种情况.</p><p>BERT - Tagger就是把原始BERT拿来做NER任务, 后续其他实验也用这个Baseline做了对比,</p><h3 id="Ablation-studies"><a href="#Ablation-studies" class="headerlink" title="Ablation studies"></a>Ablation studies</h3><h4 id="Improvement-from-MRC-or-from-BERT"><a href="#Improvement-from-MRC-or-from-BERT" class="headerlink" title="Improvement from MRC or from BERT"></a>Improvement from MRC or from BERT</h4><p>作者将预训练的BERT和其他没有预训练的Baseline放在一起做了个比较:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mrc4ner5.jpg" style="zoom: 50%;" /><p>BiDAF和QAnet都是基于MRC方法的模型, 没有经过预训练, 但仍然比LSTM要强, 这证明了本框架的有效性. 再将预训练的BERT与没预训练的模型相比, BERT的性能要优于其他模型.</p><p>作者还做了一次Case Study, 将BERT的Attention Matrix展示了出来:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mrc4ner7.jpg" style="zoom: 67%;" /><p>输入句子中的<code>Flevland</code> 与问题中的<code>geographical</code>, <code>cites</code>, <code>state</code> 相关性都比较大.</p><h4 id="How-to-Construct-Queries"><a href="#How-to-Construct-Queries" class="headerlink" title="How to Construct Queries"></a>How to Construct Queries</h4><p>问题构建的方式十分重要, 作者尝试了多种构建的问题的方法, 以Tag <code>ORG</code> 为例:</p><ol><li><strong>Position index of labels</strong>: 问题直接由Tag的索引构成, 例如”one”, “two”, “three”.</li><li><strong>Keyword</strong>: 使用Tag描述中的关键词, 如”organization”.</li><li><strong>Rule - based template filling</strong>: 用模板生成问题, 例如”which organization is mentioned in the text”.</li><li><strong>Wikipedia</strong>: 使用维基百科中对Tag的定义, 例如”an organization is an entity comprising multiple people, such as an institution or an association”.</li><li><strong>Synonyms</strong>: 使用牛津词典中找到的Tag的同义词, 例如”association”.</li><li><strong>Keyword + Synonyms</strong>: 将关键词和同义词拼接起来.</li><li><strong>Annotation guideline notes</strong>: 本文的方法, 例如”find organizations including companies, agencies and institutions”.</li></ol><p>上述方法在English OntoNotes 5.0 上的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mrc4ner6.jpg" style="zoom: 60%;" /><p>只使用索引相当于添加噪声, 反而有负影响. 其他方法都能带来一定程度的信息提示作用, 作者所使用的方法效果最佳.</p><h4 id="Zero-shot-Evaluation-on-Unseen-Labels"><a href="#Zero-shot-Evaluation-on-Unseen-Labels" class="headerlink" title="Zero - shot Evaluation on Unseen Labels"></a>Zero - shot Evaluation on Unseen Labels</h4><p>为测试模型的Zero - shot能力, 作者在CoNLL 2003上训练, 在OntoNotes 5.0上做了测试, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mrc4ner8.jpg" style="zoom: 50%;" /><p>BERT - tagger在迁移后所剩的F1仅为31.87%, 与BERT - MRC相差甚远, 证明此框架有一定泛化能力.</p><h4 id="Size-of-Training-Data"><a href="#Size-of-Training-Data" class="headerlink" title="Size of Training Data"></a>Size of Training Data</h4><p>为证明Query中注入先验知识的重要性, 作者在OntoNotes 4.0上做了使用不同数据对模型影响的实验:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mrc4ner9.jpg" style="zoom: 67%;" /><p>BERT - MRC在使用50%数据时已经能达到BERT - Tagger使用100%数据的性能. </p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>作者从<strong>抽取式MRC</strong>的角度, 使用<strong>边界模型</strong>, <strong>同时解决</strong>了<strong>Flat NER</strong>和<strong>Nested NER</strong>问题, 并在实验部分强调了Query注入先验知识对模型的影响.</p><p>但伴随这种Tagging框架的<strong>稀疏问题</strong>也不能忽视, 只能用一些小Trick去缓解, 但不能从根本上解决问题.</p><blockquote><p>虽然本文是一篇面向NER的论文, 但实际上, 在RE与NER相结合的任务, <strong>关系三元组抽取</strong>(Relational Triple Extraction, RTE)中, 有相当深远的影响.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NER </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HypE: 知识图谱超图补全</title>
      <link href="/posts/21119.html"/>
      <url>/posts/21119.html</url>
      
        <content type="html"><![CDATA[<h1 id="Knowledge-Hypergraphs-Prediction-Beyond-Binary-Relations"><a href="#Knowledge-Hypergraphs-Prediction-Beyond-Binary-Relations" class="headerlink" title="Knowledge Hypergraphs: Prediction Beyond Binary Relations"></a>Knowledge Hypergraphs: Prediction Beyond Binary Relations</h1><p>本文是论<a href="https://www.aminer.cn/pub/5ef96b048806af6ef2772134?conf=ijcai2020?f=zh" target="_blank" rel="noopener">Knowledge Hypergraphs: Prediction Beyond Binary Relations</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>现有的知识图谱均以三元组的形式被存储, 即默认地将所有的关系以二元的形式表示. 但事实上, 在广为人知的FreeBase中, 有<strong>61%</strong>的关系都是<strong>非二元</strong>关系. </p><p>虽然现在有将多元关系转化为二元关系表示的方法, 但作者认为现有的KGE方法通过二元转换后做链接预测的效果都不是很好, 因为<strong>所有关系都被强假设为二元</strong>.</p><p>例如下图中描述的<code>flies_between</code>关系:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/hype1.jpg" style="zoom:50%;" /><p>被现有的多元关系转二元关系技术分解为如下形式:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/hype2.jpg" style="zoom:50%;" /><p>左侧转换并没有损失信息, 但添加了三种额外的实体. </p><p>右侧转换可能存在歧义, 转换后以二元关系的形式存在, <code>Air Canada</code>, <code>Montreal</code>, <code>Los Angeles</code>, 之间存在关系<code>flies_between</code>是合理的, 但单独看<code>Air Canada</code>, <code>New York</code>, <code>Los Angeles</code>也存在关系<code>flies_between</code>. 参考原图, <code>Air Canada</code>不可能从<code>New York</code>飞到<code>Los Angeles</code>, 但</p><p>因此, 作者希望提出基于<strong>超图</strong>的<strong>多元关系</strong>Knowledge Embedding方法.</p><h2 id="Hypergraph"><a href="#Hypergraph" class="headerlink" title="Hypergraph"></a>Hypergraph</h2><p>首先来介绍一下超图.</p><p>超图被描述为$H=(X, E)$, $X$ 是一个有限集合, 其中的元素被称为节点或顶点, $E$ 为$X$​ 的非空子集, 被称为超边或连接.</p><p>下面展示一个例子:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/hypergraph.png" style="zoom:67%;" /><blockquote><p>图片出自<a href="https://zh.wikipedia.org/wiki/%E8%B6%85%E5%9B%BE" target="_blank" rel="noopener">维基百科</a>.</p></blockquote><p>在该超图中, 有:<br>$$<br>\begin{aligned}<br>X&amp;=\{v_1, v_2, v_3, v_4, v_5, v_6, v_7\} \\\ \\<br>E&amp;=\{e_1, e_2, e_3, e_4\}\\<br>&amp;=\{\{v_1, v_2, v_3\}, \{v_2, v_3\}, \{v_3, v_5, v_6\}, \{v_4\}\}<br>\end{aligned}<br>$$<br>对于我们所熟知的图而言, 边只能与两个顶点相连. 在超图的体系下, 边称之为超边, 能够与任意个数的顶点相连, 普通图只是在超图中的一种每条边节点数量均为2的特殊情况, 因此, 超图理论与普通图是兼容的.</p><p>而Hypergraph本身结构的<strong>高度灵活</strong>性, 对于相同的现实生活中的场景, 可能有不同的描述方法和建模方法.</p><h3 id="Knowledge-HyperGraph-Completion"><a href="#Knowledge-HyperGraph-Completion" class="headerlink" title="Knowledge HyperGraph Completion"></a>Knowledge HyperGraph Completion</h3><p>如何把Knowledge Graph Completion扩展到超图中? </p><p>现实世界的实体集为$\mathcal{E}$​​, 有限关系集为$\mathcal{R}$​​, 区别在于超图中的元组不再是三元组, 而是可以容纳更多实体的元组$\tau = r(e_1, e_2, \dots, e_k), r\in \mathcal{R}, e_i \in \mathcal{E}$​​​. ​</p><p>在超图中, 每条超边就是一个$\tau$​​​, $|r|$​​ 为关系$r$​​​​ 所对应的<strong>实体数量</strong>, 可以设置为固定的.</p><p>我们所构建的知识超图应该是真实世界元组$\tau$ 的一个<strong>子集</strong>$\tau ^ \prime \subseteq \tau$, Knowledge HyperGraph Completion的目标就是利用$\tau ^ \prime$ 找到缺失的真实知识$\tau \backslash \tau^\prime$​.</p><h2 id="HypE"><a href="#HypE" class="headerlink" title="HypE"></a>HypE</h2><p>作者先定义了$\odot(\cdot)$ 为向量的逐元素点乘求和(其实就是向量<strong>内积</strong>):</p><p>$$<br>\odot\left(\mathbf{v}_{\mathbf{1}}, \mathbf{v}_{\mathbf{2}}, \ldots, \mathbf{v}_{\mathbf{k}}\right)=\sum_{i=1}^{\ell} \mathbf{v}_{\mathbf{1}}{ }^{(i)} \mathbf{v}_{\mathbf{2}}{ }^{(i)} \ldots \mathbf{v}_{\mathbf{k}}^{(i)}<br>$$</p><p>$\mathbf{v}_j^{(i)}$ 为$\mathbf{v}_j$ 的第$i$ 个元素.</p><h3 id="HSimplE"><a href="#HSimplE" class="headerlink" title="HSimplE"></a>HSimplE</h3><p>在SimplE中, 通过学习到实体$e$​ 分别在头实体和尾实体的两个不同位置的嵌入$\mathbf{e}^{(1)},  \mathbf{e}^{(2)}$​, 和关系及其逆关系的嵌入$\mathbf{r}^{(1)}, \mathbf{r}^{(2)}$​ 来求得三元组是否为真的得分:</p><p>$$<br>\phi\left(r\left(e_{1}, e_{2}\right)\right)=\odot\left(\mathbf{r}^{(\mathbf{1})}, \mathbf{e}_{\mathbf{1}}^{(\mathbf{1})}, \mathbf{e}_{\mathbf{2}}^{(\mathbf{2})}\right)+\odot\left(\mathbf{r}^{(\mathbf{2})}, \mathbf{e}_{\mathbf{2}}^{(\mathbf{1})}, \mathbf{e}_{\mathbf{1}}^{(\mathbf{2})}\right)<br>$$</p><p>作者认为, SimplE的核心在于将<strong>三元组形式</strong>中实体可能位于任何位置的表示方式都考虑到了, 作者在这种启发下将SimplE扩展到了超图上.</p><p>在<strong>超图</strong>中, 超边能连接任意数量的顶点, 我们不可能将每个实体在每个位置上都单独学习一个Embedding, 所以我们应该用一个单独的向量来代替, 而不是像SimplE一样使用多个向量. </p><blockquote><p>作者认为, 只使用单个$\mathbf{e}$ 能够被视为$\alpha$ 个不同位置出现的实体嵌入的拼接, 例如$\mathbf{e}=\operatorname{concat}(\mathbf{e}^{(\mathbf{1})} + \mathbf{e}^{(\mathbf{2})})$​.</p><p>我认为只要<strong>保证同实体在不同位置上的表示足够不同</strong>, 就能使用单个$\mathbf{e}$.</p></blockquote><p>作者使用了一个迂回的方法, 既然不能学习所有位置, 就简单的通过<strong>平移操作</strong>来改变不同位置上同一实体的表示. 每个实体的表示因位置不同而产生变化, 那么在元组$\tau$ 中, 打分函数为:</p><p>$$<br>\begin{aligned}<br>\phi\left(r\left(e_{i}, e_{j}, \ldots, e_{k}\right)\right) &amp;=\odot\left(\mathbf{r}, \mathbf{e}_{\mathbf{i}}, \operatorname{shift}\left(\mathbf{e}_{\mathbf{j}},<br>\operatorname{len}\left(\mathbf{e}_{\mathbf{j}}\right)\cdot\frac{1} {\alpha}\right), \ldots,\right.<br>\left.\left.\operatorname{shift}\left(\mathbf{e}_{\mathbf{k}}, \operatorname{len}\left(\mathbf{e}_{\mathbf{k}}\right) \cdot\frac{(\alpha-1)} { \alpha}\right)\right)\right)<br>\end{aligned}<br>$$</p><p>其中$\alpha=|r|$​ , $\text{shift}(\mathbf{v}, x)$​ 代表将$\mathbf{v}$​ 向左平移$x$​​​ 个单位, 并将多余的部分补到右侧(参考<a href="https://github.com/baharefatemi/HypE" target="_blank" rel="noopener">源码</a>).</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/hype3.jpg" style="zoom:50%;" /><h3 id="HypE-1"><a href="#HypE-1" class="headerlink" title="HypE"></a>HypE</h3><p> HSimplE的平移操作有点太简单了, 把HSimplE的Shift操作换成了<strong>卷积</strong>就是HypE.</p><p>对于实体$e$ 在关系$r$ 中可能存在的每个位置$i$, 都有卷积核$\omega$​ 对实体的Embedding $\mathbf{e}$​​  提取特征:<br>$$<br>f(\mathbf{e}, i)=\operatorname{concat}\left(\mathbf{e} \ast \omega_{\mathrm{i} 1}, \ldots, \mathbf{e} \ast \omega_{\mathrm{in}}\right) \mathrm{P}<br>$$<br>其中$P$ 为投影矩阵. $n$ 为卷积核个数, 即在位置$i$ 上有$n$​​​ 个不同的卷积核提取特征, 再将它们拼接到一起, 最后投影回某个维度:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/hype4.jpg" style="zoom:50%;"/><p>和HSimplE相似, 把关系嵌入, 不同位置上的不同实体嵌入在一起点乘, 作为元组得分:</p><p>$$<br>\phi\left(r\left(e_{1}, \ldots, e_{|r|}\right)\right)=\odot\left(\mathbf{r}, f\left(\mathbf{e}_{\mathbf{1}}, 1\right), \ldots, f\left(\mathbf{e}_{|\mathbf{r}|},|r|\right)\right)<br>$$</p><p>即:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/hype5.jpg" style="zoom: 50%;" /><p>作者认为, 使用位置特化的卷积核好处有二:</p><ol><li>由于实体Embedding的位置信息是由卷积核额外添加进去的, 而非包含于实体Embedding本身, 所以更利于实体Embedding变得与位置无关.</li><li>位置和实体的分离使得HypE能作用于任意数量实体的Knowledge base, 这额外给予了HypE更多的鲁棒性, 即使遇到从未见过的实体, 也能有点用处.</li></ol><h3 id="Objective-Function-and-Training"><a href="#Objective-Function-and-Training" class="headerlink" title="Objective Function and Training"></a>Objective Function and Training</h3><p>HSimplE和HypE都使用小批量梯度下降训练.</p><p>负例的生成是由TransE负例生成的思路扩展到超图而来, 对于每个元组, 生成$N|r|$ 个负例, $N$ 为负样本生成率(超参).</p><p>损失函数采用交叉熵(实际上应该是温度为1的InfoNCE, 但原文写的CE, 这里暂时尊重一下原文), 最大化正例元组的打分, 最小化负例元组的打分:</p><p>$$<br>\mathcal{L}(\mathbf{r}, \mathbf{e})=\sum_{x^{\prime} \in \tau_{\text {train }}^{\prime}}-\log \left(\frac{e^{\phi\left(x^{\prime}\right)}}{e^{\phi\left(x^{\prime}\right)}+\sum_{x \in T_{n e g}\left(x^{\prime}\right)} e^{\phi(x)}}\right)<br>$$</p><p>$x$ 为所有训练集, 验证集, 测试集元组$\tau^\prime$ 的某个元组, $\tau ^ \prime_\text{train}$ 为训练集, $T_{neg}(x^\prime)$​ 为生成的负例. </p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文.</p><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>JF - 17K是前人提出的数据集, 但没有验证集, 作者随机选了20%作为验证集.</p><p>此外, 作者从FreeBase中创建了新的数据集FB - AUTO, M - FB15K. 详细的数据集创建请参考原论文附录部分. 它们的统计信息如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/hype7.jpg" style="zoom:50%;" /><h3 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h3><p>因为在超图补全上的模型很少, 所以需要抓一些之前的方法扩展到超图情况:</p><ul><li>对于处理二元关系的方法, 作者将其扩展到更多实体的情况, 如r - SimplE, m - DistMult, m - CP. </li><li>也有一些不用扩展的方法, 能直接处理更多实体, 如m - TransH.</li></ul><h3 id="Knowledge-HyperGraph-Completion-Results"><a href="#Knowledge-HyperGraph-Completion-Results" class="headerlink" title="Knowledge HyperGraph Completion Results"></a>Knowledge HyperGraph Completion Results</h3><p>HypE在这几个超图补全数据集上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/hype6.jpg" style="zoom:50%;" /><p>HypE相较于其他方法有显著提升.</p><p>因为位置特化的卷积核能将实体本身的信息和位置信息解耦, 所以作者认为HypE应该具有更高的<strong>参数利用率</strong>. 作者做了MRR随Embedding Dimensionss变化的曲线, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/hype8.jpg" style="zoom: 67%;" /><p>HypE在200维度以下时, 是完全优于Baseline HSimplE的.</p><p>HypE使用了位置特化的卷积核, 应该能处理更多实体出现在不同位置上的情况, 作者创建一个<strong>缺失位置</strong>的测试集, 其中的每个元组至少有一个实体在某个位置没有出现过, 这种情况更具挑战性. 与其他方法对比结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/hype9.jpg" style="zoom: 67%;" /><p>其他方法在缺失位置信息的情况下表现很差, 相较来说HypE确实能处理更多这种挑战性的情况, 也表明HypE确实受益于位置和实体信息解耦的设计.</p><h3 id="Knowledge-Graph-Completion-Results"><a href="#Knowledge-Graph-Completion-Results" class="headerlink" title="Knowledge Graph Completion Results"></a>Knowledge Graph Completion Results</h3><p>在现有的三元组形式数据集WN18和FB15k上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/hype10.jpg" style="zoom:50%;" /><p>HSimplE取得了相当好的表现, HypE也与其相似. </p><blockquote><p>由于这两个数据集上包含大量的逆关系导致Data Leakage存在, 使用WN18RR和FB15k-237更有说服力. 但这个实验说明面向超图的任务优化有希望提升三元组形式的数据集链接预测性能.</p></blockquote><h3 id="Ablation-Study-on-Different-Arities"><a href="#Ablation-Study-on-Different-Arities" class="headerlink" title="Ablation Study on Different Arities"></a>Ablation Study on Different Arities</h3><p>作者还在JF17K上将超边设置为不同的实体数量, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/hype11.jpg" style="zoom:50%;" /><p>超边实体数量比较多时, HSimplE表现相较于HypE来说并不好, 超边实体数量较少时, HSimplE效果比HypE稍好. 总体上来说作者提出的两种方法效果略好.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>在知识超图上的研究还比较少, 本文提出了在超图架构下的两个新数据集, 并给出了将处理三元组KGE问题迁移到超图框架下的方法.</p><p>其实与某些图游走方法的想法有一些相似之处, 显式指明多实体之间存在的关系, 而非只考虑两实体, 这些额外的附加信息可以帮助推理. 但图游走类算法更加随意, 在知识超图上的游走仅限于同关系下的实体集.</p><p>总感觉这篇论文有哪块没理解, 尤其是附录, 有机会回看.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch实现: VAE</title>
      <link href="/posts/9047.html"/>
      <url>/posts/9047.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>VAE基本原理: 详见<a href="https://adaning.github.io/posts/53598.html">变分自编码器入门</a>.</li></ul></blockquote><h1 id="Pytorch实现-VAE"><a href="#Pytorch实现-VAE" class="headerlink" title="Pytorch实现: VAE"></a>Pytorch实现: VAE</h1><p>本文是VAE的Pytorch版本实现, 并在末尾做了VAE的生成可视化.</p><p>本文的代码已经放到了Colab上, 打开设置GPU就可以复现(需要科学上网).</p><p><a href="https://colab.research.google.com/drive/1ZhmA2XxJ3oZC7A-U2mpUdB2eZZLz5NfW?usp=sharing" target="_blank" rel="noopener"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="右键我在COLAB中打开!"></a></p><p>如果你不能科学上网, 应该看不到<code>Open in Colab</code>的图标.</p><h2 id="Preparing"><a href="#Preparing" class="headerlink" title="Preparing"></a>Preparing</h2><p>先导包:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">from</span> torch <span class="token keyword">import</span> optim<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> transforms<span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> MNIST<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h2><p>CNN在MNIST上有过于明显的优势, 我们只采用纯DNN来做Auto Encoder.</p><p>随手搞一个网络结构出来就行:</p><ul><li>输入层维度: <code>input_dim = 784</code>.</li><li>过渡层维度: <code>inter_dim = 256</code>.</li><li>隐变量维度: <code>latent_dim = 2</code>, 方便后续可视化.</li></ul><pre class="line-numbers language-python"><code class="language-python">latent_dim <span class="token operator">=</span> <span class="token number">2</span>input_dim <span class="token operator">=</span> <span class="token number">28</span> <span class="token operator">*</span> <span class="token number">28</span>inter_dim <span class="token operator">=</span> <span class="token number">256</span><span class="token keyword">class</span> <span class="token class-name">VAE</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_dim<span class="token operator">=</span>input_dim<span class="token punctuation">,</span> inter_dim<span class="token operator">=</span>inter_dim<span class="token punctuation">,</span> latent_dim<span class="token operator">=</span>latent_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>VAE<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        elf<span class="token punctuation">.</span>encoder <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> inter_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>inter_dim<span class="token punctuation">,</span> latent_dim <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span>  nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>latent_dim<span class="token punctuation">,</span> inter_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>inter_dim<span class="token punctuation">,</span> input_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">reparameterize</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> mu<span class="token punctuation">,</span> logvar<span class="token punctuation">)</span><span class="token punctuation">:</span>        epsilon <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn_like<span class="token punctuation">(</span>mu<span class="token punctuation">)</span>        <span class="token keyword">return</span> mu <span class="token operator">+</span> epsilon <span class="token operator">*</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>logvar <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        org_size <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>        batch <span class="token operator">=</span> org_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        h <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        mu<span class="token punctuation">,</span> logvar <span class="token operator">=</span> h<span class="token punctuation">.</span>chunk<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        z <span class="token operator">=</span> self<span class="token punctuation">.</span>reparameterize<span class="token punctuation">(</span>mu<span class="token punctuation">,</span> logvar<span class="token punctuation">)</span>        recon_x <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>z<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>size<span class="token operator">=</span>org_size<span class="token punctuation">)</span>        <span class="token keyword">return</span> recon_x<span class="token punctuation">,</span> mu<span class="token punctuation">,</span> logvar<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>说一点细节: </p><ul><li><p>Encoder和Decoder用<code>nn.Sequential</code>的形式写, 方便后续直接使用decoder.</p></li><li><p>$p(Z\mid X_k)$ 的均值$\mu$ 和方差$\sigma^2$ 的形式上可以拆成两个小的DNN得出, 这里用一个DNN得出, 然后通过<code>torch.chunk</code>函数将均值和方差分开, 实际上是和前者等价的.</p></li><li><p>Encoder末尾千万别像网上某些例子在再接一个ReLU. 在优化过程中, 我们的隐变量$Z$ 是要逐渐趋向于$\mathcal{N}(0, I)$ 的, 如果非要加个ReLU的话, 本身假设的隐变量维度就很小, 小于0的隐变量直接就没了… Decoder在解码时直接就会因为信息不足而崩掉.</p></li><li><p>我们在这里拟合的是$\log \sigma^2$ 而不是$\sigma^2$, 所以重参数方差的表示法是<code>torch.exp(logvar / 2)</code>.</p></li></ul><h2 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h2><p>VAE的损失由<strong>重构损失</strong>和<strong>KL损失</strong>组成.</p><p>KL散度就不再推导了, 直接放结果:<br>$$<br>KL\Big(N(\mu,\sigma^2)\Big\Vert N(0,1)\Big)=\frac{1}{2}\Big(-\log \sigma^2+\mu^2+\sigma^2-1\Big)<br>$$<br>VAE的目标是<strong>最小化</strong>$Z$ 和$N(0, 1)$ 之间的KL散度, 代码只需要照着写就行了:</p><pre class="line-numbers language-python"><code class="language-python">kl_loss <span class="token operator">=</span> <span class="token keyword">lambda</span> mu<span class="token punctuation">,</span> logvar<span class="token punctuation">:</span> <span class="token operator">-</span><span class="token number">0.5</span> <span class="token operator">*</span> torch<span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> logvar <span class="token operator">-</span> mu<span class="token punctuation">.</span>pow<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">-</span> logvar<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>recon_loss <span class="token operator">=</span> <span class="token keyword">lambda</span> recon_x<span class="token punctuation">,</span> x<span class="token punctuation">:</span> F<span class="token punctuation">.</span>binary_cross_entropy<span class="token punctuation">(</span>recon_x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> size_average<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>因为MNIST是<strong>黑白二值图像</strong>, 所以的Decoder就可以用Sigmoid后的值当做灰度, 重构损失直接就用<strong>BCE</strong>了, 用MSE做重构损失尚可. 但如果是三通道图像或者是灰度图像, 还是必须使用MSE做重构损失.</p><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>先定义好训练的<code>epoch</code>和<code>batch_size</code>, 优化器随便选一个世界上最好的优化器<code>Adam(lr=1e-3)</code>:</p><pre class="line-numbers language-python"><code class="language-python">epochs <span class="token operator">=</span> <span class="token number">100</span>batch_size <span class="token operator">=</span> <span class="token number">128</span>transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>data_train <span class="token operator">=</span> MNIST<span class="token punctuation">(</span><span class="token string">'MNIST_DATA/'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>data_valid <span class="token operator">=</span> MNIST<span class="token punctuation">(</span><span class="token string">'MNIST_DATA/'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>train_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>data_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>test_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>data_valid<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">'cpu'</span><span class="token punctuation">)</span>model <span class="token operator">=</span> VAE<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> inter_dim<span class="token punctuation">,</span> latent_dim<span class="token punctuation">)</span>model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>训练的代码就不详细说了, 和一般的训练过程并无二异, 每次测试时最好把损失的两项都打印出来观察一下:</p><pre class="line-numbers language-python"><code class="language-python">best_loss <span class="token operator">=</span> <span class="token number">1e9</span>best_epoch <span class="token operator">=</span> <span class="token number">0</span>valid_losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>train_losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"Epoch {epoch}"</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>    train_loss <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>    train_num <span class="token operator">=</span> len<span class="token punctuation">(</span>train_loader<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span>    <span class="token keyword">for</span> idx<span class="token punctuation">,</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span> _<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>        batch <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        recon_x<span class="token punctuation">,</span> mu<span class="token punctuation">,</span> logvar <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        recon <span class="token operator">=</span> recon_loss<span class="token punctuation">(</span>recon_x<span class="token punctuation">,</span> x<span class="token punctuation">)</span>        kl <span class="token operator">=</span> kl_loss<span class="token punctuation">(</span>mu<span class="token punctuation">,</span> logvar<span class="token punctuation">)</span>        loss <span class="token operator">=</span> recon <span class="token operator">+</span> kl        train_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss <span class="token operator">=</span> loss <span class="token operator">/</span> batch        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> idx <span class="token operator">%</span> <span class="token number">100</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"Training loss {loss: .3f} \t Recon {recon / batch: .3f} \t KL {kl / batch: .3f} in Step {idx}"</span><span class="token punctuation">)</span>    train_losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>train_loss <span class="token operator">/</span> train_num<span class="token punctuation">)</span>    valid_loss <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>    valid_recon <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>    valid_kl <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>    valid_num <span class="token operator">=</span> len<span class="token punctuation">(</span>test_loader<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span>    model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> idx<span class="token punctuation">,</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span> _<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>test_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>            x <span class="token operator">=</span> x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>            recon_x<span class="token punctuation">,</span> mu<span class="token punctuation">,</span> logvar <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>            recon <span class="token operator">=</span> recon_loss<span class="token punctuation">(</span>recon_x<span class="token punctuation">,</span> x<span class="token punctuation">)</span>            kl <span class="token operator">=</span> kl_loss<span class="token punctuation">(</span>mu<span class="token punctuation">,</span> logvar<span class="token punctuation">)</span>            loss <span class="token operator">=</span> recon <span class="token operator">+</span> kl            valid_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            valid_kl <span class="token operator">+=</span> kl<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            valid_recon <span class="token operator">+=</span> recon<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        valid_losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>valid_loss <span class="token operator">/</span> valid_num<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"Valid loss {valid_loss / valid_num: .3f} \t Recon {valid_recon / valid_num: .3f} \t KL {valid_kl / valid_num: .3f} in epoch {epoch}"</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> valid_loss <span class="token operator">&lt;</span> best_loss<span class="token punctuation">:</span>            best_loss <span class="token operator">=</span> valid_loss            best_epoch <span class="token operator">=</span> epoch            torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'best_model_mnist'</span><span class="token punctuation">)</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Model saved"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>觉得Loss位数保留太多的可以自己设置.</p><p>下面画出训练过程中训练集和验证集上的损失曲线:</p><pre class="line-numbers language-python"><code class="language-python">plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>train_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Train'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>valid_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Valid'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Learning Curve'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>训练曲线如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vae11.png" style="zoom: 67%;" /><p>基本上Valid Loss稳定了(其实还有下降空间). 同时要保存在验证集上结果最好的模型, 因为等会还要用最好的模型做生成.</p><h2 id="Visualization"><a href="#Visualization" class="headerlink" title="Visualization"></a>Visualization</h2><p>再导俩包:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> scipy<span class="token punctuation">.</span>stats <span class="token keyword">import</span> norm<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><code>norm</code>可以在隐变量的区域内按照正态分布采样.</p><pre class="line-numbers language-python"><code class="language-python">state <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'best_model_mnist'</span><span class="token punctuation">)</span>model <span class="token operator">=</span> VAE<span class="token punctuation">(</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>state<span class="token punctuation">)</span>n <span class="token operator">=</span> <span class="token number">20</span>digit_size <span class="token operator">=</span> <span class="token number">28</span>grid_x <span class="token operator">=</span> norm<span class="token punctuation">.</span>ppf<span class="token punctuation">(</span>np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0.05</span><span class="token punctuation">,</span> <span class="token number">0.95</span><span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>grid_y <span class="token operator">=</span> norm<span class="token punctuation">.</span>ppf<span class="token punctuation">(</span>np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0.05</span><span class="token punctuation">,</span> <span class="token number">0.95</span><span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>figure <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>digit_size <span class="token operator">*</span> n<span class="token punctuation">,</span> digit_size <span class="token operator">*</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">for</span> i<span class="token punctuation">,</span> yi <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>grid_y<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> j<span class="token punctuation">,</span> xi <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>grid_x<span class="token punctuation">)</span><span class="token punctuation">:</span>        t <span class="token operator">=</span> <span class="token punctuation">[</span>xi<span class="token punctuation">,</span> yi<span class="token punctuation">]</span>        z_sampled <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span>t<span class="token punctuation">)</span>        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            decode <span class="token operator">=</span> model<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>z_sampled<span class="token punctuation">)</span>            digit <span class="token operator">=</span> decode<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token punctuation">(</span>digit_size<span class="token punctuation">,</span> digit_size<span class="token punctuation">)</span><span class="token punctuation">)</span>            figure<span class="token punctuation">[</span>                i <span class="token operator">*</span> digit_size<span class="token punctuation">:</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> digit_size<span class="token punctuation">,</span>                j <span class="token operator">*</span> digit_size<span class="token punctuation">:</span> <span class="token punctuation">(</span>j <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> digit_size            <span class="token punctuation">]</span> <span class="token operator">=</span> digitplt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>figure<span class="token punctuation">,</span> cmap<span class="token operator">=</span><span class="token string">"Greys_r"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xticks<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>yticks<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>axis<span class="token punctuation">(</span><span class="token string">'off'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>采样到$Z$ 后再给Decoder解码, 生成的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vae12.png" style="zoom: 80%;" />对于MNIST这样简单的数据集, 隐变量的某些区域已经能完成生成任务. 并且可以从图中观察到随着隐变量的变化对生成结果产生的影响.<p>从图中能够很明确的看到手写数字种类的过渡, 例如长的比较像的1, 9, 7, 都带圆弧的8, 3, 5, 再到6, 0. 但是VAE生成的内容有点点糊, 在MNSIT上影响不大, 但扩展到三通道数据时, 这个问题会变得更为显著.</p><h2 id="Pokemon"><a href="#Pokemon" class="headerlink" title="Pokemon!"></a>Pokemon!</h2><p><del>每个人都想做从零开始的宝可梦训练大师!</del> 在李宏毅老师的课程中层提到过用VAE生成神奇宝贝的事情. 下面就来尝试下. 数据集下载<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Pokemon_creation/image.rar" target="_blank" rel="noopener">点我</a>, 原始数据大小为(3, 40, 40). 本节代码没有放到Colab上, 与在MNIST上的过程大同小异, 感兴趣可以自己尝试.</p><p>这次的VAE就该用CNN了, DNN有点力不从心.</p><p>下述代码不做过多的解读, 结果也不是太好, 大家就当看个乐子.</p><p>导包:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> os<span class="token keyword">import</span> random<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> torch <span class="token keyword">import</span> optim<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader<span class="token punctuation">,</span> Dataset<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> transforms<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这次涉及到建立和读取图像数据集, 所以额外导了一些包.</p><p>然后建立图像数据集, 因为是无监督数据集, 所以比较简单:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Pokemon</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> root<span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Pokemon<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>root <span class="token operator">=</span> root        self<span class="token punctuation">.</span>image_path <span class="token operator">=</span> <span class="token punctuation">[</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>root<span class="token punctuation">,</span> x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> os<span class="token punctuation">.</span>listdir<span class="token punctuation">(</span>root<span class="token punctuation">)</span><span class="token punctuation">]</span>        random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>self<span class="token punctuation">.</span>image_path<span class="token punctuation">)</span>        <span class="token keyword">if</span> transform <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>transform <span class="token operator">=</span> transform        <span class="token keyword">if</span> train<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>images <span class="token operator">=</span> self<span class="token punctuation">.</span>image_path<span class="token punctuation">[</span><span class="token punctuation">:</span> int<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">8</span> <span class="token operator">*</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>image_path<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>images <span class="token operator">=</span> self<span class="token punctuation">.</span>image_path<span class="token punctuation">[</span>int<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">8</span> <span class="token operator">*</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>image_path<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token punctuation">]</span>    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>images<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>self<span class="token punctuation">.</span>images<span class="token punctuation">[</span>item<span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>数据集中实际上存储的是图像文件的路径, 在需要使用的时候再读出来, 我们将这一Pipeline集成在<code>transform</code>中.</p><p>接着定义CNN下的VAE:</p><pre class="line-numbers language-python"><code class="language-python">latent_dim <span class="token operator">=</span> <span class="token number">32</span>inter_dim <span class="token operator">=</span> <span class="token number">128</span>mid_dim <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>mid_num <span class="token operator">=</span> <span class="token number">1</span><span class="token keyword">for</span> i <span class="token keyword">in</span> mid_dim<span class="token punctuation">:</span>    mid_num <span class="token operator">*=</span> i<span class="token keyword">class</span> <span class="token class-name">ConvVAE</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> latent<span class="token operator">=</span>latent_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>ConvVAE<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>mid_num<span class="token punctuation">,</span> inter_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>inter_dim<span class="token punctuation">,</span> latent <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fcr2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>latent<span class="token punctuation">,</span> inter_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fcr1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>inter_dim<span class="token punctuation">,</span> mid_num<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">reparameterize</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> mu<span class="token punctuation">,</span> logvar<span class="token punctuation">)</span><span class="token punctuation">:</span>        epsilon <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn_like<span class="token punctuation">(</span>mu<span class="token punctuation">)</span>        <span class="token keyword">return</span> mu <span class="token operator">+</span> epsilon <span class="token operator">*</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>logvar <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        batch <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        h <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        mu<span class="token punctuation">,</span> logvar <span class="token operator">=</span> h<span class="token punctuation">.</span>chunk<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        z <span class="token operator">=</span> self<span class="token punctuation">.</span>reparameterize<span class="token punctuation">(</span>mu<span class="token punctuation">,</span> logvar<span class="token punctuation">)</span>        decode <span class="token operator">=</span> self<span class="token punctuation">.</span>fcr2<span class="token punctuation">(</span>z<span class="token punctuation">)</span>        decode <span class="token operator">=</span> self<span class="token punctuation">.</span>fcr1<span class="token punctuation">(</span>decode<span class="token punctuation">)</span>        recon_x <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>decode<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token operator">*</span>mid_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> recon_x<span class="token punctuation">,</span> mu<span class="token punctuation">,</span> logvar<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>结构很随意, 主要是为了满足输入和解码后的大小相同.</p><p>定义Loss, 仍然是重构损失和KL损失:</p><pre class="line-numbers language-python"><code class="language-python">kl_loss <span class="token operator">=</span> <span class="token keyword">lambda</span> mu<span class="token punctuation">,</span> logvar<span class="token punctuation">:</span> <span class="token operator">-</span><span class="token number">0.5</span> <span class="token operator">*</span> torch<span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> logvar <span class="token operator">-</span> mu<span class="token punctuation">.</span>pow<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">-</span> logvar<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>recon_loss <span class="token operator">=</span> <span class="token keyword">lambda</span> recon_x<span class="token punctuation">,</span> x<span class="token punctuation">:</span> F<span class="token punctuation">.</span>mse_loss<span class="token punctuation">(</span>recon_x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> size_average<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>重构损失使用MSE, 不能再使用BCE了, 因为RGB图像的数值不是二值的.</p><p>接下来是训练前的一些定义:</p><pre class="line-numbers language-python"><code class="language-python">epochs <span class="token operator">=</span> <span class="token number">2000</span>batch_size <span class="token operator">=</span> <span class="token number">512</span>best_loss <span class="token operator">=</span> <span class="token number">1e9</span>best_epoch <span class="token operator">=</span> <span class="token number">0</span>valid_losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>train_losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>    <span class="token keyword">lambda</span> x<span class="token punctuation">:</span> Image<span class="token punctuation">.</span>open<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>convert<span class="token punctuation">(</span><span class="token string">'RGB'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">]</span><span class="token punctuation">)</span>pokemon_train <span class="token operator">=</span> Pokemon<span class="token punctuation">(</span><span class="token string">'./pokemon/'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>pokemon_valid <span class="token operator">=</span> Pokemon<span class="token punctuation">(</span><span class="token string">'./pokemon/'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>train_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>pokemon_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>test_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>pokemon_valid<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">'cpu'</span><span class="token punctuation">)</span>model <span class="token operator">=</span> ConvVAE<span class="token punctuation">(</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>transform</code>将图像从路径中读取出来, 并通过<code>transforms.ToTensor</code>转换为<strong>0, 1之间</strong>的RGB值.</p><p>然后就开始训练, 和在MNIST上的代码相同:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"Epoch {epoch}"</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>    train_loss <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>    train_num <span class="token operator">=</span> len<span class="token punctuation">(</span>train_loader<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span>    <span class="token keyword">for</span> idx<span class="token punctuation">,</span> x <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>        batch <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        recon_x<span class="token punctuation">,</span> mu<span class="token punctuation">,</span> logvar <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        recon <span class="token operator">=</span> recon_loss<span class="token punctuation">(</span>recon_x<span class="token punctuation">,</span> x<span class="token punctuation">)</span>        kl <span class="token operator">=</span> kl_loss<span class="token punctuation">(</span>mu<span class="token punctuation">,</span> logvar<span class="token punctuation">)</span>        loss <span class="token operator">=</span> recon <span class="token operator">+</span> kl        train_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss <span class="token operator">=</span> loss <span class="token operator">/</span> batch        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> idx <span class="token operator">%</span> <span class="token number">100</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"Training loss {loss: .3f} \t Recon {recon / batch: .3f} \t KL {kl / batch: .3f} in Step {idx}"</span><span class="token punctuation">)</span>    train_losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>train_loss <span class="token operator">/</span> train_num<span class="token punctuation">)</span>    valid_loss <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>    valid_recon <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>    valid_kl <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>    valid_num <span class="token operator">=</span> len<span class="token punctuation">(</span>test_loader<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span>    model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> idx<span class="token punctuation">,</span> x <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>test_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>            x <span class="token operator">=</span> x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>            recon_x<span class="token punctuation">,</span> mu<span class="token punctuation">,</span> logvar <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>            recon <span class="token operator">=</span> recon_loss<span class="token punctuation">(</span>recon_x<span class="token punctuation">,</span> x<span class="token punctuation">)</span>            kl <span class="token operator">=</span> kl_loss<span class="token punctuation">(</span>mu<span class="token punctuation">,</span> logvar<span class="token punctuation">)</span>            loss <span class="token operator">=</span> recon <span class="token operator">+</span> kl            valid_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            valid_kl <span class="token operator">+=</span> kl<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            valid_recon <span class="token operator">+=</span> recon<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        valid_losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>valid_loss <span class="token operator">/</span> valid_num<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>            f<span class="token string">"Valid loss {valid_loss / valid_num: .3f} \t Recon {valid_recon / valid_num: .3f} \t KL {valid_kl / valid_num: .3f} in epoch {epoch}"</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> valid_loss <span class="token operator">&lt;</span> best_loss<span class="token punctuation">:</span>            best_loss <span class="token operator">=</span> valid_loss            best_epoch <span class="token operator">=</span> epoch            torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'best_model_pokemon'</span><span class="token punctuation">)</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Model saved"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>训练完VAE后, 对VAE学到的生成能力进行探索. 继续导入:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> scipy<span class="token punctuation">.</span>stats <span class="token keyword">import</span> norm<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>因为这次隐变量维度<code>latent_dim = 32</code>, 不能再一次性的将所有维度采样看VAE的生成结果. 因此, 我打算选定一个维度和其他维度组合, 观察两两组合的维度产生的效果. 为了让结果更多变些, 我打算直接让其他隐变量也随机改变:</p><pre class="line-numbers language-python"><code class="language-python">state <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'best_model_pokemon'</span><span class="token punctuation">)</span>model <span class="token operator">=</span> ConvVAE<span class="token punctuation">(</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>state<span class="token punctuation">)</span>n <span class="token operator">=</span> <span class="token number">10</span>image_size <span class="token operator">=</span> <span class="token number">40</span>grid_x <span class="token operator">=</span> norm<span class="token punctuation">.</span>ppf<span class="token punctuation">(</span>np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0.05</span><span class="token punctuation">,</span> <span class="token number">0.95</span><span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>grid_y <span class="token operator">=</span> norm<span class="token punctuation">.</span>ppf<span class="token punctuation">(</span>np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0.05</span><span class="token punctuation">,</span> <span class="token number">0.95</span><span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>selected <span class="token operator">=</span> <span class="token number">21</span>coll <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>selected<span class="token punctuation">,</span> i<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>latent_dim<span class="token punctuation">)</span> <span class="token keyword">if</span> i <span class="token operator">!=</span> selected<span class="token punctuation">]</span><span class="token keyword">for</span> idx<span class="token punctuation">,</span> <span class="token punctuation">(</span>p<span class="token punctuation">,</span> q<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>coll<span class="token punctuation">)</span><span class="token punctuation">:</span>    figure <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> image_size <span class="token operator">*</span> n<span class="token punctuation">,</span> image_size <span class="token operator">*</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> yi <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>grid_y<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> j<span class="token punctuation">,</span> xi <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>grid_x<span class="token punctuation">)</span><span class="token punctuation">:</span>            t <span class="token operator">=</span> <span class="token punctuation">[</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>latent_dim<span class="token punctuation">)</span><span class="token punctuation">]</span>            t<span class="token punctuation">[</span>p<span class="token punctuation">]</span><span class="token punctuation">,</span> t<span class="token punctuation">[</span>q<span class="token punctuation">]</span> <span class="token operator">=</span> xi<span class="token punctuation">,</span> yi            z_sampled <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span>t<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>            <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                decode <span class="token operator">=</span> model<span class="token punctuation">.</span>fcr1<span class="token punctuation">(</span>model<span class="token punctuation">.</span>fcr2<span class="token punctuation">(</span>z_sampled<span class="token punctuation">)</span><span class="token punctuation">)</span>                decode <span class="token operator">=</span> decode<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">*</span>mid_dim<span class="token punctuation">)</span>                decode <span class="token operator">=</span> model<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>decode<span class="token punctuation">)</span>                decode <span class="token operator">=</span> decode<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>                figure<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span>                i <span class="token operator">*</span> image_size<span class="token punctuation">:</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> image_size<span class="token punctuation">,</span>                j <span class="token operator">*</span> image_size<span class="token punctuation">:</span> <span class="token punctuation">(</span>j <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> image_size                <span class="token punctuation">]</span> <span class="token operator">=</span> decode    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"X: {}, Y: {}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>p<span class="token punctuation">,</span> q<span class="token punctuation">)</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>xticks<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>yticks<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>axis<span class="token punctuation">(</span><span class="token string">'off'</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>figure<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>生成效果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vae13.png" style="zoom: 150%;" /><p>就是生成结果太糊了, 但能看出来左上角这玩意的轮廓明显像沼跃鱼没进化时候的水跃鱼:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vae15.png" style="zoom:33%;" /><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vae14.png" style="zoom:150%;" /><p>右上角和左上角还有左下角可能生成的像个什么东西… 总体来说生成的效果不是很好, 非常糊, 看着感觉跟发育未完全的胚胎似的. </p><p>因为隐变量维数实在是太多了, 或许我们可以尝试更好点的办法, 找某一个神奇宝贝作为<strong>基准</strong>, 由编码器编码后得到一个现成的均值和方差, 然后再对某两个维度进行调整, 生成的结果会更贴近选定的神奇宝贝一些, 也就是使生成的结果更加合理一些.</p><p>只需要在生成隐变量时不再随机:</p><pre class="line-numbers language-python"><code class="language-python">image_path <span class="token operator">=</span> <span class="token string">'./pokemon/025MS.png'</span><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    base <span class="token operator">=</span> transform<span class="token punctuation">(</span>image_path<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> model<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>base<span class="token punctuation">)</span>    x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> model<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    h <span class="token operator">=</span> model<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    mu<span class="token punctuation">,</span> logvar <span class="token operator">=</span> h<span class="token punctuation">.</span>chunk<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>    z <span class="token operator">=</span> model<span class="token punctuation">.</span>reparameterize<span class="token punctuation">(</span>mu<span class="token punctuation">,</span> logvar<span class="token punctuation">)</span>    z <span class="token operator">=</span> z<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token keyword">for</span> idx<span class="token punctuation">,</span> <span class="token punctuation">(</span>p<span class="token punctuation">,</span> q<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>coll<span class="token punctuation">)</span><span class="token punctuation">:</span>    figure <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> image_size <span class="token operator">*</span> n<span class="token punctuation">,</span> image_size <span class="token operator">*</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> yi <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>grid_y<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> j<span class="token punctuation">,</span> xi <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>grid_x<span class="token punctuation">)</span><span class="token punctuation">:</span>            z<span class="token punctuation">[</span>p<span class="token punctuation">]</span><span class="token punctuation">,</span> z<span class="token punctuation">[</span>q<span class="token punctuation">]</span> <span class="token operator">=</span> xi<span class="token punctuation">,</span> yi            z_sampled <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span>z<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>'<span class="token comment" spellcheck="true"># ......</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>下面相同的部分被我省略掉了. 我们选定皮卡丘作为基准, 生成结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vae16.png" style="zoom:150%;" /><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vae17.png" style="zoom:150%;" /><p>有时可以维持住皮卡丘的基本形状. 但随着某些隐变量的变化, 逐渐变得混沌, 甚至换了一个物种:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vae18.png" style="zoom:150%;" /><p>还是有点看不清, 如果裁剪图片到(3, 20, 20)效果可能会好一点, 重新搭建一种更小尺寸的VAE模型:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">ConvVAE</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> latent<span class="token operator">=</span>latent_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>ConvVAE<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>mid_num<span class="token punctuation">,</span> inter_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>inter_dim<span class="token punctuation">,</span> latent <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fcr2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>latent<span class="token punctuation">,</span> inter_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fcr1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>inter_dim<span class="token punctuation">,</span> mid_num<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>然后向<code>transform</code>中添加裁剪:</p><pre class="line-numbers language-python"><code class="language-python">transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>    <span class="token keyword">lambda</span> x<span class="token punctuation">:</span> Image<span class="token punctuation">.</span>open<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>convert<span class="token punctuation">(</span><span class="token string">'RGB'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    transforms<span class="token punctuation">.</span>CenterCrop<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>将<code>image_size</code>设置为图像裁剪后的大小20, 其余代码全部不用动. 重新Train完模型, 我们依旧选择皮卡丘作为基准, 继续生成:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vae19.png" alt=""></p><p>开始也能维持住皮卡丘的基本样貌, 但多了一丝混沌的气息. 随着其他隐变量的变化, 皮卡丘长得越来越像其他的生物:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vae20.png" alt=""></p><p>继续演变, 甚至变成了右下角的某种东西:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vae21.png" alt=""></p><p>感兴趣的可以自己Train一个模型, 自己探索一下. 结果不太好可能是我搭的模型有点太随意了…</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> VAE </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introduction: Variational Auto - Encoder</title>
      <link href="/posts/53598.html"/>
      <url>/posts/53598.html</url>
      
        <content type="html"><![CDATA[<h1 id="Introduction-Variational-Auto-Encoder"><a href="#Introduction-Variational-Auto-Encoder" class="headerlink" title="Introduction: Variational Auto - Encoder"></a>Introduction: <strong>V</strong>ariational <strong>A</strong>uto - <strong>E</strong>ncoder</h1><p>变分自动编码器(<strong>VAE</strong>, <strong>V</strong>ariational <strong>A</strong>uto - <strong>E</strong>ncoder)是一种基于自编码器结构的<strong>深度生成模型</strong>.</p><p>本文对VAE更深层次的数学原理没有探讨, 一般概率基础即可放心食用, 更深层次的数学原理在文末深入阅读处给出.  </p><p>VAE与GAN有非常紧密的关系, GAN之后找个机会细说(先挖坑).</p><h2 id="自编码器"><a href="#自编码器" class="headerlink" title="自编码器"></a>自编码器</h2><blockquote><p>本节图片全部出自<a href="https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798" target="_blank" rel="noopener">Applied Deep Learning - Part 3: Autoencoders</a>.</p></blockquote><p>在介绍VAE之前, 必须要简要介绍一下<strong>自编码器</strong>(<strong>AE</strong>, <strong>A</strong>uto - <strong>E</strong>ncoder). </p><p>自编码器是一种先把输入数据压缩为某种编码, 后仅通过该编码<strong>重构</strong>出原始输入的结构. 从描述来看, AE是一种<strong>无监督</strong>方法.</p><p>AE的结构非常明确, 需要有一个<strong>压缩编码</strong>的Encoder和就一个相应<strong>解码重构</strong>的Decoder:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vae1.png" style="zoom: 50%;" /><p>Encoder能够将给定的输入$X$ 映射为<strong>编码</strong>$Z$, 即$Z=g(X)$, Decoder能将编码$Z$ 映射回与原输入相似的$\hat{X}$, 即$\hat{X}=f(Z)$. $Z$ 也被称为<strong>隐变量</strong>, 其维度必须是远远小于$X$ 的, 否则就达不到压缩编码的目的. </p><p>如果Decoder能仅依赖Encoder生成的编码$Z$ 尽可能好的还原输入数据, 那么就说明$Z$ 中真的存在某种能表征原始输入$X$ 的信息, 甚至$Z$ 的每一维都可能对应着某个输入数据变化的具体含义, 例如人脸的笑容, 褶皱, 皮肤颜色等属性.</p><p>对于压缩编码和解码重构的结构, 使用普通的神经网络, 只需要让神经元的个逐渐减少到编码的维度, 再由编码维度逐渐增大到原输入维度:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vae2.png" style="zoom: 50%;" /><blockquote><p>Encoder和Decoder的不一定是完全对称的, 甚至也不一定是同质的.</p></blockquote><p>我们希望Auto Encoder所重构的输入$\hat{X}$ 和真正输入$X$ 的差距越小越好, 所以通常使用均方误差(MSE)来作为AE的损失函数, 即$\mathcal{L}_{MSE}=\Vert X - \hat{X} \Vert ^ 2$.</p><p>AE有一种常见的变形, 称为<strong>去噪自编码器</strong>(Denoising Auto - Encoder). 这种AE在原始输入数据的基础上添加了<strong>噪声</strong>, 然后再将其送给AE, 并要求Decoder还原出不带噪声的输入数据. 这就要求Encoder和Decoder具有更强大的能力:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vae3.png" style="zoom: 50%;" /><p>AE不是我们今天的重点, 就不再展开说了.</p><p>仔细想想, Auto Encoder虽然是可以分成Encoder和Decoder两个部分, 但实际上Encoder和Decoder是没法作为两个组件单独使用的, 它们必须配套使用.</p><p>例如我们想用单独使用Decoder做生成, 我们只能把随机生成的向量输入到Decoder中, 强行让Decoder解码出一个极少概率有用的内容, 效果一定不会很好. 因为在训练时, Decoder获得的编码全部是来自于Encoder的, 而我们直接随机采样得到的向量与Encoder压根没有关联, 让Decoder解码出有效的结果是不可能的.</p><h2 id="变分自编码器"><a href="#变分自编码器" class="headerlink" title="变分自编码器"></a>变分自编码器</h2><blockquote><p>本节图片出自<a href="https://www.jeremyjordan.me/variational-autoencoders/" target="_blank" rel="noopener">Variational autoencoders.</a>, 内容讲解参考苏神的博客.</p></blockquote><p><strong>变分自编码器</strong>(<strong>VAE</strong>, <strong>V</strong>ariational <strong>A</strong>uto - <strong>E</strong>ncoder)从概率的角度描述隐空间与输入样本.</p><h3 id="隐变量-概率分布式"><a href="#隐变量-概率分布式" class="headerlink" title="隐变量 - 概率分布式"></a>隐变量 - 概率分布式</h3><p>理想形态下的生成模型可以被描述为$X = g(Z)$. 由于没法直接知道$p(X)$, 我们得引入隐变量$Z$ 来求:<br>$$<br>p(X) = \sum_Z p(X\mid Z) p(Z)<br>$$</p><p>如果我们能把输入样本$X$ 编码得到的$Z$ 控制在我们已知的某个分布中, 那么我们就可以从隐变量的分布中采样, 解码得到生成内容$\hat{X}$, 也算不错.</p><p>在这样的想法下, 将样本的隐变量建模为<strong>概率分布</strong>, 而非像AE一样把隐变量看做是离散的值:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vae5.png" style="zoom: 67%;" /><p>AE将样本编码为离散的点, 而VAE将样本编码为概率分布, 直接点就是给隐变量<strong>添加噪声</strong>.</p><p>那么在Decoder解码时, 从隐变量中<strong>随机采样</strong>, 得到采样后的向量作为Decoder的输入:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vae4.png" style="zoom: 67%;" /><p>沿着这个思路, 如果假设$p(Z) \sim \mathcal{N}(\mu, \sigma^2)$, 可以从其中采样得到$Z_1, Z_2 , \dots, Z_n$, 然后由生成模型得到$\hat{X}_1 = g(Z_1),\hat{X}_2 = g(Z_2),\dots,\hat{X}_n = g(Z_n)$, 但我们根本没法度量生成的结果$\{\hat{X}_1,\hat{X}_2,\dots,\hat{X}_n\} $ 和样本数据$\{X_1,X_2,\dots,X_n\}$ 之间的差异, 因为我们压根不知道$Z_k, X_k, \hat{X}_k$ 之间的<strong>对应关系</strong>.</p><p>没有$X, \hat{X}$ 分布的表达式, 我们就没有办法通过对齐二者分布的方法来优化模型.</p><p>所以, 我们应该在给定真实样本$X_k$ 的情况下, 假设存在分布$p(Z \mid X_k) \sim \mathcal{N}(\mu, \sigma^2)$. Decoder就可以把$p(Z \mid X_k)$ 中采样得到的$Z_k$ 还原为$X_k$, 这样保证$Z_k, X_k, \hat{X}_k$ 之间可以对应. </p><p>尽管分布内采样到的隐变量的值<strong>不完全相同</strong>, 但都应该重建回相同的输出, 这也就是把<strong>样本编码为概率分布</strong>的真正含义:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vae6.png" style="zoom: 67%;" /><p><strong>每一个样本都对应着一个自己专属的正态分布</strong>, 样本之间必定存在重合, 当采样到两个样本叠加的区域时, 解码的内容会变得介于二者之间. 按照AE中的假设, 隐变量的每维都可能有具体的含义. 若是如此, 在概率分布视角下的隐变量就可以等距采样, 通过观察控制生成的内容. 例如:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vae10.jpg" style="zoom: 67%;" /><p>从满月到半月等距采样, 应该能观察到由满月逐渐变到半月的所有月相.</p><blockquote><p>本图出自<a href="https://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017/Lecture/GAN%20(v3).pdf" target="_blank" rel="noopener">李宏毅老师课程配套Slide</a>.</p></blockquote><p>那每个分布的均值和方差要怎么求出来呢? 没什么好方法, 用神经网络来直接拟合样本对应的正态分布的均值$\mu$ 和方差$\sigma^2$ 吧:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vae7.png" style="zoom: 67%;" /><blockquote><p>但我们实际上拟合的是$\log \sigma^2$, 因为$\sigma^2$ 非负, 想要让变为负数需要加激活函数处理, 而$\log \sigma^2$ 可以直接在不加激活函数的情况下变为负值.</p></blockquote><h3 id="KL散度-防止神经网络偷懒"><a href="#KL散度-防止神经网络偷懒" class="headerlink" title="KL散度 - 防止神经网络偷懒"></a>KL散度 - 防止神经网络偷懒</h3><p>神经网络一看拟合$\mu, \sigma^2$ 的任务, 心想: “采样得到的$Z$ 是包含噪声的, 重构起来多难啊, 我直接让方差$\sigma^2$ 学出个0来, 我学一个点的拟合肯定比学一个分布的拟合简单, 美滋滋”. </p><p>神经网络很容易过拟合, 把方差学成0, 那就坏了. 如果把样本重新映射回一个点, 那么VAE就直接退化回了AE. 所以我们还是希望$Z$ 是含有噪音的(方差不为0)的分布.</p><p>对于我们之前的假设$p(Z \mid X) \sim \mathcal{N}(\mu, \sigma^2)$ 要更严格, 不但要约束$\sigma^2 \neq 0$, 还要令方差不能太大, 也不能太小.</p><p>但如果假设$p(Z \mid X) \sim \mathcal{N}(0, I)$, 还保证了模型的<strong>生成能力</strong>:</p><p>$$<br>\begin{aligned}<br>p(Z) = &amp; \sum_X p(Z \mid X) p(X) \\<br> = &amp; \sum_x \mathcal{N}(0, I) p(X)\\<br> = &amp; \mathcal{N}(0, I) \sum_X p(X) \\<br> = &amp; \mathcal{N}(0, I)<br> \end{aligned}<br>$$</p><p>在该条件下$p(Z) \sim \mathcal{N}(0, I)$, 当脱离Encoder, 即不依靠输入样本$X$ 时, 我们可以直接从$\mathcal{N}(0, I)$ 中采样来生成可靠的结果.</p><p>我们直接使用KL散度来约束$p(Z \mid X)$, 令其服从标准正态分布.</p><blockquote><p><strong>KL散度</strong>(也称为<strong>相对熵</strong>)常用于度量两个分布之间的差异性, 假设$P$ 为样本真实分布, $Q$ 为模型预测的分布, 根据KL散度有:<br>$$<br>D_{\mathrm{KL}}(P \| Q)=\mathbb{E}_{\mathrm{x} \sim P}\left[\log \frac{P(x)}{Q(x)}\right]=\mathbb{E}_{\mathrm{x} \sim P}[\log P(x)-\log Q(x)]<br>$$<br>当$P, Q$ 越接近时, $D_{\mathrm{KL}}(P \| Q)$ 就越小, 当$P, Q$ 分布完全相同时, $D_{\mathrm{KL}}(P \| Q)$ 为0.</p><p>KL散度还有两个性质:</p><ol><li>非负: KL散度是非负的.</li><li>不对称: 通常情况下, $D_{\mathrm{KL}}(P \| Q) \neq D_{\mathrm{KL}}(Q \| P)$, KL散度并不是真正意义上的距离.</li></ol></blockquote><p>求解过程如下:</p><p>$$<br>\begin{aligned}<br>&amp;KL\Big(N(\mu,\sigma^2)\Big\Vert N(0,1)\Big)\\<br>=&amp;\int \frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2} \left(\log \frac{e^{-(x-\mu)^2/2\sigma^2}/\sqrt{2\pi\sigma^2}}{e^{-x^2/2}/\sqrt{2\pi}}\right)dx\\\<br>=&amp;\int \frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2} \log \left\{\frac{1}{\sqrt{\sigma^2}}\exp\left\{\frac{1}{2}\big[x^2-(x-\mu)^2/\sigma^2\big]\right\} \right\}dx\\\<br>=&amp;\frac{1}{2}\int \frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2} \Big[-\log \sigma^2+x^2-(x-\mu)^2/\sigma^2 \Big] dx \\<br>=&amp;\frac{1}{2}(-\log\sigma^2+\mu^2+\sigma^2-1)<br>\end{aligned}<br>$$</p><p>求解时, 需要<strong>最小化</strong>KL散度.</p><p>VAE常用的损失函数为:<br>$$<br>\begin{aligned}<br>\mathcal{L} = &amp; \mathcal{L}_\mathrm{Recon} + \mathcal{L}_\mathrm{KL} \\<br>= &amp; \mathcal{D}(\hat{X}_k,X_k)^2 + KL\Big(N(\mu,\sigma^2)\Big\Vert N(0,1)\Big)<br>\end{aligned}<br>$$<br>即重构损失和KL散度两部分.</p><h3 id="梯度断裂-重参数"><a href="#梯度断裂-重参数" class="headerlink" title="梯度断裂 - 重参数"></a>梯度断裂 - 重参数</h3><p>我们想要用梯度下降来优化$p(Z \mid X_k)$ 的均值$\mu$ 和方差$\sigma $, 但”采样”这个操作是<strong>不可导</strong>的, VAE利用<strong>重参数化技巧</strong>(Reparameterization Trick)使得梯度不因采样而断裂.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vae8.png" style="zoom: 67%;" /><blockquote><p>图片来自<a href="https://www.jeremyjordan.me/variational-autoencoders/" target="_blank" rel="noopener">Variational autoencoders.</a></p></blockquote><p>原理很简单, $Z$ 的导数可以写成:<br>$$<br>\begin{aligned}&amp;\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(z-\mu)^2}{2\sigma^2}\right)dz \\<br>=&amp; \frac{1}{\sqrt{2\pi}}\exp\left[-\frac{1}{2}\left(\frac{z-\mu}{\sigma}\right)^2\right]d\left(\frac{z-\mu}{\sigma}\right)<br>\end{aligned}<br>$$<br>说明$(z - \mu) / \sigma^2 \sim \mathcal{N}(0, I)$, 从$\mathcal{N}(\mu, \sigma^2)$ 中采样, 就等价与从标准正态分布$\mathcal{N}(0, I)$ 中采样出一个$\epsilon$, 然后再通过$Z= \mu + \epsilon \times \sigma$ 缩放回去. 采样的导致梯度断裂的锅就丢给了$\epsilon$ 这个无关变量, 使得$\mu, \sigma^2$ 可以重新参与到梯度下降中优化:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vae9.png" style="zoom: 67%;" /><h2 id="深入阅读及参考资料来源"><a href="#深入阅读及参考资料来源" class="headerlink" title="深入阅读及参考资料来源"></a>深入阅读及参考资料来源</h2><ul><li><p>视频推荐: </p><ol><li>强推李宏毅: <a href="https://www.bilibili.com/video/BV1Wv411h7kN" target="_blank" rel="noopener">李宏毅2021春机器学习课程</a> 的<a href="https://www.bilibili.com/video/BV1Wv411h7kN?p=44" target="_blank" rel="noopener">p44</a> 和<a href="https://www.bilibili.com/video/BV1Wv411h7kN?p=45" target="_blank" rel="noopener">p45</a>, 以及其<a href="https://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017/Lecture/GAN%20(v3).pdf" target="_blank" rel="noopener">配套的Slide</a></li><li>通俗篇: <a href="https://www.bilibili.com/video/BV1hf4y1r7C7" target="_blank" rel="noopener">变分自动编码器(Variational AutoEncoder, VAE)，直观理解、数学推导与最新应用</a></li><li>硬核篇: <a href="https://www.bilibili.com/video/BV1q64y1y7J2" target="_blank" rel="noopener">[论文简析]VAE: Auto-encoding Variational Bayes[1312.6114]</a></li></ol></li><li><p>文章推荐 - 苏神系列博客: </p><ol><li><a href="https://spaces.ac.cn/archives/5253" target="_blank" rel="noopener">变分自编码器（一）：原来是这么一回事</a></li><li><a href="https://kexue.fm/archives/5343" target="_blank" rel="noopener">变分自编码器（二）：从贝叶斯观点出发</a></li><li><a href="https://kexue.fm/archives/5383" target="_blank" rel="noopener">变分自编码器（三）：这样做为什么能成？</a></li><li><a href="https://kexue.fm/archives/7725" target="_blank" rel="noopener">变分自编码器（六）：从几何视角来理解VAE的尝试</a></li></ol></li><li><p>外文文章推荐:</p><ol><li>VAE原论文: <a href="https://arxiv.org/abs/1312.6114" target="_blank" rel="noopener">Auto-Encoding Variational Bayes</a></li><li>关于重参数: <a href="http://gregorygundersen.com/blog/2018/04/29/reparameterization/" target="_blank" rel="noopener">The Reparameterization Trick</a></li></ol></li></ul><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>VAE是一个有严格数学推导的模型, 但在学的时候千万不要被”变分”二字给唬住了, 变分二字只是来源于VAE推导过程中所使用的KL散度.</p><p>实际上, VAE由于把样本编码为概率分布, 生成的实际上是训练样本之间的<strong>平均</strong>, 在样本的<strong>分布叠加</strong>后(高斯混合模型), VAE记录了一个样本到另一个样本之间的<strong>演化过程</strong>. 这也就是为什么VAE生成的结果会存在<strong>模糊</strong>的问题, 但仍然不妨碍它成为最强大的深度生成模型之一.</p><p>强烈推荐看苏神的博客, 苏神的见解要深刻得多.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> VAE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>知识蒸馏: Distilling the Knowledge in a Neural Network</title>
      <link href="/posts/39586.html"/>
      <url>/posts/39586.html</url>
      
        <content type="html"><![CDATA[<h1 id="Distilling-the-Knowledge-in-a-Neural-Network"><a href="#Distilling-the-Knowledge-in-a-Neural-Network" class="headerlink" title="Distilling the Knowledge in a Neural Network"></a>Distilling the Knowledge in a Neural Network</h1><p>本文是论文<a href="https://arxiv.org/abs/1503.02531" target="_blank" rel="noopener">Distilling the Knowledge in a Neural Network</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>现有机器学习中, 任何算法都可以用Ensemble的方法来提升性能, 但这样做会花费昂贵的计算资源, 并且不利于部署到真实场景中.</p><p>作者尝试提出一种<strong>把大模型知识尽可能的压缩进单个小模型</strong>的方法.</p><h2 id="Distillation"><a href="#Distillation" class="headerlink" title="Distillation"></a>Distillation</h2><p>人们希望得到的模型并不是在单一的数据集上拟合完美, 而是要求模型具有强大的<strong>泛化能力</strong>. 在某个问题的某个具体数据及上, 通常训练出的模型与真实问题会存在偏差,  存在一点点过拟合.</p><p>那么对于一个有能力的大模型, 就有希望直接利用大模型的知识, 训练一个具有更强泛化能力的小模型, 让小模型直接学习大模型的泛化能力.</p><blockquote><p>从泛化能力的角度来考虑, 知识蒸馏非常像一种<strong>正则化</strong>手段.</p></blockquote><p>训练时经常所采用的标签是独热编码, Softmax会刻意放大Logits之间的差距. 这使得模型输出的类别概率在某一类是非常大的(文中也称为<strong>Hard Target</strong>), 其他类别的概率都非常小. </p><p>但不同类别之间的<strong>相对概率</strong>仍然很重要, 例如猫的图片可能与狗有一定相似, 它一定比和苹果的相似性要低. 这种类别概率差异仍然可能存在着一些隐含的知识, 但它会被Softmax所抹除掉, 所以需要一些手段把这种知识传授给小模型.</p><p>一种可以尝试的方法是把大模型(<strong>Teacher</strong>)的预测结果和大模型的知识作为小模型(<strong>Student</strong>)的Target, 即将处理过后的大模型Logits作为Label或Label的一部分训练小模型.</p><p>既然是Softmax抹除了不同类别之间的差异, 那么可以对Softmax改动, 弱化其对隐含知识的影响.</p><p>假设神经网络在没有经过Softmax前的<strong>Logits</strong>记为$z_i$, 我们可以添加”<strong>温度</strong>“$T$ 来弱化影响, 记结果为$q_i$:</p><p>$$<br>q_{i}=\frac{\exp \left(z_{i}\right)}{\sum_{j} \exp \left(z_{j}\right)} \quad \rightarrow \quad q_{i}=\frac{\exp \left(z_{i} / T\right)}{\sum_{j} \exp \left(z_{j} / T\right)}<br>$$<br>$T=1$ 时, 就是正常的Softmax. 当$T &gt; 1$ 时, 原来的Softmax将变得<strong>更加软化</strong>, 不同类别之间的差距, 不再显得类别间的差距那么绝对. Teacher中的不同类别之间的暗含知识得到一定保留. 因为标签变得软化了, 所以熵更大, 也保存了更多的信息.</p><p>如果这个式子不够直观体现出它的作用, 我做出了不同$T$ 对Target $q_i$ 的影响变化曲线图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kd6.png" style="zoom:80%;" /><p>随着$T$ 的增大, 生成的Soft Target之间的差距会越来越小, 变得<strong>Softer</strong>.</p><p>Student将使用Hard Target和Soft Target共同训练自己, Teacher软化后的知识将作为损失函数的一部分调节Student的参数:<br>$$<br>\mathcal{L} = \mathcal{L}_{hard} + \lambda \mathcal{L}_{soft}<br>$$</p><p>$\lambda$ 为超参, 用于调节Teacher Soft Target的影响占比. 具体来说, Student应以常温$T=1$ 用Hard Target训练自己, 即损失的第一项. 同时, Teacher和Student的蒸馏时应对Softmax加以高温$T$, 即损失的第二项, 蒸馏过程的示意图如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kd7.png" style="zoom: 15%;" /><blockquote><p>该图出自<a href="https://nni.readthedocs.io/en/stable/sharings/kd_example.html" target="_blank" rel="noopener">Knowledge Distillation on NNI</a>.</p></blockquote><blockquote><p>在训练时, 必须保证Teacher和Student的温度<strong>一致</strong>, 当训练完成后, Student预测不再使用$T$, 或者说训练完成后的推断设置$T=1$.</p><p>同时, 由于温度$T$ 的影响, 梯度均缩小了$T^2$ 倍(详见下一小节最后), 所以在设置$\lambda$ 时, 需要让其尽可能大一些, 或者乘$T^2$ 倍, 才能保证两种损失的贡献度相同.</p></blockquote><h3 id="Matching-Logits-is-a-Special-Case-of-Distillation"><a href="#Matching-Logits-is-a-Special-Case-of-Distillation" class="headerlink" title="Matching Logits is a Special Case of Distillation"></a>Matching Logits is a Special Case of Distillation</h3><p>作者下面证明了直接让Student学Teacher的Logits只是蒸馏的一种特殊情况.</p><blockquote><p>下面涉及到更详细的推导过程在末尾引文连接已附上.</p></blockquote><p>假设我们处理的问题所采用的损失函数是交叉熵$C$, 梯度为$\frac{\partial C}{\partial z_i}$, Teacher模型的Logits为$v_i$, 以及其对应的概率为$p_i$, 则有:</p><p>$$<br>\frac{\partial C}{\partial z_{i}}=\frac{1}{T}\left(q_{i}-p_{i}\right)=\frac{1}{T}\left(\frac{e^{z_{i} / T}}{\sum_{j} e^{z_{j} / T}}-\frac{e^{v_{i} / T}}{\sum_{j} e^{v_{j} / T}}\right)<br>$$</p><p>当$T$ 相较于Logits充分大的时候, 可以使用泰勒展开, 有$e^{x/T}\approx1+x/T$:</p><p>$$<br>\frac{\partial C}{\partial z_{i}} \approx \frac{1}{T}\left(\frac{1+z_{i} / T}{N+\sum_{j} z_{j} / T}-\frac{1+v_{i} / T}{N+\sum_{j} v_{j} / T}\right)<br>$$</p><p>当对Logits做了零均值假设后, 有$\sum_jz_j=\sum_jv_j=0$, 结合上式有:</p><p>$$<br>\frac{\partial C}{\partial z_{i}} \approx \frac{1}{N T^{2}}\left(z_{i}-v_{i}\right)<br>$$</p><p>因此, 在较高的温度$T$ 设置下, 蒸馏等价于最小化$\frac{1}{2}(z_i - v_i)^2$, 也就是直接把Teacher和Student的Logits匹配, 所以匹配Logits是一种蒸馏的特殊情况.</p><p>当温度较低时, 对负样本的关注就比较少, 可能滤去关键信息, 但实际上这<strong>有利有弊</strong>. 有些负样本的Logits应该是非常小的负值, 这种极小的负值在高温时的作用会被放大, 作为强大的噪声影响Student. 在低温时, 这种噪声将被滤去.</p><p>所以温度的选取一般依赖于经验, 不要太高也不要太低.</p><blockquote><p>分母上有$T^2$, 所以在知识蒸馏时, $\mathcal{L}_{soft}$ 的影响被缩小了$T^2$, 所以需要在设置损失项时平衡回来.</p></blockquote><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="MNIST"><a href="#MNIST" class="headerlink" title="MNIST"></a>MNIST</h3><p>作者将知识蒸馏应用于小规模数据集MNIST, Teacher训练了一个两隐层的DNN并使用Dropout和Weight Constraints, Student网络也是两隐层的DNN但神经元个数比Teacher少, 不使用正则化手段. </p><p>作者尝试了几种不同的小模型设置, 在合适的温度下取得了与Teacher相近的表现.</p><h3 id="Speech-Recognition"><a href="#Speech-Recognition" class="headerlink" title="Speech Recognition"></a>Speech Recognition</h3><p>语音识别中, 当时比较流行的做法是用HMM, 并按照如下目标优化模型参数$\theta$:<br>$$<br>\boldsymbol{\theta}=\arg \max _{\boldsymbol{\theta}^{\prime}} P\left(h_{t} \mid \mathbf{s}_{t} ; \boldsymbol{\theta}^{\prime}\right)<br>$$<br>其中$\mathbf{s_t}$ 为$t$ 时刻的结果, $h_t$ 为$t$ 时刻的HMM隐态.</p><p>结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kd1.jpg" style="zoom: 50%;" /><p>蒸馏一个小模型出来后的结果比单独Train一个大模型的效果要好.</p><h3 id="JFT"><a href="#JFT" class="headerlink" title="JFT"></a>JFT</h3><p>JFT是一个比前面二者大得多的图像分类数据集, 这个数据集有1亿张图片, 15000个类别. </p><p>作者训练了一个通用模型和若干个专家模型, 作为没有使用知识蒸馏时的Baseline.</p><p>对若干种通用模型经常<strong>易混淆</strong>的类做一个聚类(也有可能有些类不被归纳进专家模型, 这就需要通用模型自己处理), 记为$S^m$, 作为多个JFT的子集, 将不同子集的数据交给不同的专家模型$m$ 预测, 下面是作者展示出的子集示例:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kd2.jpg" style="zoom: 50%;" /><p>对于输入的图片$\mathbf{x}$, 得到分类结果需要两步:</p><ol><li><strong>粗分</strong>: 对于每个测试数据, 由通用模型得到$n$ 个最有可能的类别, 记这些类为$k$.</li><li><strong>细分</strong>: 按照$k$, $S^m$ 的非空交集找到相应的专家模型$A_k$ , 让专家模型进行预测.</li></ol><p><strong>专家模型极易过拟合</strong>. 为了防止过拟合, 专家模型所采用的一半数据来自指定类别, 剩下一半来自全数据集, 其他类别被全部设置为一个单独的”Dustbin”类.</p><p>然后最小化所有类别的概率分布$\mathbf{q}$ 和通用模型, 专家模型得到的概率分布的KL散度.</p><blockquote><p><strong>KL散度</strong>(也称为<strong>相对熵</strong>)常用于度量两个分布之间的差异性, 假设$P$ 为样本真实分布, $Q$ 为模型预测的分布, 根据KL散度有:<br>$$<br>D_{\mathrm{KL}}(P \| Q)=\mathbb{E}_{\mathrm{x} \sim P}\left[\log \frac{P(x)}{Q(x)}\right]=\mathbb{E}_{\mathrm{x} \sim P}[\log P(x)-\log Q(x)]<br>$$<br>当$P, Q$ 越接近时, $D_{\mathrm{KL}}(P \| Q)$ 就越小, 当$P, Q$ 分布完全相同时, $D_{\mathrm{KL}}(P \| Q)$ 为0.</p><p>KL散度还有两个性质:</p><ol><li>非负: KL散度是非负的.</li><li>不对称: 通常情况下, $D_{\mathrm{KL}}(P \| Q) \neq D_{\mathrm{KL}}(Q \| P)$, KL散度并不是真正意义上的距离.</li></ol></blockquote><p>记通用模型得到的概率分布为$\mathbf{p}^g$, 专家模型得到的概率分布为$\mathbf{p}^m$, 总体分布$\mathbf{q}$ 和模型预测得到概率分布的KL散度计算方式如下:</p><p>$$<br>K L\left(\mathbf{p}^{g}, \mathbf{q}\right)+\sum_{m \in A_{k}} K L\left(\mathbf{p}^{m}, \mathbf{q}\right)<br>$$</p><p>总体来说, 最好的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kd3.jpg" style="zoom: 50%;" /><p>当时JFT的Baseline是CNN. </p><p>逐渐增大专家模型的数量, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kd4.jpg" style="zoom: 50%;" /><p>随专家数量提升, 相对提升逐渐增大.</p><p>前面说过, 专家模型极易过拟合, 如果使用3%的数据的Hard Target训练专家模型, 它更有可能过拟合, 并且在附加早停的情况下非常早就停止了.</p><p>但如果使用知识蒸馏, 把Hard Target用Soft Target代替, 仅用3%的数据训练专家模型, 不但不会很早早停, 而且还能保留专家模型的泛化能力, 效果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kd5.jpg" style="zoom: 50%;" /><p>使用Soft Target效果要好于同样使用3%的数据的Baseline的训练效果, 并且与使用全部数据的Baseline效果相近.</p><h2 id="Recommended"><a href="#Recommended" class="headerlink" title="Recommended"></a>Recommended</h2><ul><li>论文原文: <a href="https://arxiv.org/abs/1503.02531" target="_blank" rel="noopener">Distilling the Knowledge in a Neural Network</a></li><li>比较全面的文章: <a href="https://zhuanlan.zhihu.com/p/102038521" target="_blank" rel="noopener">【经典简读】知识蒸馏(Knowledge Distillation) 经典之作</a></li><li>BERT相关:<a href="https://zhuanlan.zhihu.com/p/71986772" target="_blank" rel="noopener">深度神经网络模型蒸馏Distillation</a></li><li>很详细的视频(需翻墙): <a href="https://www.youtube.com/watch?v=xKPvt3GQ1j8&ab_channel=peakqi" target="_blank" rel="noopener">Hinton：distilling knowledge in a neural network</a></li><li>数学推导(推荐看看): <a href="https://zhuanlan.zhihu.com/p/385374430" target="_blank" rel="noopener">Distilling the Knowledge in a Neural Network 理论推导</a></li></ul><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>知识蒸馏是一种将大模型的隐含知识通过某种手段提取出来, 提炼传授给小模型的<strong>模型压缩</strong>方法.</p><p>该论文发表自2014年, 当时深度学习的模型还没有发展到像现在这样的超大规模, Hinton能提出这种具有工程意义并且值得挖掘的新方向相当有远见.</p><blockquote><p>蒸馏为何有效, 人们还没有彻底摸清其中的作用原理.</p><p>甚至单个模型的<strong>自蒸馏</strong>也是有效的… 这点非常诡异, 为什么模型单单依靠样本本身却无法达到自蒸馏后的效果? 样本之间隐含的差异居然需要自己产生的产物重新喂给自己才能吸收(反刍)?</p><p>读完本论文后, 自然会产生进一步的想法. 直接把Logits蒸给小模型效果如何? 能蒸Logits为什么不直接蒸Feature呢? 要是蒸Feature也不够直接的话把参数蒸给小模型是不是也可以?  这些想法确实都可以, 或多或少都有效果.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
      <link href="/posts/60711.html"/>
      <url>/posts/60711.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>BERT: 详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>.</li></ul></blockquote><h1 id="ALBERT-A-Lite-BERT-for-Self-supervised-Learning-of-Language-Representations"><a href="#ALBERT-A-Lite-BERT-for-Self-supervised-Learning-of-Language-Representations" class="headerlink" title="ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"></a>ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</h1><p>本文是论文<a href="http://arxiv.org/abs/1909.11942" target="_blank" rel="noopener">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a>的阅读笔记和个人理解. 最近忙着毕业季, 赶巧眼病又发作了, 就拖更了几天.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>现有的NLP模型都太大了, <strong>计算资源短缺</strong>已经成为越来越显著的问题.</p><p>一般来说, 参数到达一定数量后增加参数只能带来轻微的性能提升, 模型参数过多后还容易出现过拟合, 反而导致性能下降. 作者尝试使用<strong>减少参数</strong>的多种方法, 构造一个轻量级的BERT, 能逼近原BERT的效果.</p><h2 id="ALBERT"><a href="#ALBERT" class="headerlink" title="ALBERT"></a>ALBERT</h2><p><strong>ALBERT</strong>(<strong>A</strong> <strong>L</strong>ite <strong>BERT</strong>)尝试使用三种主要的手段来节省额外的参数开销.</p><h3 id="Factorized-Embedding-Parameterization"><a href="#Factorized-Embedding-Parameterization" class="headerlink" title="Factorized Embedding Parameterization"></a>Factorized Embedding Parameterization</h3><p>在BERT系列模型架构中, Token的Embedding大小$E$ 和Encoder的Hidden Layer大小$H$ 是完全绑定的, 即$E \equiv H$, 独热编码会直接通过Embedding转换到大小为$H$ 的维度.</p><p>从<strong>建模角度</strong>来说, Word Embedding更多强调<strong>上下文无关</strong>的表示, 即Token本身在无语境时最多出现的意思, 而Hidden Layer Embedding更多强调<strong>上下文相关</strong>的表示. 但实际上我们应该希望上下文相关的部分能使用更多参数, 即$H \gg E$, 这样能最大化BERT中的参数的使用效率. </p><p>从<strong>实践角度</strong>来说, NLP经常需要非常大的字典大小$V$, 如果$E \equiv H$, 增大$H$ 的同时必然增大$E$, $H$ 和$E$ 绑在一起的思路就不太实用.</p><p>因此, ALBERT将这一部分拆分为两步, 把独热编码直接转换到$H$ 的过程拆分为两个步骤, <strong>先映射到低维嵌入空间$E$, 然后再投影到隐层大小</strong>$H$. 这样就将Embedding这部分的参数大小从$O(V \times H)$ 缩少到$O(V \times E + E \times h)$. 当$E\ll H$ 时能减少很多参数.</p><blockquote><p>例如, 词表大小$V=30000$, 在BERT中, $E=H=768$, 不考虑位置编码, Embedding的总参数为$30000 \times 768$. 在ALBERT中, $E \ne H$, 假设$E=128, H=768$, Embedding总参数为$30000 \times 128 + 128 \times 768$, 确实有减少.</p></blockquote><h3 id="Cross-Layer-Parameter-Sharing"><a href="#Cross-Layer-Parameter-Sharing" class="headerlink" title="Cross - Layer Parameter Sharing"></a>Cross - Layer Parameter Sharing</h3><p>因为BERT是Transformer Encoder堆叠起来的, 假如只用训练一组共用参数, 然后让所有层都使用这一组参数, 岂不是能大幅减少参数? 确实, ALBERT利用这种方法减少了相当多的参数量.</p><p>但即使是多层参数复用, 也有多种复用方法. 每个Transformer Encoder由FFN和Multi Head Attention组成, 所以就有<strong>仅复用Attention</strong>, <strong>仅复用FFN</strong>, <strong>直接复用Encoder</strong>三种复用方式, 作者在后文的实验中探索了这三种方式的效果.</p><p>下图为每一层中同一个Token的表示的L2距离和余弦相似度:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/albert1.jpg" style="zoom:50%;" /><p>ALBERT比BERT的曲线要平滑得多, 这说明共享参数有助于稳定网络参数.</p><blockquote><p>跨层的参数共享, 应该是ALBERT中减少参数最有效的方法, 对于共享参数对性能产生的负面效果, 在训练阶段使用一些Trick能弥补一些.</p></blockquote><h3 id="Inter-Sentence-Coherence-Loss"><a href="#Inter-Sentence-Coherence-Loss" class="headerlink" title="Inter - Sentence Coherence Loss"></a>Inter - Sentence Coherence Loss</h3><p>作者指出, NSP任务是可以偷懒的. NSP任务将同一个文档中的两个连续句子的句子对作为正例, 将不同文档的两个句子的句子对作为负例. 因此, NSP任务实际上可以划分为Topic Prediction和Coherence Prediction两个任务:</p><ul><li><strong>Topic</strong> Prediction: 预测前后两个句子的主题是否相同.</li><li><strong>Coherence</strong> Prediction: 预测前后两个句子是否是真正连续的.</li></ul><p>Topic Prediction的难度远小于Coherence Prediction, 所以NSP不一定能真正有益于模型在下游任务上的表现, 也符合其他研究人员的结论.</p><p>ALBERT也废除了NSP任务, 但从NSP的Coherence Prediction角度出发, 设计了<strong>SOP</strong>(<strong>S</strong>entence <strong>O</strong>rder <strong>P</strong>rediction)任务. SOP任务保留<strong>Coherence Prediction</strong>, 将同一个文档中顺序正确的两个连续句子的句子对作为正例, 将它们<strong>交换顺序</strong>后的句子对作为负例, 这样就消除了Topic Prediction的目标.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文, 作者将ALBERT在实验中所用到的几个设置与BERT进行了对比:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/albert2.jpg" style="zoom:50%;" /><blockquote><p>ALBERT - xxlarge的层数是12而非24, 说明此时宽的模型比深的模型效果要更好, 可能层数已经达到极限.</p><p>4096的HIdden Size与BERT - large相比已经相当大了.</p></blockquote><p>与BERT不同的是, ALBERT在90%的情况下都使用最大的文本输入长度, 仅有10%的概率使用比最大文本长度更小的输入.</p><p>并且, 作者借鉴了<a href="https://adaning.github.io/posts/34019.html#toc-heading-3">SpanBERT</a>中的<strong>N - Gram Masking</strong>, 即每次生成Mask都有$p(n)$ 的概率生成长度为$n$ 的Mask:</p><p>$$<br>p(n)=\frac{1 / n}{\sum_{k=1}^{N} 1 / k}<br>$$</p><p>作者设置最大长度$n=3$.</p><h3 id="Overall-Comparision-between-BERT-and-ALBERT"><a href="#Overall-Comparision-between-BERT-and-ALBERT" class="headerlink" title="Overall Comparision between BERT and ALBERT"></a>Overall Comparision between BERT and ALBERT</h3><p>作者将BERT的各类配置与ALBERT的各类配置在<strong>相同训练量</strong>的多个任务上做了实验, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/albert3.jpg" style="zoom:50%;" /><p>ALBERT - xxlarge应该是打榜用的, 性能超过了BERT - large. ALBERT - large的性能已经接近于BERT - large.</p><p>在作者设置的ALBERT配置中, 同等型号的ALBERT比BERT的速度要快一些, 但同等型号的ALBERT性能却要比BERT差许多. 只有<strong>跨配置</strong>比较, 才能保证性能相似, 但<strong>跨配置的ALBERT在推理速度上不占优势</strong>.</p><h3 id="Factorized-Embedding-Parameterization-1"><a href="#Factorized-Embedding-Parameterization-1" class="headerlink" title="Factorized Embedding Parameterization"></a>Factorized Embedding Parameterization</h3><p>作者调整了ALBERT - base的$E$ 大小, 分别对比了共享参数和不共享参数的情况.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/albert4.jpg" style="zoom:50%;" /><p>从结果来看, 在复用Encoder的情况下, $E=128$ 似乎是一个比较好的解, $E &gt; 128$ 性能开始退化.</p><h3 id="Cross-Layer-Parameter-Sharing-1"><a href="#Cross-Layer-Parameter-Sharing-1" class="headerlink" title="Cross - Layer Parameter Sharing"></a>Cross - Layer Parameter Sharing</h3><p>针对仅复用Attention, 仅复用FFN, 直接复用Encoder三种情况, 作者在各类下游任务上做了实验, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/albert5.jpg" style="zoom:50%;" /><p>不共享参数的情况自然是性能最好的, 共享FFN似乎比较容易掉点.共享Attention影响没有那么大. 作者坚持复用Encoder的策略.</p><h3 id="Sentence-Order-Prediction-SOP"><a href="#Sentence-Order-Prediction-SOP" class="headerlink" title="Sentence Order Prediction (SOP)"></a>Sentence Order Prediction (SOP)</h3><p>作者比较了不使用额外任务, NSP, SOP三者之间对ALBERT - base的影响, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/albert6.jpg" style="zoom:50%;" /><p>使用NSP确实会损害模型性能, 使用SOP确实会让模型涨点, 但是SST - 2的性能和不使用额外任务相当.</p><h3 id="Train-for-the-Same-Amount-of-Time"><a href="#Train-for-the-Same-Amount-of-Time" class="headerlink" title="Train for the Same Amount of Time"></a>Train for the Same Amount of Time</h3><p>在前面的实验中, ALBERT - xxlarge比BERT - large的速度要慢许多. 通常情况下, 更长的训练时间会有更好的性能, 这可能导致二者比较的不公平.</p><p>作者在此不再控制二者训练量相同, 而是将BERT和ALBERT拉到了相同训练时间下比较, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/albert12.jpg" style="zoom:50%;" /><p>在同训练时间下, ALBERT要优于BERT, 即ALBERT训练较为高效.</p><h3 id="Additional-Training-Data-and-Dropout-Effects"><a href="#Additional-Training-Data-and-Dropout-Effects" class="headerlink" title="Additional Training Data and Dropout Effects"></a>Additional Training Data and Dropout Effects</h3><p>RoBERTa和XLNet比BERT所使用的数据要多得多, 作者尝试将额外的训练数据添加到训练中, 前后对比结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/albert7.jpg" style="zoom:50%;" /><p>除去SQuAD, 其他下游任务的性能略有提升, 这是因为除去维基百科的数据集, 其他数据相对于SQuAD来说是一种噪声, 即Out of Domain, 也许维基百科数据集已经能比较有针对性的解决SQuAD问题了.</p><p>即使是训练了1M Step, ALBERT也没有找到局部最优, 所以作者在ALBERT上去掉了Dropout, 使得性能有进一步的提升:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/albert9.jpg" style="zoom:50%;" /><blockquote><p>其实很好理解为什么去掉Dropout后模型能继续训练下去, 模型本身都没法达到过拟合, 何谈防止过拟合? 在模型没能找到局部最优时, 加入正则化手段自然而然会损害训练.</p></blockquote><p>上面二者在训练过程中对ACC的影响如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/albert8.jpg" style="zoom:50%;" /><p>添加额外数据和移除Dropout后, 验证集ACC有显著提升.</p><h3 id="Current-SOTA-and-NLU-Tasks"><a href="#Current-SOTA-and-NLU-Tasks" class="headerlink" title="Current SOTA and NLU Tasks"></a>Current SOTA and NLU Tasks</h3><p>作者把流行的Baseline放到一起在GLUE上做了对比, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/albert10.jpg" style="zoom:50%;" /><p>ALEBRT(1M) 代表ALBERT在该任务上Train了1M个Step, 与RoBERTa训练量相同.</p><p>在SQuAD和RACE上的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/albert11.jpg" style="zoom:50%;" /><p>ALBERT仍然是SOTA.</p><p>附录中还有一些对加强ALBERT宽度和深度的实验, 结果都表明当深度或宽度到达一定阈值后, 性能不再能继续增加, 甚至有时会出现退化.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>ALBERT的主要贡献是减了参数, 而不是计算量. 换句话说, 只能让BERT跑起来, 但推理阶段跑的快不快就不管了. 这似乎显得ALBERT有些鸡肋了, 因为减少模型参数后它的精度仍然会受到影响, 如果继续增大深度运算量又是个门槛($H$ 比较大, 而且层数多了, 肯定会增大运算量), 这点大家好像吐槽比较多.</p><p>ALBERT能够与BERT媲美的本质可能是$H$ 增大所带来增益要超过缩小参数带来的负面效应.</p><p>ALBERT的改进点都比较偏向于工程, <del>比起论文它更像一篇炼丹报告…,</del> 看实验结果感觉应该是到ALBERT所使用的压缩方法的性能顶峰了.</p><blockquote><p>模型压缩的目的从来都不是使得小模型的效果好过大模型, 而是利用某种方式, 使得模型的参数量或计算量减少, 同时<strong>不会带来明显的性能下降</strong>.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> BERT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>UniLM: Unified Language Model Pre-training for Natural Language Understanding and Generation</title>
      <link href="/posts/14266.html"/>
      <url>/posts/14266.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>BERT: 详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>.</li></ul></blockquote><h1 id="UniLM-Unified-Language-Model-Pre-training-for-Natural-Language-Understanding-and-Generation"><a href="#UniLM-Unified-Language-Model-Pre-training-for-Natural-Language-Understanding-and-Generation" class="headerlink" title="UniLM: Unified Language Model Pre-training for Natural Language Understanding and Generation"></a>UniLM: Unified Language Model Pre-training for Natural Language Understanding and Generation</h1><p>本文是论文<a href="https://arxiv.org/abs/1905.03197" target="_blank" rel="noopener">Unified Language Model Pre-training for Natural Language Understanding and Generation</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者认为, 当前流行的语言模型有三大类, 分别是以<strong>单向</strong>(左向右或右向左)的语言模型ELMo, GPT, 以<strong>双向</strong>为代表的语言模型BERT, 以及由各类Encoder和Decoder组合使用的<strong>Seq2Seq</strong>类模型:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unilm1.jpg" style="zoom:50%;" /><p>单向, 双向, Seq2Seq类的模型的优缺点各不相同, 所擅长的下游任务也不同, 对语言的编码方式更是不同:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unilm2.jpg" style="zoom:50%;" /><p>尽管这三类模型各有千秋, 但从来没有人尝试把上述三类模型<strong>统一</strong>到同一个模型中:</p><p>因此, 作者提出<strong>UniLM</strong>(<strong>Uni</strong>fied pre-trained <strong>L</strong>anguage <strong>M</strong>odel), 将上述三大类模型统一到同一个语言模型中, 共享同一组参数.</p><h2 id="UniLM"><a href="#UniLM" class="headerlink" title="UniLM"></a>UniLM</h2><h3 id="Input-Representation"><a href="#Input-Representation" class="headerlink" title="Input Representation"></a>Input Representation</h3><p>对于输入的序列$x$, 无论是作为单向语言模型的文本段, 还是对于双向语言模型训练时所使用的文本对, 都在输入文本段前添加起始Token<code>[SOS]</code>, 添加在文本段结束时添加结束Token<code>[EOS]</code>.</p><p>此外, <code>[EOS]</code> 不光作为NLU任务中的边界标记, 还作为NLG任务中的生成结束标记.</p><p>分词时采用WordPiece.</p><h3 id="Backbone-Network-Multi-Layer-Transformer"><a href="#Backbone-Network-Multi-Layer-Transformer" class="headerlink" title="Backbone Network: Multi - Layer Transformer"></a>Backbone Network: Multi - Layer Transformer</h3><blockquote><p>本部分即Transformer Encoder.</p></blockquote><p>对输入的Token表示$\left\{\mathbf{x}_{i}\right\}_{i=1}^{|x|}$, 输入第0层Transformer中获得初始上下文表示$\mathbf{H}^{0}=\left[\mathbf{x}_{1}, \cdots, \mathbf{x}_{|x|}\right]$, 经过$L$层Transformer堆叠$\mathbf{H}^{l}=\text { Transformer }_{l}\left(\mathbf{H}^{l-1}\right), l \in[1, L]$, 获得最终表示$\mathbf{H}^{l}=\left[\mathbf{h}_{1}^{l}, \cdots, \mathbf{h}_{|x|}^{l}\right]$. </p><p>每层Transformer层中的自注意力头输出$\mathbf{A}_l$ 可以被表示为:</p><p>$$<br>\begin{aligned}<br>\mathbf{Q} &amp;=\mathbf{H}^{l-1} \mathbf{W}_{l}^{Q}, \quad \mathbf{K}=\mathbf{H}^{l-1} \mathbf{W}_{l}^{K}, \quad \mathbf{V}=\mathbf{H}^{l-1} \mathbf{W}_{l}^{V} \\<br>\mathbf{M}_{i j} &amp;=\left\{\begin{array}{ll}<br>0, &amp; \text { allow to attend } \\<br>-\infty, &amp; \text { prevent from attending }<br>\end{array}\right.\\<br>\mathbf{A}_{l} &amp;=\operatorname{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^{\top}}{\sqrt{d_{k}}}+\mathbf{M}\right) \mathbf{V}_{l}<br>\end{aligned}<br>$$</p><p>其中, $\mathbf{W}_l^Q, \mathbf{W}_l^K, \mathbf{W}_l^V$ 为线性投影矩阵, $\mathbf{M}$ 为Mask矩阵, 决定Token能够给予哪些部分注意力.</p><p>UniLM使用不同的Mask矩阵$\mathbf{M}$ 来决定模型在接下来的运算中能对哪些Token给予注意力. 所以UniLM能直接通过更改$\mathbf{M}$ 在使用同一组参数的条件下改变模型的运行模式, 非常灵活.</p><h3 id="Pre-Training-Objectives"><a href="#Pre-Training-Objectives" class="headerlink" title="Pre - Training Objectives"></a>Pre - Training Objectives</h3><p>在上一节说过, 双向语言模型, 单向语言模型, 亦或许是Seq2Seq架构下的语言模型, 都能够通过修改Self - Attention Mask的形式实现:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unilm3.jpg" style="zoom:50%;" /><p>UniLM仍然采用BERT类似的<strong>完形填空</strong>式作为目标, 采用<strong>最小化交叉熵</strong>的方式来优化模型参数.</p><p>在预训练, 作者介绍了单向, 双向, Seq2Seq三类目标在UniLM中的实现方式(其实加上NSP是四类, 但NSP仅在双向语言模型目标中使用):</p><ul><li><p><strong>Unidirectional LM</strong> - (上图中间): 对于单向语言目标, 不对输入分段, 只输入文本段$\text{S}_1$. 在左到右或右到左的单一方向是时序可见的, 因此只需要将$\mathbf{M}$ 的上三角或下三角设置为$-\infty$, 其余位置置零即可.</p></li><li><p><strong>Bidirectional LM</strong>- (上图顶端): 对于双向语言目标, 使用了NSP任务, 输入文本段$\text{S}_1, \text{S}_2$, 与BERT训练方式保持一致. Token之间是全部互相可见的, 因此$\mathbf{M}=\mathbf{0}$.</p></li><li><p><strong>Sequence-to-Sequence LM</strong> - (上图底部): 对于Seq2Seq目标, 输入源序列$\text{S}_1$ 和目标序列$\text{S}_2$, 并在 Encoder对应的$\text{S}_1$ 应该是双向可见的, 故$\mathbf{M}$ 的左侧为$\mathbf{0}$. 在Decoder对应的$\text{S}_2$, 上下文只应该单向可见, 即$\mathbf{M}$ 右侧$\text{S}_2$ 的上三角为$-\infty$, 其余为$\mathbf{0}$. </p><p>例如, 在预训练阶段, 对于Seq2Seq任务的输入$\text{S}_1=[t_1, t_2]$, 输出$\text{S}_2=[t_3, t_4, t_5]$, 将以<code>[SOS]</code>, $t_1$, $t_2$, <code>[EOS]</code>, $t_3$, $t_4$, $t_5$, <code>[EOS]</code> 的形式输入到模型中. $t_2$ 只能看见<code>[SOS]</code>, $t_1$, $t_2$, <code>[EOS]</code>, 而$t_4$ 可以看到<code>[SOS]</code>, $t_1$, $t_2$, <code>[EOS]</code>, $t_3$, $t_4$.</p><blockquote><p>在打Mask时, 如果Mask掉的部分为$\text{S}_1$ 的Token, 则有助于模型学到双向Encoder, 若Mask掉的部分为$\text{S}_2$ 的Token, 则有助于让模型学到单向Decoder. 另外, 由于<code>[EOS]</code> 是可以被Mask掉的, 在预测时可以教会模型何时该结束生成.</p><p>仔细一想其实挺巧妙的, 因为这样并不需要明确的在模型中划分出Encoder和Decoder的位置. 也就意味着整个模型的所有区域都有可能被当做是Encoder或者Decoder.</p></blockquote></li></ul><p>在UniLM实际训练过程中, 总的Loss为三者的<strong>加和</strong>. 并且对各类训练目标分配时间是<strong>均匀</strong>的, 即有$\frac{1}{3}$ 时间采用双向语言训练目标, $\frac{1}{3}$ 时间采用Seq2Seq训练目标, 从左到右和从右到左的单向语言训练目标各有$\frac{1}{6}$ 时间 . 因为只是对Mask做出了调整, 因此<strong>与BERT完全兼容</strong>, UniLM参数直接使用BERT - LARGE初始化.</p><blockquote><p>这三种类型的训练难度可能不同, 这里将它们平均考虑应该是出于探索, 不带来过多的麻烦.</p></blockquote><h3 id="Fine-Tuning-on-Downstream-NLU-and-NLG-Tasks"><a href="#Fine-Tuning-on-Downstream-NLU-and-NLG-Tasks" class="headerlink" title="Fine - Tuning on Downstream NLU and NLG Tasks"></a>Fine - Tuning on Downstream NLU and NLG Tasks</h3><ul><li>对NLU任务, 采用<code>[SOS]</code> 处的输出作为整个输入的表示, 记为$\mathbf{h}_1^L$, 可以由此计算文本在分类任务上的概率$\operatorname{softmax}\left(\mathbf{h}_{1}^{L} \mathbf{W}^{C}\right)$, $C$ 为类别数, 最大化类别标签的概率即可, 该部分与BERT一致.</li><li>对NLG任务, 令输入的源序列$\text{S}_1$, 目标序列$\text{S}_2$, 以<code>[SOS]</code>, $\text{S}_1$, <code>[EOS]</code>, $\text{S}_2$, <code>[EOS]</code> 的形式输入. 训练目标为最大化在给定上下文条件下, 被Mask部分原来内容的概率. 但在精调阶段, 只对$\text{S}_2$ 中的内容打Mask.</li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的实验参数设置请参考原论文.</p><h3 id="Abstractive-Summarization"><a href="#Abstractive-Summarization" class="headerlink" title="Abstractive Summarization"></a>Abstractive Summarization</h3><p>在CNN / DailyMail上的抽取式摘要和生成式摘要结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unilm4.jpg" style="zoom:50%;" /><p>Gigaword上生成式摘要结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unilm5.jpg" style="zoom:50%;" /><p>UniLM要比Baseline有小幅提升, 并几乎全面领先.</p><h3 id="Question-Answering-QA"><a href="#Question-Answering-QA" class="headerlink" title="Question Answering (QA)"></a>Question Answering (QA)</h3><h4 id="Extractive-QA"><a href="#Extractive-QA" class="headerlink" title="Extractive QA"></a>Extractive QA</h4><p>抽取式QA的在SQuAD上的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unilm6.jpg" style="zoom:50%;" /><p>在CoQA上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unilm7.jpg" style="zoom:50%;" /><h4 id="Generative-QA"><a href="#Generative-QA" class="headerlink" title="Generative QA"></a>Generative QA</h4><p>生成式QA在CoQA上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unilm8.jpg" style="zoom:50%;" /><p>相较于指针生成网络, 在生成式QA上有非常明显的进步.</p><h3 id="Question-Generation"><a href="#Question-Generation" class="headerlink" title="Question Generation"></a>Question Generation</h3><p>SQuAD上问题生成结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unilm9.jpg" style="zoom:50%;" /><p>作者还使用UniLM所生成的问题让QA中的模型结果涨了四个点:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unilm10.jpg" style="zoom:50%;" /><p>据作者所述, 这应该是一种<strong>数据增强</strong>方法, 用生成的500w个可回答的问题, 以及经过更改后不可回答的400w个问题, 反喂给QA的UniLM微调少量Epoch, 结果有提升.</p><h3 id="Response-Generation"><a href="#Response-Generation" class="headerlink" title="Response Generation"></a>Response Generation</h3><p>对话生成结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unilm11.jpg" style="zoom:50%;" /><p>与Baseline相比, 若假设数据集所采取的评价指标是有效的, UniLM已经超过人类表现, 甚至更加精简.</p><h3 id="GLUE-Benchmark"><a href="#GLUE-Benchmark" class="headerlink" title="GLUE Benchmark"></a>GLUE Benchmark</h3><p>在GLUE上的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/unilm12.jpg" style="zoom:50%;" /><p>UniLM达到了新SOTA.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>同是出自MSRA的论文, UniLM与<a href="https://adaning.github.io/posts/46395.html">MASS</a>所尝试的思路是完全相反的.</p><p>如果说MASS是将BERT搬到了Seq2Seq上, 那么UniLM则是将Seq2Seq搬入了BERT体系, 用Mask来实现Seq2Seq的思路还是挺巧妙的, 个人感觉UniLM比MASS要有趣一些. 从结果上来看, UniLM将各类任务推向了新SOTA.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> BERT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MASS: Masked Sequence to Sequence Pre - training for Language Generation</title>
      <link href="/posts/46395.html"/>
      <url>/posts/46395.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识;</p><ul><li>BERT: 详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>.</li><li>Transformer: 详见<a href="https://adaning.github.io/posts/6744.html">Transformer精讲</a>.</li></ul></blockquote><h1 id="MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation"><a href="#MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation" class="headerlink" title="MASS: Masked Sequence to Sequence Pre-training for Language Generation"></a>MASS: Masked Sequence to Sequence Pre-training for Language Generation</h1><p>本文是论文<a href="https://arxiv.org/abs/1905.02450" target="_blank" rel="noopener">MASS: Masked Sequence to Sequence Pre-training for Language Generation</a>的阅读笔记和个人理解. 最近一个月在忙毕设相关, 所以有些论文看完还没有做笔记, 本文属于对这段时间没有写笔记的论文填坑之一.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>在Pre - training, Fine - tuning下的BERT是没有办法处理生成问题的, 而自回归的结构能较好的处理序列生成问题, 所以作者尝试继续将效果比较好的BERT重新迁移回Transformer的<strong>Seq2Seq</strong>框架下, 并能与<strong>Encoder - Decoder</strong>相兼容.</p><blockquote><p>该模型与<a href="https://adaning.github.io/posts/1394.html">BART</a>属于相同出发点的模型, 建议也学习一下BART.</p></blockquote><h2 id="MASS"><a href="#MASS" class="headerlink" title="MASS"></a>MASS</h2><h3 id="Sequence-to-Sequence-Learning"><a href="#Sequence-to-Sequence-Learning" class="headerlink" title="Sequence to Sequence Learning"></a>Sequence to Sequence Learning</h3><p>先来回顾一下标准的Seq2Seq训练过程. </p><p>在序列生成任务中, 输入输出为句子对$(x, y)\in(\mathcal{X},\mathcal{Y})$, $x$ 包含$m$ 个Token的源句子, 即$x=(x_1, x_2,\dots, x_m)$, $y$ 为包含$n$ 个Token的目标句子, 即$y=(y_1, y_2,\dots, y_n)$.</p><p>在Seq2Seq问题下, 目标为最大化模型参数$\theta$ 下的条件概率$P(y \mid x;\theta)$, 根据极大似然, 并结合语言模型中自回归的特性, 条件概率由链式法则能够被拆分, 目标函数为:<br>$$<br>\begin{aligned}<br>L(\theta ;(\mathcal{X}, \mathcal{Y}))&amp;=\sum_{(x, y) \in(\mathcal{X}, \mathcal{Y})} \log P(y \mid x ; \theta)\\<br>&amp;=\sum_{(x, y) \in(\mathcal{X}, \mathcal{Y})} \log \prod_{t=1}^{n} P\left(y_{t} \mid y_{&lt;t}, x ; \theta\right)<br>\end{aligned}<br>$$<br>其中, $y_{&lt;t}$ 为$t$ 时刻前自回归模型的输出.</p><p>一种最普遍的训练法为将Seq2Seq中Encoder的每个时刻隐态输出全部拿到, 然后用Attention机制在Decoder端对隐态输出加权解码.</p><h3 id="Masked-Sequence-to-Sequence-Pre-Training"><a href="#Masked-Sequence-to-Sequence-Pre-Training" class="headerlink" title="Masked Sequence to Sequence Pre - Training"></a>Masked Sequence to Sequence Pre - Training</h3><p>在<strong>MASS</strong>(<strong>MA</strong>sked <strong>S</strong>equence to <strong>S</strong>equence Pre - training for Language Generation)中, 不再采用句子对的方法训练模型, 而是采用<strong>去噪编码器</strong>的方式训练模型. </p><p>在Encoder端, 对于给定的Token数量为$m$ 的单句$x \in \mathcal{X}$, 假设<strong>连续</strong>地将位置$u \to v,(0&lt;u&lt;v&lt;m)$ 上的Token打上Mask, 打上Mask后的部分记为$x^{\backslash u:v}$, 打Mask前的实际内容被记为$x^{u:v}$. 那么$k=v-u+1$ 就为一共打上Mask的数量. </p><p>在Decoder端, Decoder的目标为重构加噪的文本, 即预测出被Mask掉的内容, 由于<strong>Teacher Forcing</strong>, 训练时的Decoder的输入为移位后的$x^{u:v}$, 其余全为Mask. </p><p>标准Seq2Seq的目标函数在引入Mask后就应该被改写为:</p><p>$$<br>\begin{aligned}<br>L(\theta ; \mathcal{X}) &amp;=\frac{1}{|\mathcal{X}|} \Sigma_{x \in \mathcal{X}} \log P\left(x^{u: v} \mid x^{\backslash u: v} ; \theta\right) \\<br>&amp;=\frac{1}{|\mathcal{X}|} \Sigma_{x \in \mathcal{X}} \log \prod_{t=u}^{v} P\left(x_{t}^{u: v} \mid x_{&lt;t}^{u: v}, x^{\backslash u: v} ; \theta\right)<br>\end{aligned}<br>$$</p><p>对于上述过程, 举出一个例子:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mass1.jpg" style="zoom: 50%;" /><p>其中<code>_</code>代表<code>[MASK]</code>.</p><p>我们将长度为$m=8$ 的句子中$x_3, x_4, x_5, x_6$ 连续地打上Mask, 在Decoder端, 除去Encoder中被Mask掉的$x_3, x_4, x_5$, 其余输入全部为Mask. </p><blockquote><p>对于为何Decoder处将不需要被预测的Token都变成Mask, 作者的解释是这样能更好利用Encoder的编码, 结合Attention, 使得Encoder和Decoder更好的联合训练.</p></blockquote><p>连续Mask的长度$k$ 为超参数. 作者巧妙地利用不同$k$ 的取值将BERT和GPT统一在了同一个框架中:</p><ul><li><p>BERT:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mass2.jpg" style="zoom: 50%;" /><p>当$k=1$ 时, Decoder没有输入任何额外信息, 单纯利用Encoder的信息预测出被Mask掉的Token. 从双向利用上下文的角度来说, BERT是MASS的一种特殊情况.</p></li><li><p>GPT:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mass12.jpg" style="zoom: 50%;" /><p>当$k=m$ 时, MASS直接就变成了自回归式的生成模型, 需要预测所有的Token. Encoder没有给Decoder任何信息.</p></li><li><p>MASS:</p><p>MASS便是介于BERT和GPT二者之间的, $k\in(1, m)$.</p></li></ul><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mass3.jpg" style="zoom: 50%;" /><p>因此, MASS最大的特点也是在BERT和GPT之间做了一个<strong>折中</strong>, 以Mask的形式进一步将BERT和GPT的特点结合到同一个框架下.</p><h3 id="Discussions"><a href="#Discussions" class="headerlink" title="Discussions"></a>Discussions</h3><p>这部分作者简述了MASS的几个特点:</p><ul><li><p>Encoder和Decoder联合训练. 在其他语言模型或BERT中都只是单独训练Encoder或Decoder. </p><blockquote><p>这点是针对预训练来说的, MASS将问题转化为Seq2Seq后, 当然可以做到对于任何问题, 都用联合训练好的Encoder和Decoder解决.</p></blockquote></li><li><p>连续的Mask能带给Encoder更好的NLU能力和Decoder更好的解码能力.</p></li><li><p>使Decoder能从Encoder获取更多的信息, 而不是获取Decoder自身生成的信息.</p></li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文.</p><h3 id="Neural-Meachine-Translation"><a href="#Neural-Meachine-Translation" class="headerlink" title="Neural Meachine Translation"></a>Neural Meachine Translation</h3><p>无监督机器翻译上的BLEU得分如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mass4.jpg" style="zoom: 50%;" /><p>MASS在无监督机器翻译上表现还可以, 除了英语翻译到法语, 剩下的数据集基本上涨了一个点.</p><p>与其他预训练方法相比, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mass5.jpg" style="zoom: 50%;" /><p>MASS比其他的预训练方法平均涨了3个点左右.</p><p>作者还展示了MASS在少资源状态下的机器翻译下的效果:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mass6.jpg" style="zoom: 50%;" /><blockquote><p>这个实验的Baseline选取还是拿没有预训练的模型和做过预训练的MASS去Fine Tuning后的结果进行比较… 自然是预训练后又微调的效果好. 所以少资源状态下的实验结果和结论都没有说服力, 在下一小节中的低资源实验也是类似的情况, 不再特意说明.</p></blockquote><h3 id="Text-Summarization"><a href="#Text-Summarization" class="headerlink" title="Text Summarization"></a>Text Summarization</h3><p>文本摘要任务上的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mass7.jpg" style="zoom: 50%;" /><p>在高资源状态下, 相较于Baseline, 效果有一些提升, 但似乎没有很大.</p><p>与其他的预训练方法比较结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mass8.jpg" style="zoom: 50%;" /><p>MASS结果有些许提升.</p><h3 id="Conversational-Response-Generation"><a href="#Conversational-Response-Generation" class="headerlink" title="Conversational Response Generation"></a>Conversational Response Generation</h3><p>对话生成任务的PPL如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mass9.jpg" style="zoom: 50%;" /><p>与BERT相比, MASS的PPL低了很多.</p><h3 id="Analysis-of-MASS"><a href="#Analysis-of-MASS" class="headerlink" title="Analysis of MASS"></a>Analysis of MASS</h3><h4 id="Study-of-Different-k"><a href="#Study-of-Different-k" class="headerlink" title="Study of Different k"></a>Study of Different k</h4><p>作者探究了$k$ 值的设定对各类任务性能的影响:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mass10.jpg" style="zoom: 50%;" /><p>非常神奇的是所有任务几乎都在为$k=50\% \ast m$ 效果最好.</p><h4 id="Ablation-Study-of-MASS"><a href="#Ablation-Study-of-MASS" class="headerlink" title="Ablation Study of MASS"></a>Ablation Study of MASS</h4><p>作者对MASS的改动做了消融实验:</p><ol><li>将MASS的连续Mask方式改为离散, 记为Discrete.</li><li>将Decoder输入的Token不再打Mask, 记为Feed.</li></ol><p>在en - fr上的无监督机器翻译结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mass11.jpg" style="zoom: 50%;" /><p>MASS的这两个改动确实有性能提升, 但似乎没有特别大的变化.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>MASS与BART属于相同类型的模型, 二者处理思路都是重新把<strong>BERT拉回到Seq2Seq框架</strong>下做<strong>生成</strong>任务. 这二者的侧重点不同, MASS更侧重对<strong>Decoder</strong>的改进, 而BART更侧重对<strong>加噪方法</strong>做出调整. </p><p>从实验结果来看, MASS的性能提升并没有很大, 似乎其有效性也受到争议, 主要原因是实验设计<strong>说服力不够</strong>.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> BERT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SpanBERT: Improving Pre-training by Representing and Predicting Spans</title>
      <link href="/posts/34019.html"/>
      <url>/posts/34019.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>BERT: 详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>.</li></ul></blockquote><h1 id="SpanBERT-Improving-Pre-training-by-Representing-and-Predicting-Spans"><a href="#SpanBERT-Improving-Pre-training-by-Representing-and-Predicting-Spans" class="headerlink" title="SpanBERT: Improving Pre-training by Representing and Predicting Spans"></a>SpanBERT: Improving Pre-training by Representing and Predicting Spans</h1><p>本文是论文<a href="https://www.aclweb.org/anthology/2020.tacl-1.5/" target="_blank" rel="noopener">SpanBERT: Improving Pre-training by Representing and Predicting Spans</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者认为, 许多NLP任务中所涉及到的推理任务是关于<strong>多个</strong>Text Token之间的, 而非基于单个Token之间的. </p><p>例如, 在回答问题<code>Which NFL team won Super Bowl 50?</code>时, 直接给出答案<code>Denver Broncos</code>比在<code>Denver</code>后给出<code>Broncos</code>要困难的多, 但前者却更贴近与现实场景, 难度也更大.</p><p>作者尝试提出SpanBERT来解决这种Span Level Prediction的问题.</p><h2 id="SpanBERT"><a href="#SpanBERT" class="headerlink" title="SpanBERT"></a>SpanBERT</h2><p>SpanBERT通过三种方式来帮助模型理解语言:</p><ol><li>将Token Level Mask替换为<strong>Span Level</strong> Mask.</li><li>引入一种新的辅助目标来<strong>SBO</strong>帮助模型训练.</li><li>只使用<strong>单个句子</strong>训练, 而非BERT所使用的句子对.</li></ol><h3 id="Span-Masking"><a href="#Span-Masking" class="headerlink" title="Span Masking"></a>Span Masking</h3><p>在BERT中, 对于给定的输入序列$X=\left(x_{1}, x_{2}, \ldots, x_{n}\right)$, 其编码表示为:<br>$$<br>\operatorname{enc}\left(x_{1}, x_{2}, \ldots, x_{n}\right)=\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{n}<br>$$<br>但在BERT中, 打Mask是对单个Token做的. 在SpanBERT中, 作者使用Span Level的Mask, 对一连串的Token做指定长度的Mask. 通过<strong>均匀分布</strong>随机采样得到Span Mask的初始位置.</p><p>而Span Mask的长度是通过<strong>迭代随机采样</strong>的来的, 继续扩展Span的几率服从<strong>几何分布</strong>$\ell \sim \operatorname{Geo}(p=0.2)$, 最大的Span长度$\ell_{\max }=10$. 即连续采样, 每次都有$p=0.2$ 的几率继续扩展Span, 每次都有$q = 1 - p = 0.8$ 的几率停止扩展Span. 其概率分布如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spanbert2.jpg" style="zoom:33%;" /><p>那么Span长度的期望值为3.8, 其推导过程如下:<br>$$<br>\begin{aligned}<br>&amp;p = 0.2 \\<br>&amp;q = 1 - p = 0.8 \\<br>&amp;p^\prime = \frac{p}{1-q^{10}} = 0.224 \\<br>\end{aligned}<br>$$<br>$p^\prime$ 代表Span最大长度为10以下的概率的归一化. $1-q^{10}$ 为当前Span不超过10的概率和, 需要用$p$ 去除以这个值, 让$p^\prime$ 排除掉Span长度大于10的情况.</p><blockquote><p>参考<a href="https://zhuanlan.zhihu.com/p/75893972" target="_blank" rel="noopener">SpanBert：对 Bert 预训练的一次深度探索</a>的评论区.</p></blockquote><p>故限定Span最大长为10的期望为:<br>$$<br>\begin{aligned}<br>E(x) &amp;= p^\prime \sum_{n=1}^{10}n q^{n-1} \\<br> &amp;= p^\prime (1 + 2q + 3q^2 + \dots + 10q^9) \\<br>  &amp;= 0.224 \cdot 16.9469=3.797 \approx3.8<br>\end{aligned}<br>$$<br>即Span长度期望为3.8, 如果向上取整就是4.</p><p>Span Masking将Token Level的Mask变更为了Span Level的Mask, 任务的训练难度更为复杂.</p><h3 id="Span-Boundary-Objective"><a href="#Span-Boundary-Objective" class="headerlink" title="Span Boundary Objective"></a>Span Boundary Objective</h3><p>作者希望Span的<strong>结尾</strong>能尽可能多的表示出Span<strong>内部</strong>的内容, 因此作者直接引入一个新的辅助目标来实现.</p><blockquote><p>SBO的提出应该也受到一些其他模型的启发, 例如<strong>ERNIE(Baidu)</strong>, <strong>BERT WWM</strong>等模型, 都是基于Span Level Mask做的额外处理.</p></blockquote><p>对于Span外部的起始边界和结束边界$(s, e)$, 作者希望通过某种方式$f(\cdot)$, 来根据其边界两侧的表示$\mathbf{x}_{s-1}, \mathbf{x}_{e+1}$, 以及<strong>每个Span内部的Token</strong> $x_i$ 所对应的位置编码$\mathbf{p}_{i-s+1}$ 来得到预测结果$\mathbf{y}_i$, 从而预测出Span内的每个Token:</p><p>$$<br>\mathbf{y}_{i}=f\left(\mathbf{x}_{s-1}, \mathbf{x}_{e+1}, \mathbf{p}_{i-s+1}\right)<br>$$</p><p>这里作者简单的使用两层FFN和GeLU来将$\mathbf{y}_i$ 转换为$x_i$:</p><p>$$<br>\begin{array}{l}<br>\mathbf{h}_{0}=\left[\mathbf{x}_{s-1} ; \mathbf{x}_{e+1} ; \mathbf{p}_{i-s+1}\right] \\<br>\mathbf{h}_{1}=\text { LayerNorm }\left(\operatorname{GeLU}\left(\mathbf{W}_{1} \mathbf{h}_{0}\right)\right) \\<br>\mathbf{y}_{i}=\text { LayerNorm }\left(\operatorname{GeLU}\left(\mathbf{W}_{2} \mathbf{h}_{1}\right)\right)<br>\end{array}<br>$$</p><p>$[\cdot]$ 代表拼接操作.</p><p>SBO将和MLM的损失函数共同优化, 二者之间是简单的加和关系即可:<br>$$<br>\begin{aligned}<br>\mathcal{L}\left(x_{i}\right) &amp;=\mathcal{L}_{\mathrm{MLM}}\left(x_{i}\right)+\mathcal{L}_{\mathrm{SBO}}\left(x_{i}\right) \\<br>&amp;=-\log P\left(x_{i} \mid \mathbf{x}_{i}\right)-\log P\left(x_{i} \mid \mathbf{y}_{i}\right)<br>\end{aligned}<br>$$<br>那么根据SBO, 模型可能会学到一些关于Span的内容, 因为SBO要求模型必须用边界信息来猜Span内部指定位置的内容.</p><p>下面给出一个说明SBO的例子:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spanbert1.jpg" style="zoom: 50%;" /><p>在该例中, $\text{football}$ 被选中, 将在其附近给长度为4的区域打上Mask, 这个词应该根据被Mask的区域的<strong>边界表示</strong>$\mathbf{x}_3, \mathbf{x}_9$, 以及$\text{football}$ 在Span Mask中的<strong>相对位置</strong>的编码$\mathbf{p}_3$ 共同预测出来.</p><p>所以在该例中, 损失函数为MLM预测$\text{football}$ 和SBO预测$\text{football}$ 的损失之和:<br>$$<br>\begin{aligned}<br>\mathcal{L}(\text { football }) &amp;=\mathcal{L}_{\mathrm{MLM}}(\text { football })+\mathcal{L}_{\mathrm{SBO}}(\text { football }) \\<br>&amp;=-\log P\left(\text { football } \mid \mathbf{x}_{7}\right)-\log P\left(\text { football } \mid \mathbf{x}_{4}, \mathbf{x}_{9}, \mathbf{p}_{3}\right)<br>\end{aligned}<br>$$</p><h3 id="Single-Sequence-Training"><a href="#Single-Sequence-Training" class="headerlink" title="Single - Sequence Training"></a>Single - Sequence Training</h3><p>在SpanBERT中, 作者舍弃了BERT的句子对训练法, 仅使用<strong>单句训练</strong>, 并抛弃NSP任务, 理由如下: </p><ol><li>句子对的引入限制了<strong>单句最长文本长度</strong>. 使用单句训练, 最长文本长度可以直接<strong>翻倍</strong>.</li><li>句子对上下不相关时, 会引入非常大的<strong>噪声</strong>.</li></ol><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文.</p><h3 id="Extractive-Question-Answering"><a href="#Extractive-Question-Answering" class="headerlink" title="Extractive Question Answering"></a>Extractive Question Answering</h3><p>抽取式QA的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spanbert3.jpg" style="zoom:33%;" /><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spanbert4.jpg" style="zoom:33%;" /><p>在这些数据集上的提升趋势是一致的, 每个数据集都有提升.</p><h3 id="Coreference-Resolution"><a href="#Coreference-Resolution" class="headerlink" title="Coreference Resolution"></a>Coreference Resolution</h3><p><strong>指代消解</strong>的任务目标是将文本中提到的同一实体的不同表述找出来, 即<strong>将同一事物的不同自然语言描述链接到文本中的同一事物</strong>.</p><p>对指代消解任务, 每个Mention Span $x$, 它与之前文本中对应的多个定长Span $y^\prime \in Y$ 都有一个得分$s(x, y)$, 根据得分使用Softmax能够判断出$y$ 是哪个Span的指代:</p><p>$$<br>P(y)=\frac{e^{s(x, y)}}{\sum_{y^{\prime} \in Y} e^{s\left(x, y^{\prime}\right)}}<br>$$</p><p>其中Span Pair的打分函数$s(x, y)$ 是由FFN得来的:</p><p>$$<br>\begin{aligned}<br>s(x, y) &amp;=s_{m}(x)+s_{m}(y)+s_{c}(x, y) \\<br>s_{m}(x) &amp;=\mathrm{FFNN}_{m}\left(\mathbf{g}_{\mathrm{x}}\right) \\<br>s_{c}(x, y) &amp;=\mathrm{FFNN}_{c}\left(\mathbf{g}_{\mathrm{x}}, \mathbf{g}_{\mathbf{y}}, \phi(x, y)\right)<br>\end{aligned}<br>$$</p><p>在这里, $\mathbf{g}_x, \mathbf{g}_y$ 代表两个Transformer提取出Span<strong>两个端点</strong>的隐态输出和Span内部的<strong>Attention的加权求和</strong>后的拼接向量, $\text{FFNN}_m, \text{FFNN}_c$ 代表两个有一层隐层的前馈神经网络, $\phi(x, y)$ 代表人工构建的特征.</p><blockquote><p>和论文<a href="https://arxiv.org/abs/1908.09091" target="_blank" rel="noopener">BERT for Coreference Resolution: Baselines and Analysis</a>的使用方法一致, BERT系列指代消解模型是BiLSTM + Attention系列模型在C2F - Coref上的升级.</p><p>更多关于神经网络指代消解的论文, 还可以参考:</p><ul><li><a href="https://arxiv.org/abs/1707.07045" target="_blank" rel="noopener">End-to-end Neural Coreference Resolution</a></li><li><a href="https://arxiv.org/abs/1804.05392" target="_blank" rel="noopener">Higher-order Coreference Resolution with Coarse-to-fine Inference</a></li></ul></blockquote><p>指代消解的数据集OntoNotes上表现如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spanbert5.jpg" style="zoom:33%;" /><p>相较于其他的BERT Baseline, SpanBERT有些许提升, 并且提升在每个数据集上都是一致的.</p><h3 id="Relation-Extraction"><a href="#Relation-Extraction" class="headerlink" title="Relation Extraction"></a>Relation Extraction</h3><p>在关系抽取的数据集TACRED上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spanbert6.jpg" style="zoom:33%;" /><p>SpanBERT在关系抽取上的标现达到平均水平, 在Recall上进步比较大.</p><h3 id="GLUE"><a href="#GLUE" class="headerlink" title="GLUE"></a>GLUE</h3><p>GLUE上的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spanbert7.jpg" style="zoom:33%;" /><p>相较于其他BERT Baseline, SpanBERT提升也是全面的. 尤其是在二分类数据集QNLI上提升比较大, 我认为这可能是SBO带来的提升.</p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><h4 id="Masking-Schemes"><a href="#Masking-Schemes" class="headerlink" title="Masking Schemes"></a>Masking Schemes</h4><p>作者将Subword, Whole Words, Named Entities, Noun Phrases, Geometric Spans几种Mask策略放在一起做了对比:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spanbert8.jpg" style="zoom:33%;" /><p>能够看出, SpanBERT中的Geometric Spans几乎是最有效的, 但在Coreference上似乎没有与其他提升一致. 但其他的策略也都不如BERT最开始使用的Subword Tokens策略要好.</p><h4 id="Auxiliary-Objectives"><a href="#Auxiliary-Objectives" class="headerlink" title="Auxiliary Objectives"></a>Auxiliary Objectives</h4><p>不同的辅助目标对Span BERT的影响如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/spanbert9.jpg" style="zoom:33%;" /><p>如果使用NSP任务, 性能会比不使用NSP并单句训练的SpanBERT有损, 加上SBO任务后会使得Coreference上的表现大幅提升(相较于上表).</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>SpanBERT作为一种BERT的改进方案, 提出了更完备的Span Masking方案. SpanBERT加入了SBO任务, 使用边界信息和被Mask的Token在Span内的相对位置信息能预测出Span内部的内容. 也没有使用句子对作为训练方式, 抛弃了对性能有损害的NSP任务.</p><p>SpanBERT在各类任务上性能皆超过BERT, 应该算作是一种比较有效的改进方式, 方法也比较巧妙.</p><blockquote><p>我感觉效果全面提升的原因主要是增大了模型的训练难度, 并且包含有一定的随机性, 在预测时, 需要对每个Span内的Token都做预测, 大大的强化了Span左右两端判断Span内部内容的能力, 这样比漫无目的的Mask要有效得多, 在不同的位置编码下, 需要判断Span内部不同的内容, 进一步的提高了模型对两端信息的利用率, 使得两端的隐态更有意义, 更有内容.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> BERT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding</title>
      <link href="/posts/51848.html"/>
      <url>/posts/51848.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>BERT: 详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>.</li></ul></blockquote><h1 id="StructBERT-Incorporating-Language-Structures-into-Pre-training-for-Deep-Language-Understanding"><a href="#StructBERT-Incorporating-Language-Structures-into-Pre-training-for-Deep-Language-Understanding" class="headerlink" title="StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding"></a>StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding</h1><p>本文是论文<a href="http://arxiv.org/abs/1908.04577" target="_blank" rel="noopener">StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>虽然BERT和RoBERTa将注意力放在了NLU问题上, 并大幅度的化简NLU相关的任务难度, 但仍然没有把<strong>语言结构信息</strong>集成进去.</p><blockquote><p>如”研表究明, 汉字序顺并不定一影阅响读. 比如当你看完这句话后, 才发这现里的字全是都乱的”. 尤其在使用了Self  Attention后, Token之间的最短距离恒为1, 自然语言中的单词顺序可能对BERT没那么重要了, 语言的结构可能也就得不到关注.</p></blockquote><p>因此, 作者希望改进BERT的训练方式, 使其能够注意到句子中的结构信息的变化.</p><h2 id="StructBERT"><a href="#StructBERT" class="headerlink" title="StructBERT"></a>StructBERT</h2><p>作者在保留BERT现有的两个训练任务的基础上, 额外引入两个<strong>辅助任务</strong>, 分别是Word Structural Objective和Sentence Structural Objective, 即从<strong>单词</strong>和<strong>句子</strong>的两个角度来提升BERT对语言结构的理解.</p><h3 id="Input-Representation-and-Transformer-Encoder"><a href="#Input-Representation-and-Transformer-Encoder" class="headerlink" title="Input Representation and Transformer Encoder"></a>Input Representation and Transformer Encoder</h3><p>和BERT一样, 对于输入序列(可能是单句子也可能是句子对)的每个Token $t_i$, 将它的Word Embedding, Segment Embedding, Position Embedding相加作为其初始表示$\mathbf{x}_i$, 然后能通过对$L$ 层Transformer Encoder的堆叠获得其在当前语境下的表示$\mathbf{h}_i$.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert2.jpg" style="zoom: 50%;" /><p>同理, 输入句子时, 句向量$\mathbf{X}=\left\{\mathbf{x}_{i}\right\}_{i=1}^{N}$ 会被$L$ 层Transformer Encoder编码为$\mathbf{H}^l$:<br>$$<br>\mathbf{H}^{l}=\text { Transformer }_{l}\left(\mathbf{H}^{l-1}\right)<br>$$<br>其中$l \in [1, L]$, $\mathbf{H}^0 = X$, $\mathbf{H}^{L}=\left[\mathbf{h}_{1}^{L}, \cdots, \mathbf{h}_{N}^{L}\right]$.</p><h3 id="Word-Structural-Objective"><a href="#Word-Structural-Objective" class="headerlink" title="Word Structural Objective"></a>Word Structural Objective</h3><p>作者认为, 一个好的语言模型必须能够通过句子中打乱顺序的单词组<strong>恢复</strong>出单词的原来顺序, 而BERT仍不能正确的做到这一点. 针对这点, 作者直接将这种想法作为训练任务补充到BERT的训练当中.</p><p>该任务的目标为:<br>$$<br>\arg \max _{\theta} \sum \log P\left(\operatorname{pos}_{1}=t_{1}, \operatorname{pos}_{2}=t_{2}, \ldots, \operatorname{pos}_{K}=t_{K} \mid t_{1}, t_{2}, \ldots, t_{K}, \theta\right)<br>$$<br>$\theta$ 为参数, $K$ 为打乱顺序的子序列长度. </p><p>如果$K$ 比较大, 意味着模型需要对比较长的序列重建, 噪声比较多, $K$ 比较小, 模型只需要对比较短的子序列重建, 噪声相应的也比较少. 为达到模型鲁棒性和模型重建能力的平衡, 作者设定$K=3$.</p><p>该任务与原始MLM训练目标是不冲突的, 能够<strong>联合训练</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/structbert1.jpg" style="zoom:33%;" /><p>在输入为<code>[MASK]</code>的地方应该能被预测出正确被Mask掉的Token, 对于输入打乱的地方应该能根据$\mathbf{h}_i^L$ 恢复出正确顺序的Token.</p><blockquote><p>Word Structural Objective主要针对单句子任务.</p></blockquote><h3 id="Sentence-Structural-Objective"><a href="#Sentence-Structural-Objective" class="headerlink" title="Sentence Structural Objective"></a>Sentence Structural Objective</h3><p>除去单词级别的结构, 还需要关注句子和句子之间的结构.</p><p>在BERT训练中, 使用的是<strong>NSP任务</strong>, 而在RoBERTa中提出NSP任务由于过于<strong>简单</strong>, 有害于模型性能. </p><p>其实NSP任务也不是不能用, 必须增加它的任务难度. 在StructBERT中, 沿着组装句子的思路, NSP被<strong>改进</strong>成一个<strong>三分类问题</strong>, 分别令当前句子$S_1$ 与另一个句子$S_2$ 组合, $S_2$ 可能是以下的其中一种:</p><ol><li>$S_2$ 为$S_1$ 的<strong>下</strong>一句, 此时任务为原始的NSP任务.</li><li>$S_2$ 为$S_1$ 的<strong>上</strong>一句.</li><li>$S_2$ 为<strong>其他文档</strong>的<strong>随机</strong>一句.</li></ol><p>这三种情况均为<strong>等概率</strong>发生, 即发生概率均为$\frac{1}{3}$, <code>[SEP]</code> 的添加与BERT相同, 采用<code>[CLS]</code>处的输出做三分类结果. 示意图如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/structbert2.jpg" style="zoom: 50%;" /><blockquote><p>Sentence Structural Objective主要针对句子对任务.</p></blockquote><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的实验设置请参照原论文.</p><h3 id="General-Language-Understanding"><a href="#General-Language-Understanding" class="headerlink" title="General Language Understanding"></a>General Language Understanding</h3><h4 id="GLUE-benchmark"><a href="#GLUE-benchmark" class="headerlink" title="GLUE benchmark"></a>GLUE benchmark</h4><p>在GLUE上的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/structbert3.jpg" style="zoom: 50%;" /><p>StructBERT在GLUE上的平均表现超过了其他的PLM.</p><h4 id="SNLI"><a href="#SNLI" class="headerlink" title="SNLI"></a>SNLI</h4><p>在自然语言推理的数据集SNLI上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/structbert4.jpg" style="zoom:33%;" /><h3 id="Extractive-Question-Answering"><a href="#Extractive-Question-Answering" class="headerlink" title="Extractive Question Answering"></a>Extractive Question Answering</h3><p>在抽SQuAD1.1上的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/structbert5.jpg" style="zoom:33%;" /><p>即使没有使用任何的数据增强和额外数据, StructBERT还是仅次于使用了数据增强和额外数据的XLNet.</p><blockquote><p>其实打乱顺序这种处理也可以看做是一种数据增强.</p></blockquote><h3 id="Effect-of-Different-Structural-Objectives"><a href="#Effect-of-Different-Structural-Objectives" class="headerlink" title="Effect of Different Structural Objectives"></a>Effect of Different Structural Objectives</h3><p>消融实验便是针对StructBERT的两个额外任务做的:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/structbert6.jpg" style="zoom:33%;" /><p>前三个数据集是单句任务, 在去掉Word Structural Objective后, 前三个任务的性能有退化. 后三个是句子对任务, 去掉Sentence Structural Objective后对后三个任务影响也比较大. 证明了这两种新增的任务的有效性.</p><p>下图分别是Word Prediction Loss, Word Prediction Acc, Sentence Prediction Loss, Sentence Prediction Acc随着训练步长的增长的变化曲线:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/structbert7.jpg" style="zoom: 50%;" /><p>红色代表原始BERT, 蓝色为StructBERT, 绿色为StructBERT在Masked Token任务上的表现.</p><p>作者认为, 加入Word Structural Objective使得MLM任务做的更好, 加入Sentence Structural Objective明显使得任务变得更难.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>StructBERT通过在原有任务的基础上从Word Level和Sentence Level添加了两个新的辅助训练任务, 使得BERT能够关注一些语言结构, 想法非常非常简单, 但效果却出乎意料的好. </p><p>StructBERT属于对BERT的改进, 没有太多值得进一步的建议.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BART和mBART</title>
      <link href="/posts/1394.html"/>
      <url>/posts/1394.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li><p>Transformer: 详见<a href="https://adaning.github.io/posts/6744.html">Transformer精讲</a>.</p></li><li><p>BERT, GPT: 详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>.</p></li></ul></blockquote><h1 id="BART和mBART"><a href="#BART和mBART" class="headerlink" title="BART和mBART"></a>BART和mBART</h1><p>本文是如下论文的阅读笔记和个人理解:</p><ul><li><a href="https://arxiv.org/abs/1910.13461" target="_blank" rel="noopener">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a></li><li><a href="http://arxiv.org/abs/2001.08210" target="_blank" rel="noopener">Multilingual Denoising Pre-training for Neural Machine Translation</a></li></ul><h2 id="BART"><a href="#BART" class="headerlink" title="BART"></a>BART</h2><h3 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h3><p>Transformer浑身都是宝, 相较于直接延续Transformer的Seq2Seq架构, 更广为使用的是把<strong>Encoder</strong>和<strong>Decoder</strong>的单独堆叠, 分别对应着只使用Encoder的BERT和只使用Decoder的GPT. 如果只是单纯的使用其中的某一部分, 就会造成两个鸿沟:</p><ul><li><p><strong>BERT</strong>: 具备<strong>双向语言理解</strong>能力的却不具备做<strong>生成任务</strong>的能力.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bart1.jpg" style="zoom: 50%;" /></li><li><p><strong>GPT</strong>: 拥有<strong>自回归</strong>特性的却不能更好的从<strong>双向理解</strong>语言.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bart2.jpg" style="zoom: 50%;" /><p>因此, 作者希望将二者融合, 让模型保留双向语言理解能力的同时, 也能解决生成任务.</p></li></ul><h3 id="BART-Model"><a href="#BART-Model" class="headerlink" title="BART Model"></a>BART Model</h3><p><strong>BART</strong>(<strong>B</strong>idirectional and <strong>A</strong>uto - <strong>R</strong>egressive <strong>T</strong>ransformers)的结构非常的简单, 既然只使用Encoder或者只使用Decoder不能让鱼和熊掌兼得, 那就重新把它们组装回来:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bart3.jpg" style="zoom: 50%;" /><p>左侧为BERT(Transformer Encoder), 右侧为GPT(Transformer Decoder).</p><p>所以BART采用的其实还是<strong>标准Transformer</strong>结构, 如果把Encoder和Decoder组装回来, 就又成了标准Transformer:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer.jpg" style="zoom: 50%;" /><p>但是, BART所采用的<strong>输入数据</strong>和<strong>训练目标</strong>和Transformer<strong>完全不一样</strong>, 换句话说, 作者希望BART所做的事情和Transformer是完全不一样的, 这也是BART与Transformer的最大区别.</p><p>作者指出, BART和BERT有两点最大的  不同:</p><ol><li>除了对Encoder的堆叠外, 还使用了Decoder, 并添加了Encoder和Decoder之间的<strong>Cross Attention</strong>(其实就是结构和标准Transformer一样).</li><li>BERT在预测时加了额外的<strong>FFN</strong>, 而BART没使用FFN. 整体来说, 包含了Decoder的BART只是比同级别的BERT多了<strong>10%</strong>的参数.</li></ol><h3 id="Pre-Training-BART"><a href="#Pre-Training-BART" class="headerlink" title="Pre - Training BART"></a>Pre - Training BART</h3><p>BART使用的是类似BERT的<strong>Denoising AutoEncoder</strong>的形式来训练的, 即模型需要对被添加噪声的数据<strong>去噪</strong>, <strong>恢复</strong>出原始数据.</p><blockquote><p>我猜测, 之所以BART名字是仿照BERT, 而不是仿照Transformer最大原因, 是因为BERT和BART都是去噪自编码器, 而Transformer不是.</p></blockquote><p>BART允许对原始数据做<strong>任意形式</strong>的噪声干扰, 作者提出了五种可行的添加噪声的方式:</p><ul><li><strong>Token Masking</strong>: 与BERT打<code>[Mask]</code>的策略完全相同.</li><li><strong>Token Deletion</strong>: 直接随机删除某些Token.</li><li><strong>Text Infilling</strong>: 同时选中多个连续的Token, 仅替换成一个<code>[Mask]</code>, 或者在原始数据中随机插入Mask Token(即使没有数据缺失). 模型不知道<code>[Mask]</code>对应的是多少个Token, 也不知道<code>[Mask]</code>是否有效. 这就要求模型有强大的学习能力.</li><li><strong>Sentence Permutation</strong>: 将一个文档中的句子之间的顺序打乱.</li><li><strong>Document Rotation</strong>: 从文档中随机选定一个Token作为整个文档的起始Token, 对文档Rotation. 该方法令模型能识别文档的起始Token.</li></ul><p>上述五种方式的示例如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bart4.jpg" style="zoom: 33%;" /><blockquote><p>即使有Decoder, 也不需要让Encoder和Decoder对应的部分对齐. 在极端情况下, 所有的原始信息都丢失了, 此时BART相当于Language Model. BART添加噪声的方式不单包含了Token Masking, 还包含了更为复杂的噪声, 它们也可以互相组合. 想从这些噪声中按照原语序恢复出句子还是非常困难的, 因为它们包含了缺失, 乱序的情况. </p></blockquote><p>在输入加噪的数据后, BART先通过Encoder双向编码, 再通过Decoder用自回归极大似然解码, 恢复出原始序列.</p><h3 id="Fine-Tuning-BART"><a href="#Fine-Tuning-BART" class="headerlink" title="Fine - Tuning BART"></a>Fine - Tuning BART</h3><p>BART在预训练后, 需要对下游任务微调. 对于不同的任务, BART有不同的使用方法:</p><ul><li><p><strong>Sequence Classification</strong>: 对序列分类任务, Encoder和Decoder以相同的数据输入, 利用Decoder的最后一次输出代表整个句子的表示, 类似于BERT中的<code>[CLS]</code>. 只不过BART是有Decoder的, 所以需要让Decoder的输出作为整个句子的表示.</p></li><li><p><strong>Token Classification</strong>: 对于Token分类任务, Encoder和Decoder也以相同的数据输入, BART也将Decoder中所对应Token的输出作为分类的隐藏状态.</p></li><li><p><strong>Sequence Generation</strong>: 对于序列生成任务, BART直接适应生成任务, 对Encoder和Decoder微调即可. 该类任务包括文本摘要, 问答等.</p></li><li><p><strong>Machine Translation</strong>: 机器翻译任务比较特殊, 因为它的任务输入和输出是两种不同的语言. </p><p>结合先前在机器翻译上的研究, 额外添加一个专门用于外语映射的Encoder(例如其他语言映射到英语)将有助于模型性能的提升. 所以BART需要训练一个新的Encoder来将源语言与目标语言语义空间对齐, 来替代掉BART原来的Word Embedding, 在完成对齐后, BART将把源语言转换为目标语言, 与Transformer保持一致.</p><p>训练分为了两个阶段:</p><ol><li>一阶段中, 对BART的大多数参数冻结, 只更新随机初始化的源语言Encoder, BART的位置编码, BART Encoder的第一层Self - Attention投影矩阵.</li><li>二阶段中, 对整个BART(包含后来添加的Encoder)中的所有参数做少次迭代.</li></ol></li></ul><p>对于上述四种任务的处理概括为下图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bart5.jpg" style="zoom: 45%;" /><p>左侧为处理分类问题的示意图, 右侧为处理机器翻译的示意图.</p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>详细的实验设置请参照原论文.</p><h4 id="Comparsion-Pre-Training-Objectives"><a href="#Comparsion-Pre-Training-Objectives" class="headerlink" title="Comparsion Pre - Training Objectives"></a>Comparsion Pre - Training Objectives</h4><p>作者做了各不同预训练目标的模型的效果对比, 这些模型并不是原始论文中的模型, 而是作者或多或少调整过的. 其中所使用的模型分别类似于:</p><ul><li>Language Model: GPT.</li><li>Permuted Language Model: XLNet.</li><li>Masked Language Mode: BERT.</li><li>Multitask Masked Language Model: UniLM.</li><li>Masked Seq - to - Seq: MASS.</li></ul><p>实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bart6.jpg" style="zoom: 50%;" /><p>从实验中看到, 使用Text Infilling的效果非常好, 只使用Document Rotation和Sentence Shuffling的效果比较差. 并且, 预训练的有效性高度取决于任务, 自回归式的模型有利于解决生成类任务.</p><h4 id="Discriminative-Tasks"><a href="#Discriminative-Tasks" class="headerlink" title="Discriminative Tasks"></a>Discriminative Tasks</h4><p>各类Large模型在SQuAD和GLUE上的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bart7.jpg" style="zoom: 50%;" /><p>RoBERTa和BART的表现相似, 但是BART能够在不牺牲性能的情况下将任务扩展到生成任务上, 这对BART来说是一个独天得厚的优势, 因为扩展只带来了将近10%的参数量增长.</p><h4 id="Generation-Tasks"><a href="#Generation-Tasks" class="headerlink" title="Generation Tasks"></a>Generation Tasks</h4><h5 id="Summarization"><a href="#Summarization" class="headerlink" title="Summarization"></a>Summarization</h5><p>在摘要任务上的实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bart8.jpg" style="zoom: 50%;" /><h5 id="Dialogue"><a href="#Dialogue" class="headerlink" title="Dialogue"></a>Dialogue</h5><p>在对话任务上的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bart9.jpg" style="zoom: 50%;" /><h5 id="Abstractive-QA"><a href="#Abstractive-QA" class="headerlink" title="Abstractive QA"></a>Abstractive QA</h5><p>抽象问答数据集上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bart10.jpg" style="zoom: 50%;" /><h4 id="Machine-Translation"><a href="#Machine-Translation" class="headerlink" title="Machine Translation"></a>Machine Translation</h4><p>在机器翻译任务上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bart11.jpg" style="zoom: 50%;" /><p>Baseline是标准Transformer. Fixed BART和Tuned BART分别代表单向翻译和使用<strong>反向翻译</strong>的BART. 没有反向翻译的BART效果不太好, 可能涉及到过拟合, 使用反向翻译后应该增强了泛化能力, 效果得到了改善.</p><blockquote><p>反向翻译指的是, 将目标语言输入模型得到源语言的预测结果, 其中预测错误的内容将视为噪声, 将该预测结果再重新输入新的模型, 让它生成目标语言, 这是一种常见的<strong>文本数据增强</strong>方式.</p></blockquote><h2 id="mBART"><a href="#mBART" class="headerlink" title="mBART"></a>mBART</h2><blockquote><p>但我不是MT方向的, 所以实验部分我只挑着看了比较关键的部分. 之所以把mBART也写在这篇文章中, 是因为mBART只是作为BART的多语言版本, 没有本质上的结构变化, 作者展示了一种基于BART处理机器翻译问题的方法, 比较有趣, 在本文中作为扩展内容. </p></blockquote><p><strong>mBART</strong>(<strong>M</strong>ultilingual <strong>B</strong>idirectional and <strong>A</strong>uto - <strong>R</strong>egressive <strong>T</strong>ransformers)是BART的<strong>多语言</strong>版本, 用于处理不同语言之间的<strong>机器翻译</strong>问题. </p><h3 id="mBART-Model"><a href="#mBART-Model" class="headerlink" title="mBART Model"></a>mBART Model</h3><p>mBART仍然是分为<strong>Pre - Training</strong>和<strong>Fine - Tuning</strong>两个阶段. Pre - Training将使用多个语种的语料作为预训练数据.</p><h4 id="Pre-Training-mBART"><a href="#Pre-Training-mBART" class="headerlink" title="Pre - Training mBART"></a>Pre - Training mBART</h4><p>预训练阶段延续了BART的做法, 仍然采用<strong>Denoising AutoEncoder</strong>的方法令mBART的每条数据在<strong>单语言</strong>内训练. 目标是将单语言文本加噪干扰后再恢复回来. 作者采取了两种BART中的加噪方式: </p><ul><li><strong>Sentence Permutation</strong>: 打乱句子和句子之间的顺序.</li><li><strong>Word - Span masking</strong>: 连续的Mask掉一些内容, 并且只用一个<code>[Mask]</code>替换.</li></ul><p>除此外, mBART的初衷是多语言模型, 必须将语种的信息加入. 在文本输入结束后, 在句子末尾处需要加上句子结尾标识<code>&lt;\s&gt;</code>和对应语言的标识<code>[LID]</code>. </p><h4 id="Fine-Tuning-mBART"><a href="#Fine-Tuning-mBART" class="headerlink" title="Fine - Tuning mBART"></a>Fine - Tuning mBART</h4><p>微调阶段才针对机器翻译任务训练. 用<code>[LID]</code>替换Decoder原来的第一个输入<code>[Start]</code>, 表明要翻译成哪个语种.</p><p>Sentence Level MT和Document Level MT的主要区别就在于文本的长度不同, Document Level MT更困难一些.</p><p>上述方法概括为下图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mbart1.jpg" style="zoom: 67%;" /><p>左侧为预训练阶段, 每条数据使用单语言文本并加噪, mBART将其恢复为原来的单语言文本. 右侧为微调阶段, 针对某两种语言之间做微调, 输入为源语言, 期望输出为目标语言, Decoder的首次输入为目标语言的<code>&lt;LID&gt;</code>.</p><h3 id="Language-Transfer"><a href="#Language-Transfer" class="headerlink" title="Language Transfer"></a>Language Transfer</h3><p>作者额外提出了一种新的语言迁移的无监督机器翻译方式.</p><p>常见的的反向翻译示意图如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mbart2.jpg" style="zoom: 50%;" /><p>先用预训练权重初始化翻译模型, 然后对目标语言到源语言的翻译模型做训练, 生成源语言的文本作为扩充数据, 再将之前的平行语料和新生成源语言的文本共同作为训练数据, 训练源语言到目标语言的模型.</p><p>而作者提出的语言迁移方法如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mbart3.jpg" style="zoom: 50%;" /><p>直接在预训练阶段就注入多语言的平行语料, 使得模型能学习到不同语种之间潜在的共性, 在Fine Tuning后, 直接就能应对相似语种的翻译任务.</p><h3 id="Experiments-1"><a href="#Experiments-1" class="headerlink" title="Experiments"></a>Experiments</h3><p>详细的实验设置请参照原论文.</p><p>文中mBART25代表用25种语言预训练出的mBART. 文章中实验所涉及到的语言代码分别如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mbart4.jpg" style="zoom: 50%;" /><p>在低资源数据集上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mbart5.jpg" style="zoom: 50%;" /><p>Random代表不对每种翻译任务使用预训练, 直接在该任务上训练. mBART25效果要好.</p><p>在高资源数据集上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mbart6.jpg" style="zoom: 50%;" /><p>结果显示, 当数据集大小增加后, 不做预训练的效果反而是要好一点, mBART和Random表现相近.</p><p>作者尝试了不同语种到英文的Fine Tuning, 并使用不同的Testting Language观察不同Fine Tuning Language对最终结果的影响:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/mbart7.jpg" style="zoom: 33%;" /><p>灰色的部分为同一个语系中的语言. </p><p>这个表格意味着:</p><ul><li>主对角线上Fine Tuning时所采用的语料为<code>X-EN</code>, Testing时测试的任务为<code>X-X</code>.</li><li>同一X轴上代表使用的Fine Tuning Language相同(<code>X-EN</code>), 但采用了不同的Testing Language.</li><li>同一Y轴上代表采用了不同的Fine Tuning Language, 但使用的Testing Language相同(<code>X-X</code>).</li></ul><p>除去翻译最好的是自己的语言外, 次优的一般都是同一语系下的其他语言. 确实说明了语言之间存在一定的共性.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>BART算是BERT和GPT的集大成者, 它的结构和标准Transformer一致, 但与Transformer不同点在于<strong>数据输入</strong>和<strong>训练目标</strong>, 以<strong>自回归式去噪自动编码器</strong>的形式存在. </p><p>因为使用了Transformer Decoder, 使得BART具有了BERT不具备的处理生成任务的能力, 实验结果表明, 没有损失性能, 也没有添加过多的参数.</p><p>作者尝试了包括Masking在内的多种<strong>噪声</strong>的添加方式, 这些噪声的干扰非常强大, 强制要求模型也要有与之匹配的预测能力.</p><p>mBART作为BART多语言版本, 给出了一种基于BART的多语言机器翻译上的处理思路, 也揭示了机器翻译中同一语系下不同语种之间的一些潜在共性.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GAKE: Graph Aware Knowledge Embedding</title>
      <link href="/posts/60222.html"/>
      <url>/posts/60222.html</url>
      
        <content type="html"><![CDATA[<h1 id="GAKE-Graph-Aware-Knowledge-Embedding"><a href="#GAKE-Graph-Aware-Knowledge-Embedding" class="headerlink" title="GAKE: Graph Aware Knowledge Embedding"></a>GAKE: Graph Aware Knowledge Embedding</h1><p>本文是论文<a href="https://www.aclweb.org/anthology/C16-1062/" target="_blank" rel="noopener">GAKE: Graph Aware Knowledge Embedding</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>在现有的KGE方法中, 都是基于<strong>三元组</strong>的, 但三元组之间是相互<strong>孤立</strong>的, 并没有利用上图信息.</p><p>作者尝试提出一种<strong>融入图结构信息</strong>的KGE方法.</p><h2 id="GAKE"><a href="#GAKE" class="headerlink" title="GAKE"></a>GAKE</h2><p>KG中的知识以图的结构按如下方式存在着:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gake1.jpg" style="zoom: 50%;" /><p>作者将图中的结构信息划分为三类:</p><ul><li><strong>邻居上下文</strong>(Neighbor Context)</li><li><strong>边上下文</strong>(Edge Context)</li><li><strong>路径上下文</strong>(Path Context)</li></ul><p>如果能够处理图信息, 那么与其他KGE方法相比, 能够对<strong>Triple, Path, Edge</strong>同时建模:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gake2.jpg" style="zoom: 50%;" /><p>能够看到, 之前的方法一般都以三元组的方式对待KG, 即使融入路径信息, 也从未考虑图中的<strong>边信息</strong>.</p><h3 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h3><p>有向图可以被表示为$G=(V, E)$, $V, E$ 均为$G$ 中的Subject, 在GAKE中, 知识图谱$G=(V, E)$ 中的每一种<strong>顶点</strong>(实体)和每一种<strong>边</strong>(关系), 都是GAKE要学习的Embedding.</p><p>作者更一般的将$V, E$ 统称Subject $S$, 其中$s=(t, k)$. 当$t=0$ 时代表$s$ 为顶点, $t=1$ 时代表$s$ 为边. $k$ 为$s$ 在Subject Set $S$ 中的索引.</p><p>在给出Subject的定义后, 也能给出$s$ <strong>上下文</strong>的定义. 对于给定的$s_i$, 其图上下文$c(s_i)$ 为与$s_i$ 相关的Subject Set $\left\{s_w \mid s_w \in S, s_w \text{ relevant to }s_i \right\}$. 因为Subject是很抽象的, 所以对于<strong>不同类型的Subject会有不同的Context定义</strong>.</p><p>作者还定义了如下符号: $C(s)$ 代表$s$ 的图上下文, $\phi(s)$ 代表取$s$ 的Embedding, $\pi(C(s))$ 代表获取$s$ 在上下文情况下的表示, $\theta$ 为模型参数.</p><h3 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h3><p>GAKE是一种利用图结构信息的KGE范式, 其目标为预测图中缺失的Subject $s_i$, 那么缺失Subject $s_i$ 的概率应该在其上下文已知的情况下得到最大化:<br>$$<br>P\left(s_{i} \mid c\left(s_{i}\right)\right)=\frac{\exp \left(\phi\left(s_{i}\right)^{\top} \pi\left(c\left(s_{i}\right)\right)\right)}{\sum_{j=1}^{|S|} \exp \left(\phi\left(s_{j}\right)^{\top} \pi\left(c\left(s_{i}\right)\right)\right)}<br>$$<br>其中, $\pi(\cdot)$ 为聚合函数, 在这里先用简单的<strong>平均求和</strong>来计算各种上下文因素对$s_i$ 的影响:<br>$$<br>\pi\left(c\left(s_{i}\right)\right)=\frac{1}{\left|c\left(s_{i}\right)\right|} \sum_{s_{j} \in c\left(s_{i}\right)} \phi\left(s_{j}\right)<br>$$</p><h4 id="Neighbor-Context"><a href="#Neighbor-Context" class="headerlink" title="Neighbor Context"></a>Neighbor Context</h4><p>给定一个Subject $s_i$, 以$s_i$ 是实体为例, 它的上下文是它的每个一阶邻居和它们之间的关系. 所以$s_i$ 为实体时, 邻居上下文和$s_i$ 的所有相关三元组无异.</p><p>然后采用<strong>极大似然</strong>优化$s_i$ 的概率:<br>$$<br>O_{N}=\sum_{s_{i} \in S} \sum_{c_{N}\left(s_{i}\right) \in C_{N}\left(s_{i}\right)} \log p\left(s_{i} \mid c_{N}\left(s_{i}\right)\right)<br>$$<br>$C_N(s_i)$ 是$s_i$ 的Neighbor Context.</p><blockquote><p>作者没有说明Relation的邻居上下文是什么样的, 代码里也只是构造的实体邻居, 应该是本来就没有定义.</p></blockquote><h4 id="Path-Context"><a href="#Path-Context" class="headerlink" title="Path Context"></a>Path Context</h4><p>KG中的路径直接或间接的反映了<strong>实体之间</strong>的关系. 因此, 作者定义Path Context $c_P(s_i)$ 为<strong>Random Walk</strong>生成的一系列<strong>顶点</strong>和<strong>边</strong>的集合.</p><p>与Neighbor Context类似, 也是用<strong>极大似然</strong>去优化:<br>$$<br>O_{P}=\sum_{s_{i} \in S} \sum_{c_{P}\left(s_{i}\right) \in C_{P}\left(s_{i}\right)} \log p\left(s_{i} \mid c_{P}\left(s_{i}\right)\right)<br>$$</p><h4 id="Edge-Context"><a href="#Edge-Context" class="headerlink" title="Edge Context"></a>Edge Context</h4><p>对于一个实体, 所有与之<strong>直接相连</strong>的<strong>边</strong>都可能反映实体本身的情况. 因此, 当Subject为顶点时, Edge Context $c_E(s_i)$ 为$s_i$ 的边集.</p><p>同样, 还是用<strong>极大似然</strong>去优化:<br>$$<br>O_{E}=\sum_{s_{i} \in S} \log p\left(s_{i} \mid c_{E}\left(s_{i}\right)\right)<br>$$</p><h4 id="Context-extension"><a href="#Context-extension" class="headerlink" title="Context extension"></a>Context extension</h4><p>在作者提出的范式下, 如果有其他形式的Graph Context也可以继续<strong>扩展</strong>, 因为$c(s_i)$ 的含义可以随着对Context的定义变化而改变, 保证了一定的<strong>灵活性</strong>.</p><h3 id="Attention-Mechanism"><a href="#Attention-Mechanism" class="headerlink" title="Attention Mechanism"></a>Attention Mechanism</h3><p>在现实生活中, 每个Subject Context对Subject的本身的影响可能是不同的, 如果使用之前说的平均求和就略显不妥. 作者使用Attention机制对$s_i$ 的上下文元素加权求和, 使得模型更加灵活, 也更契合场景.</p><p> $a(s)$ 代表$s$ 所占的注意力权重, Attention计算方式如下:<br>$$<br>a\left(s_{i}\right)=\frac{\exp \left(\theta_{i}\right)}{\sum_{s_{j} \in C\left(s_{i}\right)} \exp \left(\theta_{j}\right)}<br>$$<br>$\theta$ 为要优化的参数.</p><p>在Attention机制下, $s_i$ 会受到其上下文元素的<strong>加权影响</strong>:<br>$$<br>\pi\left(c\left(s_{i}\right)\right)=\sum_{s_{j} \in c\left(s_{i}\right)} a\left(s_{j}\right) \phi\left(s_{j}\right)<br>$$<br>当模型预测<code>English</code>时, 对上下文所分配的各类权重是不同的:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gake3.jpg" style="zoom: 50%;" /><p>深暖色代表分配的注意力权重多, 冷色代表分配的注意力权重少. <code>United_States, Nationality-1, SpeakLanguage</code> 分配的权重就比较大, 因为它们能直接或间接的推导出<code>English</code>.</p><h3 id="Model-Learning"><a href="#Model-Learning" class="headerlink" title="Model Learning"></a>Model Learning</h3><p>在训练模型时, 作者将上述三种目标函数<strong>整合</strong>到一起:<br>$$<br>O=\lambda_{N} O_{N}+\lambda_{P} O_{P}+\lambda_{E} O_{E}<br>$$<br>$\lambda_N, \lambda_P, \lambda_E$ 分别是Neighbor Context, Path Context, Edge Context所对应的占比, 总和为1. 在后续实验中, 作者定义$\lambda_N=0.8, \lambda_P=0.1, \lambda_E=0.1$, 强调了Neighbor Context的作用.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文.</p><h3 id="Triple-Classification"><a href="#Triple-Classification" class="headerlink" title="Triple Classification"></a>Triple Classification</h3><p>作者在FB15K上做了三元组分类实验, 并与其他方法对比:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gake4.jpg" style="zoom: 50%;" /><p>GAKE要优于其他方法, 达到近乎90%左右的准确率.</p><p>为了更好展示Attention在GAKE中起到的作用, 作者对预测<code>Terminate2:JudgementDay</code>时的其他<strong>路径上下文</strong>所占的<strong>注意力权重</strong>展示出来:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gake5.jpg" style="zoom: 50%;" /><p><code>Action</code>和<code>Sequel</code>所占的比重非常大, 因为终结者2是一部动作电影, 还是续集. <code>Genre</code>占的比重比较少, 因为每部电影都有所对应的体裁, 最起码在这条路径中, 体裁没那么重要.</p><h3 id="Link-Prediction"><a href="#Link-Prediction" class="headerlink" title="Link Prediction"></a>Link Prediction</h3><p>作者还做了链接预测任务, 在FB15K上的实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gake6.jpg" style="zoom: 50%;" /><p>相较于其他早期的KGE方法, GAKE的性能有明显提升.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>本文发布于COLING2016, 算是一篇相对比较早的文章了. 作者提出了在KGE中同时使用Neighbor Context, Path Context, Edge Context三种上下文来<strong>融入图结构信息</strong>, 并用<strong>Attention</strong>去增强图结构对Subject的表示, 取得了比较好的效果.</p><blockquote><p>但仔细想想, GAKE其实并没有强调Relation在KG中的不对等地位, 对Entity和Relation统一的使用Subject概括. 其实在后续的KGE的研究方向表明, Relation的作用是至关重要的, Entity的作用可能比Relation小. </p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HAKE: Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction</title>
      <link href="/posts/19612.html"/>
      <url>/posts/19612.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>RotatE: 详见<a href="https://adaning.github.io/posts/60792.html">RotatE: Relational Rotation in Complex Vector Space</a></li></ul></blockquote><h1 id="Learning-Hierarchy-Aware-Knowledge-Graph-Embeddings-for-Link-Prediction"><a href="#Learning-Hierarchy-Aware-Knowledge-Graph-Embeddings-for-Link-Prediction" class="headerlink" title="Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction"></a>Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction</h1><p>本文是论文<a href="https://arxiv.org/abs/1911.09419" target="_blank" rel="noopener">Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction</a>的阅读笔记和个人理解.</p><p>这篇讲解之所以把<strong>RotatE</strong>作为前置知识, 是因为HAKE与RotatE有很大关系, 并且通篇论文的论述也是与RotatE的对比. 本文有一部分参考于<a href="https://zhuanlan.zhihu.com/p/107646185" target="_blank" rel="noopener">AAAI 2020 | 中科大：可建模语义分层的知识图谱补全方法</a>.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>现有的KGE方法将注意力聚焦在对关系模式的建模上, 但没有对<strong>语义层级</strong>结构的建模, 而这种层级结构在现实世界的应用中是普遍存在的.</p><p>例如, <code>mammal</code>, <code>dog</code>, <code>run</code>, <code>move</code> 处于不同语义层级中, <code>rose</code>, <code>peony</code>, <code>truck</code>, <code>lorry</code>处于相同的语义层级中. 语义分层的现象在知识图谱所使用的<strong>三元组</strong>结构中是随处可见的. 并且可以进一步的将这种分层现象抽象成<strong>树形结构</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/hake10.jpg" style="zoom: 25%;" /><p>在这棵树中, 语义层级由上至下逐渐变得具体, 离根部越近的节点语义层级越高, 越抽象.</p><blockquote><p>在本文中, 作者定义如下符号:</p><p>$$<br>[\mathbf{a} \circ \mathbf{b}]_{i}=[\mathbf{a}]_{i} \cdot[\mathbf{b}]_{i}<br>$$</p><p>$[\mathbf{h}]_i$ 代表第$i$ 个实体的Embedding, $\lVert\cdot\rVert_1$, $\lVert \cdot \rVert_2$ 分别代表L1范数和L2范数.</p></blockquote><h2 id="HAKE"><a href="#HAKE" class="headerlink" title="HAKE"></a>HAKE</h2><p>有了RotatE在复数空间建模的前车之鉴, HAKE希望在<strong>极坐标</strong>中直接对语义层次建模. 在极坐标中, 用<strong>极径(模长)</strong>$r$ 和<strong>角度</strong>$\rho$ 来表示在二维坐标系中的一个点, 模长和角度能完美的对<strong>语义层级</strong>建模:</p><ul><li><strong>模长</strong>部分对<strong>不同层级</strong>的实体建模.</li><li><strong>角度</strong>部分对<strong>同一层级</strong>的实体建模.</li></ul><p>想通过头实体$\mathbf{h}$ 在关系$\mathbf{r}$ 的作用下找到尾实体$\mathbf{t}$, 需要在极坐标中按照极坐标的运算规则来找到:</p><ul><li><strong>模长</strong>部分对应着固定角度下的<strong>放缩</strong>.</li><li><strong>角度</strong>部分对应着固定模长时的<strong>旋转</strong>.</li></ul><p>所以, 能把之前抽象出的树形结构直接用极坐标来表达:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/hake2.jpg" style="zoom: 50%;" /><h3 id="Hierarchy-Aware-Knowledge-Graph-Embedding"><a href="#Hierarchy-Aware-Knowledge-Graph-Embedding" class="headerlink" title="Hierarchy - Aware Knowledge Graph Embedding"></a>Hierarchy - Aware Knowledge Graph Embedding</h3><h4 id="Modulus-Part"><a href="#Modulus-Part" class="headerlink" title="Modulus Part"></a>Modulus Part</h4><p>模长部分的关系被定义为对头实体模长的<strong>放缩</strong>:<br>$$<br>\mathbf{h}_{m} \circ \mathbf{r}_{m}=\mathbf{t}_{m}, \text { where } \mathbf{h}_{m}, \mathbf{t}_{m} \in \mathbb{R}^{k}, \text { and } \mathbf{r}_{m} \in \mathbb{R}_{+}^{k}<br>$$</p><p>模长所对应的距离函数为:</p><p>$$<br>d_{r, m}\left(\mathbf{h}_{m}, \mathbf{t}_{m}\right)=\lVert\mathbf{h}_{m} \circ \mathbf{r}_{m}-\mathbf{t}_{m}\rVert_{2}<br>$$</p><p>作者规定实体嵌入可以为负, 但约束Relation Embedding必须为<strong>正</strong>, 因为符号有助于尾实体的预测. 例如, 三元组$(h, r, t_1)$ 为正例, $(h, r, t_2)$ 为负例, 目标为最小化$d_r(\mathbf{h}_m, \mathbf{t}_{1, m})$, 最大化$d_r(\mathbf{h}_m, \mathbf{t}_{2, m})$. </p><ul><li>对于正样本, $[\mathbf{h}]_i, [\mathbf{t}_1]_i$ 倾向于使用同一个符号, 因为$[\mathbf{r}_m]_i&gt;0$. </li><li>对于负例, $[\mathbf{h}_m]_i, [\mathbf{t}_{2,m}]_i$ 可能是随机的符号. </li></ul><p>这样, $d_r(\mathbf{h}_m, \mathbf{t}_{2, m})$ 会倾向于比$d_r(\mathbf{h}_m, \mathbf{t}_{1, m})$ 更大, 更有利于优化.</p><h4 id="Phrase-Part"><a href="#Phrase-Part" class="headerlink" title="Phrase Part"></a>Phrase Part</h4><p>相位部分的关系被定义为角度的<strong>旋转</strong>:</p><p>$$<br>\mathbf{h}_p+\mathbf{r}_p \approx\mathbf{t}_p<br>$$<br>通过<strong>求余</strong>的方式来解决相位的周期问题:</p><p>$$<br>\left(\mathbf{h}_{p}+\mathbf{r}_{p}\right) \bmod 2 \pi=\mathbf{t}_{p}, \text { where } \mathbf{h}_{p}, \mathbf{r}_{p}, \mathbf{t}_{p} \in[0,2 \pi)^{k}<br>$$</p><p>由于相位具有<strong>周期性</strong>, 所以相位所使用的距离度量函数应该与模长部分不同. 不能以简单的相位之差$\mathbf{h}_p + \mathbf{r}_p - \mathbf{t}_p$ 来度量两实体之间在相位部分的距离.</p><p>作者在这里使用了$\sin$ 来描述三元组中两实体的距离:</p><p>$$<br>d_{r, p}\left(\mathbf{h}_{p}, \mathbf{t}_{p}\right)=\lVert\sin \left(\left(\mathbf{h}_{p}+\mathbf{r}_{p}-\mathbf{t}_{p}\right) / 2\right)\rVert_{1}<br>$$</p><p>因为$\sin$ 是有负值的, 所以这里用L1范数(即绝对值)来约束$\sin$ 的结果.</p><blockquote><p>该距离度量能很好的描述三者之间的相位距离关系.</p><p>因为对相位除了2, 所以周期变为$\pi$, 所以每当头实体的相位, 和关系相位, 与尾实体的相位的差$\mathbf{h}_p + \mathbf{r}_p - \mathbf{t}_p = k\pi, k\in(2k+1)$时, $(\mathbf{h}_p + \mathbf{r}_p - \mathbf{t}_p)/2 = \frac{\pi}{2}k, k\in (2k+1)$, 可以取到$\sin$ 函数的最大值1, 即此时距离最大.</p></blockquote><h4 id="Map-into-Polar-Coordinate-System"><a href="#Map-into-Polar-Coordinate-System" class="headerlink" title="Map into Polar Coordinate System"></a>Map into Polar Coordinate System</h4><p>综合前两个小节, HAKE被整合进了极坐标系统中, 实体能在极坐标下唯一的被二维坐标标识.</p><p>$$<br>\left\{\begin{array}{l}<br>\mathbf{h}_{m} \circ \mathbf{r}_{m}=\mathbf{t}_{m}, \text { where } \mathbf{h}_{m}, \mathbf{t}_{m} \in \mathbb{R}^{k}, \mathbf{r}_{m} \in \mathbb{R}_{+}^{k} \\<br>\left(\mathbf{h}_{p}+\mathbf{r}_{p}\right) \bmod 2 \pi=\mathbf{t}_{p}, \text { where } \mathbf{h}_{p}, \mathbf{t}_{p}, \mathbf{r}_{p} \in[0,2 \pi)^{k}<br>\end{array}\right.<br>$$</p><p>距离函数是<strong>模长部分</strong>的距离函数和<strong>相位部分</strong>的组合:</p><p>$$<br>d_{r}(\mathbf{h}, \mathbf{t})=d_{r, m}\left(\mathbf{h}_{m}, \mathbf{t}_{m}\right)+\lambda d_{r, p}\left(\mathbf{h}_{p}, \mathbf{t}_{p}\right)<br>$$</p><p>其中$\lambda$ 为可学习的参数.</p><p>HAKE的打分函数如下:</p><p>$$<br>f_{r}(\mathbf{h}, \mathbf{t})=-d_{r}(\mathbf{h}, \mathbf{t})=-d_{r, m}(\mathbf{h}, \mathbf{t})-\lambda d_{r, p}(\mathbf{h}, \mathbf{t})<br>$$</p><p>因为<strong>距离越大</strong>, <strong>打分应该越小</strong>, 所以这里在距离前面加上一个负号.</p><p>下图是HAKE与常见KGE模型的打分函数对比:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/hake1.jpg" style="zoom: 50%;" /><p>在跑实验时, 作者还发现了一种<strong>混合距离度量</strong>, 比原度量更加有效:<br>$$<br>d_{r, m}^{\prime}(\mathbf{h}, \mathbf{t})=\lVert\mathbf{h}_{m} \circ \mathbf{r}_{m}+\left(\mathbf{h}_{m}+\mathbf{t}_{m}\right) \circ \mathbf{r}_{m}^{\prime}-\mathbf{t}_{m}\rVert_{2}<br>$$</p><p>下面这个式子与上式是<strong>等价</strong>的:</p><p>$$<br>d_{r, m}^{\prime}(\mathbf{h}, \mathbf{t})=\lVert\mathbf{h}_{m} \circ\left(\left(\mathbf{r}_{m}+\mathbf{r}_{m}^{\prime}\right) /\left(1-\mathbf{r}_{m}^{\prime}\right)\right)-\mathbf{t}_{m}\rVert_{2}<br>$$</p><p>作者在后续实验发现这种方式有很大提升.</p><blockquote><p>这个混合度量还有一些疑问, 以后研究.</p></blockquote><h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><p>HAKE与RotatE一样, 使用<strong>最大化间隔</strong>的损失函数来优化:</p><p>$$<br>L=-\log \sigma\left(\gamma-d_{r}(\mathbf{h}, \mathbf{t})\right)<br>-\sum_{i=1}^{n} p\left(h_{i}^{\prime}, r, t_{i}^{\prime}\right) \log \sigma\left(d_{r}\left(\mathbf{h}_{i}^{\prime}, \mathbf{t}_{i}^{\prime}\right)-\gamma\right)<br>$$</p><p>其中$\gamma$ 为间隔, $\sigma$ 为Sigmoid函数, $(h_i^\prime, r, t_i^\prime)$ 为负三元组. </p><p>HAKE也使用了RotatE中提到的<strong>自对抗负采样</strong>, 我在<a href="https://adaning.github.io/posts/60792.html#Self-Adversarial-Negative-Sampling">RotatE</a>中已经有过讲解:<br>$$<br>p\left(h_{j}^{\prime}, r, t_{j}^{\prime} \mid\left\{\left(h_{i}, r_{i}, t_{i}\right)\right\}\right)=\frac{\exp \alpha f_{r}\left(\mathbf{h}_{j}^{\prime}, \mathbf{t}_{j}^{\prime}\right)}{\sum_{i} \exp \alpha f_{r}\left(\mathbf{h}_{i}^{\prime}, \mathbf{t}_{i}^{\prime}\right)}<br>$$</p><p>其中$\alpha$ 为Temperature. </p><blockquote><p>除了带来性能上的提升外, HAKE在后续与RotatE对比时也显得更加<strong>公平</strong>.</p></blockquote><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文.</p><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><p>作者分别在WN18RR, FB15k - 237, YAGO3 - 10上做了实验, 它们的统计信息如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/hake3.jpg" style="zoom: 50%;" /><p>实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/hake4.jpg" style="zoom: 50%;" /><p>HAKE将相较于RotatE有一些提升, ModE和RotatE差不多.</p><h3 id="Relation-Embeddings-Analysis"><a href="#Relation-Embeddings-Analysis" class="headerlink" title="Relation Embeddings Analysis"></a>Relation Embeddings Analysis</h3><h4 id="Modulus-Part-1"><a href="#Modulus-Part-1" class="headerlink" title="Modulus Part"></a>Modulus Part</h4><p>作者把关系中所对应的实体<strong>模长</strong>分布直方图画了出来:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/hake5.jpg" style="zoom: 50%;" /><p>对于以关系$\mathbf{r}_m$作者有如下期望:</p><ul><li>头实体相比于尾实体的语义层次更<strong>高</strong>时, 关系的模长$\mathbf{r}_{m}=\mathbf{t}_{m} / \mathbf{h}_{m}&gt;1$.</li><li>头实体相比于尾实体的语义层次更<strong>低</strong>时, 关系的模长$\mathbf{r}_{m}=\mathbf{t}_{m} / \mathbf{h}_{m}&lt;1$.</li><li>头实体与尾实体位于<strong>相同</strong>的语义层次时, 关系的模长$\mathbf{r}_{m}=\mathbf{t}_{m} / \mathbf{h}_{m} \approx 1$.</li></ul><p>在作者展示出的六种关系中, 作者猜想得到了佐证.</p><h4 id="Phrase-Part-1"><a href="#Phrase-Part-1" class="headerlink" title="Phrase Part"></a>Phrase Part</h4><p>因为在上幅图中, <code>_similar_to</code>和<code>friend</code>没能利用模长做出区分, 作者将它们的相位分布做了出来:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/hake6.jpg" style="zoom: 50%;" /><p>大多数的相位区分都集中在$\pi$ 的整数倍, 当相位差为$\pi$ 时, 实体可以被区分开.</p><h3 id="Entity-Embedding-Analysis"><a href="#Entity-Embedding-Analysis" class="headerlink" title="Entity Embedding Analysis"></a>Entity Embedding Analysis</h3><p>将RotatE的实部, 虚部, HAKE的模长部分, 相位部分视为是二维坐标系中的坐标, 将它们的每个维度放到2D平面上可视化:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/hake9.jpg" style="zoom: 67%;" /><p>在RotatE中, 不同语义层级关系的实体是不能被区分开的, HAKE能保证不同语义层级的实体之间有一定的间隔, 相同语义层级之间的实体大致相同.</p><blockquote><p>相同语义层级之间的实体可能被多元关系所包含, 它们从图(b)上来看是不好区分的, 暴露了HAKE不擅长处理多元关系的特性.</p></blockquote><h3 id="Ablation-Studies"><a href="#Ablation-Studies" class="headerlink" title="Ablation Studies"></a>Ablation Studies</h3><p>作者在三个数据集上做了相应的消融实验, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/hake7.jpg" style="zoom: 50%;" /><p>$\mathbf{b}$ 代表作者所称的混合距离度量.</p><p>如果只使用模长的部分, 性能是非常差的. 但如果只使用相位部分, HAKE将退化为<strong>pRotatE</strong>. 把二者混合性能可以得到进一步提升. 如果再采用混合距离度量性能还会有点提升.</p><h3 id="Comparsion-with-Other-Related-Work"><a href="#Comparsion-with-Other-Related-Work" class="headerlink" title="Comparsion with Other Related Work"></a>Comparsion with Other Related Work</h3><p>在FB15k上与四种TKRL的模型对比:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/hake8.jpg" style="zoom: 50%;" /><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>HAKE与RotatE相似, RotatE利用了<strong>复数</strong>空间的旋转建模关系, HAKE利用<strong>极坐标</strong>中的<strong>角度</strong>和<strong>模长</strong>建模语义层级结构.</p><p>作者着重与RotatE做了对比, 它确实比RotatE能针对语义层级关系做处理.</p><p>HAKE也十分的简洁, 但和RotatE一样, 理论上它还是只能做<strong>一对一</strong>的建模, 不善于处理多对一, 一对多, 多对多的<strong>多元关系</strong>.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for KGE</title>
      <link href="/posts/21954.html"/>
      <url>/posts/21954.html</url>
      
        <content type="html"><![CDATA[<h1 id="ReInceptionE-Relation-Aware-Inception-Network-with-Joint-Local-Global-Structural-Information-for-KGE"><a href="#ReInceptionE-Relation-Aware-Inception-Network-with-Joint-Local-Global-Structural-Information-for-KGE" class="headerlink" title="ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for KGE"></a>ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for KGE</h1><p>本文是论文<a href="https://www.aclweb.org/anthology/2020.acl-main.526.pdf" target="_blank" rel="noopener">ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for Knowledge Graph Embedding</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者观察到<strong>基于卷积</strong>的方法存在如下限制:</p><ol><li>ConvE中没有融入<strong>结构化信息</strong>.</li><li>ConvE的性能仍然被<strong>交互次数</strong>所限制.</li></ol><p>而KBAT直接从<strong>多跳</strong>的<strong>图</strong>角度出发, 考虑N阶<strong>邻居</strong>的关系和它们本身对中心节点的影响, 但是仍需多跳推理, 所以它只考虑了邻居的局部信息. 因此, 利用<strong>局部</strong>和<strong>全局</strong>信息结合可能会从中受益.</p><p>例如, 在下图中, 对于<code>(Jack London, nationality, ?)</code>这个问题, 我们能够从局部邻居观察到<code>(Jack London, place_lived, Oakland)</code>存在, 因为<code>Oakland</code>在Embedding中与<code>America</code>非常近, 所以也会增加<code>America</code>的预测分数. 同时, 对于关系<code>nationality</code>的全局信息, 即它存在的三元组所对应的头实体集合和尾实体集合也能帮助我们提升结果得出<code>America</code>的概率:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/reinceptione1.jpg" style="zoom: 50%;" /><blockquote><p>这也可能会<strong>潜在</strong>的导致局部和全局信息<strong>冲突</strong>所带来的<strong>混淆</strong>问题, 即多个实体的分数<strong>高度近似</strong>, 模型将会很难区分它们. 可能说的比较极端.</p><p>此外, 在这个例子中, <code>nationality</code>应该不是在<a href="https://adaning.github.io/posts/53954.html">这篇论文</a>中提出的<strong>笛卡尔积关系</strong>, 因为头实体和尾实体的数量实在是太大了.</p></blockquote><p>根据上述Conv类模型存在的缺陷, 作者希望用Inception来增加头实体和关系之间的交互次数, 用Attention丰富局部和全局信息.</p><h2 id="ReInceptionE"><a href="#ReInceptionE" class="headerlink" title="ReInceptionE"></a>ReInceptionE</h2><p><strong>ReInceptionE</strong>(<strong>Re</strong>lation - aware <strong>Inception</strong> network with joint local-global structural information for knowledge graph <strong>E</strong>mbedding) 将Inception和KBAT的优势结合到了一起, 作者先用Inception在实体和关系之间<strong>交互</strong>, 生成头实体和关系之间的<strong>查询向量</strong>, 然后从<strong>局部</strong>和<strong>全局</strong>两个角度集成信息, 用<strong>Attention</strong>把信息融合到一起.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/reinceptione7.jpg" style="zoom: 50%;" /><h3 id="Inception-based-Query-Encoder"><a href="#Inception-based-Query-Encoder" class="headerlink" title="Inception - based Query Encoder"></a>Inception - based Query Encoder</h3><blockquote><p>除去和ConvE实体关系嵌入的<strong>拼接方式</strong>不同, 这小节基本就是在讲Inception, 有基础可以跳过.</p></blockquote><p>作者指出, 在ConvE中其实并没有最大化实体和关系的交互次数. 因为ConvE是通过<strong>Concat</strong>的方式把二者拼接到一起, 只有最中间的<strong>交界处</strong>才有卷积核对二者的交互. 为了能够尽可能多的使头实体嵌入$\mathbf{v}_h$ 和关系嵌入$\mathbf{v}_r$ 整合, 作者使用<strong>Stack</strong>的方式在Channel维上把二者<strong>堆叠</strong>到一起, 这样在抽取特征时直接就能对二者共同交互:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/reinceptione2.jpg" style="zoom: 50%;" /><blockquote><p><a href="https://adaning.github.io/posts/14355.html">CoPER</a>也是针对这个点改进的, 只不过使用的是<strong>参数生成</strong>的方法, 避免了Concatenate这种加性操作.</p></blockquote><p>在Inception中, 在这里, 先使用$1\times1$ 卷积能提供<strong>最直接</strong>的实体嵌入$\mathbf{v}_h$ 和关系嵌入$\mathbf{v}_r$ 的信息整合作用:</p><p>$$<br>\mathbf{v}_{1 \times 1}=\operatorname{Relu}\left(\left[\mathbf{v}_{h} \lVert\mathbf{v}_{r}\right] \ast \omega_{1 \times 1}\right)<br>$$</p><p>其中$\ast$ 为卷积操作, $\omega_{1\times1}$ 为$1\times1$ 的卷积核权重. $\lVert$ 为Stack操作. </p><p>在整合过后, 由于Inception通过多尺度的方式捕获<strong>不同角度</strong>的高阶信息, 所以会使用不同大小的卷积核, $\mathbf{v}_{2\times2}, \mathbf{v}_{3\times3}$ 分别代表在$\mathbf{v}_{1\times1}$ 后接一个$2\times2$ 和$3\times3$ 卷积的结果.</p><blockquote><p>Inception中, 不同大小卷积核抽取后的特征图可能宽高不同, 使用<strong>Padding</strong>可以保证它们的特征图不变, 以正确的Stack到一起.</p></blockquote><p>接着, 在Inception中, 还使用了两个$3\times3$ 的卷积代替一个$5\times5$ 的卷积, 以此捕获<strong>大空间</strong>上的交互:</p><p>$$<br>\mathbf{v}_{2(3 \times 3)}=\operatorname{Relu}\left(\operatorname{Relu}\left(\mathbf{v}_{1 \times 1}^{2(3 \times 3)} \ast \omega_{3 \times 3}^{1}\right) \ast \omega_{3 \times 3}^{2}\right)<br>$$</p><p>$\mathbf{v}_{1 \times 1}^{2(3 \times 3)}$ 为输入的交互特征, 即$1\times1$ 卷积后的结果,  $\omega_{3\times3}^1, \omega_{3\times3}^2$ 分别为两个$3\times3$ 的卷积核参数.</p><p>最后, 把前面的多尺度信息Stack起来, 然后打平, 扔进一层FC层:<br>$$<br>\begin{aligned}<br>\mathbf{v}_{q} &amp;=\text { Inception }\left(\mathbf{v}_{h}, \mathbf{v}_{r}\right) \\<br>&amp;=\operatorname{Relu}\left(\operatorname{vec}\left(\left[\mathbf{v}_{1 \times 1}\lVert \mathbf{v}_{2 \times 2} \lVert \mathbf{v}_{3 \times 3} \lVert \mathbf{v}_{2(3 \times 3)}\right]\right) \mathbf{W}\right)<br>\end{aligned}<br>$$</p><p>$\mathbf{W}$ 为一层FC的参数.</p><p>整体交互的输出结果$\mathbf{v}_q$, 视作是头实体嵌入$\mathbf{v}_h$ 和关系嵌入$\mathbf{v}_r$ 之间的查询结果.</p><h3 id="Relation-Aware-Local-Attention"><a href="#Relation-Aware-Local-Attention" class="headerlink" title="Relation - Aware Local Attention"></a>Relation - Aware Local Attention</h3><p>下面, 作者从<strong>图视角</strong>对<strong>局部信息</strong>用<strong>KBAT</strong>的方式做集成.</p><p>在上小节中, 已经用Inception获得了头实体嵌入$\mathbf{v}_h$ 和关系嵌入$\mathbf{v}_r$ 之间的查询结果$\mathbf{v}_q$. </p><p>在图视角中, 头实体$h$ 的邻居集为$\mathcal{N}_{q}=\left\{n_{i}=\left(e_{i}, r_{i}\right) \mid\left(e_{i}, r_{i}, h\right) \in \mathcal{G}\right\}$, 对于每个邻居$n_i = \left(e_i, r_i\right)$, 都可以计算它们的查询编码:<br>$$<br>\mathbf{v}_{n_{i}}=\operatorname{Inception}\left(\mathbf{v}_{e_{i}}, \mathbf{v}_{r_{i}}\right)<br>$$</p><p>接着用$\mathbf{v}_q$ 和$\mathbf{v}_{ni}$ 计算Attention Score $s_i$:</p><p>$$<br>s_{i}=\operatorname{LeakyRelu}\left(\mathbf{W}_{1}\left[\mathbf{W}_{2} \mathbf{v}_{q} \lVert \mathbf{W}_{3} \mathbf{v}_{n_{i}}\right]\right)<br>$$</p><p>然后计算Attention Weight $\alpha_i$:</p><p>$$<br>\alpha_{i}=\frac{\exp \left(s_{i}\right)}{\sum_{n_{j} \in \mathcal{N}_{a}} \exp \left(s_{j}\right)}<br>$$</p><p>为了保留原始查询嵌入的信息, 这里也加上了类似<strong>残差</strong>的操作:</p><p>$$<br>\mathbf{v}_{n}=\operatorname{Relu}\left(\sum_{n_{i} \in \mathcal{N}_{q}} \alpha_{i} \mathbf{W}_{3} \mathbf{v}_{n_{i}}\right)+\mathbf{W}_{2} \mathbf{v}_{q}<br>$$</p><p>把上述<strong>Relation - Aware Attention</strong>操作记为下式:</p><p>$$<br>\mathbf{v}_{n}=Re A t t\left(\mathbf{V}_{n}, \mathbf{v}_{q}\right)<br>$$</p><p>$\mathbf{V}_n = \left\{ \mathbf{v}_n \mid n_i \in \mathcal{N}_q \right\}$ 为局部邻居向量的集合.</p><h3 id="Relation-Aware-Global-Attention"><a href="#Relation-Aware-Global-Attention" class="headerlink" title="Relation - Aware Global Attention"></a>Relation - Aware Global Attention</h3><p>每个实体的局部邻居数量都不同, 可能会导致<strong>稀疏性</strong>. 稀疏性会影响KGE的准确率. 作者根据关系$r$, 提取出同一种关系的头实体$\mathcal{H}_{r}=\left\{e_{i} \mid\left(e_{i}, r, e_{j}\right) \in \mathcal{G}\right\}$ 所共享的信息, 以及尾实体$\mathcal{T}_r=\left\{e_{j} \mid\left(e_{i}, r, e_{j}\right) \in \mathcal{G}\right\}$ 共享的一些隐含信息.</p><p>即构建下面的这两个子图<code>Relation - aware global heads</code>和<code>Relation - aware global tails</code>, 对每个关系所对应的头尾实体集合用Attention做了聚合.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/reinceptione1.jpg" style="zoom: 50%;" /><p>上述操作只需要将同关系所对应的头实体集$\mathcal{H}_r$ 视为查询向量$\mathbf{v}_q$ 的邻居, 尾实体集$\mathcal{T}_r$ 也视为$\mathbf{v}_q$ 的邻居, 再做Relation - Aware Local Attention就可以了:</p><p>$$<br>\mathbf{v}_{r h}=Re A t t\left(\mathbf{V}_{r h}, \mathbf{v}_{q}\right)<br>$$</p><p>$\mathbf{V}_{rh}=\left\{\mathbf{v}_{h_{ri}}\mid h_{ri} \in \mathcal{H}_r\right\}$ 是对应关系中的所有<strong>头实体</strong>集合.<br>$$<br>\mathbf{v}_{r t}=ReAtt\left(\mathbf{V}_{r t}, \mathbf{v}_{q}\right)<br>$$</p><p>$\mathbf{V}_{rt}=\left\{\mathbf{v}_{t_{ri}}\mid t_{ri} \in \mathcal{T}_r\right\}$ 是对应关系中的所有<strong>尾实体</strong>集合.</p><h3 id="Joint-Relation-Aware-Attention"><a href="#Joint-Relation-Aware-Attention" class="headerlink" title="Joint Relation - Aware Attention"></a>Joint Relation - Aware Attention</h3><p>最后, 将前面包含局部信息的查询向量$\mathbf{v}_n$, 包含全局信息的查询向量$\mathbf{v}_{rh}, \mathbf{v}_{rt}$ 一并拼接起来, 再做最后一次线性投影, 就得到了最终的查询向量$\mathbf{v}_q^\prime$:</p><p>$$<br>\mathbf{v}_{q}^{\prime}=\mathbf{W}_{4}\left[\mathbf{v}_{n}\lVert\mathbf{v}_{r h} \lVert \mathbf{v}_{r t}\right]+\mathbf{b}<br>$$</p><p>$\mathbf{W}_4, \mathbf{b}$ 为权重矩阵和偏置项.</p><p>打分函数为计算查询嵌入$\mathbf{v}_q^\prime$ 与尾实体嵌入$\mathbf{v}_t$ 的<strong>点积相似度</strong>:<br>$$<br>f(h, r, t)=\mathbf{v}_{q}^{\prime T} \mathbf{v}_{t}<br>$$</p><p>然后通过最大化正例三元组的条件概率的方式来调整模型参数:</p><p>$$<br>p(t\mid h, r)=\frac{\exp (\lambda f(h, r, t))}{\sum_{\left(h, r, t^{\prime}\right) \in \mathcal{G}^{\prime} \cup\{(h, r, t)\}}\exp \left(\lambda f\left(h, r, t^{\prime}\right)\right)}<br>$$</p><p>$\lambda$ 为平滑参数, $\mathcal{G}^\prime$ 为负采样获得的三元组集合.</p><p>总体上用交叉熵来优化模型:</p><p>$$<br>\mathcal{L}=-\frac{1}{|\mathcal{E}|} \sum_{i=0}^{|\mathcal{E}|} \log p\left(t_{i} \mid h_{i}, r_{i}\right)<br>$$</p><p>$\mathcal{E}$ 为$\mathcal{G}$ 中的有效实体总数.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的超参数设置请参照原论文.</p><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><p>作者在WN18RR和FB15k - 237这两个数据集上与诸多Baseline进行了比较:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/reinceptione3.jpg" style="zoom: 50%;" /><p>Baseline有很多是基于图或基于CNN的方法. 右上角角标b代表从<a href="">另一篇论文</a>重新运行代码得出的结果, a代表作者从原论文中得到的结果.</p><p>ConvKB, CapsE, KBGAT的评估协议是不公平的, 我建议不要关注这三个模型的结果.</p><p>ReInceptionE其实也并没有超出其他Baseline很多的性能, 感觉效果其实<strong>差不多</strong>.</p><blockquote><p>明显在关系比较少的WN18RR上效果比关系很多的FB15k - 237上效果要好, 我估计是因为Attention的问题. Attention在数据量小的情况下非常脆弱. 关系数量多了但数据总量没怎么增长时, 依赖关系作出判断的模型会因关系的长尾问题暴露出很大缺陷.</p></blockquote><h3 id="Impact-of-Different-Modules"><a href="#Impact-of-Different-Modules" class="headerlink" title="Impact of Different Modules"></a>Impact of Different Modules</h3><p>作者探究了在两个数据集上ReInceptionE各组件的影响:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/reinceptione4.jpg" style="zoom: 50%;" /><p><code>ReInception w/o N</code>是模型不使用局部信息, <code>ReInception w/o E</code>是不使用全局信息. 单独使用InceptionE的效果就比ConvE好很多, 在加入局部信息后, 效果比KBAT也有改善. 与只使用局部信息相比, 只使用全局信息更不利于模型区分三元组. 二者结合的情况下, 效果最佳.</p><h3 id="Evaluation-on-Different-Relation-Types"><a href="#Evaluation-on-Different-Relation-Types" class="headerlink" title="Evaluation on Different Relation Types"></a>Evaluation on Different Relation Types</h3><p>在WN18RR和FB15k - 237上对不同种类的关系预测结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/reinceptione5.jpg" style="zoom: 50%;" /><p>相比于ConvE和InceptionE, ReInceptionE有改善, 看起来加入Inception的作用要更大.</p><h3 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h3><p>作者还分析了两个小案例:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/reinceptione6.jpg" style="zoom: 50%;" /><p>通过Attention学习得到的重要邻居往往权重都比较高, 它们都是与Target相关或能够利用它推断出Target的三元组.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>ReInceptionE使用<strong>Inception</strong>作为增加实体与关系交互的基本组件, 并在<strong>KBAT</strong>的基础上集成<strong>局部</strong>和<strong>全局</strong>信息, 用<strong>Relation Aware Attention</strong>的方式将二者融合到一起, </p><p>我之前一直都想做一种基于图的多尺度关系感知的Attention, 和这篇论文思路几乎一致.<br>有句话说得好:</p><center>你觉得你idea足够多, 一定是你读的Paper还不够.</center><p><strong>增加交互</strong>来提升性能的方法应该是有上限的, 基于CNN的方法核心就是最大化实体Embedding和关系Embedding的交互, 如果再以交互为核心扩展基于CNN的方法, 可能从中的<strong>收益不大</strong>.</p><blockquote><p>从本文中看到, 即使利用CV早期做出的某个具体BottleNeck套过来, 尤其是引入早期CV中效果最好的多尺度的BottleNeck来提升交互次数, 也仍然没有带来质的飞跃. 所以不得不让人怀疑继续增加交互的有效性.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> CNN </tag>
            
            <tag> GNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KBAT: Learning Attention-based Embeddings for Relation Prediction in KGs</title>
      <link href="/posts/37736.html"/>
      <url>/posts/37736.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>GAT: 详见<a href="https://adaning.github.io/posts/31958.html">图神经网络入门</a>.</li></ul></blockquote><h1 id="Learning-Attention-based-Embeddings-for-Relation-Prediction-in-Knowledge-Graphs"><a href="#Learning-Attention-based-Embeddings-for-Relation-Prediction-in-Knowledge-Graphs" class="headerlink" title="Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs"></a>Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs</h1><p>本文是论文<a href="https://arxiv.org/abs/1906.01195" target="_blank" rel="noopener">Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者观察到, 虽然基于CNN的模型能生成关于三元组更丰富的信息, 但处理三元组时是<strong>独立</strong>的, 并没有覆盖<strong>周围邻居</strong>的复杂信息. 现有的方法要么只关注实体特征, 要么不会对实体和关系的特征共同嵌入.</p><p>作者希望利用<strong>Attention机制</strong>来共同捕捉节点与邻居的实体和关系特征.</p><h2 id="KBAT"><a href="#KBAT" class="headerlink" title="KBAT"></a>KBAT</h2><h3 id="Graph-Attention-Network"><a href="#Graph-Attention-Network" class="headerlink" title="Graph Attention Network"></a>Graph Attention Network</h3><blockquote><p>这部分对GAT有基础的可以直接跳过, 作者在原论文中描述GAT的式子与GAT本身论文中没区别.</p></blockquote><p>对于有$N$ 个节点的图, 输入节点特征为$\mathbf{x}=\left\{\vec{x}_{1}, \vec{x}_{2}, \ldots, \vec{x}_{N}\right\}$, 在经过一层GNN变换后, 节点特征被更新为$\mathbf{x}^{\prime}=\left\{\vec{x}_{1}^{\prime}, \vec{x}_{2}^{\prime}, \ldots, \vec{x}_{N}^{\prime}\right\}$. </p><p>在注意力机制中, 边$(e_i, e_j)$ 的注意力值$e_{ij}$ 由下式决定:<br>$$<br>e_{i j}=a\left(\mathbf{W} \overrightarrow{x_{i}}, \mathbf{W} \overrightarrow{x_{j}}\right)<br>$$<br>$\mathbf{W}$ 为线性变换矩阵, 能将节点特征投影到更高的维度, $a$ 是可选择的Attention Function.</p><p>在更新节点隐态信息时, 将在GCN的基础上对节点的邻居<strong>加权聚合</strong>:<br>$$<br>\overrightarrow{x_{i}^{\prime}}=\sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j} \mathbf{W} \overrightarrow{x_{j}}\right)<br>$$<br>$\alpha_{ij}$ 是用<strong>Softmax</strong>对节点$i$ 所有邻居$j$ 的注意力值由$e_{ij}$ 计算得到的<strong>注意力权重</strong>. </p><p>与Transformer类似, GAT中引入了<strong>多头注意力</strong>, 更新方程如下:<br>$$<br>\overrightarrow{x_{i}^{\prime}}=\lVert_{k=1}^{K} \sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k} \overrightarrow{x_{j}}\right)<br>$$<br>$\lVert$ 代表Concat操作, $\sigma$ 为非线性函数, $K$ 代表多头的头数. $\alpha_{ij}^k$ 代表第$k$ 个头内归一化的注意力权重, $\mathbf{W}^k$ 是第$k$ 个头的线性变换矩阵.</p><p>最后一层将GAT输出作为Embedding, 将多头的信息通过<strong>平均</strong>的方式整合:<br>$$<br>\overrightarrow{x_{i}^{\prime}}=\sigma\left(\frac{1}{K} \sum_{k=1}^{K} \sum_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k} \overrightarrow{x_{j}}\right)<br>$$</p><h3 id="Relations-are-Important"><a href="#Relations-are-Important" class="headerlink" title="Relations are Important"></a>Relations are Important</h3><p>在KG中, 相同的实体在不同关系的作用下意义会变得完全不一样, 所以<strong>关系嵌入</strong>是非常重要的. </p><p>例如, 在下述场景中, <code>Christopher Nolan</code>出现在了两个不同的三元组中, 所对应的关系分别是<code>directed</code>和<code>brother_of</code>, 他的身份分别是导演和哥哥:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kbat1.jpg" style="zoom: 50%;" /><p>因此, 在将GAT引入KGE时, 必须将<strong>关系嵌入</strong>也一并纳入注意力的计算范畴.</p><p>在每层GAT中, $\mathbf{H}, \mathbf{G}$ 分别代表输入时实体嵌入和关系嵌入矩阵. $\mathbf{H}^\prime, \mathbf{G}^\prime$ 为输出时实体嵌入, 关系嵌入矩阵. 特别定义$t_{ij}^k=(e_i, r_k, e_j)$ 为三元组.</p><p>我们从输入实体嵌入矩阵$\mathbf{H}$ 中取出实体表示$\vec{h_i}, \vec{h_j}$, 从输入关系嵌入矩阵$\mathbf{G}$ 中取出关系表示$\vec{g_k}$, 获取三元组$t_{ij}^k$ 整体在嵌入空间中的表示$\vec{c_{i j k}}$:<br>$$<br>\vec{c_{i j k}}=\mathbf{W}_{1}\left[\vec{h}_{i}\lVert\vec{h}_{j}\lVert \vec{g}_{k}\right]<br>$$<br>$\mathbf{W}_1$ 为线性变换矩阵.</p><p>然后利用整体表示再次经过变换, 对$c_{ijk}$ 变换后用<strong>LeakyReLU</strong>过滤得到Attention Score $b_{ijk}$:<br>$$<br>b_{i j k}=\operatorname{LeakyReLU}\left(\mathbf{W}_{2} c_{i j k}\right)<br>$$<br>$\mathbf{W}_2$ 是线性变换矩阵.</p><p>根据Attention Score, 用Softmax可以得出三元组的Attention Weight $\alpha_{ijk}$:<br>$$<br>\begin{aligned}<br>\alpha_{i j k} &amp;=\operatorname{softmax}_{j k}\left(b_{i j k}\right) \\<br>&amp;=\frac{\exp \left(b_{i j k}\right)}{\sum_{n \in \mathcal{N}_{i}} \sum_{r \in \mathcal{R}_{i n}} \exp \left(b_{i n r}\right)}<br>\end{aligned}<br>$$<br>$\mathcal{N}_i$ 代表节点$i$ 的邻居节点集, $\mathcal{R}_{in}$ 代表节点$i$ 与邻居$n$ 之间的关系集.</p><p>因此, KBAT的前半部分与GAT相比, 只是把关系嵌入$g_k$ 塞进了GAT中:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kbat3.jpg" style="zoom: 33%;" /><p>在更新节点表示时, 对实体$i$ 为头实体的三元组$\vec{c_{ijk}}$ 加权求和得到新的节点表示:<br>$$<br>\overrightarrow{h_{i}^{\prime}}=\sigma\left(\sum_{j \in \mathcal{N}_{i}} \sum_{k \in \mathcal{R}_{i j}} \alpha_{i j k} \vec{c_{i j k}}\right)<br>$$<br>和GAT相同, 也可以采用<strong>多头注意力</strong>的方式来增强GAT的稳定性:<br>$$<br>\overrightarrow{h_{i}^{\prime}}=\lVert_{m=1}^{M} \sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j k}^{m} c_{i j k}^{m}\right)<br>$$<br>其中$\lVert$ 代表Concat操作, $M$ 为注意力头数. </p><p>我们已经获取了实体在单层GAT的更新后的表示, 也需要对关系做一些改变, 保证节点嵌入更新的同时, 关系嵌入也是在更新的:<br>$$<br>G^{\prime}=G \cdot \mathbf{W}^{R}<br>$$<br>$\mathbf{W}^R$ 是权重矩阵, 可以改变关系嵌入的维度.</p><h3 id="Final-Output-Layer"><a href="#Final-Output-Layer" class="headerlink" title="Final Output Layer"></a>Final Output Layer</h3><p>在最终KBAT输出时, 需要聚合多头注意力的信息, 并对节点表示做进一步处理.</p><p>输出节点表示时, 与GAT类似, 将多个头的信息直接<strong>求平均</strong>聚合作为节点表示:<br>$$<br>\overrightarrow{h_{i}^{\prime}}=\sigma\left(\frac{1}{M} \sum_{m=1}^{M} \sum_{j \in \mathcal{N}_{i}} \sum_{k \in \mathcal{R}_{i j}} \alpha_{i j k}^{m} c_{i j k}^{m}\right)<br>$$<br>但是初始信息可能会随着提取特征而丢失, 作者在最终的输出前把初始节点嵌入$\mathbf{H}^t$ 再加回来:<br>$$<br>\mathbf{H}^{\prime \prime}=\mathbf{W}^{E} \mathbf{H}^{t}+\mathbf{H}^{f}<br>$$<br>$\mathbf{W}^E$ 是为了让维度能够匹配的变换矩阵, $\mathbf{H}^f$ 为最终层前的KBAT节点表示输出.</p><blockquote><p>其实就是类似<strong>残差</strong>的方法, 但是在原论文中却对残差只字未提.</p></blockquote><p>至此, 整个KBAT的运算过程如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kbat4.jpg" style="zoom: 50%;" /><p>图中绿色圆圈为关系嵌入, 黄色圆圈为实体嵌入. 对图中的流程总结描述如下:</p><ol><li>先将头实体, 尾实体, 关系的嵌入<strong>拼接</strong>起来.</li><li>经过GAT层的<strong>多头注意力</strong>后得到两个<strong>实体更新</strong>后的表示, 同时将<strong>关系嵌入</strong>做一次<strong>更新</strong>. </li><li>在下个GAT层前, 将更新后的头实体, 尾实体, 关系表示<strong>拼接</strong>起来.</li><li>再次经过<strong>GAT</strong>层. 得到两个实体的输出.</li><li>将最初的实体嵌入通过一次<strong>投影</strong>到与最终输出维度相匹配的维度, 和GAT输出的实体表示<strong>相加</strong>.</li><li>利用关系嵌入和实体嵌入共同计算<strong>Loss</strong>.</li></ol><p>最后, 作者谈了一下多层GAT能够聚合N阶邻居信息的优势:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kbat2.jpg" style="zoom: 50%;" /><p>在图中, 多阶邻居的信息能够以多次迭代的方式被有可能相关联的实体所捕捉.</p><blockquote><p>这种优势也只来自于GAT本身, 并不是由于KBAT的特殊性.</p></blockquote><h3 id="Training-Objective"><a href="#Training-Objective" class="headerlink" title="Training Objective"></a>Training Objective</h3><p>训练目标和TransE完全一致, 损失函数采用<strong>Hinge Loss</strong>:<br>$$<br>L(\Omega)=\sum_{t_{i j} \in S} \sum_{t_{i j}^{\prime} \in S^{\prime}} \max \left\{d_{t_{i j}^{\prime}}-d_{t_{i j}}+\gamma, 0\right\}<br>$$<br>$\gamma$ 为间隔, 距离的度量函数采用TransE的一范数, 即$d_{t_{ij}} = \lVert \vec{h_i} + \vec{g_k} - \vec{h_j} \rVert_1$.</p><p>也同样使用了负采样, $S$ 为有效三元组, $S^\prime$ 为负样本三元组:<br>$$<br>S^{\prime}=\underbrace{\left\{t_{i^{\prime} j}^{k} \mid e_{i}^{\prime} \in \mathcal{E} \backslash e_{i}\right\}}_{\text {replace head entity }} \cup \underbrace{\left\{t_{i j^{\prime}}^{k} \mid e_{j}^{\prime} \in \mathcal{E} \backslash e_{j}\right\}}_{\text {replace tail entity }}<br>$$</p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>KBAT在做链接预测时仍然像其他图方法一样, 沿用了<strong>Encoder - Decoder</strong>架构.</p><p>作者的模型中使用<a href="https://adaning.github.io/posts/52280.html">ConvKB</a>作为解码器, 作者的目标是使用卷积层捕获三元组$t_{ij}^k$ 的全局信息. ConvKB的打分函数如下:<br>$$<br>f\left(t_{i j}^{k}\right)=\left(\prod_{m=1}^{\Omega} \operatorname{ReLU}\left(\left[\vec{h}_{i}, \vec{g}_{k}, \vec{h}_{j}\right] \ast \omega^{m}\right)\right) \cdot \mathbf{W}<br>$$<br>其中$\omega^m$ 为第$m$ 个卷积核, $\Omega$ 为卷积核总数, $\ast$ 为卷积操作. $\mathbf{W}$ 代表获取最终三元组得分的线性变换矩阵.</p><p>然后用软间隔损失来优化ConvKB:<br>$$<br>\mathcal{L}=\sum_{t_{i j}^{k} \in\left\{S \cup S^{\prime}\right\}} \log \left(1+\exp \left(l_{t_{i j}^{k}} \cdot f\left(t_{i j}^{k}\right)\right)\right)+\frac{\lambda}{2}|\mathbf{W}|_{2}^{2} \\<br>\text { where } l_{t_{i j}^{k}}=\left\{\begin{array}{ll}<br>1 &amp; \text { for } t_{i j}^{k} \in S \\<br>-1 &amp; \text { for } t_{i j}^{k} \in S^{\prime}<br>\end{array}\right.<br>$$</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的参数设置请参照原论文. </p><blockquote><p><strong>这篇论文的实验结果有很大问题, 在文末的Summary里会提到.</strong></p></blockquote><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><p>作者将训练过程分为两步:</p><ol><li>先用Hinge Loss单独训练KBAT作为<strong>Encoder</strong>, 使得GAT有编码能力.</li><li>再用Soft Margin Loss训练ConvKB作为<strong>Decoder</strong>, 使得ConvKB有解码能力.</li></ol><blockquote><p>原文并没有说是KBAT和ConvKB联合训练还是单独训练ConvKB, 但在代码中是ConvKB直接加载了已经训练好的KBAT的Embedding, 然后继续训练. 也就是直接使用KBAT的Embedding当做ConvKB的Embedding<strong>初始化</strong>, 同时训练Embedding和ConvKB, <strong>并没有对Embedding做冻结</strong>.</p><p>作者针对此问题有<a href="https://github.com/deepakn97/relationPrediction/issues/14" target="_blank" rel="noopener">回复</a>, 但我仍感觉这样做可能会把KBAT提取好的Embedding信息<strong>破坏</strong>掉, 而非在KBAT的基础上去找ConvKB的更优解.</p></blockquote><p>作者在五个常用数据集做了链接预测实验, 统计信息如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kbat5.jpg" style="zoom: 50%;" /><p>在WN18RR和FB15k - 237上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kbat6.jpg" style="zoom: 33%;" /><p>R - GCN在FB15k - 237和WN18RR上的结果没有超过ConvE, 但KBAT超过了. WN18RR上的Hits@1表现不太好. 作者在后面分析了原因.</p><p>在NELL - 995和Kinship上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kbat7.jpg" style="zoom: 33%;" /><p>在这两个数据集上来看KBAT效果不错, R - GCN就不太行.</p><h3 id="PageRank-Analysis"><a href="#PageRank-Analysis" class="headerlink" title="PageRank Analysis"></a>PageRank Analysis</h3><p>作者假设复杂的多跳信息在稠密图中比在稀疏图中要更难捕获, 作者设计了类似ConvE中的实验, 探究了PageRank对模型的影响, 即测试将DistMult替换成KBAT后MRR的相对变化:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kbat10.jpg" style="zoom: 33%;" /><p>基本上PageRank越高, KBAT进步就越明显. 作者将在WN18RR上表现不好归因于WN18RR的高度稀疏和分级结构.</p><h3 id="Attention-Values-vs-Epochs"><a href="#Attention-Values-vs-Epochs" class="headerlink" title="Attention Values vs Epochs"></a>Attention Values vs Epochs</h3><p>下图是FB15k - 237中注意力值随epoch增大的变化情况:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kbat8.jpg" style="zoom: 50%;" /><p>下图是WN18RR中注意力值随epoch增大的变化情况:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kbat9.jpg" style="zoom: 50%;" /><p>总体来说, 随epoch的增大, 多跳邻居会被分配给更多的注意力.</p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>作者逐渐移除模型中的信息, 做出NELL - 995上MR随epoch的变化曲线:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kbat11.jpg" style="zoom: 50%;" /><p>红色是作者的模型曲线, 绿色是移除多跳路径信息的曲线, 蓝色是移除关系信息的曲线. </p><blockquote><p>作者也没说如何移除, 估计是用随机初始化来代替.</p></blockquote><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>KBAT将<strong>GAT</strong>引入了KGE领域, 结合KG中的十分重要的<strong>关系</strong>, 利用<strong>图注意力网络</strong>的聚合特性, 获取实体之间的<strong>多跳</strong>信息, 并取得了比R - GCN更好的效果.</p><p>虽然有实验, 但是本论文除了将GAT引入KG领域中, 感觉没有什么创新点… 文中有很大篇幅都在介绍GAT. 有一些小改动也没有给出说明.</p><p>后续有一些<a href="https://arxiv.org/abs/1911.03903" target="_blank" rel="noopener">论文</a>说明KBAT给出的代码导致了Test Data存在Leakage, 详见<a href="https://github.com/deepakn97/relationPrediction/issues/28" target="_blank" rel="noopener">此处</a>, 所以实验结果大多<strong>站不住脚</strong>, 很多疑问都没有必要再追究了. 但使用GAT在思路上应该仍然有效.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> GNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CompGCN: Composition-based Multi-Relational Graph Convolutional Networks</title>
      <link href="/posts/45769.html"/>
      <url>/posts/45769.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>GCN: 详见<a href="https://adaning.github.io/posts/31958.html">图神经网络入门</a></li><li>R - GCN: 详见<a href="https://adaning.github.io/posts/3226.html">R - GCN: Modeling Relational Data with Graph Convolutional Networks</a></li></ul></blockquote><h1 id="Composition-based-Multi-Relational-Graph-Convolutional-Networks"><a href="#Composition-based-Multi-Relational-Graph-Convolutional-Networks" class="headerlink" title="Composition-based Multi-Relational Graph Convolutional Networks"></a>Composition-based Multi-Relational Graph Convolutional Networks</h1><p>本文是论文<a href="https://arxiv.org/abs/1911.03082" target="_blank" rel="noopener">Composition-based Multi-Relational Graph Convolutional Networks</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者观察到, 现在GCN在图建模中存在两个问题:</p><ol><li><p>现实世界中的图经常是<strong>有向图</strong>, 但现在图领域的研究常常专注于简单的无向图问题.</p></li><li><p>GCN的<strong>过参数化</strong>问题很严重, 并且<strong>只学习节点表示</strong>限制了模型表达.</p></li></ol><p>作者希望提出一种对节点和关系<strong>联合嵌入</strong>的GCN模型来应对上述问题.</p><h2 id="CompGCN"><a href="#CompGCN" class="headerlink" title="CompGCN"></a>CompGCN</h2><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/compgcn1.jpg" style="zoom: 33%;" /><h3 id="GCN-and-R-GCN"><a href="#GCN-and-R-GCN" class="headerlink" title="GCN and R - GCN"></a>GCN and R - GCN</h3><p>对于图$\mathcal{G}=(\mathcal{V}, \mathcal{E}, \mathcal{X})$, $\mathcal{V}, \mathcal{E}, \mathcal{X}$ 分别代表图中存在的顶点, 边, 和节点的初始特征. </p><p>让我们先来回顾一下经典的<a href="https://adaning.github.io/posts/31958.html">GCN</a>的节点特征更新方程:</p><p>$$<br>\boldsymbol{H}^{k+1}=f\left(\hat{\boldsymbol{A}} \boldsymbol{H}^{k} \boldsymbol{W}^{k}\right)<br>$$<br>其中$\hat{\boldsymbol{A}}$ 是由度矩阵和自邻接矩阵归一化得到的.</p><p>在<a href="https://adaning.github.io/posts/3226.html">R - GCN</a>中, 引入了对不同关系$\mathcal{R}$ 的特化处理, 图结构变为了$\mathcal{G}=(\mathcal{V}, \mathcal{R}, \mathcal{E}, \mathcal{X})$:<br>$$<br>\boldsymbol{H}^{k+1}=f\left(\hat{\boldsymbol{A}} \boldsymbol{H}^{k} \boldsymbol{W}_{r}^{k}\right)<br>$$</p><p>其中, $\boldsymbol{W}_r$ 为关系特化的变换矩阵.</p><p>但和大多数只嵌入节点的常规GCN方法不同, CompGCN同时嵌入<strong>节点</strong>和<strong>关系</strong>, 图结构信息变为$\mathcal{G}=(\mathcal{V}, \mathcal{R}, \mathcal{E}, \mathcal{X}, \mathcal{Z})$, $\mathcal{Z}$ 代表<strong>初始化</strong>的关系特征.</p><p>边的种类也被作者额外区分, 能对<strong>逆边</strong>和<strong>自环边</strong>加以区分, 即:<br>$$<br>\left.\mathcal{E}^{\prime}=\mathcal{E} \cup\left\{\left(v, u, r^{-1}\right) \mid(u, v, r) \in \mathcal{E}\right\} \cup\{(u, u, \top) \mid u \in \mathcal{V})\right\}<br>$$</p><p>相应的, 关系也被加以区分, 即$\mathcal{R}^{\prime}=\mathcal{R} \cup \mathcal{R}_{i n v} \cup\{\top\}$, 其中$\mathcal{R}_{i n v}=\left\{r^{-1} \mid r \in \mathcal{R}\right\}$. </p><h3 id="Relation-Based-Composition"><a href="#Relation-Based-Composition" class="headerlink" title="Relation - Based Composition"></a>Relation - Based Composition</h3><p>引入关系嵌入后, 与KGE方法一样, 作者利用关系嵌入$\mathbf{e}_s$ 和实体嵌入$\mathbf{e}_r$ 的<strong>组合操作</strong>将节点和关系的信息作为整体共同$\mathbf{e}_o$, 纳入GCN的更新方程:<br>$$<br>\boldsymbol{e}_{o}=\phi\left(\boldsymbol{e}_{s}, \boldsymbol{e}_{r}\right)<br>$$</p><p>其中, $\phi: \mathbb{R}^{d} \times \mathbb{R}^{d} \rightarrow \mathbb{R}^{d} $ 是关系和实体嵌入的组合方式.</p><p>将关系表示为Embedding减轻了图神经网络中针对关系的<strong>过参数化</strong>问题.</p><blockquote><p>我个人的理解是, 在R - GCN中, 关系特化以变换矩阵$\boldsymbol{W}_r$ 的形式存在, 而CompGCN中以Embedding的形式存在, 减少了每种关系所对应的参数个数. Embedding是信息的低维表征, <strong>万物皆可Embedding</strong>.</p></blockquote><p>作者尝试了三种实体关系组合方式:</p><ul><li><strong>Subtraction</strong>(<strong>Sub</strong>): $\phi\left(\mathbf{e}_{s}, \mathbf{e}_{r}\right)=\mathbf{e}_{s}-\mathbf{e}_{r}$.</li><li><strong>Multiplication</strong>(<strong>Mult</strong>): $\phi\left(\boldsymbol{e}_{s}, \boldsymbol{e}_{r}\right)=\boldsymbol{e}_{s} \ast \boldsymbol{e}_{r}$.</li><li><strong>Circular - correlation</strong>(<strong>Corr</strong>): $\phi\left(\boldsymbol{e}_{s}, \boldsymbol{e}_{r}\right)=\boldsymbol{e}_{s} \star \boldsymbol{e}_{r}$.</li></ul><p>分别是相减, 相乘, 循环相关运算. 循环相关运算出自论文<a href="https://dl.acm.org/doi/10.5555/3016100.3016172" target="_blank" rel="noopener">Holographic embeddings of knowledge graphs</a>, 即HoLE的论文. 三阶时的计算方法与行列式的主对角线计算中组合方式是相似的:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/compgcn11.png" style="zoom: 25%;" /><p>循环相关运算相比于前两种更为复杂, 考虑了更多向量间的<strong>不同组合</strong>方式.</p><blockquote><p>通过关系实体组合操作, 关系和实体的嵌入组合特征将在同一特征空间下对节点信息更新.</p></blockquote><h3 id="CompGCN-Update-Equation"><a href="#CompGCN-Update-Equation" class="headerlink" title="CompGCN Update Equation"></a>CompGCN Update Equation</h3><p>普通GCN的节点表示更新方程如下:</p><p>$$<br>\boldsymbol{h}_{v}=f\left(\sum_{(u, r) \in \mathcal{N}(v)} \boldsymbol{W}_{r} \boldsymbol{h}_{u}\right)<br>$$</p><p>其中, $\mathcal{N}(v)$ 是节点$v$ 的<strong>出边邻居</strong>, $(u,r)$ 是邻居节点和相对应的和关系. $\boldsymbol{h}_u$ 为节点$u$ 的特征.</p><p>而CompGCN引入关系实体嵌入的组合后, 的节点更新方式如下:</p><p>$$<br>\boldsymbol{h}_{v}=f\left(\sum_{(u, r) \in \mathcal{N}(v)} \boldsymbol{W}_{\lambda(r)} \phi\left(\boldsymbol{x}_{u}, \boldsymbol{z}_{r}\right)\right)<br>$$</p><p>其中$\lambda(r)$ 针对<strong>边的种类</strong>做了特化, 同一种关系在不同的边类型上也许会得到区分. 作者将边分为<strong>出边</strong>, <strong>入边</strong>, 和<strong>自环边</strong>三种. 在这里$\lambda(r)=\text{dir}(r)$:</p><p>$$<br>\boldsymbol{W}_{\mathrm{dir}(r)}=\left\{\begin{array}{ll}<br>\boldsymbol{W}_{O}, &amp; r \in \mathcal{R} \\<br>\boldsymbol{W}_{I}, &amp; r \in \mathcal{R}_{i n v} \\<br>\boldsymbol{W}_{S}, &amp; r=\top(\text { self-loop })<br>\end{array}\right.<br>$$</p><p>在该模式下, 一些以前的方法能够被纳入CompGCN的框架之中:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/compgcn3.jpg" style="zoom: 33%;" /><p>从CompGCN的身上能偶找到这些过去方法的影子.</p><p>关系嵌入同节点嵌入的聚合一样, 也需要做类似的更新, 但关系没有邻居之类的概念, 直接用简单的投影即可:<br>$$<br>\boldsymbol{h}_{r}=\boldsymbol{W}_{\mathrm{rel}} \boldsymbol{z}_{r}<br>$$</p><p>$\boldsymbol{W_{\text{rel}}}$ 是<strong>与关系无关</strong>的变换矩阵, 它只是将初始关系嵌入$\boldsymbol{z}_{r}$ 投影到关系嵌入空间而已.</p><p>与R - GCN一样, CompGCN也使用<strong>基函数分解</strong>来减轻GCN的过参数化. 但用途仅为获得<strong>初始化</strong>的关系嵌入$\boldsymbol{z}_r$:<br>$$<br>\boldsymbol{z}_{r}=\sum_{b=1}^{\mathcal{B}} \alpha_{b r} \boldsymbol{v}_{b}<br>$$<br>基函数分解并没有在后续更新关系(边)的Embedding中使用到.</p><p>上述为CompGCN在<strong>只有一层</strong>情况下的说明.</p><h3 id="CompGCN-in-Multilayer"><a href="#CompGCN-in-Multilayer" class="headerlink" title="CompGCN in Multilayer"></a>CompGCN in Multilayer</h3><p>对于多层的情况, 只需要将上面的方程改写为含有层数$k$ 的形式:<br>$$<br>\boldsymbol{h}_{v}^{k+1}=f\left(\sum_{(u, r) \in \mathcal{N}(v)} \boldsymbol{W}_{\lambda(r)}^{k} \phi\left(\boldsymbol{h}_{u}^{k}, \boldsymbol{h}_{r}^{k}\right)\right)<br>$$</p><p>关系表示$\boldsymbol{h}_r^k$ 的更新也按照多层形式写出:</p><p>$$<br>\boldsymbol{h}_{r}^{k+1}=\boldsymbol{W}_{\mathrm{rel}}^{k} \boldsymbol{h}_{r}^{k}<br>$$</p><p>注意, $\boldsymbol{W}_{\text{rel}}$ 每层参数都是不同的, 多层GCN也对关系嵌入多次迭代更新.</p><p>作者对比了CompGCN与普通GCN方法的优势:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/compgcn2.jpg" style="zoom: 50%;" /><p>$K$ 为GCN的层数, $d$ 为Embedding维度, $\mathcal{B}$ 为基的数量, $\lvert \mathcal{R} \rvert$ 为关系个数.</p><p>CompGCN复杂度中的第一项来自于与层数相关的$\boldsymbol{W}_{\text{rel}}, \boldsymbol{W}_{\lambda(r)}$, 第二, 三项分别来自于基函数分解中的基$\boldsymbol{v}_{b}$ 和标量$\alpha_{b r}$.</p><p>相较于其他的GCN算法, 尤其是R - GCN, CompGCN保持了相对少的参数量.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的实验参数设置请参照原论文.</p><h3 id="Link-Prediction"><a href="#Link-Prediction" class="headerlink" title="Link Prediction"></a>Link Prediction</h3><p>CompGCN在链接预测上的玩法仍然沿用了R - GCN中<strong>Encoder - Decoder</strong>的架构</p><p>R - GCN使用的Decoder是DistMult, 解码时关系嵌入需要额外训练, 并且存在实体嵌入和关系嵌入的方法<strong>不匹配</strong>问题, 这就割裂了实体嵌入和关系嵌入的关系. 但CompGCN在编码阶段就引入了<strong>关系嵌入</strong>, 不会存在关系和实体嵌入训练不匹配的问题:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/compgcn6.jpg" style="zoom: 50%;" /><h4 id="Main-Result"><a href="#Main-Result" class="headerlink" title="Main Result"></a>Main Result</h4><p>作者在FB15k - 237和WN18RR上做了Link Prediction, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/compgcn4.jpg" style="zoom: 50%;" /><p>无论是比传统的KGE方法, 还是其他基于图的KGE方法, CompGCN的综合表现比较不错, 在WN18RR上的Hits@10比RotatE少了三个点.</p><h4 id="Comparison-in-CompGCN"><a href="#Comparison-in-CompGCN" class="headerlink" title="Comparison in CompGCN"></a>Comparison in CompGCN</h4><p>作者主要比较了不同GCN方法在不同Decoder下的链接预测性能表现, 其形式为$\mathbf{X}+\mathbf{M}(\mathbf{Y})$, $\mathbf{X}$ 代表某种KGE方法的打分函数, $\mathbf{M}$ 代表用来获取实体和关系Embedding的GCN方法名称, $\mathbf{Y}$ 代表组合操作符.</p><p>在FB15k - 237上的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/compgcn5.jpg" style="zoom: 50%;" /><p>几乎所有的GCN方法会导致TransE的性能退化, 但CompGCN没有. 可能因为关系和实体的联合学习. 不同的组合操作所擅长的优化指标是不相同的.</p><h4 id="Scalability-of-CompGCN"><a href="#Scalability-of-CompGCN" class="headerlink" title="Scalability of CompGCN"></a>Scalability of CompGCN</h4><h5 id="Effect-of-Varying-Relation-Basis-Vectors"><a href="#Effect-of-Varying-Relation-Basis-Vectors" class="headerlink" title="Effect of Varying Relation Basis Vectors"></a>Effect of Varying Relation Basis Vectors</h5><p>作者比较了FB15k - 237上不同基向量个数对相对MRR的影响:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/compgcn7.jpg" style="zoom: 50%;" /><p>当基向量个数为237, 即关系不使用基向量时性能达到最好. 但采用更少的基向量并不会带来非常明显的性能衰减.</p><h5 id="Effect-of-Number-of-Relations"><a href="#Effect-of-Number-of-Relations" class="headerlink" title="Effect of Number of Relations"></a>Effect of Number of Relations</h5><p>作者比较了FB15k - 237中, 对于相同的基函数为5的情况下, 不同关系数量上的相对MRR:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/compgcn9.jpg" style="zoom: 50%;" /><p>即使固定基函数为5, 仍然能比较好的应对繁多的关系.</p><blockquote><p>上述两个实验说明关系之间可能存在着大量线性相关, 少量的基向量完全可以重新组合表示它们.</p></blockquote><h5 id="Comparison-with-R-GCN"><a href="#Comparison-with-R-GCN" class="headerlink" title="Comparison with R - GCN"></a>Comparison with R - GCN</h5><p>作者对比了R - GCN和CompGCN在FB15k - 237的不同关系数量子集中的表现:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/compgcn8.jpg" style="zoom: 50%;" /><p>仅仅在5个基函数的情况下, CompGCN全面优于R - GCN.</p><h3 id="Node-and-Graph-Classification"><a href="#Node-and-Graph-Classification" class="headerlink" title="Node and Graph Classification"></a>Node and Graph Classification</h3><p>在节点和图分类任务中, CompGCN表现如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/compgcn10.jpg" style="zoom: 33%;" /><p>这两个数据集上, 对于简单的节点分类和图分类问题, CompGCN表现良好.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>CompGCN进一步地将Knowledge Embedding中对待关系的方法引入了GCN领域, 在之前的GCN类方法中, 常不将关系嵌入引入, 而在KGE领域中, 将关系和实体嵌入共同使用是非常普遍的.</p><p>CompGCN算是<strong>集大成者</strong>, 作者考虑了很多关于GCN<strong>过参数化</strong>的内容. 同时兼顾<strong>有向</strong>, <strong>多关系</strong>, <strong>支持关系嵌入</strong>三种特性. </p><p>作者着重论述了CompGCN的各方面优势, 例如复杂度, 可扩展性, 以及具体任务的性能等.</p><blockquote><p>作者在文中使用的是非参数化的组合方式, 其实也可以尝试参数化的组合, 这样也会引入更高的复杂度. </p><p>作者也没有考虑引入Attention来增强边对节点的影响, 而是使用了对边类型的特化来处理, 可能Attention的泛用性更强一些.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> GNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KEQA: Knowledge Graph Embedding Based Question Answering</title>
      <link href="/posts/15146.html"/>
      <url>/posts/15146.html</url>
      
        <content type="html"><![CDATA[<h1 id="Knowledge-Graph-Embedding-Based-Question-Answering"><a href="#Knowledge-Graph-Embedding-Based-Question-Answering" class="headerlink" title="Knowledge Graph Embedding Based Question Answering"></a>Knowledge Graph Embedding Based Question Answering</h1><p>本文是论文<a href="https://dl.acm.org/doi/abs/10.1145/3289600.3290956" target="_blank" rel="noopener">Knowledge Graph Embedding Based Question Answering</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者发现KBQA中的三个挑战问题:</p><ol><li>谓词(关系)有多种<strong>自然语言</strong>的表示法.</li><li>实体也会有严重的模糊性而产生大量候选答案, 即实体的<strong>歧义问题</strong>.</li><li>用户的问题领域可能是<strong>开放</strong>的, KG也有可能是不完整的, 这对鲁棒性有一定的要求.</li></ol><p>Knowledge Embedding可以将实体映射为低维向量, KG中的<strong>关系信息</strong>会被保存, 各种<strong>KRL</strong>会非常有益于下游任务. 因此, 作者希望提出一种基于<strong>KGE</strong>方法, 并能够回答所有<strong>自然语言问题</strong>的框架.</p><p>KGE能够保证得到的单词表示的<strong>质量</strong>, 因为每个嵌入表示都是与<strong>整个KG交互</strong>的结果, 并且它也能保持相似的关系或实体之间的<strong>表示相似性</strong>.</p><blockquote><p>在本文中, 作者只关注了最简单的QA, 直接使用一个<strong>三元组</strong>就能描述整个问题, 而非多跳推理的问题.</p></blockquote><h2 id="KEQA"><a href="#KEQA" class="headerlink" title="KEQA"></a>KEQA</h2><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/keqa1.jpg" style="zoom: 50%;" /><p>KEQA的大致思路是通过某种结构, 对自然语言中的<strong>整个句子</strong>抽取出与Knowledge Embedding<strong>相似</strong>的表示, 即希望用句子抽取后的表示空间<strong>等价于</strong>Knowledge Embedding的空间.</p><h3 id="Predicate-and-Head-Entity-Learning-Models"><a href="#Predicate-and-Head-Entity-Learning-Models" class="headerlink" title="Predicate and Head Entity Learning Models"></a>Predicate and Head Entity Learning Models</h3><h4 id="Knowledge-Graph-Embedding"><a href="#Knowledge-Graph-Embedding" class="headerlink" title="Knowledge Graph Embedding"></a>Knowledge Graph Embedding</h4><p>对于头实体$\mathbf{e}_h$, 谓词(关系)$\mathbf{p}_\ell$, KGE方法能通过打分函数$f(\mathbf{e}_t, \mathbf{p}_\ell)$得出对应的尾实体$\mathbf{e}_t$, 即$\mathbf{e}_t \approx f(\mathbf{e}_h, \mathbf{p}_\ell)$. 通过KGE, 我们能获得一个KG中的实体嵌入表示集$\mathbf{E}$ 和谓词嵌入表示集$\mathbf{P}$.</p><blockquote><p>当KGE训练完成时, 实体和关系的表示将会<strong>固定</strong>下来, 这样才能保存住KG的信息. 若继续在后续训练时更新Embedding, 将会对原有信息扰动. 所以KGE只是做了空间指示作用.</p></blockquote><h4 id="Neural-Network-Based-Predicate-Representation-Learning"><a href="#Neural-Network-Based-Predicate-Representation-Learning" class="headerlink" title="Neural Network Based Predicate Representation Learning"></a>Neural Network Based Predicate Representation Learning</h4><p>有了KGE中获取的$\mathbf{P}, \mathbf{E}$, 接着需要将自然语言中的<strong>谓词</strong>表示(在三元组中也是关系)与KGE空间相<strong>对齐</strong>, 作者在这里采用了比较简单的<strong>双向LSTM + Attention</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/keqa2.jpg" style="zoom: 50%;" /><blockquote><p>其实在作者发布的代码中, 用的是<strong>GRU</strong>, 不过都大差不差了.</p></blockquote><p>图中在绿色向量$\mathbf{r}_j$ 和拼接后的向量$\mathbf{h}_j$ 之间还有一个FC层没画出来.</p><p>先将自然语言的Token转换为Embedding(图中灰色), 记为$\mathbf{x}_j$, 然后再用双向LSTM获取双向表示$\mathbf{h}_j$ (图中红色蓝色).</p><p>下述数学表达都是LSTM, 左右双向同理, 下面只是单向:<br>$$<br>\begin{array}{l}<br>\mathrm{f}_{j}=\sigma\left(\mathbf{W}_{x f} \mathbf{x}_{j}+\mathbf{W}_{h f} \overleftarrow{\mathbf{h}}_{j+1}+\mathbf{b}_{f}\right) \\<br>\mathbf{i}_{j}=\sigma\left(\mathbf{W}_{x i} \mathbf{x}_{j}+\mathbf{W}_{h i} \overleftarrow{\mathbf{h}}_{j+1}+\mathbf{b}_{i}\right) \\<br>\mathbf{o}_{j}=\sigma\left(\mathbf{W}_{x o} \mathbf{x}_{j}+\mathbf{W}_{h o} \overleftarrow{\mathbf{h}}_{j+1}+\mathbf{b}_{o}\right) \\<br>\mathbf{c}_{j}=\mathbf{f}_{j} \circ \mathbf{c}_{j+1}+\mathbf{i}_{j} \tanh \left(\mathbf{W}_{x c} \mathbf{x}_{j}+\mathbf{W}_{h c} \overleftarrow{\mathbf{h}}_{j+1}+\mathbf{b}_{c}\right) \\<br>\overleftarrow{\mathbf{h}}_{j}=\mathbf{o}_{j} \circ \tanh \left(\mathbf{c}_{j}\right)<br>\end{array}<br>$$</p><p>将最初的$\mathbf{x}_j$ 和双向LSTM抽取得到的$\mathbf{h}_j$ 拼到一起, 计算Attention Score $q_j$ 和注意力权重$\alpha_j$ (图中紫色):<br>$$<br>\begin{aligned}<br>\alpha_{j} &amp;=\frac{\exp \left(q_{j}\right)}{\sum_{i=1}^{L} \exp \left(q_{i}\right)} \\<br>q_{j} &amp;=\tanh \left(\mathbf{w}^{\top}\left[\mathbf{x}_{j} ; \mathbf{h}_{j}\right]+b_{q}\right)<br>\end{aligned}<br>$$<br>将注意力权重与$\mathbf{h}_j$ 加权, 与$\mathbf{x}_j$ 拼接得到隐态$\mathbf{s}_j=[\mathbf{x}_j;\alpha_j\mathbf{h}_j]$. 之后再经过一个FC层得到最终的结果$\mathbf{r}_j$.</p><p>然后对求得的每个Token的最终表示$\mathbf{r}_j$ (图中绿色) 求个平均, 得到与KGE中谓词表示$\mathbf{p}_{\ell}$ 相似的表示$\hat{\mathbf{p}}_{\ell}$:<br>$$<br>\hat{\mathbf{p}}_{\ell}=\frac{1}{L} \sum_{j=1}^{L} \mathbf{r}_{j}^{\top}<br>$$</p><p>其中$L$ 是问题的句子长度.</p><h4 id="Neural-Network-based-Head-Entity-Learning-Model"><a href="#Neural-Network-based-Head-Entity-Learning-Model" class="headerlink" title="Neural Network based Head Entity Learning Model"></a>Neural Network based Head Entity Learning Model</h4><p>对于一个自然语言问题, 作者用类似获取谓词表示的方法获取头实体表示$\hat{\mathbf{e}}_{h}$, 用同样的架构获取头实体的表示, 也与KGE空间相对齐.</p><blockquote><p>这样直接把NER这步给省略掉了, 规避了自然语言中实体表示的模糊性问题.</p></blockquote><h3 id="Head-Entity-Detection-Model"><a href="#Head-Entity-Detection-Model" class="headerlink" title="Head Entity Detection Model"></a>Head Entity Detection Model</h3><p>因为自然语言问题中的候选实体可能过多, 所以需要一个<strong>头实体检测模型</strong>来减少问题中的候选实体, 从一个句子中选择一个或几个连续的Token作为实体名称.</p><p>同样是使用双向LSTM去处理整个句子:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/keqa3.jpg" style="zoom: 50%;" /><p>先获取词嵌入$\mathbf{x}_j$, 再使用双向LSTM获得$\mathbf{h}_j$, 后接一个FC层和Softmax, 能获得两个概率, 分别是该Token属于实体名的概率$\text{HED}_{\text{entity}}$, 和Token不属于实体名的概率$\text{HED}_\text{non}$.</p><p>这样, 相连的HED可能是同一个实体, 而不相连的HED可能是多个实体名. 在获取完实体名后, 再从KG中找到对应的<strong>候选实体集</strong>, 很大程度上缩小了与KG中实体匹配的范围.</p><h3 id="Joint-Search-on-Embedding-Spaces"><a href="#Joint-Search-on-Embedding-Spaces" class="headerlink" title="Joint Search on Embedding Spaces"></a>Joint Search on Embedding Spaces</h3><p>如果只是想简单的缩小模型与KGE空间中的距离, 只需计算模型抽取出的表示与KGE空间中表示的范数. 但这显然没有考虑过KGE中保留的<strong>关系信息</strong>.</p><p>基于前面的谓词学习模型, 实体学习模型, 头实体检测模型, 作者提出了对三种模型的<strong>联合距离度量</strong>:</p><p>$$<br>\begin{aligned}<br>\underset{(h, \ell, t) \in C} {\operatorname{minimize}} \quad\lVert\mathbf{p}_{\ell}-\hat{\mathbf{p}}_{\ell}\rVert_{2}+\beta_{1}\lVert\mathbf{e}_{h}-\hat{\mathbf{e}}_{h}\rVert_{2}+\beta_{2}\lVert f\left(\mathbf{e}_{h}, \mathbf{p}_{\ell}\right)-\hat{\mathbf{e}}_{t}\rVert_{2}<br> \\\ -\beta_{3} \operatorname{sim}\left[n(h), \mathrm{HED}_{\text {entity}}\right]-\beta_{4} \operatorname{sim}\left[n(\ell), \mathrm{HED}_{\text {non}}\right]<br>\end{aligned}<br>$$</p><p>其中, $\beta_1,\beta_2,\beta_3,\beta_4$ 是预定义的超参, 即损失中四项的权重, 用来衡量所对应的影响力. $n(\cdot)$ 代表取实体或谓词的具体名的操作.</p><p>第一, 二项均用于缩小模型抽取出的头实体和谓词表示与KGE空间中相应表示的距离, 第三项用于缩小尾实体表示与KGE中相应实体表示的距离. 第四项和第五项针对头实体检测模型, $\operatorname{sim}[\cdot;\cdot]$ 是度量两个<strong>字符串</strong>之间相似性的函数, 在这里最大化标出的头实体和目标实体之间的相似度, 以及句子的非实体部分和真实谓词之间的相似度.</p><p>KEQA的框架执行流程如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/keqa4.jpg" style="zoom: 67%;" /><p>大致流程是:</p><ol><li>分别<strong>训练</strong>谓词学习模型, 实体学习模型, 头实体检测模型. 训练谓词和实体表示模型时最小化模型结果与KGE相应表示的距离. 训练头实体检测模型时采用极大似然.</li><li>训练结束后, 面对每个问题, 分别用谓词学习模型和实体学习模型抽取出问题的谓词表示$\hat{\mathbf{p}}_\ell$ 和头实体表示$\hat{\mathbf{e}}_h$.</li><li>使用头实体检测模型检测出问题中出现的头实体$\text{HED}_{\text{entity}}$.</li><li>在KG中找到与检测出的头实体所匹配的候选集$\mathcal{C}$, 然后用KGE的打分函数$f(\cdot)$, 将$\hat{\mathbf{p}}_\ell$ 和$\hat{\mathbf{e}}_h$ 预测出相应的尾实体.</li><li>根据预测出的尾实体, 计算出与当前三元组<strong>联合距离最小</strong>的三元组, 作为答案返回.</li></ol><blockquote><p>头实体检测模型训练时采用的损失与联合距离度量略有不同, 训练时采用的是极大似然, 但在计算联合距离时采用的是字符串相似度.</p><p>KEQA对不同的损失项的优化实际上是<strong>分步</strong>运行的, 并且不同损失项针对的是不同模型.</p></blockquote><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>实验部分详细的参数请参照原论文.</p><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><p>因为本文探讨的是<strong>简单问题</strong>的回答, 作者除了采用FB2M, FB5M外, 还使用了从FB2M中的子集数据集SimpleQuestions, 统计信息如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/keqa5.jpg" style="zoom: 67%;" /><h3 id="Effectiveness-of-KEQA"><a href="#Effectiveness-of-KEQA" class="headerlink" title="Effectiveness of KEQA"></a>Effectiveness of KEQA</h3><p>KEQA在FB2M, FB5M上的实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/keqa8.jpg" style="zoom: 67%;" /><p>其中, KEQA_noEmbed是<strong>随机</strong>得到的Embedding, 没有经过KGE训练.</p><p>相较于提出的Baseline, 提升比较大. 在引入Knowledge Embedding后, 模型性能有了进一步的提升, 但其实在没引入KGE模型的情况下, 本身的效果也还可以, 因为它仍然超过了其他的Baseline.</p><h3 id="Generalizability-and-Robustness-Evaluation"><a href="#Generalizability-and-Robustness-Evaluation" class="headerlink" title="Generalizability and Robustness Evaluation"></a>Generalizability and Robustness Evaluation</h3><p>为检测KEQA的<strong>鲁棒性</strong>, 作者从两个角度做了实验:</p><ol><li>对KEQA使用不同的KGE方法.</li><li>对所有的谓词划分为三个组, 然后将训练集, 验证集, 测试集的问题按照谓词划分为三组. 这样测试集中出现的谓词肯定没有在训练集和验证集中出现过, 三类数据的谓词都是相互独立的. 按照该方法将SimpleQuestions改进为SimpleQ_Missing.</li></ol><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/keqa6.jpg" style="zoom: 67%;" /><p>不同的KGE方法对结果其实没有太大的影响. 最简单的TransE就能有不错的结果. 对于新的数据集, 在TransE的帮助下仍然有40%左右的精度, 比不依赖KGE方法要高一些.</p><h3 id="Parameter-Analysis"><a href="#Parameter-Analysis" class="headerlink" title="Parameter Analysis"></a>Parameter Analysis</h3><p>为了探究联合距离度量中每项的贡献程度, 作者分别采用三种设置:</p><ul><li>Only_Keep: <strong>仅保留</strong>五项中的该度量.</li><li>Remove: 从五项中<strong>删除</strong>该项度量.</li><li>Accumulate: <strong>逐项添加</strong>度量.</li></ul><p>实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/keqa7.jpg" style="zoom: 67%;" /><p>仅保留实体相关项的精度都非常低, 而仅保留谓词项的精度都比较高, 这也侧面说明了关系在KGE中发挥的重要作用, 实际上就是在针对关系学习. </p><p>在第一二项均保留的情况下, 结合谓词信息, 实体信息能够得到充分的利用.</p><p>第五项仍然使得模型性能涨了一个点, 说明问题中的某些Token可能和谓词<strong>共享信息</strong>.</p><blockquote><p>我认为第三项和第一二项可能是<strong>近似等价</strong>的, 而不是<strong>互为补充</strong>的.</p><p>因为第三项的引入并没有使得模型性能提升产生多大的变化, 但仅保留时仍然发挥了相当重要的作用, 而且在删除第三项时也没有对性能产生太大的影响. </p><p>即仅保留第三项时没有第一二项, 性能好. 删除第三项, 剩下其余四项, 性能没有明显变化. 在已经有第一二项的情况下, 加上第三项, 几乎无提升.</p></blockquote><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>KEQA是一种基于<strong>Knowledge Embedding</strong>的<strong>问答</strong>框架, 能从繁杂的<strong>自然语言</strong>中直接抽取出<strong>谓词表示</strong>和<strong>实体表示</strong>, 缓解了自然语言的<strong>模糊性</strong>和<strong>歧义性</strong>问题. 并通过<strong>头实体检测</strong>模型过<strong>滤掉</strong>非常多的候选实体三元组, <strong>缩小搜索范围</strong>. 同时, 作者充分结合了KGE能够保存<strong>关系信息</strong>的特性, 提出了<strong>联合距离度量</strong>.</p><p>除此外, 我个人认为联合度量中第三项的必要性是有待商榷的.</p><p>BERT具有类似KGE获取表示的能力, 在依托KGE方法的KEQA的框架下, 结合一些<strong>上下文相关</strong>的KGE方法可能会有奇效, 例如<a href="https://adaning.github.io/posts/42304.html">CoKE</a>, <a href="https://adaning.github.io/posts/28100.html">CoLAKE</a>, 因为它们与KEQA类似能够利用<strong>自然语言信息</strong>的能力, 而不单单是利用知识本身.</p><p>本文只关注于最基本的单跳简单问题, 该如何扩展到多跳复杂问题?</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KBQA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CoPER: Contextual Parameter Generation for Knowledge Graph Link Prediction</title>
      <link href="/posts/14355.html"/>
      <url>/posts/14355.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>ConvE: 详见<a href="https://adaning.github.io/posts/42031.html">ConvE: Convolutional 2D Knowledge Graph Embeddings</a></li></ul></blockquote><h1 id="Contextual-Parameter-Generation-for-Knowledge-Graph-Link-Prediction"><a href="#Contextual-Parameter-Generation-for-Knowledge-Graph-Link-Prediction" class="headerlink" title="Contextual Parameter Generation for Knowledge Graph Link Prediction"></a>Contextual Parameter Generation for Knowledge Graph Link Prediction</h1><p>本文是论文<a href="https://ojs.aaai.org/index.php/AAAI/article/view/5693" target="_blank" rel="noopener">Contextual Parameter Generation for Knowledge Graph Link Prediction</a>的阅读笔记和个人理解. 本文是CMU发布的<strong>参数生成</strong>系列论文中的一篇, 系列文章请参见<a href="https://zhuanlan.zhihu.com/p/210942711" target="_blank" rel="noopener">参数生成（Parameter Generation/Adaption）相关论文整理</a>.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者发现, 现在的KGE模型中Relation Embedding经常被限制为<strong>加性</strong>的, 这使得模型在处理不同关系变换时的表达能力被<strong>限制</strong>.</p><p>为解决此问题, 作者使用<strong>上下文参数生成</strong>方法, 去除掉加性限制. 更具体的, 作者将关系作为<strong>上下文</strong>来做Knowledge Embedding, 下图是作者改进<strong>ConvE</strong>的示意图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/coper1.jpg" style="zoom: 33%;" /><p>如图构想所示, Relation Embedding作为用于<strong>生成</strong>操作Entity Embedding的<strong>参数</strong>信息而存在. 这种方式被作者称为<strong>CoPER</strong>(<strong>Co</strong>ntextual <strong>P</strong>arameters from <strong>E</strong>mbedded <strong>R</strong>elations).</p><h2 id="Limited-Expressive-Power"><a href="#Limited-Expressive-Power" class="headerlink" title="Limited Expressive Power"></a>Limited Expressive Power</h2><p>现在Knowledge Embedding框架普遍可以由如下四部组成:</p><p>$$<br>\begin{array}{lr}<br>\mathbf{e}_{s}=\mathbf{E} e_{s}, &amp;\quad \text { (embedding) }\\<br>\mathbf{r}=\mathbf{R} r, &amp;\quad \text { (embedding) }\\<br>\mathbf{z}=h_{\phi}\left(\mathbf{e}_{s}, \mathbf{r}, \ldots\right), &amp;\quad\text { (merge) }\\<br>\text {ans}=f_{\theta}(\mathbf{z}, \ldots), &amp;\quad \text { (prediction) }<br>\end{array}<br>$$</p><p>其中$e_s , r$ 分别是头实体和关系的<strong>独热编码</strong>, $\mathbf{E}, \mathbf{R}$ 是实体和关系的Embedding矩阵. $h$ 为<strong>Merge Function</strong>, 参数为$\phi$, 用于获取关系和头实体的聚合表示. $f$ 为<strong>Prediction Function</strong>, 参数为$\theta$, 用于预测出尾实体.</p><p>四步可以概括为获取头实体表示, 获取关系表示, KGE聚合, 投影预测.</p><p>Merge和Prediction的具体操作在不同的KGE方法中通常是不同的, 作者以<strong>ConvE</strong>为例, 具体的说明了表达是如何被限制的. 在ConvE中, 后两步的数学表达如下:<br>$$<br>\begin{array}{lr}<br>\mathbf{z}=\operatorname{Conv2D}\left(\operatorname{Reshape}\left(\left[\mathbf{e}_{s} ; \mathbf{r}\right]\right)\right), &amp; \text { (merge) } \\<br>\hat{e}_{t} =f_{\theta}(\mathbf{z}), &amp; \text { (prediction) }<br>\end{array}<br>$$</p><p>ConvE中, Merge Function为将头实体和关系的嵌入表示Concat, 后Reshape成二维数据, 然后用<strong>二维卷积</strong>获取聚合后的表示$\mathbf{z}$, $f_\theta$ 为一次线性变换, 即$\theta$ 代表投影矩阵.</p><p>考虑一种最简单的情况, 当Merge Function为Concat + Linear时的场景:</p><p>$$<br>h_{\phi}\left(\mathbf{e}_{s}, \mathbf{r}\right)=\phi \cdot\left[\mathbf{e}_{s} ; \mathbf{r}\right]<br>$$</p><p>$[\cdot; \cdot]$ 为Concat, $\phi$ 为单层线性投影. 正是因为Concat在操作上使得$\mathbf{e}_s, \mathbf{r}$ 之间是<strong>相互独立</strong>的, $h_\phi (\mathbf{e}_s; \mathbf{r})$ 可以被写为$h_\phi (\mathbf{e}_s; \mathbf{r})=\phi_s \mathbf{e}_s + \phi_r\mathbf{r}$, 所以$\mathbf{e}_s, \mathbf{r}$ 只能做<strong>加性交互</strong>. ConvE在大多数情况下都符合上述情况, 唯独除了卷积核在对$\mathbf{e}_s$ 和$\mathbf{r}$ 的<strong>交界处</strong>做交互的情况.</p><blockquote><p>作者所提出的问题仅适用于ConvE这种没有进行<strong>模块堆叠</strong>的方法, 因为ConvE仅仅只使用了一层卷积. </p><p>同样也不适用于能够同时覆盖$\mathbf{e}_o$ 和$\mathbf{e}_r$ 的操作. 如果操作能够覆盖二者, 那么二者之间将不单进行简单的加和运算, 还会有其他部分的交互运算.</p><p>如果是多层卷积堆叠, 随着堆叠加深, $\mathbf{e}_s$ 和$\mathbf{r}$ 交界处所映射的范围也越来越大, 直到感受野可以覆盖住整个特征图, 每次卷积都可以对整块数据交互, 即使是使用了Concat, 也不符合作者所述的情况.</p></blockquote><p>作者举出了一个非常简单的例子(Toy Example)来详细说明一些KGE模型在应对这些场景时所暴露的问题:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/coper2.jpg" style="zoom: 50%;" /><p>如果以<strong>三元组</strong>作为结构, 那么$\mathbf{e_2}, \mathbf{e_3}$ 可以按照如下方式表示:<br>$$<br>\begin{array}{l}<br>\mathbf{e}_{2}=\phi_{e} \mathbf{e}_{0}+\phi_{r} \mathbf{r}_{0} \\<br>\mathbf{e}_{3}=\phi_{e} \mathbf{e}_{0}+\phi_{r} \mathbf{r}_{1} \\<br>\mathbf{e}_{3}=\phi_{e} \mathbf{e}_{1}+\phi_{r} \mathbf{r}_{0} \\<br>\mathbf{e}_{2}=\phi_{e} \mathbf{e}_{1}+\phi_{r} \mathbf{r}_{1}<br>\end{array}<br>$$</p><p>将上述四个式子可以分别做减法得到如下两个$\mathbf{e}_2, \mathbf{e}_3$ 的表示式:</p><p>$$<br>\begin{array}{l}<br>\left(\mathbf{e}_{2}-\mathbf{e}_{3}\right)=\phi_{r}\left(\mathbf{r}_{0}-\mathbf{r}_{1}\right) \\<br>\left(\mathbf{e}_{3}-\mathbf{e}_{2}\right)=\phi_{r}\left(\mathbf{r}_{0}-\mathbf{r}_{1}\right)<br>\end{array}<br>$$</p><p>这两个表示式非常的矛盾, 除非满足如下情况:</p><ul><li>$\mathbf{e}_2 = \mathbf{e}_3$: 将导致$\mathbf{e}_2, \mathbf{e}_3$无法区分.</li><li>$\phi_r=0$ 或$\mathbf{r}_0 = \mathbf{r}_1$: 将导致$\mathbf{r}_1, \mathbf{r}_2$ 无法区分或失效.</li><li>$\phi_e=0$ 或$\mathbf{e}_0 = \mathbf{e}_1$: 将导致$\mathbf{e}_0, \mathbf{e}_1$ 无法区分.</li></ul><p>即模型<strong>无法处理</strong>Toy Example的情况. 主要原因就在于, 虽然Merge Function操作对象表面是$\mathbf{e}_s$ 和$\mathbf{r}$, 但做后续操作的时候还是近似对它们<strong>单独处理</strong>的.</p><h2 id="CoPER"><a href="#CoPER" class="headerlink" title="CoPER"></a>CoPER</h2><p>在CoPER中, Relation被定义为, 为了产生尾实体, 应该对头实体进行<strong>何种操作</strong>. 因此, $h$ 只作用于$\mathbf{e}_s$, 而$\mathbf{r}$ 将用于生成$f$ 的参数$\theta$. $f$ 的参数将不再由学习得来, 而由<strong>CPG</strong>(<strong>C</strong>ontextual <strong>P</strong>arameter <strong>G</strong>neration)模块的<strong>输出</strong>得来, CPG是一种能够对于给定的Relation独热编码$r$ 给出操作头实体的参数$\theta$ 的模块.</p><h3 id="Parameter-Generator-Network"><a href="#Parameter-Generator-Network" class="headerlink" title="Parameter Generator Network"></a>Parameter Generator Network</h3><p>有多种方式可以通过$r$ 来获取$\theta$. 最简单的一种是通过<strong>查表</strong>:<br>$$<br>g_{\text {lookup }}(r)=\mathbf{W}_{\text {lookup }} r<br>$$</p><p>但是这种方式不能在跨关系时实现信息共享, 而且非常容易<strong>过拟合</strong>, 尤其是面对训练集中所出现的关系.</p><p>换一种方法, 也可以是简单的<strong>Linear Projection</strong>:<br>$$<br>g_{\text {linear }}(r)=\mathbf{W}_{\text {linear }} \mathbf{R} r+\mathbf{b}<br>$$</p><p>这样, 通过一个权重矩阵$\mathbf{W}$, 不同的关系嵌入$\mathbf{r}$ 可能<strong>共享</strong>一些潜在的<strong>共性</strong>, 因为$\mathbf{r}$ 会被重新线性组合. </p><p>一般非常大的线性变换矩阵很容易造成<strong>过拟合</strong>, 而MLP可以作为一种线性变换的<strong>低秩近似</strong>:<br>$$<br>g_{\mathrm{MLP}}(r)=\operatorname{MLP}(\mathbf{R} r)<br>$$</p><p>当然也可以是其他更复杂的结构, 这里只列举了最简单的三种.</p><h3 id="Enhanced-Expressive-Power"><a href="#Enhanced-Expressive-Power" class="headerlink" title="Enhanced Expressive Power"></a>Enhanced Expressive Power</h3><p>其实作者的解决思路非常的简朴, 把<strong>加性变成乘性</strong>的不就完了么. 还是针对前面的Toy Example, 构建最简单的情况. 假设Merge Function有$h_\phi(\mathbf{e}_s)=\mathbf{e}_s$, $f$ 是最简单的线性投影, 即$f_\theta(x)=\theta x$, $\theta=g_{\text{linear}}(r)=\mathbf{WR}r+\mathbf{b}=\mathbf{Wr}+b$, 因此: </p><p>$$<br>\hat{e}_{t}=f_{\theta}\left(h_{\phi}\left(\mathbf{e}_{s}\right)\right)=f_{\theta}\left(\mathbf{e}_{s}\right)=\theta \mathbf{e}_{s}=(\mathbf{W r}+\mathbf{b}) \mathbf{e}_{\mathbf{s}}<br>$$</p><p>这使得Relation Embedding将以<strong>乘性</strong>地作用于$\mathbf{e}_s$, Toy Example重新表示为如下四个式子:</p><p>$$<br>\begin{array}{l}<br>\mathbf{e}_{2}=\left(\mathbf{W r}_{0}+\mathbf{b}\right) \mathbf{e}_{0} \\<br>\mathbf{e}_{3}=\left(\mathbf{W r}_{0}+\mathbf{b}\right) \mathbf{e}_{1} \\<br>\mathbf{e}_{2}=\left(\mathbf{W} \mathbf{r}_{1}+\mathbf{b}\right) \mathbf{e}_{1} \\<br>\mathbf{e}_{3}=\left(\mathbf{W} \mathbf{r}_{1}+\mathbf{b}\right) \mathbf{e}_{0}<br>\end{array}<br>$$</p><p>还是将上式做减法, 能得出在两个三元组下的表示结果是不同的:</p><p>$$<br>\begin{array}{l}<br>\mathbf{e}_{3}-\mathbf{e}_{2}=\left(\mathbf{W r}_{0}+\mathbf{b}\right)\left(\mathbf{e}_{1}-\mathbf{e}_{0}\right) \\<br>\mathbf{e}_{3}-\mathbf{e}_{2}=\left(\mathbf{W} \mathbf{r}_{1}+\mathbf{b}\right)\left(\mathbf{e}_{0}-\mathbf{e}_{1}\right)<br>\end{array}<br>$$</p><p>除去一组解$\mathbf{e}_0=\mathbf{e}_1$, 其他解都可以使得该式有意义:</p><p>$$<br>\mathbf{W}\left(\mathbf{e}_{1}-\mathbf{e}_{0}\right)\left(\mathbf{r}_{0}+\mathbf{r}_{1}\right)+2 \mathbf{b}\left(\mathbf{e}_{1}-\mathbf{e}_{0}\right)=0<br>$$</p><p>因此, $\mathbf{e}_2, \mathbf{e}_3$ 找到了更合适的表示. </p><h3 id="CoPER-ConvE"><a href="#CoPER-ConvE" class="headerlink" title="CoPER - ConvE"></a>CoPER - ConvE</h3><p>基于上述方法, 作者将ConvE改写为如下过程:<br>$$<br>\begin{aligned}<br>\mathbf{z} &amp;=\operatorname{Conv2D}\left(\operatorname{Reshape}\left(\mathbf{e}_{s}\right)\right)\\<br>\theta &amp;=g(r)\\<br>\hat{e}_{t} &amp;=f_{\theta}(\mathbf{z})=\theta_{1}+\theta_{2: D_{\theta}} \mathbf{z}<br>\end{aligned}<br>$$</p><p>即投影层的参数是由$\mathbf{r}$ 生成的, $\theta_1, \theta_2$ 是由$\theta$ 分割而来, 即$\theta = [\theta_1; \theta_2]$.</p><h3 id="CoPER-MINERVA"><a href="#CoPER-MINERVA" class="headerlink" title="CoPER - MINERVA"></a>CoPER - MINERVA</h3><p>同样, 对于基于LSTM的方法MINERVA也可以改写为下述式子:</p><p>$$<br>\mathbf{h}_{i}=\mathbf{L S T M}\left(\mathbf{h}_{i-1},\left[\mathbf{e}_{i} ; \mathbf{r}_{i-1}\right]\right)<br>$$</p><p>$$<br>\begin{array}{lr}<br>\mathbf{o}_{i}=\operatorname{MLP}\left(\left[\mathbf{h}_{i} ; \mathbf{e}_{i} ; \mathbf{r}_{q}\right]\right), &amp;\quad \text { (merge) }\\<br>a_{j}=\operatorname{Categorical}\left(\mathbf{A}_{i} \mathbf{o}_{i}\right)&amp;\quad \text { (prediction) }<br>\end{array}<br>$$<br>这个模型不是很了解, 在此不做说明了.</p><p>上述两种方法更改前与更改后的对比图如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/coper3.jpg" style="zoom: 50%;" /><p>主要区别在于不将实体嵌入和关系嵌入一起输入模型, 仅将关系嵌入作为对头实体嵌入操作参数生成的依据.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>作者将当前SOTA的模型在五个常用数据集上做了实验, 统计信息如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/coper5.jpg" style="zoom: 50%;" /><p>$N_e$ 为实体数量, $N_r$ 为关系数量, $\tilde{N}_a$ 为每个问题平均的答案数量, $\tilde{d}$ 为平均节点度.</p><p>下面是实验结果:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/coper4.jpg" style="zoom: 50%;" /><p>效果显著的变好了, 涨点特别多. </p><p>除涨点外, 最为显著的是其<strong>收敛提速</strong>效果:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/coper6.jpg" style="zoom: 50%;" /><p>在不将实体嵌入和关系嵌入一并Merge过后, 收敛提速效果极其明显. 但是加速效果具体应该有多少似乎与数据集没有固定的规律.</p><p>消融实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/coper7.jpg" style="zoom: 50%;" /><p>“CoPER - PL - ConvE”代表作者前面所述的查表的方法. 而CoPER指的是使用$g_{\text{linear}}$ 或者$g_{\text{MLP}}$ 的方法.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>CoPER针对作者提出的场景解决方法简单而有效, 将<strong>参数生成</strong>应用到链接预测任务上, 将<strong>关系视为上下文</strong>, 生成操作头实体的函数参数. </p><p>但作者的方法仅能用于改进<strong>无交互性操作</strong>的模型, 例如ConvE等.</p><p>这篇论文其实可以与类似的方法<a href="https://adaning.github.io/posts/20145.html">ConvR</a>进行对比, 这二者同样都可以针对<strong>ConvE</strong>做出改进, 也同样都是对<strong>关系的特化</strong>处理. </p><p>ConvR侧重于从结构的角度将关系嵌入直接作为卷积核, 而CoPER - ConvE将关系嵌入作为参数生成的上下文依据, 生成投影所需的参数.</p><p>所以甚至可以集二者之优, 沿着CoPER中参数生成的思路去改进ConvR? 或许一定程度上可以缓解ConvR中关系嵌入没有<strong>深层次化</strong>的问题.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>R-GCN: Modeling Relational Data with Graph Convolutional Networks</title>
      <link href="/posts/3226.html"/>
      <url>/posts/3226.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>GNN: 详见<a href="https://adaning.github.io/posts/31958.html">图神经网络入门</a></li></ul></blockquote><h1 id="Modeling-Relational-Data-with-Graph-Convolutional-Networks"><a href="#Modeling-Relational-Data-with-Graph-Convolutional-Networks" class="headerlink" title="Modeling Relational Data with Graph Convolutional Networks"></a>Modeling Relational Data with Graph Convolutional Networks</h1><p>本文是论文<a href="https://arxiv.org/abs/1703.06103" target="_blank" rel="noopener">Modeling Relational Data with Graph Convolutional Networks</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>大规模知识图谱仍然不完整, 并且知识图谱中还没有针对<strong>图结构</strong>建模的方法, 而节点缺失的信息经常可以由邻居<strong>编码</strong>而来.</p><p>在KG中, 经常以Entity Classification和Link Prediction作为任务来衡量模型对KG补全的能力:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rgcn1.jpg" style="zoom: 50%;" /><p>图中红色字体为缺失信息, 对应着实体类型和关系.</p><p>在大规模知识图谱中, <strong>多关系数据</strong>特性十分显著, 作者希望针对多关系数据, 提出一种基于图的方法处理知识图谱补全问题.</p><h2 id="R-GCN"><a href="#R-GCN" class="headerlink" title="R - GCN"></a>R - GCN</h2><p>有向且有标签的多图被表示为$G=(\mathcal{V, E, R})$, 其中节点$v_i \in \mathcal{V}$, 边$(v_i, r, v_j) \in \mathcal{E}$, 关系类型$r \in \mathcal{R}$.</p><h3 id="Relational-Graph-Conovlutional-Network"><a href="#Relational-Graph-Conovlutional-Network" class="headerlink" title="Relational Graph Conovlutional Network"></a>Relational Graph Conovlutional Network</h3><p>R - GCN是一种GCN的扩展形式, GCN能够聚合周围邻居的信息. </p><p>GCN遵循<strong>消息传递</strong>机制:</p><p>$$<br>h_{i}^{(l+1)}=\sigma\left(\sum_{m \in \mathcal{M}_{i}} g_{m}\left(h_{i}^{(l)}, h_{j}^{(l)}\right)\right)<br>$$</p><p>其中$h_{i}^{(l)}$ 是节点$v_i$ 第$l$ 层的隐藏状态, $\mathcal{M_i}$ 为节点$v_i$ 的<strong>入边</strong>. $\sigma$ 为非线性激活函数, 例如$\operatorname{ReLU}$.</p><p> $g_m(\cdot, \cdot)$ 为邻居节点的<strong>聚合方式</strong>, 一般只用简单的线性变换作为替代, 即$g_m(h_i, h_j) = Wh_j$.</p><blockquote><p>这部分有问题请参见<a href="https://adaning.github.io/posts/31958.html">图神经网络入门</a>中的GCN和消息传递部分.</p></blockquote><p>R - GCN针对不同种类的关系进行了特化处理, 即针对不同的关系采用<strong>不同的聚合方式</strong>, 在这里是使用了不同的线性变换矩阵$W_r$. 其更新方程为:<br>$$<br>h_{i}^{(l+1)}=\sigma\left(\sum_{r \in \mathcal{R}} \sum_{j \in \mathcal{N}_{i}^{r}} \frac{1}{c_{i, r}} W_{r}^{(l)} h_{j}^{(l)}+W_{0}^{(l)} h_{i}^{(l)}\right)<br>$$</p><p>其中$c_{i, r}$ 代表归一化因子, 可以由学习得来, 或规定$c_{i, r} = |\mathcal{N}_i^r|$. $W_0$ 代表节点自身闭环所对应的变换矩阵.</p><p>针对节点$v_i$ 及其所有邻居$\mathcal{N_i}$, 分别考虑$v_i$ 与邻居$v_j$ 二者之间的关系$r$, 施加以不同的关系变换$W_r$, 再将它们求和并归一化, 加上自身的闭环影响, 在激活函数的影响下获得下一层的节点表示.</p><p>对于有向异质图(有向多关系图), R - GCN每个节点表示的更新方式大致可以由下图描述:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rgcn2.jpg" style="zoom: 50%;" /><p>蓝色块对应着不同关系下蓝色的不同邻居对红色中心节点的影响, 每种关系分别经过变换求和得到绿色块, 对所有关系的绿色块求和并激活, 得到红色中心节点的新表示.</p><p>同一类型关系的不同指向被视为不同关系(同关系下的出入被视为是不同的).</p><blockquote><p>对于关系非常多的图, 每一种关系都对应着一个独特的变换矩阵, <strong>参数量</strong>将是巨大的, 增加了潜在的<strong>过拟合</strong>风险.</p></blockquote><h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><p>针对上小节图神经网络过参数化的问题, 作者提出两种减少参数从而减轻过拟合的方法. 分别是<strong>基函数分解</strong>和<strong>对角块分解</strong>.</p><h4 id="Basic-Decomposition"><a href="#Basic-Decomposition" class="headerlink" title="Basic Decomposition"></a>Basic Decomposition</h4><p>基函数分解将所有关系的权重矩阵视为不同系数和基的<strong>线性组合</strong>:</p><p>$$<br>W_{r}^{(l)}=\sum_{b=1}^{B} a_{r b}^{(l)} V_{b}^{(l)}<br>$$</p><p>其中$B$ 为基的数量, $a_{rb}$ 是不同关系$r$ 下的系数, $V_b$ 是线性基.</p><p>这种表示方法可以被视为是不同关系下的<strong>权重共享</strong>, 对于少关系的情况能够缓解过拟合现象, 因为关系之间的基是相互共享的, 基总会被其他关系所频繁更新.</p><h4 id="Block-Diagonal-Decomposition"><a href="#Block-Diagonal-Decomposition" class="headerlink" title="Block - Diagonal Decomposition"></a>Block - Diagonal Decomposition</h4><p>对角块分解将变换矩阵$W_r$ 拆分为$B$ 个大小<strong>相同</strong>的块:<br>$$<br>W_{r}^{(l)}=\bigoplus_{b=1}^{B} Q_{b r}^{(l)}<br>$$</p><p>其中$Q_{b r}^{(l)} \in \mathbb{R}^{\left(d^{(l+1)} / B\right) \times\left(d^{(l)} / B\right)}$, $\bigoplus$ 为生成对角阵的操作$\operatorname{diag}$.</p><p>对角块分解又可以视为是一种$W_r$ 的一种<strong>稀疏性约束</strong>, 除去块的位置其余位置都是0, 与未分解时相比更为稀疏. 而且由于分块的影响, 在每个块内部, 隐特征的联系将比块与块之间更加<strong>紧密</strong>.</p><p>至此, R - GCN已经能够作为一个单独的层, 按照层级结构堆叠获得$L$ 层的多层输出. 如果没有节点的初始特征, 可以采用<strong>Embedding</strong>的方式获取.</p><blockquote><p>原文中说的独热 + 单层线性变换获取的稠密表示实际上就是Embedding.</p></blockquote><h3 id="Entity-Classification"><a href="#Entity-Classification" class="headerlink" title="Entity Classification"></a>Entity Classification</h3><p>对于实体分类问题, R - GCN能直接将堆叠过后最后一层的节点输出作为Logits, 预测实体类别:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rgcn3.jpg" style="zoom: 67%;" /><p>将最后一层的激活函数更换为$\operatorname{softmax}$, 接着最小化<strong>多分类交叉熵</strong>即可:</p><p>$$<br>\mathcal{L}=-\sum_{i \in \mathcal{Y}} \sum_{k=1}^{K} t_{i k} \ln h_{i k}^{(L)}<br>$$</p><p>其中$\mathcal{Y}$ 为有标签的节点集合, $K$ 为类别总数.</p><h3 id="Link-Prediction"><a href="#Link-Prediction" class="headerlink" title="Link Prediction"></a>Link Prediction</h3><p>对于链接预测问题, 通常在三元组$(s, r, o)$ 上(也就是边$\mathcal{E}$) 预测缺失的尾实体$o$. 但R - GCN只能根据某节点不同关系的邻居获得自身的表示, 并不能结合关系信息做尾实体的预测.</p><p>因此, 作者将R - GCN集成进<strong>Auto Encoder</strong>的框架, 将R - GCN视为一个获取所有节点编码的<strong>Encoder</strong>, 再用其他的KGE模型对节点(实体)表示<strong>打分</strong>, 以完成链接预测任务:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rgcn4.jpg" style="zoom: 67%;" /><p>在这里, 作者选用DistMult作为Decoder. 其打分函数如下:</p><p>$$<br>f(s, r, o)=e_{s}^{T} R_{r} e_{o}<br>$$</p><p>DistMult是基于<strong>语义匹配</strong>的模型, 对每个不同的关系, DistMult都有不同的对角矩阵$R_r$ 来变换头实体$e_s$, 然后与尾实体$e_o$ 做<strong>相似度匹配</strong>. 所有的Embedding, $e_s, e_o$ 都来源于R - GCN.</p><blockquote><p>DistMult虽然也对关系做了特化处理, 但只靠<strong>对角矩阵</strong>做变换显然是无法应对多元化关系的.</p><p>所以R - GCN的在链接预测上的性能可能受到了<strong>约束</strong>.</p></blockquote><p>然后用<strong>BCE</strong>作为损失函数优化:<br>$$<br>\mathcal{L}=-\frac{1}{(1+\omega)|\hat{\mathcal{E}}|} \sum_{(s, r, o, y) \in \mathcal{T}} y \log l(f(s, r, o))+<br>(1-y) \log (1-l(f(s, r, o)))<br>$$</p><p>其中$\omega$ 代表每个正样本采样多少个负样本. $l$ 为$\operatorname{sigmoid}$ 函数. </p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的超参数设置和Trick请参照原论文.</p><h3 id="Entity-Classification-Experiments"><a href="#Entity-Classification-Experiments" class="headerlink" title="Entity Classification Experiments"></a>Entity Classification Experiments</h3><p>作者分别在四个实体分类数据集AIFB, MUTAG, BGS, AM上进行了实体分类任务, 数据集的统计信息如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rgcn5.jpg" style="zoom: 50%;" /><p>实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rgcn6.jpg" style="zoom: 50%;" /><p>在AIFB和AM上的效果都要高于其他模型, 而MUTAG和BGS是<strong>特定领域</strong>的数据集, 与其他两个数据集都不太一样, 导致了R - GCN的性能差距. </p><blockquote><p>作者提到, 对所有邻居都采用<strong>相同权重</strong>的归一化方式可能会有损性能, 一种潜在的解决方案是采用<strong>注意力机制</strong>, 通过学习分配给周围邻居不同的权重.</p><p>其实就是同年10月份提出的GAT, 而R - GCN发布于同年3月份.</p></blockquote><h3 id="Link-Prediction-Experiments"><a href="#Link-Prediction-Experiments" class="headerlink" title="Link Prediction Experiments"></a>Link Prediction Experiments</h3><p>作者主要在FB15k, WN18, FB15k - 237上做了链接预测的实验.</p><p>同样是针对关系特化的模型, 作者做出了在FB15k的验证集中不同度对MRR的影响曲线:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rgcn7.jpg" style="zoom: 50%;" /><p>作者认为R - GCN在度比较高, 即上下文信息比较多时处理很占优势, 而DistMult在度比较低时占优势.</p><blockquote><p>可能也只是作者的一个推测, DistMult在度比较低的时候并没有比R - GCN好特别多.</p></blockquote><p>观察到R - GCN和DistMult上的<strong>互补</strong>优势, 作者尝试将两种模型融合, 仍然采用DistMult的打分函数, 但Embedding分别来自于已经训练好的不同模型:<br>$$<br>f(s, r, t)_{\mathrm{R}-\mathrm{GCN}+}=\alpha f(s, r, t)_{\mathrm{R}-\mathrm{GCN}}+(1-\alpha) f(s, r, t)_{\text {DistMult }}<br>$$<br>$\alpha$ 为选择模型得分的权重. 在FB15k中设置为$\alpha=0.4$, 即来自DistMult的打分要多一些, R - GCN少一些, 因为作者不希望改进后的模型性能显著高于纯R - GCN.</p><p>在FB15k, WN18上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rgcn8.jpg" style="zoom: 50%;" /><p>在关系比较少的WN18上, R - GCN并不占优势, 但在FB15k上的表现十分不错. DistMult作为关系特化的模型也在FB15k上显示出一些优势, 但仍不及能够对对称, 反对称, 逆关系同时建模的ComplEx.</p><p>在FB15k - 237上结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rgcn9.jpg" style="zoom: 50%;" /><p>R - GCN比其他方法效果要好特别多, 因为把DistMult也塞进去了, 所以比DistMult也显著的好.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>R - GCN是一篇比较早的GNN论文, 已经作为一种KGE中常用的图方法Baseline出现, 也正是R - GCN使得大家对图方法在KG上的应用提起了注意.</p><p>R - GCN在GCN的基础上对<strong>多关系</strong>进行特化, 并针对图神经网络的<strong>过拟合</strong>问题提出了两种可替代的解决方法, <strong>基函数分解</strong>和<strong>对角块分解</strong>. 也提出了图神经网络在处理Entity Classification和Link Prediction问题上的框架, 其实主要还是把R - GCN的输出作为一种<strong>Node Embedding</strong>的方法来使用.</p><p>作者在文中还提出了两种可以优化的地方:</p><ol><li>Decoder还可以替换为任意的KGE模型, 例如能对更多关系建模的ComplEx.</li><li>对邻居节点的权重分配可以通过<strong>Attention</strong>学习得来.</li></ol>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> GNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RotatE: Relational Rotation in Complex Vector Space</title>
      <link href="/posts/60792.html"/>
      <url>/posts/60792.html</url>
      
        <content type="html"><![CDATA[<h1 id="RotatE-Knowledge-Grpah-Embedding-by-Relational-Rotation-Complex-Space"><a href="#RotatE-Knowledge-Grpah-Embedding-by-Relational-Rotation-Complex-Space" class="headerlink" title="RotatE: Knowledge Grpah Embedding by Relational Rotation Complex Space"></a>RotatE: Knowledge Grpah Embedding by Relational Rotation Complex Space</h1><p>本文是论文<a href="https://openreview.net/forum?id=HkgEQnRqYQ" target="_blank" rel="noopener">ROTATE: KNOWLEDGE GRAPH EMBEDDING BY RELATIONAL ROTATION IN COMPLEX SPACE</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者的出发点如下:</p><ol><li>在许多任务中的成功推断都严重依赖于<strong>关系</strong>模式.</li><li>现有的Knowledge Embedding Model都不能对<strong>所有关系</strong>同时建模.</li></ol><p>基于这两个出发点, 作者希望能构建出一种对<strong>所有关系</strong>建模的模型.</p><h2 id="RotatE"><a href="#RotatE" class="headerlink" title="RotatE"></a>RotatE</h2><h3 id="Modeling-and-Inferring-Relation-Patterns"><a href="#Modeling-and-Inferring-Relation-Patterns" class="headerlink" title="Modeling and Inferring Relation Patterns"></a>Modeling and Inferring Relation Patterns</h3><p>假设有三实体$x, y, z$, 存在关系$r$, 作者定义了如下三种关系的形式:</p><ol><li><p><strong>对称</strong>关系和<strong>反对称</strong>关系:</p><p>对称关系:  如果$x$ 能通过变换$r$ 得出$y$, 那么$y$ 必然能通过同样的$r$ 得出$x$.</p><p>反对称关系: 若$x$ 能通过$r$ 找到$y$, 那么$y$ 必然不能通过同样的$r$ 找到$x$, 或者说$y$ 对$x$ 必然不存在关系$r$. </p><p>即:</p></li></ol><p>$$<br>\begin{aligned}<br>r(x, y) &amp;\Rightarrow r(y, x) \\<br>r(x, y) &amp;\Rightarrow \neg r(y, x)<br>\end{aligned}<br>$$</p><ol start="2"><li><p><strong>互逆</strong>关系: </p><p>如果$r_1$ 和$r_2$ 互为逆关系, $x$ 必然能通过$r_1$ 得到$y$, 同时$y$ 也能通过$r_1$ 的逆变换$r_2$ 得到$x$, 即:</p></li></ol><p>$$<br>r_{2}(x, y) \Rightarrow r_{1}(y, x)<br>$$</p><ol start="3"><li><p><strong>组合</strong>关系:</p><p>如果$r_3$ 是由变换$r_1$ 和变换$r_2$ 组成的, 那么它们之间存在<strong>递进关系</strong>, 即:</p></li></ol><p>$$<br>r_{2}(\mathrm{x}, \mathrm{y}) \wedge \mathrm{r}_{3}(\mathrm{y}, \mathrm{z}) \Rightarrow \mathrm{r}_{1}(\mathrm{x}, \mathrm{z})<br>$$</p><blockquote><p>我这里将所有的”关系”都解释为”<strong>变换</strong>“, 是因为KGE模型的本质是在构造一种”<strong>变换方式</strong>“, 使得头实体尽可能的能够通过特定的<strong>关系变换</strong>后找到<strong>唯一</strong>对应尾实体.</p></blockquote><p>而现有的方法往往不能对上述常见的三种关系模式进行建模:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rotate1.jpg" style="zoom: 50%;" /><p>除了ConvE是神经网络型的KGE模型无法分析, 其他模型都没有同时对所有关系建模:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rotate2.jpg" style="zoom: 50%;" /><p>而作者接下来提出的RotatE却能处理列出的所有情况.</p><h3 id="Modeling-Relations-as-Rotations-in-Complex-Vector-Space"><a href="#Modeling-Relations-as-Rotations-in-Complex-Vector-Space" class="headerlink" title="Modeling Relations as Rotations in Complex Vector Space"></a>Modeling Relations as Rotations in Complex Vector Space</h3><h4 id="Euler’s-Inspired"><a href="#Euler’s-Inspired" class="headerlink" title="Euler’s Inspired"></a>Euler’s Inspired</h4><p>RotatE的灵感来源于<strong>欧拉公式</strong>:<br>$$<br>e^{i \theta}=\cos \theta+i \sin \theta<br>$$</p><p>在复数空间内, 虚数乘法的运算意义为<strong>旋转</strong>(Rotation), 这一特性能使RotatE很好地对作者所定义的关系进行建模, 记复数空间内的三点为$x, y, z$, 旋转角度为$\theta$:</p><ol><li>对称关系: 将$x$ 旋转$\theta=\pi$ 得到$y$, 在$y$ 处再次旋转$\theta=\pi$ 回到$x$.</li><li>互逆关系: 将$x$ 旋转$\theta_1$ 得到$y$, 在$y$ 处旋转$\theta_2=-\theta_1$, 即反方向旋转$\theta_1$, 可以回到$x$.</li><li>组合关系: 将$x$ 先旋转$\theta_1$, 记为$y$, 再旋转$\theta_2$ 记为$z$. 该过程等价于从$x$ 处旋转$\theta_3=\theta_1+\theta_2$, 直接得到$z$.</li></ol><p>因此, 我们只需要在复平面上做头实体$h$ 和关系$r$ 的<strong>元素按位乘</strong>就可以了:</p><p>$$<br>\mathbf{t}=\mathbf{h} \circ \mathbf{r}<br>$$</p><p>其中$\circ$ 代表Hadmard Product(元素按位乘). 因为关系变换被定义为旋转, 所以需要限制$\mathbf{r}$ 中的每个维度的$|r_i|=1$, 这样就可以让角度$\theta \in [-\pi, \pi]$. </p><p>距离函数被作者定义为:<br>$$<br>d_{r}(\mathbf{h}, \mathbf{t})=\lVert\mathbf{h} \circ \mathbf{r}-\mathbf{t}\rVert<br>$$</p><p>即头实体通过旋转后与三元组真实尾实体的距离越近越好.</p><h4 id="Connection-to-TransE"><a href="#Connection-to-TransE" class="headerlink" title="Connection to TransE"></a>Connection to TransE</h4><p>TransE仅在<strong>一维</strong>数轴上做关系变换, 而将关系变换定义为旋转的RotatE可以在<strong>二维</strong>复平面上做关系变换:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rotate3.jpg" style="zoom: 50%;" /><p>一维数轴的限制导致TransE无法处理对称关系, 除非将关系变为0向量, 但这会引入更多的问题.</p><h3 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h3><h4 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h4><p>RotatE仍然采用类似TransE的<strong>Hinge Loss</strong>, 并加上<strong>负采样</strong>:<br>$$<br>L=-\log \sigma\left(\gamma-d_{r}(\mathbf{h}, \mathbf{t})\right)-\sum_{i=1}^{n} \frac{1}{k} \log \sigma\left(d_{r}\left(\mathbf{h}_{i}^{\prime}, \mathbf{t}_{i}^{\prime}\right)-\gamma\right)<br>$$</p><h4 id="Self-Adversarial-Negative-Sampling"><a href="#Self-Adversarial-Negative-Sampling" class="headerlink" title="Self - Adversarial Negative Sampling"></a>Self - Adversarial Negative Sampling</h4><p>通常情况下, 模型认为打分函数越高的三元组越真实, 而模型已经知道打分比较低的三元组不真实了, 就没有必要<strong>过多</strong>的注意这类三元组. 而对于那些模型认为是正确, 实际是错误的三元组, 需要被模型提起注意. </p><p>所以作者按照如下分布采样:<br>$$<br>p\left(h_{j}^{\prime}, r, t_{j}^{\prime} \mid\left\{\left(h_{i}, r_{i}, t_{i}\right)\right\}\right)=\frac{\exp \alpha f_{r}\left(\mathbf{h}_{j}^{\prime}, \mathbf{t}_{j}^{\prime}\right)}{\sum_{i} \exp \alpha f_{r}\left(\mathbf{h}_{i}^{\prime}, \mathbf{t}_{i}^{\prime}\right)}<br>$$<br>其中$f_r$ 为KGE模型的打分函数, $\alpha$ 为采样率. 得分比较高的三元组更容易被采样到, 因为它们更容易把模型迷惑, 得分比较低的三元组将被少量采样到, 因为模型已经有一定能力判断它们是否正确.</p><p>因为每次负样本采样都是昂贵的, 不单是采样可以使用计算得出的概率, 它们还可以在计算Loss时复用, 将作为负样本的<strong>权重</strong>:<br>$$<br>L=-\log \sigma\left(\gamma-d_{r}(\mathbf{h}, \mathbf{t})\right)-\sum_{i=1}^{n} p\left(h_{i}^{\prime}, r, t_{i}^{\prime}\right) \log \sigma\left(d_{r}\left(\mathbf{h}_{i}^{\prime}, \mathbf{t}_{i}^{\prime}\right)-\gamma\right)<br>$$</p><p>打分本身较低的三元组所对应的负采样权重较低, 打分较高的三元组权重较高. 即<strong>越容易迷惑模型的三元组权重越高</strong>.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的超参设置请参照原论文.</p><p>除了RotatE, 作者还提出了其变体pRotatE, 它的实体嵌入的<strong>模长</strong>是被<strong>限制</strong>的, 即$|h_i| = |t_i| = C$, 它的距离函数被相应的改写为$2C \lVert\sin \frac{\theta_h + \theta_r - \theta_r}{2}\rVert$.</p><p>理论上它与RotatE一样, 也能同时处理多种关系. 但因缺少了模长信息, 所以它处理的效果应该会稍差一些. 反过来看, pRotatE也能说明旋转建模所带来的强大能力.</p><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><p>作者在FB15k, WN18上的实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rotate4.jpg" style="zoom: 33%;" /><p>RotatE表现出很强大的性能, 和变体差不多.</p><p>作者在FB15k - 237, WN18RR上的实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rotate5.jpg" style="zoom: 33%;" /><p>RotatE比ConvE的结果要好一些. 但对于MR来说, RotatE的结果要好得多, 说明ConvE对于某些关系学习到的特征是<strong>模糊</strong>的.</p><h3 id="Relation-Pattern-Experiments"><a href="#Relation-Pattern-Experiments" class="headerlink" title="Relation Pattern Experiments"></a>Relation Pattern Experiments</h3><p>在Countries上的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rotate6.jpg" style="zoom: 33%;" /><p>在最困难的S3上, RotatE要显著好于其他方法.</p><p>WN18上的各类关系所对应的$r$ 值如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rotate7.jpg" style="zoom: 33%;" /><p><code>similar_to</code>是明显的对称关系,  <code>hypernym, hyponym</code>是一对明显的逆关系, 二者的组合刚好是什么也不做, <code>for1, for2, winner</code>三者的组合也被学习到了. RotatE很好的掌握了这些关系的变换方式.</p><h3 id="Negative-Sampling-Techniques"><a href="#Negative-Sampling-Techniques" class="headerlink" title="Negative Sampling Techniques"></a>Negative Sampling Techniques</h3><p>分别在3个基准数据集上, 三种负采样的效果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rotate8.jpg" style="zoom: 33%;" /><p>明显作者提出的自对抗负采样效果要好.</p><p>为了能更公平的对比TransE, ComplEx, RotatE, 作者将自对抗负采样同时用于其他两种方法, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rotate9.jpg" style="zoom: 33%;" /><p>RotatE仍然是最优秀的方法. 但TransE比RotatE在S3上稍好一些, 作者解释为Countries不包含TransE最不擅长的对称关系, 在这个数据集上显示出TransE的强大.</p><h3 id="Results-by-Relation-Category"><a href="#Results-by-Relation-Category" class="headerlink" title="Results by Relation Category"></a>Results by Relation Category</h3><p>作者在FB15k上按照多类别统计的实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rotate10.jpg" style="zoom: 33%;" /><p>其实从原理上来讲, RotatE应该不擅长处理一对多, 多对一, 多对多关系. 不过看起来结果还不错.</p><blockquote><p>个人观点: 可能是RotatE采用了比较大的Embedding Size, 在高维中或许点和点之间的距离比想象的要远, 即这些点在高维空间中可能是可分的.</p></blockquote><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>在联结主义被大肆吹鼓的今天看到一篇这样的文章实属不易. </p><p>RotatE将关系变换定义为<strong>旋转</strong>, 在<strong>复数空间</strong>建模, 建模的方式<strong>简洁而优雅</strong>. 同时提出了一种优化的自对抗<strong>负采样</strong>算法, 能让模型聚焦于更<strong>容易混淆</strong>的三元组. RotatE还是一种线性复杂度的算法, 所以易于扩展到大规模的KG中.</p><p>其实在后面Appendix还有在YAGO3 - 10上做的实验, 还有自对抗负采样的Ablation Study, 算是很扎实了.</p><p>虽然RotatE十分简洁, 但它仍然具有以下缺点:</p><ol><li>与TransE相同, 只能对<strong>一对一</strong>关系建模. 这个缺陷在实验中并没有被很好的体现, 我认为是较大的Embedding Size遮盖了这个缺陷.</li><li>旋转操作并不能区分关系变换的<strong>前后顺序</strong>. 例如”父亲的儿子”和”儿子的父亲”在旋转中是一样的, 因为$\theta_3 = \theta_1+\theta_2=\theta_2+\theta_1$.</li></ol>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SACN: End-to-end Structure-Aware Convolutional Networks for KBC</title>
      <link href="/posts/63236.html"/>
      <url>/posts/63236.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>GNN: 详见<a href="https://adaning.github.io/posts/31958.html">图神经网络入门</a></li><li>ConvE: 详见<a href="https://adaning.github.io/posts/42031.html">ConvE: Convolutional 2D Knowledge Graph Embeddings</a></li><li>ConvKB: 详见<a href="https://adaning.github.io/posts/52280.html">ConvKB: A Novel Embedding Model for KB Completion Based on CNN</a></li></ul></blockquote><h1 id="End-to-end-Structure-Aware-Convolutional-Networks-for-Knowledge-Base-Completion"><a href="#End-to-end-Structure-Aware-Convolutional-Networks-for-Knowledge-Base-Completion" class="headerlink" title="End-to-end Structure-Aware Convolutional Networks for Knowledge Base Completion"></a>End-to-end Structure-Aware Convolutional Networks for Knowledge Base Completion</h1><p>本文是论文<a href="http://arxiv.org/abs/1811.04441" target="_blank" rel="noopener">End-to-end Structure-Aware Convolutional Networks for Knowledge Base Completion</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者注意到ConvE中存在如下不足:</p><ol><li>ConvE中没有融入太多的<strong>结构信息</strong>(特指<strong>图结构信息和节点属性信息</strong>).</li><li>ConvE没有像TransE一样保留<strong>平移</strong>的特性, 即$e_s + e_r \approx e_o$.</li></ol><p>基于上述两点不足, 作者希望能够在ConvE架构下融入图中的结构信息, 并保留类似平移的特性. </p><p>作者观察到, <a href="https://adaning.github.io/posts/52280.html">ConvKB</a>中存在类似保留平移特性的方法, 它与ConvE有几点不同:</p><ol><li>ConvKB只使用了<strong>1D</strong>卷积, 而ConvE使用了<strong>2D</strong>卷积. </li><li>ConvKB使用了<strong>Stack</strong>, ConvE使用的是<strong>Reshape</strong>.</li><li><strong>损失函数</strong>不同.</li></ol><p>同时, ConvKB的作者也指出, 在特殊情况下ConvKB可以退化成TransE, 即能够保留平移特性. 受到ConvKB的启发, 作者提出了<strong>结构感知</strong>的Conv系列方法.</p><h2 id="SACN"><a href="#SACN" class="headerlink" title="SACN"></a>SACN</h2><p><strong>SACN</strong>(end-to-end <strong>S</strong>tructure - <strong>A</strong>ware <strong>C</strong>onvolutional <strong>N</strong>etwork)将融入结构信息的过程设计为<strong>Encoder - Decoder</strong>架构. 通过Encoder捕获图结构信息, 然后用Decoder从编码中解码出尾实体Embedding.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/sacn1.jpg" style="zoom:50%;" /><h3 id="Weighted-Graph-Convolutional-Layer"><a href="#Weighted-Graph-Convolutional-Layer" class="headerlink" title="Weighted Graph Convolutional Layer"></a>Weighted Graph Convolutional Layer</h3><p><strong>W</strong>eighted <strong>G</strong>raph <strong>C</strong>onvolutional <strong>N</strong>etwork(<strong>WGCN</strong>)是GCN的一种扩展类型, 它将多关系图看做是<strong>多个单关系的子图</strong>. 因此对于每种不同的关系, WGCN都能决定子图中的节点以多少权重被集成, 即给予图结构多种不同的关系以不同的权重. 在SACN中, 它充当Encoder, 用于<strong>提取图结构信息</strong>.</p><p>其隐态更新方程如下:<br>$$<br>h_{i}^{l+1}=\sigma\left(\sum_{j \in \mathbf{N}_{\mathbf{i}}} \alpha_{t}^{l} g\left(h_{i}^{l}, h_{j}^{l}\right)\right)<br>$$</p><p>其中, $\alpha_t$ 为<strong>关系特化</strong>的权重. $T$ 为关系的总数, $t \in [1, T]$. $g$ 为聚合方式.</p><p>如下图所示, 红色的中心节点周围有4个相邻节点, 但只有3种不同的关系, 它们以3种不同的权重被聚合.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/sacn8.jpg" style="zoom: 33%;" /><p>在WGCN中, 聚合方式采用了最简单的线性变换:<br>$$<br>g\left(h_{i}^{l}, h_{j}^{l}\right)=h_{j}^{l} W^{l}<br>$$</p><p>$W^l$ 为第$l$ 层的线性变换矩阵.</p><p>因为在考虑中心节点$i$ 的邻居节点$\mathbf{N_i}$ 时没有考虑节点自身到自身的<strong>闭环</strong>, 所以还是将闭环添加进来:<br>$$<br>h_{i}^{l+1}=\sigma\left(\sum_{j \in \mathbf{N}_{\mathbf{i}}} \alpha_{t}^{l} h_{j}^{l} W^{l}+h_{i}^{l} W^{l}\right)<br>$$</p><p>相当于给闭环分配了一种特殊的关系$\text{self-loop}$, 且权重$\alpha _{\text{self-loop}}=1$.</p><p>公式中的闭环是可以进行合并的:</p><p>$$<br>\begin{aligned}<br>h_{i}^{l+1}&amp;=\sigma\left(\sum_{j \in \mathbf{N}_{\mathbf{i}}} \alpha_{t}^{l} h_{j}^{l} W^{l}+h_{i}^{l} W^{l}\right) \\<br>&amp;=\sigma\left[\left(\sum_{i \in \mathbf{N}_i} \alpha_t^l h_j^l + h_i^l\right) W^l \right]<br>\end{aligned}<br>$$</p><p>在计算时, 将每种关系的邻接矩阵$A_t$ 分别乘以它们对应的权重$\alpha_t$, 视为第$l$ 层整张图的邻接矩阵$A^l$, 能一次性更新<strong>所有</strong>不同关系的节点隐态, 整个过程写成矩阵形式如下:<br>$$<br>\begin{aligned}<br>H^{l+1} &amp;= \sigma \left[ \left( \sum_{t=1}^T \left(\alpha_t^l A_t \right) + I \right) H^l W^l  \right] \\\<br>&amp;=\sigma\left(A^{l} H^{l} W^{l}\right)<br>\end{aligned}<br>$$<br>计算流程如下图所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/sacn2.jpg" style="zoom:50%;" /><p>最左侧的一堆矩阵表示<strong>闭环</strong>的单位矩阵$I$ 和代表<strong>不同关系</strong>的邻接矩阵$A_t$, 第二个矩阵代表它们在不同关系$t$ 下与对应权重$\alpha_t$ 的加权和$A^l$. 第三个矩阵为$H^l$, 第四个为$W^l$.</p><blockquote><p>但WGCN没有像经典GCN一样将<strong>度</strong>的信息集成进来.</p></blockquote><h3 id="Node-Attributes"><a href="#Node-Attributes" class="headerlink" title="Node Attributes"></a>Node Attributes</h3><p>作者提到, 在当前KG中有一部分<strong>属性</strong>三元组, 即<code>(entity, relation, attribute)</code>, 例如<code>(Tom, people.person.gender, male)</code>. </p><p>这种<strong>属性三元组</strong>建模会带来两种潜在问题:</p><ol><li>属性与一般节点不同, 它<strong>不能再延伸</strong>出其他的节点, 会导致属性特征非常<strong>稀疏</strong>.</li><li>由于其稀疏性, 属性特征中的0值可能会产生歧义, 可能是节点没有特殊的属性, 也可能是节点丢失了属性. 会影响KGE准确率.</li></ol><p>如果需要减少过多的属性节点, 作者提出一种方法, 每一种属性将作为一个单独的<strong>属性节点</strong>. </p><blockquote><p>这里有很大疑问, 作者的<a href="https://github.com/JD-AI-Research-Silicon-Valley/SACN/issues/7" target="_blank" rel="noopener">回复</a>也比较模糊. 建议自行阅读原论文. 关于这块的吐槽在后面构建FB15k - 237 - Attr时我会详细说.</p></blockquote><p>WGCN同时使用了图结构信息和属性信息, 这也就是”<strong>Structure Awared</strong>“的来源.</p><h3 id="Conv-TransE"><a href="#Conv-TransE" class="headerlink" title="Conv - TransE"></a>Conv - TransE</h3><p>Conv - TransE在SACN中扮演Decoder的角色. 它能像TransE一样保留<strong>平移</strong>特性. 它仍然沿用ConvE的架构, 但它的核心点在于: <strong>不进行Reshape</strong>. </p><p>作者认为正是ConvE中的Reshape操作将头实体嵌入$e_o$ 和关系嵌入$e_r$ 转化为2D向量才使得平移特性不能保存. ConvKB中的<strong>Stack</strong>操作却没有破坏原本的$e_o, e_r$形状.</p><p>因此, 从Encoder(WGCN) 中得到实体嵌入$e_o$后, 与关系嵌入$e_r$ 一起<strong>Stack</strong>起来, 不经过Reshape, 直接用<strong>宽度为2的2D卷积</strong>抽取得到Feature map. </p><blockquote><p>WGCN只训练了Entity Embedding, Relation Embedding此时还是刚初始化的状态.</p></blockquote><p>后面的流程和<strong>ConvE</strong>一样, 将Feature map<strong>打平</strong>, 再用<strong>投影层</strong>投回Embedding的维度, 和整个Embedding矩阵相乘得到Logits. 最后用Sigmoid得到概率, <strong>BCE</strong>计算损失:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/sacn9.jpg" style="zoom: 25%;" /><blockquote><p>ConvKB采用的是宽度为1的2D卷积, 所以作者将ConvKB视为Conv - TransE的一种特殊情况.</p></blockquote><p>卷积运算的数学描述如下:</p><p>$$<br>\begin{aligned}<br>m_{c}\left(e_{s}, e_{r}, n\right)&amp;= \sum_{\tau=0}^{K-1} \omega_{c}(\tau, 0) \hat{e}_{s}(n+\tau)+\omega_{c}(\tau, 1) \hat{e}_{r}(n+\tau) \\<br>M_c(e_s, e_r) &amp;= \left[ m_c(e_s, e_r, 0), \dots, m_c(e_s, e_r, F^L - 1)\right]<br>\end{aligned}<br>$$</p><p>其中$K$ 为卷积核宽度, $\omega_{c}$ 为卷积核权重.</p><blockquote><p>卷积核能够分别对$e_s, e_r$ 加权, 并将二者相加, 作者认为这样保留了基于平移的性质.</p><p>我个人认为后面的投影层可能会破坏这种性质.</p></blockquote><p>打分函数如下:</p><p>$$<br>\psi\left(e_{s}, e_{o}\right)=f\left(\operatorname{vec}\left(\mathbf{M}\left(e_{s}, e_{r}\right)\right) W\right) e_{o}<br>$$</p><p>其中$\mathbf{M}$ 为在WGCN上编码后的卷积操作, $\operatorname{vec}(\cdot)$ 为Flatten操作. $W$ 为投影层的投影矩阵, $f$ 为非线性激活函数. $e_o$ 为尾实体的Embedding.</p><p>与ConvE相同, 得到得分后再用对数几率函数得到概率:<br>$$<br>p\left(e_{s}, e_{r}, e_{o}\right)=\sigma\left(\psi\left(e_{s}, e_{o}\right)\right)<br>$$</p><p>得到概率后用<strong>BCE</strong>做损失函数优化.</p><p>SACN的打分函数整体形式与ConvE相同:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/sacn3.jpg" style="zoom:50%;" /><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的实验参数请参照原论文.</p><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>除了两个Benchmark数据集FB15k - 237和WN18RR, 作者还利用实体的<strong>属性特征</strong>构建了一个新的数据集FB15k - 237 - Attr. 它是作者从FB24k抽取了FB15k - 237中的所有实体属性所构建的数据集. 它具有14541个节点, 203种属性, 484种关系. 共计78334个属性三元组, 这将近8w个三元组被作者全部并入训练集中.</p><blockquote><p>花了很久看这里, 也不知道作者到底是如何具体构建的.</p><p>在实际的FB15k - 237 - Attr中, 作者直接将所有FB24k中同实体的属性三元组拿了过来. 但有些相同含义的三元组在FB15k - 237中是已经存在的, 这些已经存在的三元组没有被删除. 这样一来, 这些属性不单以”关系”的身份存在, 也以”属性”的身份存在, 而且在属性三元组中, 有些属性节点得到了合并, 但有些又没有, 感觉有点奇怪.</p><p>单纯从后续的实验结果来看, 使用该数据集是会涨点的.</p></blockquote><p>作者将SACN在FB15k - 237和WN18RR上做了Link Prediction:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/sacn4.jpg" style="zoom: 33%;" /><p>Conv - TransE相比较于ConvE有一定提升, 在引入WGCN后的SACN又有一定提升, 如果让SACN使用属性信息还会有一点点提升.</p><h3 id="Convergence-Analysis"><a href="#Convergence-Analysis" class="headerlink" title="Convergence Analysis"></a>Convergence Analysis</h3><p>作者分析了SACN + Attr(绿), SACN(红), Conv - TransE(黄)在Hits@1和MRR上的收敛性:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/sacn5.jpg" style="zoom:50%;" /><p>Conv - TransE在Epoch较少时性能是比SACN要好的, 随着轮数的增加, SACN性能反超了Conv - TransE. </p><p>而加入属性信息后, SACN性能得到了完全的提升.</p><h3 id="Kernel-Size-Analysis"><a href="#Kernel-Size-Analysis" class="headerlink" title="Kernel Size Analysis"></a>Kernel Size Analysis</h3><p>作者分析了不同卷积核大小对性能的影响:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/sacn6.jpg" style="zoom: 33%;" /><p>增大卷积核的大小会获得一点性能上的提升, 对于不同的数据集有不同的最优超参数设置.</p><h3 id="Node-Indegree-Analysis"><a href="#Node-Indegree-Analysis" class="headerlink" title="Node Indegree Analysis"></a>Node Indegree Analysis</h3><p>作者分析了节点入度对结果的影响:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/sacn7.jpg" style="zoom: 33%;" /><p>在度不够高时, SACN要略好于Conv - TransE, 在度比较大时, Conv - TransE会好于SACN. 作者解释为高入度导致SACN学到的邻居特征被平滑了.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>SACN是一种基于<strong>Encoder - Decoder</strong>架构的卷积KGE方法, 保留了TransE基于<strong>平移</strong>的特性, 并集成了<strong>图结构</strong>和<strong>属性</strong>信息. WGCN也是一种GCN的扩展形式, 可以单独针对多关系作为基本的GNN组件存在.</p><p>WGCN抽取Entity Embedding的同时没有对Relation Embedding做更新, 或许可以利用图中的边对Relation Embedding增益, 例如尝试用Entity Embedding和Relation Embedding的联合训练.</p><p>关于属性节点的部分没太搞懂, 感觉比较模糊.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> GNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch实现: BERT</title>
      <link href="/posts/52648.html"/>
      <url>/posts/52648.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>Transformer: 详见<a href="https://adaning.github.io/posts/6744.html">Transformer精讲</a></li><li>BERT: 详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a></li><li><a href="https://adaning.github.io/posts/63679.html">Pytorch实现: Transformer</a></li></ul><p><strong>2022.04.03</strong>: 修正Pre Norm效果好于Post Norm的错误描述.</p></blockquote><h1 id="Pytorch实现-BERT"><a href="#Pytorch实现-BERT" class="headerlink" title="Pytorch实现: BERT"></a>Pytorch实现: BERT</h1><p>本文是BERT的Pytorch版本实现. 实现并没有完全参照BERT原论文中的设置, 有些细枝末节的地方可能没有考虑进去, 每个人实现的方法可能也不同, 可以不必过于纠结这些. BERT的实现比Transformer更简单, 因为不用考虑Decoder.</p><p>本文参考如下文章:</p><ul><li><a href="https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html#BertModel" target="_blank" rel="noopener">Hugging Face的BERT实现</a></li><li><a href="https://wmathor.com/index.php/archives/1457/" target="_blank" rel="noopener">BERT 的 PyTorch 实现</a></li></ul><p>本文的代码已经放到了Colab上, 打开设置GPU就可以复现(需要科学上网).</p><p><a href="https://colab.research.google.com/drive/13PZiE-UQlGy-gSF_KZ-L9wq9RYsBbXDj?usp=sharing" target="_blank" rel="noopener"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="右键我在COLAB中打开!"></a></p><p>如果你不能科学上网, 应该看不到<code>Open in Colab</code>的图标.</p><h2 id="Preparing"><a href="#Preparing" class="headerlink" title="Preparing"></a>Preparing</h2><p>先导包:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> random<span class="token keyword">import</span> re<span class="token keyword">from</span> math <span class="token keyword">import</span> sqrt <span class="token keyword">as</span> msqrt<span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">import</span> Adadelta<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader<span class="token punctuation">,</span> Dataset<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>定义与BERT相关的参数:</p><pre class="line-numbers language-python"><code class="language-python">max_len <span class="token operator">=</span> <span class="token number">30</span>max_vocab <span class="token operator">=</span> <span class="token number">50</span>max_pred <span class="token operator">=</span> <span class="token number">5</span>d_k <span class="token operator">=</span> d_v <span class="token operator">=</span> <span class="token number">64</span>d_model <span class="token operator">=</span> <span class="token number">768</span>  <span class="token comment" spellcheck="true"># n_heads * d_k</span>d_ff <span class="token operator">=</span> d_model <span class="token operator">*</span> <span class="token number">4</span>n_heads <span class="token operator">=</span> <span class="token number">12</span>n_layers <span class="token operator">=</span> <span class="token number">6</span>n_segs <span class="token operator">=</span> <span class="token number">2</span>p_dropout <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token number">1</span><span class="token comment" spellcheck="true"># BERT propability defined</span>p_mask <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token number">8</span>p_replace <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token number">1</span>p_do_nothing <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">-</span> p_mask <span class="token operator">-</span> p_replacedevice <span class="token operator">=</span> <span class="token string">"cuda"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><p>max_len: 输入序列的最大长度.</p></li><li><p>max_vocab: 字典的最大大小.</p></li><li><p>max_pred: Mask时最大的Mask数量.</p></li><li><p>d_k, d_v: 自注意力中K和V的维度, Q的维度直接用K的维度代替, 因为这二者必须始终相等.</p></li><li><p>d_model: Embedding的大小.</p></li><li><p>d_ff: 前馈神经网络的隐藏层大小, 一般是d_model的四倍.</p></li><li><p>n_heads: 多头注意力的头数.</p></li><li><p>n_layers: Encoder的堆叠层数.</p></li><li><p>n_segs: 输入BERT的句子段数. 用于制作Segment Embedding.</p></li><li><p>p_dropout: BERT中所有dropout的概率.</p></li><li><p>p_mask, p_replace, p_do_nothing:</p><ul><li><p><strong>80%</strong>的概率将被选中的单词替换为<code>[MASK]</code>.</p></li><li><p><strong>10%</strong>的概率将被选中的单词替换为<strong>随机词</strong>.</p></li><li><p><strong>10%</strong>的概率对被选中的单词<strong>不进行替换</strong>.</p></li></ul></li></ul><h2 id="Mask-and-GELU"><a href="#Mask-and-GELU" class="headerlink" title="Mask and GELU"></a>Mask and GELU</h2><p>在BERT中没有Decoder, 所以我们的Mask实际上只为<strong>Padding</strong>服务. </p><h3 id="Mask"><a href="#Mask" class="headerlink" title="Mask"></a>Mask</h3><p>因为预期的Token输入序列大小为<code>[batch, seq_len]</code>, 如果token中的索引与pad索引相同, 那么则</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_pad_mask</span><span class="token punctuation">(</span>tokens<span class="token punctuation">,</span> pad_idx<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''    suppose index of [PAD] is zero in word2idx    tokens: [batch, seq_len]    '''</span>    batch<span class="token punctuation">,</span> seq_len <span class="token operator">=</span> tokens<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>    pad_mask <span class="token operator">=</span> tokens<span class="token punctuation">.</span>data<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>pad_idx<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    pad_mask <span class="token operator">=</span> pad_mask<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> seq_len<span class="token punctuation">)</span>    <span class="token keyword">return</span> pad_mask<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="GELU"><a href="#GELU" class="headerlink" title="GELU"></a>GELU</h3><p>在BERT中采用GELU作为激活函数, 它与ReLU相比具有一些概率上的性质:<br>$$<br>\displaylines{<br>\operatorname{GELU}(x)=x P(X \leq x)= x \Phi(x)=x \cdot \frac{1}{2}[1+\operatorname{erf}(x / \sqrt{2})] \\<br> or \\<br>0.5 x\left(1+\tanh \left[\sqrt{2 / \pi}\left( x+ 0.044715 x^{3}\right)\right]\right)<br>}<br>$$<br>第二行的是GELU的近似表达式. 它实现起来非常简单:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">gelu</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''    Two way to implements GELU:    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))    or    0.5 * x * (1. + torch.erf(torch.sqrt(x, 2)))     '''</span>    <span class="token keyword">return</span> <span class="token punctuation">.</span><span class="token number">5</span> <span class="token operator">*</span> x <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">.</span> <span class="token operator">+</span> torch<span class="token punctuation">.</span>erf<span class="token punctuation">(</span>x <span class="token operator">/</span> msqrt<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>两种方式均可, 我这里采用第一种.</p><h2 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h2><p>BERT中含有三种编码, Word Embedding, Position Embedding, Segment Embedding:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert2.jpg" style="zoom: 50%;" /><p>其中Position Embedding不像Transformer中是用正余弦编码计算得到, 而是通过学习获得.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Embeddings</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Embeddings<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>seg_emb <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>n_segs<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>word_emb <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>max_vocab<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>pos_emb <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>max_len<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p_dropout<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> seg<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">'''        x: [batch, seq_len]        '''</span>        word_enc <span class="token operator">=</span> self<span class="token punctuation">.</span>word_emb<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># positional embedding</span>        pos <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>long<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>        pos <span class="token operator">=</span> pos<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        pos_enc <span class="token operator">=</span> self<span class="token punctuation">.</span>pos_emb<span class="token punctuation">(</span>pos<span class="token punctuation">)</span>        seg_enc <span class="token operator">=</span> self<span class="token punctuation">.</span>seg_emb<span class="token punctuation">(</span>seg<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>word_enc <span class="token operator">+</span> pos_enc <span class="token operator">+</span> seg_enc<span class="token punctuation">)</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># return: [batch, seq_len, d_model]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里的LayerNorm有些版本的实现加了, 有些没有加, 看个人爱好吧.</p><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>这里的点积缩放注意力和多头注意力完全和Transformer一致, 不再细说, 直接照搬过来就行.</p><h3 id="Scaled-DotProduct-Attention"><a href="#Scaled-DotProduct-Attention" class="headerlink" title="Scaled DotProduct Attention"></a>Scaled DotProduct Attention</h3><p>$$<br>\operatorname{Attention}(Q, K, V) = \operatorname{softmax}(\frac{QK^T}{\sqrt{d_k}})V<br>$$</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">ScaledDotProductAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>ScaledDotProductAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> Q<span class="token punctuation">,</span> K<span class="token punctuation">,</span> V<span class="token punctuation">,</span> attn_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>        scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>Q<span class="token punctuation">,</span> K<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">/</span> msqrt<span class="token punctuation">(</span>d_k<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># scores: [batch, n_heads, seq_len, seq_len]</span>        scores<span class="token punctuation">.</span>masked_fill_<span class="token punctuation">(</span>attn_mask<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1e9</span><span class="token punctuation">)</span>        attn <span class="token operator">=</span> nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span>scores<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># context: [batch, n_heads, seq_len, d_v]</span>        context <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>attn<span class="token punctuation">,</span> V<span class="token punctuation">)</span>        <span class="token keyword">return</span> context<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi - Head Attention"></a>Multi - Head Attention</h3><p>$$<br>\begin{aligned}<br>\operatorname{MultiHead}(Q, K, V) &amp;= \operatorname{Concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_h)W^O \\<br>\text{where } \text{head}_i &amp;= \operatorname{Attention}(QW^Q_i, KW^K_i, VW^V_i)<br>\end{aligned}<br>$$</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>MultiHeadAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>W_Q <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_k <span class="token operator">*</span> n_heads<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>W_K <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_k <span class="token operator">*</span> n_heads<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>W_V <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_v <span class="token operator">*</span> n_heads<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>n_heads <span class="token operator">*</span> d_v<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> Q<span class="token punctuation">,</span> K<span class="token punctuation">,</span> V<span class="token punctuation">,</span> attn_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">'''        Q, K, V: [batch, seq_len, d_model]        attn_mask: [batch, seq_len, seq_len]        '''</span>        batch <span class="token operator">=</span> Q<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        <span class="token triple-quoted-string string">'''        split Q, K, V to per head formula: [batch, seq_len, n_heads, d_k]        Convenient for matrix multiply opearation later        q, k, v: [batch, n_heads, seq_len, d_k / d_v]        '''</span>        per_Q <span class="token operator">=</span> self<span class="token punctuation">.</span>W_Q<span class="token punctuation">(</span>Q<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> d_k<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        per_K <span class="token operator">=</span> self<span class="token punctuation">.</span>W_K<span class="token punctuation">(</span>K<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> d_k<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        per_V <span class="token operator">=</span> self<span class="token punctuation">.</span>W_V<span class="token punctuation">(</span>V<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> d_v<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        attn_mask <span class="token operator">=</span> attn_mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># context: [batch, n_heads, seq_len, d_v]</span>        context <span class="token operator">=</span> ScaledDotProductAttention<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>per_Q<span class="token punctuation">,</span> per_K<span class="token punctuation">,</span> per_V<span class="token punctuation">,</span> attn_mask<span class="token punctuation">)</span>        context <span class="token operator">=</span> context<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>            batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads <span class="token operator">*</span> d_v<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># output: [batch, seq_len, d_model]</span>        output <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>context<span class="token punctuation">)</span>        <span class="token keyword">return</span> output<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Feed-Forward-Neural-Network"><a href="#Feed-Forward-Neural-Network" class="headerlink" title="Feed Forward Neural Network"></a>Feed Forward Neural Network</h2><p>BERT中的FFN实现将激活函数换为了GELU:<br>$$<br>\operatorname{FFN}(x)=\operatorname{GELU}(xW_1+b_1)W_2 + b_2<br>$$</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">FeedForwardNetwork</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>FeedForwardNetwork<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_ff<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_ff<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p_dropout<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>gelu <span class="token operator">=</span> gelu    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>gelu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我把残差部分移到了EncoderLayer的设计中, 见下节.</p><h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p>我对Encoder的实现进行了调整. 在Encoder中控制两个Sub Layer的Layer Norm和Residual connection. </p><p>在论文<a href="https://openreview.net/pdf?id=B1x8anVFPr" target="_blank" rel="noopener">ON LAYER NORMALIZATION IN THE TRANSFORMER ARCHITECTURE</a>中曾经提到, Transformer中Layer Norm的位置加的有问题, 在Sub Layer后加Layer Norm被称为Post Norm, 它是不规范的. Layer Norm如果调整到Sub Layer前会对训练有很大帮助, 称为<strong>Pre Norm</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/prenorm.jpg" style="zoom: 50%;" /><blockquote><p>事实上, 截止到2022年4月3日, 研究表明, Pre Norm并不是比Post Norm效果好, 而是因为Pre Norm比Post Norm更好训练. 实际上, 如果能够训练完全, Post Norm效果是比Pre Norm好的, 原因见<a href="https://spaces.ac.cn/archives/9009" target="_blank" rel="noopener">为什么Pre Norm的效果不如Post Norm？</a>.</p></blockquote><p>我这里采用了Pre Norm版本的实现:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">EncoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>EncoderLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>norm1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>norm2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>enc_attn <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>ffn <span class="token operator">=</span> FeedForwardNetwork<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> pad_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">'''        pre-norm        see more detail in https://openreview.net/pdf?id=B1x8anVFPr        x: [batch, seq_len, d_model]        '''</span>        residual <span class="token operator">=</span> x        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>enc_attn<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> pad_mask<span class="token punctuation">)</span> <span class="token operator">+</span> residual        residual <span class="token operator">=</span> x        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>ffn<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x <span class="token operator">+</span> residual<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Pooler"><a href="#Pooler" class="headerlink" title="Pooler"></a>Pooler</h2><p>Pooler是<a href="https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html#BertModel" target="_blank" rel="noopener">Hugging Face实现BERT</a>时加上的额外组件. NSP任务需要提取<code>[CLS]</code>处的特征, Hugging Face的做法是将<code>[CLS]</code>处的输出接上一个FC, 并用tanh激活, 最后再接上二分类输出层. 他们将这一过程称为”<strong>Pool</strong>“.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Pooler</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Pooler<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>tanh <span class="token operator">=</span> nn<span class="token punctuation">.</span>Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">'''        x: [batch, d_model] (first place output)        '''</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>因为额外添加了一个FC层, 所以能增强表达能力, 同样提升了训练难度.</p><h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p>现在大框架中的Embedding, EncoderLayer, Pooler已经定义好了, 只需要额外定义输出时需要的其他组件. 在NSP任务输出时需要额外定义一个二分类输出层<code>next_cls</code>, 还有MLM任务输出所需的<code>word_classifier</code>, 以及前向传递<code>forward</code>.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">BERT</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>BERT<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> Embeddings<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>encoders <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>            EncoderLayer<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_layers<span class="token punctuation">)</span>        <span class="token punctuation">]</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>pooler <span class="token operator">=</span> Pooler<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>next_cls <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>gelu <span class="token operator">=</span> gelu        <span class="token comment" spellcheck="true"># Sharing weight between some fully connect layer</span>        shared_weight <span class="token operator">=</span> self<span class="token punctuation">.</span>pooler<span class="token punctuation">.</span>fc<span class="token punctuation">.</span>weight        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc<span class="token punctuation">.</span>weight <span class="token operator">=</span> shared_weight        <span class="token comment" spellcheck="true"># Sharing weight between word embedding and classifier</span>        shared_weight <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>word_emb<span class="token punctuation">.</span>weight        self<span class="token punctuation">.</span>word_classifier <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> max_vocab<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>word_classifier<span class="token punctuation">.</span>weight <span class="token operator">=</span> shared_weight    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tokens<span class="token punctuation">,</span> segments<span class="token punctuation">,</span> masked_pos<span class="token punctuation">)</span><span class="token punctuation">:</span>        output <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>tokens<span class="token punctuation">,</span> segments<span class="token punctuation">)</span>        enc_self_pad_mask <span class="token operator">=</span> get_pad_mask<span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>encoders<span class="token punctuation">:</span>            output <span class="token operator">=</span> layer<span class="token punctuation">(</span>output<span class="token punctuation">,</span> enc_self_pad_mask<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># output: [batch, max_len, d_model]</span>        <span class="token comment" spellcheck="true"># NSP Task</span>        hidden_pool <span class="token operator">=</span> self<span class="token punctuation">.</span>pooler<span class="token punctuation">(</span>output<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        logits_cls <span class="token operator">=</span> self<span class="token punctuation">.</span>next_cls<span class="token punctuation">(</span>hidden_pool<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Masked Language Model Task</span>        <span class="token comment" spellcheck="true"># masked_pos: [batch, max_pred] -> [batch, max_pred, d_model]</span>        masked_pos <span class="token operator">=</span> masked_pos<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># h_masked: [batch, max_pred, d_model]</span>        h_masked <span class="token operator">=</span> torch<span class="token punctuation">.</span>gather<span class="token punctuation">(</span>output<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> index<span class="token operator">=</span>masked_pos<span class="token punctuation">)</span>        h_masked <span class="token operator">=</span> self<span class="token punctuation">.</span>gelu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>h_masked<span class="token punctuation">)</span><span class="token punctuation">)</span>        logits_lm <span class="token operator">=</span> self<span class="token punctuation">.</span>word_classifier<span class="token punctuation">(</span>h_masked<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># logits_lm: [batch, max_pred, max_vocab]</span>        <span class="token comment" spellcheck="true"># logits_cls: [batch, 2]</span>        <span class="token keyword">return</span> logits_cls<span class="token punctuation">,</span> logits_lm<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>做个说明:</p><ol><li>为了减少模型训练上的负担, 这里对<code>pooler</code>的<code>fc</code>和MLM输出时使用的<code>fc</code>做了权重共享, 也对Word Embedding和<code>word_classifier</code>的权重做了共享.</li><li><code>torch.gather</code>能收集特定维度的指定位置的数值. <code>h_masked</code>使用的<code>gather</code>是为了检索<code>output</code>中 <code>max_len</code>维度上被Mask的位置上的表示, 总大小<code>[batch, max_pred, d_model]</code>. 因为<code>masked_pos</code>大小为<code>[batch, max_pred, d_model]</code>. 可能我表述不太清楚, 请参照<a href="https://adaning.github.io/posts/1216.html">Pytorch之张量进阶操作</a>中的例子理解.</li><li>没在模型中使用<code>Softmax</code>和<code>Simgoid</code>的原因是<code>nn.CrossEntropyLoss</code>自带将Logits转为概率的效果.</li></ol><h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>这部分主要是准备数据.</p><h3 id="Text-data"><a href="#Text-data" class="headerlink" title="Text data"></a>Text data</h3><p>采用一段简单的对话来作为原始数据. 为了方便起见, 这里没有采用<strong>Subword</strong>.</p><pre class="line-numbers language-python"><code class="language-python">test_text <span class="token operator">=</span> <span class="token punctuation">(</span>    <span class="token string">'Hello, how are you? I am Romeo.\n'</span>  <span class="token comment" spellcheck="true"># R</span>    <span class="token string">'Hello, Romeo My name is Juliet. Nice to meet you.\n'</span>  <span class="token comment" spellcheck="true"># J</span>    <span class="token string">'Nice meet you too. How are you today?\n'</span>  <span class="token comment" spellcheck="true"># R</span>    <span class="token string">'Great. My baseball team won the competition.\n'</span>  <span class="token comment" spellcheck="true"># J</span>    <span class="token string">'Oh Congratulations, Juliet\n'</span>  <span class="token comment" spellcheck="true"># R</span>    <span class="token string">'Thank you Romeo\n'</span>  <span class="token comment" spellcheck="true"># J</span>    <span class="token string">'Where are you going today?\n'</span>  <span class="token comment" spellcheck="true"># R</span>    <span class="token string">'I am going shopping. What about you?\n'</span>  <span class="token comment" spellcheck="true"># J</span>    <span class="token string">'I am going to visit my grandmother. she is not very well'</span>  <span class="token comment" spellcheck="true"># R</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># we need [MASK] [SEP] [PAD] [CLS]</span>word2idx <span class="token operator">=</span> <span class="token punctuation">{</span>f<span class="token string">'[{name}]'</span><span class="token punctuation">:</span> idx <span class="token keyword">for</span> idx<span class="token punctuation">,</span>            name <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'PAD'</span><span class="token punctuation">,</span> <span class="token string">'CLS'</span><span class="token punctuation">,</span> <span class="token string">'SEP'</span><span class="token punctuation">,</span> <span class="token string">'MASK'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true"># {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}</span>sentences <span class="token operator">=</span> re<span class="token punctuation">.</span>sub<span class="token punctuation">(</span><span class="token string">"[.,!?\\-]"</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">,</span> test_text<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">)</span>word_list <span class="token operator">=</span> list<span class="token punctuation">(</span>set<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>holdplace <span class="token operator">=</span> len<span class="token punctuation">(</span>word2idx<span class="token punctuation">)</span><span class="token keyword">for</span> idx<span class="token punctuation">,</span> word <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>word_list<span class="token punctuation">)</span><span class="token punctuation">:</span>    word2idx<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token operator">=</span> idx <span class="token operator">+</span> holdplaceidx2word <span class="token operator">=</span> <span class="token punctuation">{</span>idx<span class="token punctuation">:</span> word <span class="token keyword">for</span> word<span class="token punctuation">,</span> idx <span class="token keyword">in</span> word2idx<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>vocab_size <span class="token operator">=</span> len<span class="token punctuation">(</span>word2idx<span class="token punctuation">)</span><span class="token keyword">assert</span> len<span class="token punctuation">(</span>word2idx<span class="token punctuation">)</span> <span class="token operator">==</span> len<span class="token punctuation">(</span>idx2word<span class="token punctuation">)</span>token_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> sentence <span class="token keyword">in</span> sentences<span class="token punctuation">:</span>    token_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">[</span>        word2idx<span class="token punctuation">[</span>s<span class="token punctuation">]</span> <span class="token keyword">for</span> s <span class="token keyword">in</span> sentence<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里需要编写两个函数:</p><ul><li><code>padding</code>: 句子长度不够时, 用<code>[PAD]</code>补全.</li><li><code>masking_produce</code>: 按照BERT论文中提到的Mask方式.</li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">padding</span><span class="token punctuation">(</span>ids<span class="token punctuation">,</span> n_pads<span class="token punctuation">,</span> pad_symb<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> ids<span class="token punctuation">.</span>extend<span class="token punctuation">(</span><span class="token punctuation">[</span>pad_symb <span class="token keyword">for</span> _ <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_pads<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">masking_procedure</span><span class="token punctuation">(</span>cand_pos<span class="token punctuation">,</span> input_ids<span class="token punctuation">,</span> masked_symb<span class="token operator">=</span>word2idx<span class="token punctuation">[</span><span class="token string">'[MASK]'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    masked_pos <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    masked_tokens <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> pos <span class="token keyword">in</span> cand_pos<span class="token punctuation">:</span>        masked_pos<span class="token punctuation">.</span>append<span class="token punctuation">(</span>pos<span class="token punctuation">)</span>        masked_tokens<span class="token punctuation">.</span>append<span class="token punctuation">(</span>input_ids<span class="token punctuation">[</span>pos<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> p_mask<span class="token punctuation">:</span>            input_ids<span class="token punctuation">[</span>pos<span class="token punctuation">]</span> <span class="token operator">=</span> masked_symb        <span class="token keyword">elif</span> random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token punctuation">(</span>p_mask <span class="token operator">+</span> p_replace<span class="token punctuation">)</span><span class="token punctuation">:</span>            rand_word_idx <span class="token operator">=</span> random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> vocab_size <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span>            input_ids<span class="token punctuation">[</span>pos<span class="token punctuation">]</span> <span class="token operator">=</span> rand_word_idx    <span class="token keyword">return</span> masked_pos<span class="token punctuation">,</span> masked_tokens<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我们需要给句子加上<code>[CLS], [&#39;SEP&#39;], [MASK]</code>来符合BERT的输入格式. 并且保持样本中句子相邻和不相邻的比例是半对半, 即有一半样本中输入句子对是相邻的, 有一半不相邻.</p><p>这里简单的用两个句子的index是否相邻来判断上下文是否相邻, 不是很严谨, 只用于自己实现测试.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">make_data</span><span class="token punctuation">(</span>sentences<span class="token punctuation">,</span> n_data<span class="token punctuation">)</span><span class="token punctuation">:</span>    batch_data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    positive <span class="token operator">=</span> negative <span class="token operator">=</span> <span class="token number">0</span>    len_sentences <span class="token operator">=</span> len<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 50% sampling adjacent sentences, 50% sampling not adjacent sentences</span>    <span class="token keyword">while</span> positive <span class="token operator">!=</span> n_data <span class="token operator">/</span> <span class="token number">2</span> <span class="token operator">or</span> negative <span class="token operator">!=</span> n_data <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">:</span>        tokens_a_idx <span class="token operator">=</span> random<span class="token punctuation">.</span>randrange<span class="token punctuation">(</span>len_sentences<span class="token punctuation">)</span>        tokens_b_idx <span class="token operator">=</span> random<span class="token punctuation">.</span>randrange<span class="token punctuation">(</span>len_sentences<span class="token punctuation">)</span>        tokens_a <span class="token operator">=</span> sentences<span class="token punctuation">[</span>tokens_a_idx<span class="token punctuation">]</span>        tokens_b <span class="token operator">=</span> sentences<span class="token punctuation">[</span>tokens_b_idx<span class="token punctuation">]</span>        input_ids <span class="token operator">=</span> <span class="token punctuation">[</span>word2idx<span class="token punctuation">[</span><span class="token string">'[CLS]'</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">+</span> tokens_a <span class="token operator">+</span> <span class="token punctuation">[</span>word2idx<span class="token punctuation">[</span><span class="token string">'[SEP]'</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">+</span> tokens_b <span class="token operator">+</span> <span class="token punctuation">[</span>word2idx<span class="token punctuation">[</span><span class="token string">'[SEP]'</span><span class="token punctuation">]</span><span class="token punctuation">]</span>        segment_ids <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>            <span class="token number">1</span> <span class="token operator">+</span> len<span class="token punctuation">(</span>tokens_a<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token number">1</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> len<span class="token punctuation">(</span>tokens_b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>        n_pred <span class="token operator">=</span> min<span class="token punctuation">(</span>max_pred<span class="token punctuation">,</span> max<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> int<span class="token punctuation">(</span>len<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        cand_pos <span class="token operator">=</span> <span class="token punctuation">[</span>i <span class="token keyword">for</span> i<span class="token punctuation">,</span> token <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>                    <span class="token keyword">if</span> token <span class="token operator">!=</span> word2idx<span class="token punctuation">[</span><span class="token string">'[CLS]'</span><span class="token punctuation">]</span> <span class="token operator">and</span> token <span class="token operator">!=</span> word2idx<span class="token punctuation">[</span><span class="token string">'[SEP]'</span><span class="token punctuation">]</span><span class="token punctuation">]</span>                random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>cand_pos<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># shuffle all candidate position index, to sampling maksed position from first n_pred</span>        masked_pos<span class="token punctuation">,</span> masked_tokens <span class="token operator">=</span> masking_procedure<span class="token punctuation">(</span>            cand_pos<span class="token punctuation">[</span><span class="token punctuation">:</span>n_pred<span class="token punctuation">]</span><span class="token punctuation">,</span> input_ids<span class="token punctuation">,</span> word2idx<span class="token punctuation">[</span><span class="token string">'[MASK]'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># zero padding for tokens</span>        padding<span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> max_len <span class="token operator">-</span> len<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span><span class="token punctuation">)</span>        padding<span class="token punctuation">(</span>segment_ids<span class="token punctuation">,</span> max_len <span class="token operator">-</span> len<span class="token punctuation">(</span>segment_ids<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># zero padding for mask</span>        <span class="token keyword">if</span> max_pred <span class="token operator">></span> n_pred<span class="token punctuation">:</span>            n_pads <span class="token operator">=</span> max_pred <span class="token operator">-</span> n_pred            padding<span class="token punctuation">(</span>masked_pos<span class="token punctuation">,</span> n_pads<span class="token punctuation">)</span>            padding<span class="token punctuation">(</span>masked_tokens<span class="token punctuation">,</span> n_pads<span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>tokens_a_idx <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">==</span> tokens_b_idx <span class="token operator">and</span> positive <span class="token operator">&lt;</span> <span class="token punctuation">(</span>n_data <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            batch_data<span class="token punctuation">.</span>append<span class="token punctuation">(</span>                <span class="token punctuation">[</span>input_ids<span class="token punctuation">,</span> segment_ids<span class="token punctuation">,</span> masked_tokens<span class="token punctuation">,</span> masked_pos<span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">]</span><span class="token punctuation">)</span>            positive <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token keyword">elif</span> <span class="token punctuation">(</span>tokens_a_idx <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">!=</span> tokens_b_idx <span class="token operator">and</span> negative <span class="token operator">&lt;</span> <span class="token punctuation">(</span>n_data <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            batch_data<span class="token punctuation">.</span>append<span class="token punctuation">(</span>                <span class="token punctuation">[</span>input_ids<span class="token punctuation">,</span> segment_ids<span class="token punctuation">,</span> masked_tokens<span class="token punctuation">,</span> masked_pos<span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">]</span><span class="token punctuation">)</span>            negative <span class="token operator">+=</span> <span class="token number">1</span>    <span class="token keyword">return</span> batch_data<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>除了Tokens需要添加Zero Padding, Mask也要添加Zero Padding, 因为每个样本中添加Mask的数量是不定的.</p><p>此外, 这里实现逻辑不是很好, 可以针对循环优化.</p></blockquote><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>其实上面已经把数据准备的工作做完了, 下面就直接继承<code>Dataset</code>实现自己的数据集:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">BERTDataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_ids<span class="token punctuation">,</span> segment_ids<span class="token punctuation">,</span> masked_tokens<span class="token punctuation">,</span> masked_pos<span class="token punctuation">,</span> is_next<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>BERTDataset<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>input_ids <span class="token operator">=</span> input_ids        self<span class="token punctuation">.</span>segment_ids <span class="token operator">=</span> segment_ids        self<span class="token punctuation">.</span>masked_tokens <span class="token operator">=</span> masked_tokens        self<span class="token punctuation">.</span>masked_pos <span class="token operator">=</span> masked_pos        self<span class="token punctuation">.</span>is_next <span class="token operator">=</span> is_next    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>input_ids<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>input_ids<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>segment_ids<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>masked_tokens<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>masked_pos<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>is_next<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Training-and-Evaluation"><a href="#Training-and-Evaluation" class="headerlink" title="Training and Evaluation"></a>Training and Evaluation</h2><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>下面是训练代码, 没有什么值得注意的地方.</p><pre class="line-numbers language-python"><code class="language-python">batch_size <span class="token operator">=</span> <span class="token number">6</span>batch_data <span class="token operator">=</span> make_data<span class="token punctuation">(</span>token_list<span class="token punctuation">,</span> n_data<span class="token operator">=</span>batch_size<span class="token punctuation">)</span>batch_tensor <span class="token operator">=</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>ele<span class="token punctuation">)</span> <span class="token keyword">for</span> ele <span class="token keyword">in</span> zip<span class="token punctuation">(</span><span class="token operator">*</span>batch_data<span class="token punctuation">)</span><span class="token punctuation">]</span>dataset <span class="token operator">=</span> BERTDataset<span class="token punctuation">(</span><span class="token operator">*</span>batch_tensor<span class="token punctuation">)</span>dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>model <span class="token operator">=</span> BERT<span class="token punctuation">(</span>n_layers<span class="token punctuation">)</span>lr <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span>epochs <span class="token operator">=</span> <span class="token number">500</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>optimizer <span class="token operator">=</span> Adadelta<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># training</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> one_batch <span class="token keyword">in</span> dataloader<span class="token punctuation">:</span>        input_ids<span class="token punctuation">,</span> segment_ids<span class="token punctuation">,</span> masked_tokens<span class="token punctuation">,</span> masked_pos<span class="token punctuation">,</span> is_next <span class="token operator">=</span> <span class="token punctuation">[</span>ele<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token keyword">for</span> ele <span class="token keyword">in</span> one_batch<span class="token punctuation">]</span>        logits_cls<span class="token punctuation">,</span> logits_lm <span class="token operator">=</span> model<span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> segment_ids<span class="token punctuation">,</span> masked_pos<span class="token punctuation">)</span>        loss_cls <span class="token operator">=</span> criterion<span class="token punctuation">(</span>logits_cls<span class="token punctuation">,</span> is_next<span class="token punctuation">)</span>        loss_lm <span class="token operator">=</span> criterion<span class="token punctuation">(</span>logits_lm<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> max_vocab<span class="token punctuation">)</span><span class="token punctuation">,</span> masked_tokens<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        loss_lm <span class="token operator">=</span> <span class="token punctuation">(</span>loss_lm<span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss <span class="token operator">=</span> loss_cls <span class="token operator">+</span> loss_lm        <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">10</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'Epoch:{epoch + 1} \t loss: {loss:.6f}'</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h3><p>这里只采用了单个样本模拟Evaluation的过程.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Using one sentence to test</span>test_data_idx <span class="token operator">=</span> <span class="token number">3</span>model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    input_ids<span class="token punctuation">,</span> segment_ids<span class="token punctuation">,</span> masked_tokens<span class="token punctuation">,</span> masked_pos<span class="token punctuation">,</span> is_next <span class="token operator">=</span> batch_data<span class="token punctuation">[</span>test_data_idx<span class="token punctuation">]</span>    input_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    segment_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>segment_ids<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    masked_pos <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>masked_pos<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    masked_tokens <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>masked_tokens<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    logits_cls<span class="token punctuation">,</span> logits_lm <span class="token operator">=</span> model<span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> segment_ids<span class="token punctuation">,</span> masked_pos<span class="token punctuation">)</span>    input_ids<span class="token punctuation">,</span> segment_ids<span class="token punctuation">,</span> masked_tokens<span class="token punctuation">,</span> masked_pos<span class="token punctuation">,</span> is_next <span class="token operator">=</span> batch_data<span class="token punctuation">[</span>test_data_idx<span class="token punctuation">]</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"========================================================"</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Masked data:"</span><span class="token punctuation">)</span>    masked_sentence <span class="token operator">=</span> <span class="token punctuation">[</span>idx2word<span class="token punctuation">[</span>w<span class="token punctuation">]</span> <span class="token keyword">for</span> w <span class="token keyword">in</span> input_ids <span class="token keyword">if</span> idx2word<span class="token punctuation">[</span>w<span class="token punctuation">]</span> <span class="token operator">!=</span> <span class="token string">'[PAD]'</span><span class="token punctuation">]</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>masked_sentence<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># logits_lm: [batch, max_pred, max_vocab]</span>    <span class="token comment" spellcheck="true"># logits_cls: [batch, 2]</span>    cpu <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cpu'</span><span class="token punctuation">)</span>    pred_mask <span class="token operator">=</span> logits_lm<span class="token punctuation">.</span>data<span class="token punctuation">.</span>max<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>cpu<span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>    pred_next <span class="token operator">=</span> logits_cls<span class="token punctuation">.</span>data<span class="token punctuation">.</span>max<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>data<span class="token punctuation">.</span>to<span class="token punctuation">(</span>cpu<span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    bert_sentence <span class="token operator">=</span> masked_sentence<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>    original_sentence <span class="token operator">=</span> masked_sentence<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>masked_pos<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        pos <span class="token operator">=</span> masked_pos<span class="token punctuation">[</span>i<span class="token punctuation">]</span>        <span class="token keyword">if</span> pos <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            <span class="token keyword">break</span>        bert_sentence<span class="token punctuation">[</span>pos<span class="token punctuation">]</span> <span class="token operator">=</span> idx2word<span class="token punctuation">[</span>pred_mask<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span>        original_sentence<span class="token punctuation">[</span>pos<span class="token punctuation">]</span> <span class="token operator">=</span> idx2word<span class="token punctuation">[</span>masked_tokens<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"BERT reconstructed:"</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>bert_sentence<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Original sentence:"</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>original_sentence<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"===============Next Sentence Prediction==============="</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'Two sentences are continuous? {True if is_next else False}'</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'BERT predict: {True if pred_next else False}'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出:</p><pre><code>========================================================Masked data:[&#39;[CLS]&#39;, &#39;team&#39;, &#39;[MASK]&#39;, &#39;[MASK]&#39;, &#39;you&#39;, &#39;i&#39;, &#39;am&#39;, &#39;romeo&#39;, &#39;[SEP]&#39;, &#39;hello&#39;, &#39;romeo&#39;, &#39;my&#39;, &#39;name&#39;, &#39;is&#39;, &#39;juliet&#39;, &#39;nice&#39;, &#39;to&#39;, &#39;meet&#39;, &#39;you&#39;, &#39;[SEP]&#39;]BERT reconstructed:[&#39;[CLS]&#39;, &#39;hello&#39;, &#39;how&#39;, &#39;are&#39;, &#39;you&#39;, &#39;i&#39;, &#39;am&#39;, &#39;romeo&#39;, &#39;[SEP]&#39;, &#39;hello&#39;, &#39;romeo&#39;, &#39;my&#39;, &#39;name&#39;, &#39;is&#39;, &#39;juliet&#39;, &#39;nice&#39;, &#39;to&#39;, &#39;meet&#39;, &#39;you&#39;, &#39;[SEP]&#39;]Original sentence:[&#39;[CLS]&#39;, &#39;hello&#39;, &#39;how&#39;, &#39;are&#39;, &#39;you&#39;, &#39;i&#39;, &#39;am&#39;, &#39;romeo&#39;, &#39;[SEP]&#39;, &#39;hello&#39;, &#39;romeo&#39;, &#39;my&#39;, &#39;name&#39;, &#39;is&#39;, &#39;juliet&#39;, &#39;nice&#39;, &#39;to&#39;, &#39;meet&#39;, &#39;you&#39;, &#39;[SEP]&#39;]===============Next Sentence Prediction===============Two sentences are continuous? TrueBERT predict: True</code></pre>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Transformer </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ConvR: Adaptive Convolution for Multi-Relational Learning</title>
      <link href="/posts/20145.html"/>
      <url>/posts/20145.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>ConvE: 详见<a href="https://adaning.github.io/posts/42031.html">ConvE: Convolutional 2D Knowledge Graph Embeddings</a></li></ul></blockquote><h1 id="ConvR-Adaptive-Convolution-for-Multi-Relational-Learning"><a href="#ConvR-Adaptive-Convolution-for-Multi-Relational-Learning" class="headerlink" title="ConvR: Adaptive Convolution for Multi-Relational Learning"></a>ConvR: Adaptive Convolution for Multi-Relational Learning</h1><p>本文是论文<a href="https://www.researchgate.net/publication/334601450_Adaptive_Convolution_for_Multi-Relational_Learning" target="_blank" rel="noopener">Adaptive Convolution for Multi-Relational Learning</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>即使ConvE在KGE上利用CNN取得了突破性的成就, 但它的设计仍然<strong>忽略了实体和关系之间的交互性</strong>, 限制了Link Prediction的性能.</p><p>在ConvE中, 是将头实体和关系Reshape, 然后用<strong>标准卷积</strong>的卷积核进行运算, 而实际包含有实体和关系Embedding交互的只有卷积核经过的中间那一条(图中用红色标出):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/convr1.jpg" style="zoom:50%;" /><p>少量的交互次数使得关系对实体Embedding的影响降低了许多, 如果能进一步提升交互应该能改善性能.</p><p>因此, 作者同样是希望利用<strong>卷积</strong>, <strong>最大化实体和关系间的交互次数</strong>, 从而提升卷积类模型在链接预测的性能.</p><h2 id="ConvR"><a href="#ConvR" class="headerlink" title="ConvR"></a>ConvR</h2><p>ConvR其实非常简单, 它仍然没有脱离ConvE的框架. 它仅仅只是将卷积核直接拿掉了, “<strong>关系就是卷积核</strong>“.</p><p>ConvR的精髓在于, 适应性的将Relation Embedding构建成卷积核, 与Entity Embedding交互:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/convr2.jpg" style="zoom:50%;" /><p>这样每次卷积都是100%的交互.</p><h4 id="The-ConvR-Model"><a href="#The-ConvR-Model" class="headerlink" title="The ConvR Model"></a>The ConvR Model</h4><p>与ConvE一样, 先将Entity Embedding Reshape成2D矩阵$\mathbf{S} \in \mathbb{R}_{e}^{d_{e}^{h} \times d_{e}^{w}}$, 且$d_e = d_e^h d_e^w$. 在ConvE中已经论证过在2D上做卷积运算比1D卷积的优越性了.</p><p>然后需要用Relation Embedding构建成卷积核. 首先将Relation Embedding划分为大小相同的若干小块$\mathbf{r}^{(1)}, \cdots, \mathbf{r}^{(c)}$, 该过程称为<strong>Split</strong>.</p><p>因为是2D卷积, 对于每个小块$\mathbf{r}^{(\ell)} \in \mathbb{R}^{d_{r} / c}$, 都重新Reshape成高$h$, 宽$w$ 的卷积核$\mathbf{R}^{(\ell)} \in \mathbb{R}^{h \times w}$, $d_{r}=chw$.</p><blockquote><p>若拿$chw$ 作为$d_r$, 可能导致维数过大.</p></blockquote><p>在卷积时就采用<strong>关系特化</strong>的卷积核$\mathbf{R}^{(\ell)}$进行运算, 卷积过程的数学表示如下:</p><p>$$<br>c_{m, n}^{(\ell)}=f\left(\sum_{i, j} s_{m+i-1, n+j-1} \times r_{i, j}^{(\ell)}\right)<br>$$</p><p>其中$f$ 为非线性函数. 每个卷积核都能产生大小为$\mathbb{R}^{\left(d_{e}^{h}-h+1\right) \times\left(d_{e}^{w}-w+1\right)}$ 的特征图$\mathbf{C}^{(\ell)}$:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/convr3.jpg" style="zoom:50%;" /><p>最后, 将前面卷积产生的所有特征图$\mathbf{C}^{(1)}, \cdots, \mathbf{C}^{(c)}$全部<strong>打平</strong>, 然后<strong>堆叠</strong>成一个向量$\mathbf{c}$, 然后通过一个投影层将其转化为大小为$\mathbb{R}^{d_{e}}$ 的向量, 与尾实体向量$\mathbf{o}$ 做点积, 打分函数如下:<br>$$<br>\psi(s, r, o)=f(\mathbf{W} \mathbf{c}+\mathbf{b})^{\top} \mathbf{o}<br>$$</p><p>其中$\mathbf{W} \in \mathbb{R}^{d_{e} \times c\left(d_{e}^{h}-h+1\right)\left(d_{e}^{w}-w+1\right)}$, $\mathbf{b} \in \mathbb{R}^{d_{e}}$. 形式与ConvE<strong>完全一致</strong>.</p><h3 id="Parameter-Learning"><a href="#Parameter-Learning" class="headerlink" title="Parameter Learning"></a>Parameter Learning</h3><p>首先计算尾实体的概率:<br>$$<br>p_{o}^{s, r}=\sigma(\psi(s, r, o))<br>$$<br>其中$\sigma$ 是对数几率函数, 即$\sigma(x)=\frac{1}{1+e^{-x}}$.</p><p>与ConvE提出的1 - n Scoring相同, 然后用<strong>二分类交叉熵</strong>(BCE)损失函数进行优化:<br>$$<br>\mathcal{L}(s, r)=- \frac{1}{|\mathcal{E}|} \sum_{o \in \mathcal{E}} y_{o}^{s, r} \log \left(p_{o}^{s, r}\right)+<br>\left(1-y_{o}^{s, r}\right) \log \left(1-p_{o}^{s, r}\right)<br>$$</p><p>其他小Trick例如Dropout, BN, Label Smoothing仍然沿用ConvE.</p><h3 id="Advantages-over-ConvE"><a href="#Advantages-over-ConvE" class="headerlink" title="Advantages over ConvE"></a>Advantages over ConvE</h3><p>既然是ConvE的优化方法, 作者将着重论证ConvR比ConvE的优越性. 主要有两点:</p><ol><li><p><strong>交互次数</strong>比ConvE变多了, 每次卷积都是Entity Embedding和Relation Embedding的交互.</p></li><li><p>参数比ConvE少了, 因为”<strong>关系就是卷积核</strong>“.</p><p><strong>ConvE</strong>:$\mathcal{O}\left(d|\mathcal{E}|+d|\mathcal{R}|+c h w+c d\left(2 d^{h}-h+1\right)\left(d^{w}- w + 1\right)\right)$</p><p><strong>ConvR</strong>: $\mathcal{O}\left(d_{e}|\mathcal{E}|+d_{r}|\mathcal{R}|+c d_{e}\left(d_{e}^{h}-h+1\right)\left(d_{e}^{w}-w+1\right)\right)$</p><p>ConvR少掉的部分$chw$ 被集成进了Relation Embedding中.</p></li></ol><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的实验参数设置请参照原论文.</p><h4 id="Link-Prediction-Results"><a href="#Link-Prediction-Results" class="headerlink" title="Link Prediction Results"></a>Link Prediction Results</h4><p>作者主要探究了ConvR在WN18, FB15K, WN18RR, FB15K - 237上的性能:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/convr4.jpg" style="zoom: 50%;" /><p>WN18和FB15K这两个数据集是存在<strong>大量逆关系</strong>的, 结果可能是过拟合了的.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/convr5.jpg" style="zoom:50%;" /><p>主要的对比还是集中在卷积类的模型, ConvR比ConvE有了小幅提升, 比ConvKB的效果也要好.</p><p>其中所采用的最佳配置如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/convr6.jpg" style="zoom:50%;" /><blockquote><p>能看到, 如果使用ConvR, $d_r=chw$, 在列出的最佳结果中$d_r$ 都是远比$d_e$ 大的.</p></blockquote><h3 id="Parameter-Efficiency"><a href="#Parameter-Efficiency" class="headerlink" title="Parameter Efficiency"></a>Parameter Efficiency</h3><p>作者探究了不同卷积核大小和不同数量对模型性能的影响:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/convr7.jpg" style="zoom:50%;" /><p>总体来说各类参数对性能影响不大, 可以认为算法对超参是比较<strong>不敏感</strong>的.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>从卷积核<strong>自身结构</strong>下手, 与关系信息融合是一个非常好的思路. ConvR的设计即减少了参数量, 也增加了实体关系嵌入的交互次数.</p><p>但是以结构为基础的融合也是一把<strong>双刃剑</strong>, 我认为ConvR还存在两个重要的问题:</p><ul><li>虽然提升了关系和实体之间的交互次数, 但受制于直接拿关系Embedding作为卷积核的方式, ConvR无法像其他卷积类方法把模型做得更<strong>深</strong>. 一般来讲, 卷积层数越深, 所抽取到的特征也就越高阶, 也就意味着<strong>更复杂的实体与关系交互</strong>, 但在ConvR中只能做一层卷积.</li><li>因为卷积的特殊机制, $d_e$ 和$d_r$ 大概率不相同, 可能会对下游任务使用产生一定影响(许多场景要求$d_e$ 和$d_r$ 是<strong>相同维度</strong>的, 所以还需要<strong>额外维度压缩</strong>). 作者所给出的示例$d_r$ 都比较大, 如果强行调整至一致可能会导致性能衰减. </li></ul>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LightCAKE: A Lightweight Framework for Context-Aware Knowledge Graph Embedding</title>
      <link href="/posts/21282.html"/>
      <url>/posts/21282.html</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>2021.03.09</strong>: 修正关于引入逆三元组的影响.</p><p><strong>2021.04.18</strong>: 更新一篇更早的类似论文<a href="https://adaning.github.io/posts/60222.html">GAKE</a>.</p></blockquote><h1 id="LightCAKE-A-Lightweight-Framework-for-Context-Aware-Knowledge-Graph-Embedding"><a href="#LightCAKE-A-Lightweight-Framework-for-Context-Aware-Knowledge-Graph-Embedding" class="headerlink" title="LightCAKE: A Lightweight Framework for Context-Aware Knowledge Graph Embedding"></a>LightCAKE: A Lightweight Framework for Context-Aware Knowledge Graph Embedding</h1><p>本文是<a href="https://arxiv.org/abs/2102.10826" target="_blank" rel="noopener">LightCAKE: A Lightweight Framework for Context-Aware Knowledge Graph Embedding</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者提出, 现有的KGE模型无法很好地平衡<strong>图上下文</strong>信息与模型计算的<strong>复杂度</strong>.</p><p>作者指出, 许多的KGE模型都忽略了图中所蕴含的上下文信息(其实用图的<strong>多跳信息</strong>概括更为生动). 而作者将图中蕴含的信息分为两种, <strong>实体</strong>上下文信息和<strong>关系</strong>上下文信息:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lightcake1.jpg" style="zoom:50%;" /><p>图中普通虚线代表需要被预测的关系, 而红色虚线所圈的内容是对预测有重大帮助的部分. 许多模型直接将二者全部忽略, 或是只关注其中的一种. </p><blockquote><p>左侧图中从<strong>实体邻居</strong>的角度出发, 特朗普是拜登的一阶邻居, 考虑二者之间的关系将对预测有很大帮助.</p><p>右侧图中从<strong>关系对应的实体</strong>对角度出发, “president_of” 关系下所有的头尾实体都对预测特朗普是否是总统有帮助.</p></blockquote><p>因此, 作者希望提出一种轻量级的<strong>框架</strong>, <strong>LightCAKE</strong>(<strong>Light</strong>weight Framework for <strong>C</strong>ontext-<strong>A</strong>ware <strong>K</strong>nowledge Graph <strong>E</strong>mbedding) 来解决上述问题. 既然涉及到图信息的集成, 那么采用<strong>图算法</strong>更为合适. 例如GNN, 能够很好地聚合邻居节点的信息, 也能够捕获一部分来自高阶邻居的<strong>多跳信息</strong>.</p><h2 id="LightCAKE"><a href="#LightCAKE" class="headerlink" title="LightCAKE"></a>LightCAKE</h2><h3 id="Context-Star-Graph"><a href="#Context-Star-Graph" class="headerlink" title="Context Star Graph"></a>Context Star Graph</h3><p>在说明框架的运作方式之前, 先对实体上下文和关系上下文下个定义:</p><ul><li><strong>Entity Context</strong>: 对于头实体$h$, 实体上下文被定义为$h$ 的邻居, $\mathcal{C}_{\text {ent }}(h)=\{(r, t) \mid(h, r, t) \in \mathcal{G}\}$.</li><li><strong>Relation Context</strong>: 对于关系$r$, 关系上下文被定义为$r$ 下的全部头尾实体对, $\mathcal{C}_{r e l}(r)=\{(h, t) \mid(h, r, t) \in \mathcal{G}\}$.</li></ul><p>在图结构中, 实体和关系的上下文能<strong>一次性</strong>的构建:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lightcake2.jpg" style="zoom:50%;" /><p>作者在论文中计算实体上下文$\mathcal{C}_{\text {ent }}(h)$ 时只考虑了$h$ 的出边邻居, 忽略了入边邻居. 对于三元组$(h, r, t) \in \mathcal{G}$, 作者都创建一个逆三元组$(t, r^{-1}, h)$, 使得头实体和尾实体之间的边变为双向边(图中虚线部分). </p><blockquote><p>Inverse Triplet是否会导致Inverse Leakage? </p><p>实际上这里不会导致标签泄露. 在FB15K和WN18中引入的大量逆关系导致Leakage的原因并不是因为主动引入了逆三元组. 而是因为原本的三元组构建中就包含逆三元组, 在划分训练集和测试集的时候, 将部分互逆三元组对划分到了训练集和测试集两部分, 所以在测试集中的三元组可以由训练集过拟合得到, 从而导致的标签泄露.</p><p>而这里主动引入逆三元组, 若原数据集中不存在互逆三元组对, 那么训练时是不会将信息泄露给测试集的. 比如原来的三元组存在于训练集, 引入的新逆三元组也应该属于训练集, 这样就不会造成标签泄露.</p><p>事实上, 现在很多KGE方法都是这样做的.</p></blockquote><h3 id="LightCAKE-Details"><a href="#LightCAKE-Details" class="headerlink" title="LightCAKE Details"></a>LightCAKE Details</h3><p>LightCAKE被作者分为两部分:</p><ol><li>将实体或关系上下文<strong>编码</strong>进Embedding(<strong>Encode</strong>).</li><li><strong>迭代聚合</strong>上下文节点的信息(<strong>Attention</strong>).</li></ol><p>每次迭代的<strong>更新方程</strong>如下:</p><p>$$<br>\begin{array}{l}<br>e_{h}^{(l+1)}=e_{h}^{(l)}+\sum\limits_{\left(r^{\prime}, t^{\prime}\right) \in \mathcal{C}_{\text {ent }}(h)} \alpha_{h,\left(r^{\prime}, t^{\prime}\right)}^{(l)} \phi_{\text {ent }}\left(e_{r^{\prime}}, e_{t^{\prime}}\right) \\<br>e_{r}^{(l+1)}=e_{r}^{(l)}+\sum\limits_{\left(h^{\prime}, t^{\prime}\right) \in \mathcal{C}_{r e l}(r)} \beta_{r,\left(h^{\prime}, t^{\prime}\right)}^{(l)} \phi_{\text {rel }}\left(e_{h^{\prime}}, e_{t^{\prime}}\right)<br>\end{array}<br>$$</p><p>每次在原来Embedding的基础上<strong>有权重地聚合</strong>了图结构中的多跳信息. 其中$\phi$ 代表编码函数, $\phi(\cdot): \mathbb{R}^{d} \times \mathbb{R}^{d} \rightarrow \mathbb{R}$. $l$ 是迭代的次数, $0\leq l\leq L$. </p><p>$\alpha_{h,\left(r^{\prime}, t^{\prime}\right)}^{(l)}, \beta_{r,\left(h^{\prime}, t^{\prime}\right)}^{(l)}$ 分别是在实体上下文星图和关系上下文星图中对其他节点的<strong>注意力</strong>:<br>$$<br>\begin{aligned}<br>\alpha_{h,\left(r^{\prime}, t^{\prime}\right)}^{(l)}=\frac{\exp \left(\psi\left(e_{h}^{(l)}, e_{r^{\prime}}^{(l)}, e_{t^{\prime}}^{(l)}\right)\right)}{\sum_{\left(r^{\prime \prime}, t^{\prime \prime}\right) \in \mathcal{C}_{e n t}(h)} \exp \left(\psi\left(e_{h}^{(l)}, e_{r^{\prime \prime}}^{(l)}, e_{t^{\prime \prime}}^{(l)}\right)\right)} \\<br>\beta_{r,\left(h^{\prime}, t^{\prime}\right)}^{(l)}=\frac{\exp \left(\psi\left(e_{h^{\prime}}^{(l)}, e_{r}^{(l)}, e_{t^{\prime}}^{(l)}\right)\right)}{\sum_{\left(h^{\prime \prime}, t^{\prime \prime}\right) \in \mathcal{C}_{r e l}(r)} \exp \left(\psi\left(e_{h^{\prime \prime}}^{(l)}, e_{r}^{(l)}, e_{t^{\prime \prime}}^{(l)}\right)\right)}<br>\end{aligned}<br>$$</p><p>其中, $\psi$ 是具体的打分函数, 也就是具体的KGE方法, 例如TransE, DistMult等.</p><p>通过$L$ 次迭代, 就能得到一组<strong>上下文增强</strong>的Embedding $e_{h}^{(L)}, e_{r}^{(L)}, e_{t}^{(L)}$. 然后分别计算出在已知头实体和尾实体条件下未知关系$r$ 条件概率:<br>$$<br>p(r \mid h, t)=\frac{\exp \left(\psi\left(e_{h}^{(L)}, e_{r}^{(L)}, e_{t}^{(L)}\right)\right)}{\sum_{r^{\prime} \in \mathcal{R}} \exp \left(\psi\left(e_{h}^{(L)}, e_{r^{\prime}}^{(L)}, e_{t}^{(L)}\right)\right)}<br>$$<br>根据计算得出的条件概率$p(r \mid h, t)$, 用<strong>极大似然</strong>来优化Embedding:<br>$$<br>\mathcal{L}=-\frac{1}{|\mathcal{D}|} \sum_{i=0}^{|\mathcal{D}|} \log p\left(r_{i} \mid h_{i}, t_{i}\right)<br>$$<br>对于作者文中所采用的两种方法TransE和DistMult, 作者将它们的打分函数代入后写出了改进方法的详细形式, 在此没有必要列出了.</p><p>作者注意到, 与图相关的GNN算法存在<strong>过参数化</strong>的问题, 所以整套LightCAKE框架<strong>没有添加任何额外的参数</strong>.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>作者认为, 使用Relation Prediction与使用Link Prediction一定程度上是等价的, 所以这里采用了Relation Prediction作为评估任务.</p><blockquote><p>我个人认为还是有些不同, Relation Prediction的难度比Link Prediction难度要小得多.</p></blockquote><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><p>作者选用了现在效果比较好的Baseline, ComplEx, SimplE, RotatE, DRUM, R - GCN和改进后的TransE, DistMult性能做了对比:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lightcake3.jpg" style="zoom:50%;" /><p>改进后的方法不但性能上比现在已有的方法性能要好, 而且性能与没改进前要高许多(针对MRR来说).</p><h3 id="Ablation-Study-and-Analysis-on-Number-of-Iterations"><a href="#Ablation-Study-and-Analysis-on-Number-of-Iterations" class="headerlink" title="Ablation Study and Analysis on Number of Iterations"></a>Ablation Study and Analysis on Number of Iterations</h3><p>作者在消融实验中<strong>逐步引入图多跳信息</strong>, 查看引入的信息对性能产生的影响, 也探究了<strong>迭代次数</strong>对Embedding产生的影响. 作者将引入信息的程度分为四个等级:</p><ol><li><strong>不引入</strong>任何图结构的信息.</li><li>只引入<strong>关系</strong>上下文信息, 记为$\mathcal{L}_{rel}$.</li><li>只引入<strong>实体</strong>上下文信息, 记为$\mathcal{L}_{ent}$.</li><li>引入<strong>实体和关系</strong>上下文信息, 记为$\mathcal{L}$.</li></ol><p>实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lightcake4.jpg" style="zoom: 50%;" /><p>随着引入信息的逐渐增加, 模型性能逐渐提高. 引入实体信息后的提升比引入关系信息要大. </p><p>关于迭代轮数, 迭代过多次数反而会导致性能下降, 控制在3 ~ 4 次为宜.</p><h3 id="Efficiency-Analysis"><a href="#Efficiency-Analysis" class="headerlink" title="Efficiency Analysis"></a>Efficiency Analysis</h3><p>作者将改进前后和基于图的算法(R - GCN)做了效率上的比较:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lightcake5.jpg" style="zoom: 50%;" /><p>DistMult在改进后增加了L倍的复杂度, 但却获得了实体和关系上的图结构信息<strong>双加成</strong>, 效果提升显著. 而R - GCN的时间复杂度比较高, 没有利用图上的关系信息.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>LightCAKE是一个<strong>轻量级</strong>的<strong>集成图多跳信息</strong>的KGE框架, 无需添加任何额外的参数, 仅用部分计算量就将关系和实体上下文信息全部集成了起来, 整体非常简洁. 作者也注意到了GNN中的过参数化问题, 这点很宝贵. 但实验部分采用了Relation Prediction作为评估, 我仍然期待它在Link Prediction上的效果.</p><p>对我的启发:</p><ol><li>要利用好<strong>图结构优势</strong>, 图中蕴含着多种<strong>多跳信息</strong>, 不能白白浪费它们. 从可解释的角度, 图结构也占有优势.</li><li>注意各类方法在KGE中的<strong>过参数化</strong>(尤其是GNN), 有时候参数过多也不是啥好事, 可能不利于优化, 还增加了时间复杂度.</li><li>引入实体, 关系双角度信息, 能更好的刻画节点. 如果有更多角度的信息, 能更精确的找到节点定位.</li></ol><blockquote><p>经评论区老哥指路, LightCAKE的Entity Context的定义和<a href="https://adaning.github.io/posts/60222.html">GAKE(COLING2016)</a>的<strong>Neighbor Context</strong>定义是一样的, 并且同样也是用Attention分配上下文的对Embedding的影响, 用极大似然优化. 只是GAKE还额外定义了Path Context和Edge Context, 而LightCAKE可以使用别的打分函数. 整体上来讲, 这两篇论文非常像.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introduction: Graph Neural Network</title>
      <link href="/posts/31958.html"/>
      <url>/posts/31958.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>图结构基础知识(数据结构相关内容, 自行查阅).</li></ul><p><strong>2021.04.06</strong>: 更新GraphSAGE的理解.</p></blockquote><h1 id="Introduction-Graph-Neural-Network"><a href="#Introduction-Graph-Neural-Network" class="headerlink" title="Introduction: Graph Neural Network"></a>Introduction: Graph Neural Network</h1><p>本文介绍的是GNN方面入门级别的知识, 其实这坑早就挖下了, 但是一直都没有机会补. 部分内容出自<a href="https://aistudio.baidu.com/aistudio/course/introduce/1956" target="_blank" rel="noopener">飞桨图神经网络7日打卡营</a>, 训练营的切入的角度避开了复杂的数学推导, 方便GNN入门.</p><blockquote><p>关于实现, 现在还没有统一的比较成熟的图学习框架, 无论是PyTorch还是TF, 都需要自己手动实现. </p><p>有两个图学习框架<a href="https://github.com/rusty1s/pytorch_geometric" target="_blank" rel="noopener">PyG</a> 和<a href="https://github.com/dmlc/dgl" target="_blank" rel="noopener">DGL</a>是大家用的比较多的, 但是都有一些问题, 目前来看PyG的人气比DGL要高得多. 还有百度飞桨的框架PGL, 感谢飞桨PGL在图学习开源上做出的贡献.</p></blockquote><p>文中所涉及的所有论文和图片出处在结尾都会提供.</p><h2 id="Graph-and-Graph-Learning"><a href="#Graph-and-Graph-Learning" class="headerlink" title="Graph and Graph Learning"></a>Graph and Graph Learning</h2><p>诸如图的有向, 无向, 邻接矩阵, 矩阵的度之类的<strong>基本概念</strong>就不再赘述, 如果你对图结构本身还不太熟悉的话, 去重温一下<strong>数据结构</strong>就好.</p><p>在现实生活中, 图结构有多种很相关的实例. 例如分子结构, 交通流量, 社交网络, <strong>知识图谱</strong>等… 非常多的复杂问题都能够被图所表示, 因为<strong>图本来就是一种表示能力极强的结构</strong>, 这使得许多利用简单结构不能被表示的问题得以表示. 也正是因为<strong>欧式数据</strong>和<strong>非欧数据</strong>在处理问题上的差异, 导致我们需要探索一种在非欧数据问题上生效的方法.</p><p>如果从图本身的角度来看, 单张图本身可以看做是茫茫众多图中的一个, 如果完成任务需要依赖于不同的图, 可以被称为是<strong>图级别任务</strong>,</p><p>如果从图结构的角度来看, 每张图中的<strong>节点</strong>和<strong>边</strong>都能够用作在不同的任务中. 节点或许可以代表问题中的某个<strong>实物</strong>, 边可以表示不同节点(实物)和节点之间的<strong>联系</strong>. 使用它们两个也就照应着<strong>节点级任务</strong>和<strong>边级别任务</strong>.</p><p>按照飞桨训练营中对图学习的划分, 图学习算法可以分为三大类:</p><ol><li><strong>图游走类算法</strong>(图嵌入算法): DeepWalk, Node2Vec等.</li><li><strong>图神经网络算法</strong>: GCN, GAT, GraphLSTM等.</li><li><strong>知识图谱嵌入算法</strong>: TransE, TransR, RotatE等.</li></ol><p>现在主流的知识图谱嵌入和图嵌入有一些区别, 所以单独列了一类. 因为现在的Knowledge Embedding方法大多采用<strong>三元组</strong>的形式来获取嵌入表示. 当然也有结合图的算法, 例如R - GCN等.</p><p>图嵌入和图神经网络互有交集, 但图嵌入更侧重于只得到节点的<strong>低维表示</strong>, 而图神经网络侧重于将任务<strong>端到端</strong>的解决:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial1.jpg" style="zoom:50%;" /><blockquote><p>关于GNN发展过程中在<strong>频域</strong>, <strong>空域</strong>上的一些内容在入门阶段很晦涩, 涉及到相当多的<strong>数学推导</strong>, 由于个人的基础不扎实在这里就不做误导了. 如果只是使用它, 不推荐关心这部分内容.</p><p>但如果是专门做GNN的研究, 这方面知识是非常有必要的. 并且需要掌握所涉及的推导过程.</p><p>给几个关于这方面的补充资料吧:</p><ul><li><a href="https://blog.csdn.net/yyl424525/article/details/100058264" target="_blank" rel="noopener">图卷积网络 GCN Graph Convolutional Network（谱域GCN）的理解和详细推导</a></li><li><a href="https://blog.csdn.net/qq_44015059/article/details/105341555" target="_blank" rel="noopener">基于频域GCN理论基础(拉普拉斯矩阵与傅里叶变换)</a></li></ul></blockquote><h2 id="Graph-Walking"><a href="#Graph-Walking" class="headerlink" title="Graph Walking"></a>Graph Walking</h2><p>图游走算法可以说是为<strong>图嵌入</strong>所服务的, 就好像Word Embedding对于NLP的地位一样, 为下游任务服务. 图嵌入可以得到一个节点在图中的表示(向量). 图游走就是通过某种游走算法将图转化为<strong>序列</strong>, 再使用类似NLP获取词嵌入的方式得到节点表示的方法.</p><h3 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h3><p>最早的游走思路是借鉴了NLP在处理词嵌入时所采用的<strong>Word2Vec</strong>, 在Word2Vec中, <strong>中心词语义可以由周围邻近的词语</strong>义来”决定”, Skip - Gram便是一种根据中心词预测上下文来获取中心词嵌入表示的方法:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/skipgram.png" style="zoom: 50%;" /><p>人们观察到, 社交网络中的用户行为经常会受到邻近用户的行为影响. 在图结构中, 只需要将中心思想变为<strong>中心节点含义由周围的节点决定</strong>, 本质上还是不变的. 所以就有可能将Word2Vec迁移到图领域用于获取节点嵌入表示. </p><h3 id="DeepWalk"><a href="#DeepWalk" class="headerlink" title="DeepWalk"></a>DeepWalk</h3><p>DeepWalk非常简单的在图中做<strong>随机游走</strong>, 即在当前节点的邻近节点(包括<strong>自身</strong>)随机游走, 当游走到<strong>最大长度</strong>时停止. 所以它是一个<strong>可以重复遍历的DFS</strong>.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial2.jpg" style="zoom: 67%;" /><p>在这里定义一般的随机游走公式, 在节点$v$ 处游走到下一个节点$x$ 的概率为:<br>$$<br>{P}\left(c_{i}=x \mid c_{i-1}=v\right)=\left\{\begin{array}{cc}<br>\frac{\pi_{vx}}{Z}, &amp; \text { if }(v, x) \in E \\<br>0, &amp; \text { otherwise }<br>\end{array}\right.<br>$$<br>其中$Z$ 为归一化因子, $\pi_{vx}$ 是在被归一化之前算法得出的关于$v$ 游走到$x$ 的某个依据值.</p><p>对于DeepWalk来说, 只要与节点$v$ 相邻的节点概率是相等的, 所以有:</p><p>$$<br>P\left(c_{i}=x \mid c_{i-1}=v\right)=\left\{\begin{array}{cc}<br>\frac{1}{|N(v)|}, &amp; \text { if }(v, x) \in E \\<br>0, &amp; \text { otherwise }<br>\end{array}\right.<br>$$</p><p>拿到了这个图的遍历序列, 就能将它作为一个序列输入到Word2Vec中, 就得到了节点的表示.</p><h3 id="Node2Vec"><a href="#Node2Vec" class="headerlink" title="Node2Vec"></a>Node2Vec</h3><p>对于DeepWalk来说, 随机游走显得有些过于漫无目的, 没有偏好. 而且在DeepWalk中, 只考虑了使用DFS游走的方式. 而在数据结构中可知, 图的游走方式是有<strong>DFS</strong>和<strong>BFS</strong>两种的:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial3.jpg" style="zoom: 67%;" /><p>在Node2Vec中, 考虑了上述问题, 希望能够让游走的方式更加丰富一点. 只需要将图游走的一般公式做如下替换:<br>$$<br>\pi_{vx} =\alpha_{pq}(t, x) \cdot w_{vx}<br>$$<br>$v$为当前节点, $t$ 为上一个节点, $w_{vx}$为$v$ 和$x$ 之间的权值, 其中$\alpha_{pq}(t, x)$ 为:<br>$$<br>\alpha_{p q}(t, x)=\left\{\begin{array}{ll}<br>\frac{1}{p}, &amp; \text { if } d_{t x}=0 \\<br>1, &amp; \text { if } d_{t x}=1 \\<br>\frac{1}{q}, &amp; \text { if } d_{t x}=2<br>\end{array}\right.<br>$$<br>$d_{tx}$ 代表节点$t$ 到$x$ 的距离, 即当前节点$v$ 的一阶邻居. 而$p, q$ 则是两个参数, 它们能控制如何游走:</p><ul><li>$p$ 能控制有多大的概率”回头”, 即从当前节点$v$ 重新回到前一节点$t$, 如下图$v\rightarrow t$).</li><li>$q$ 控制游走策略倾向于DFS或是BFS:<ul><li>$q&gt;1$ 时倾向于BFS, 如下图$v\rightarrow x_1$.</li><li>$q&lt;1$ 时倾向于DFS, 如下图$v\rightarrow x_2$.</li></ul></li><li>$p=q=1$ 时, $\pi_{vx}=w_{vx}$.</li></ul><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial4.jpg" style="zoom: 67%;" /><p>这样游走有如下好处:</p><ol><li>结合了图的<strong>权重</strong>对游走的影响.</li><li>能够让模型<strong>自己学习</strong>如何游走合适(不是仅仅执行DFS, 也在某些时候BFS).</li></ol><h2 id="Graph-Neural-Network"><a href="#Graph-Neural-Network" class="headerlink" title="Graph Neural Network"></a>Graph Neural Network</h2><h3 id="Graph-Convolutional-Network"><a href="#Graph-Convolutional-Network" class="headerlink" title="Graph Convolutional Network"></a><strong>G</strong>raph <strong>C</strong>onvolutional <strong>N</strong>etwork</h3><p><strong>图卷积网络</strong>(<strong>G</strong>raph <strong>C</strong>onvolutional <strong>N</strong>etwork, <strong>GCN</strong>)才彻彻底底的将卷积的概念从谱域扩展到了空域上. GCN将欧式结构上的卷积扩展到非欧结构上的卷积.</p><p>在CV中的卷积被定义为: <strong>将某个像素点周围的像素以不同权重叠加起来</strong>. 那么扩展到图结构中这种非欧结构中, 对应的像素应该变为<strong>节点</strong>, <strong>即将某个节点周围的邻居以不同权重叠加起来</strong>, 如下所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial5.jpg" style="zoom: 33%;" /><p>就像普通的CNN一样, GCN也是以若干层堆叠提取特征的方式发挥作用:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial6.jpg" style="zoom: 33%;" /><p>或是加上图池化与读出层做分类任务:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial7.jpg" style="zoom: 33%;" /><p>其中, 每一层GCN所对应节点隐态的更新方式为:<br>$$<br>H^{(l+1)}=\sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})<br>$$<br>其中$\hat{A}$ 为图的自邻接矩阵(有节点自身的闭环, 即$\tilde{A} = A + I$), $D$ 为度矩阵, $H^{(l)}$ 为第$l$ 层GCN的节点表示. $W^{(l)}$ 很好理解, 第$l$ 层的线性变换矩阵, 也就是”神经网络”(DNN).</p><p>以下图为例(图中有节点到自身的<strong>闭环</strong>):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial8.jpg" style="zoom: 67%;" /><p>它所对应的邻接矩阵$A$, 度矩阵$D$ 分别为:<br>$$<br>\tilde{A}=\begin{bmatrix}\color{red}{1}&amp;\color{red}{1}&amp;\color{red}{1}&amp;0&amp;0&amp;0&amp;0\\\color{red}{1}&amp;\color{red}{1}&amp;\color{red}{1}&amp;0&amp;0&amp;0&amp;0\\\color{red}{1}&amp;\color{red}{1}&amp;\color{red}{1}&amp;\color{red}{1}&amp;0&amp;0&amp;0\\0&amp;0&amp;\color{red}{1}&amp;\color{red}{1}&amp;\color{red}{1}&amp;\color{red}{1}&amp;\color{red}{1}\\0&amp;0&amp;0&amp;\color{red}{1}&amp;\color{red}{1}&amp;\color{red}{1}&amp;\color{red}{1}\\0&amp;0&amp;0&amp;\color{red}{1}&amp;\color{red}{1}&amp;\color{red}{1}&amp;0\\0&amp;0&amp;0&amp;\color{red}{1}&amp;\color{red}{1}&amp;0&amp;\color{red}{1}\end{bmatrix}<br>\tilde{D}=\begin{bmatrix}\color{red}{3}&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\\0&amp;\color{red}{3}&amp;0&amp;0&amp;0&amp;0&amp;0\\0&amp;0&amp;\color{red}{4}&amp;0&amp;0&amp;0&amp;0\\0&amp;0&amp;0&amp;\color{red}{5}&amp;0&amp;0&amp;0\\0&amp;0&amp;0&amp;0&amp;\color{red}{4}&amp;0&amp;0\\0&amp;0&amp;0&amp;0&amp;0&amp;\color{red}{3}&amp;0\\0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;\color{red}{3}\end{bmatrix}\<br>$$</p><p>其中所需要用到的$\tilde{D}^{-\frac{1}{2}}$ 为下矩阵:</p><p>$$<br>\tilde{D}^{-\frac{1}{2}}=\begin{bmatrix}\color{red}{\frac{1}{\sqrt{3}}}&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\\0&amp;\color{red}{\frac{1}{\sqrt{3}}}&amp;0&amp;0&amp;0&amp;0&amp;0\\0&amp;0&amp;\color{red}{\frac{1}{\sqrt{4}}}&amp;0&amp;0&amp;0&amp;0\\0&amp;0&amp;0&amp;\color{red}{\frac{1}{\sqrt{5}}}&amp;0&amp;0&amp;0\\0&amp;0&amp;0&amp;0&amp;\color{red}{\frac{1}{\sqrt{4}}}&amp;0&amp;0\\0&amp;0&amp;0&amp;0&amp;0&amp;\color{red}{\frac{1}{\sqrt{3}}}&amp;0\\0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;\color{red}{\frac{1}{\sqrt{3}}}\end{bmatrix}<br>$$<br>为了方便理解更新方式, 我们先做如下方式的<strong>简化</strong>:<br>$$<br>\begin{aligned}<br>H^{(l+1)}&amp;=\sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})\\\  &amp;\Downarrow  \\\ H^{(l+1)}&amp;=\sigma(\tilde{A}H^{(l)}W^{(l)})<br>\end{aligned}<br>$$<br>即<strong>先不考虑度对更新的影响</strong>. $\tilde{A}H^{(l)}$ 的含义是:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial9.jpg" style="zoom: 67%;" /><p>$l+1$ 层的第0节点表示是$l$ 层第0, 1, 2 节点表示的和. 这和CNN非常相似, 能根据邻接矩阵来判断邻居, 然后将邻居信息求和. 在计算下一层节点表示的过程中, 隐含着一种机制(或是框架), <strong>消息传递</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial10.jpg" style="zoom: 67%;" /><p>0号节点<strong>接收</strong>了来自0, 1, 2号节点的信息, 并<strong>更新</strong>了自己的信息. 消息传递的过程也就是这两步:</p><ol><li>边上的源节点向目标节点<strong>发送</strong>信息.</li><li>目标节点对接收到的特征进行<strong>聚合</strong>.</li></ol><p>既然已经能够完成特征更新的整个流程, 为什么要引入$\tilde{D}^{-\frac{1}{2}}$ 呢? 如果只使用邻接矩阵做加权, 周围人对你的评价可能并不是真实的:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial11.jpg" style="zoom: 50%;" /><p>例如新垣结衣的周围的人非常多(<strong>度非常大</strong>), 她的特征可能会因为多人的评价而变得非常大, 从而对你的评价可能就不那么准确, 在训练时也容易导致梯度消失或梯度爆炸. 相反, 可能你的好友更加的了解你(<strong>度比较小</strong>), 对你的评价也更准确. 对所有节点一视同仁会导致度大的节点特征越来越大, 度小的节点越来越小.</p><p>因此, 我们可以使用<strong>度</strong>来衡量邻居信息的<strong>重要性</strong>, 这里的$\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$ 在做的事情实际上是用度矩阵对$A$ 做了<strong>Renormalization</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial12.jpg" style="zoom: 50%;" /><p>度$d$ 越大, 信息就越少, $\frac{1}{\sqrt{d}}$ 就越小. </p><blockquote><p>这里采用<strong>Renormalization</strong>是有说法的, 想深入了解可以看<a href="https://www.zhihu.com/question/426784258/answer/1536731121" target="_blank" rel="noopener">GCN中的拉普拉斯矩阵如何归一化？</a>.</p><p>之所以没有采用”对称归一化”这个说法, 是因为矩阵并没有真正的得到归一化, 原论文中的表述也是”Renormalization”.</p></blockquote><p>下面来总结一下GCN的流程:</p><ol><li><p>使用$\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$ 进行节点之间的<strong>特征传递</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial13.jpg" style="zoom: 25%;" /></li><li><p>对每一个节点过一层<strong>DNN</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial14.jpg" style="zoom: 25%;" /></li><li><p><strong>重复</strong>上面两步多次, 实现多层GCN, 并能获得每个节点的表示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial15.jpg" style="zoom: 25%;" /></li><li><p>根据取得的节点表示$H^{(l)}$将其用于<strong>下游任务</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial16.jpg" style="zoom: 50%;" /></li></ol><h3 id="Graph-Attention-Network"><a href="#Graph-Attention-Network" class="headerlink" title="Graph Attention Network"></a>Graph Attention Network</h3><p>在GCN中的边权重是通过<strong>度</strong>来控制的, 这种度量仅与度有关, 而且不可学习权重的分配方式. </p><p>在深度学习背景下, 我们更希望能够模型能够<strong>自己学习</strong>如何分配权重. 在深度学习中, 关于学习权重分配的分配方式, 人们很自然而然的就想到了<strong>Attention</strong>, 它也确实非常适合去做这件事情. <strong>图注意力网络</strong>(<strong>G</strong>raph <strong>At</strong>tention Network, <strong>GAT</strong>)应运而生.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial17.jpg" style="zoom: 50%;" /><p>GAT通过对调整当前节点$i$ 对其他节点$j$ 的权重来调整, 在这里只考虑节点$i$ 的<strong>一阶邻居</strong> $j \in \mathcal{N}_{i}$.<br>$$<br>e_{i j}=a\left(\mathbf{W} \vec{h}_{i}, \mathbf{W} \vec{h}_{j}\right)<br>$$<br>GAT中的Attention计算方式如下:<br>$$<br>\displaylines{<br>\alpha_{i j}=\operatorname{softmax}_{j}\left(e_{i j}\right)=\frac{\exp \left(e_{i j}\right)}{\sum_{k \in \mathcal{N}_{i}} \exp \left(e_{i k}\right)}<br>\\\ \Downarrow \\<br>\alpha_{i j}=\frac{\exp \left(\operatorname{LeakyReLU}\left(\overrightarrow{\mathbf{a}}^{T}\left[\mathbf{W} \vec{h}_{i} | \mathbf{W} \vec{h}_{j}\right]\right)\right)}{\sum_{k \in \mathcal{N}_{i}} \exp \left(\text { LeakyReLU }\left(\overrightarrow{\mathbf{a}}^{T}\left[\mathbf{W} \vec{h}_{i} | \mathbf{W} \vec{h}_{k}\right]\right)\right)}<br>}<br>$$<br>其中$\overrightarrow{\mathbf{a}}$ 是一个权重向量, 也可以被视作是一个<strong>单层神经网络</strong>, $\mathbf{W}$ 为权重矩阵, 能够学习到输入特征$\overrightarrow{h}$ 中更高级的特征. GAT计算各节点的高阶特征, 后计算各节点对当前节点的重要程度, 并经过LeakyReLU激活, 最后用Softmax做归一化, 求得Attention权重:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial19.jpg" style="zoom: 33%;" /><p>对特征的聚合方式如下:<br>$$<br>\vec{h}_{i}^{\prime}=\sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j} \mathbf{W} \vec{h}_{j}\right)<br>$$<br>$\sigma$ 是非线性的激活函数.</p><p>与Transformer一样, GAT也支持<strong>多头特征聚合</strong>:<br>$$<br>\vec{h}_{i}^{\prime}=\operatorname{\lVert}\limits_{k=1}^{K} \sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k} \vec{h}_{j}\right)<br>$$<br>其中$||$ 代表Concatenation. 即将多个头的特征Concat起来. 当然也可以采用求平均的方式来适应不同的场景:<br>$$<br>\vec{h}_{i}^{\prime}=\sigma\left(\frac{1}{K} \sum_{k=1}^{K} \sum_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k} \vec{h}_{j}\right)<br>$$<br>GAT总体来说如下所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial18.jpg" style="zoom: 40%;" /><p>图中三种颜色的线代表有三个头, 学习到了不同的权重分配方式, 最后再通过某种聚合方式聚合获得$\overrightarrow{h_1^\prime}$.</p><p>因此, GAT不但将权重调整为与两个节点都相关的函数, 而且还是可学习的, 它同样遵守消息传递框架.</p><h3 id="Message-Passing-Neural-Network"><a href="#Message-Passing-Neural-Network" class="headerlink" title="Message Passing Neural Network"></a>Message Passing Neural Network</h3><p><strong>消息传递网络</strong>(<strong>M</strong>essage <strong>P</strong>assing <strong>N</strong>eural <strong>N</strong>etwork, <strong>MPNN</strong>)并非是某种具体的图神经网络, 而是对图神经网络更新权重方式的一种<strong>范式</strong>或是一种<strong>框架</strong>. 我在前面介绍GCN, GAT时曾多次提到这个词, 因为它们都是<strong>基于邻居聚合</strong>的模型, 都属于Spatial GNN, 大多数的空域GNN都是可以被消息传递网络实现的.</p><p>基于消息传递的 Graph Neural Network的通用公式如下:</p><p>$$<br>h_{l}^{(t)}(v)=\color{green}{f}\left(h_{l}^{(t-1)}, \color{red}{\mathcal{F}}\left\{\color{blue}{h_{l}^{(t-1)}(u) \mid u \in N(v)}\right\}\right)<br>$$</p><p>其中$h_{l}^{(t-1)}(u)$ 代表邻居的消息发送, $\mathcal{F}$ 代表聚合函数, 可以是<strong>Max</strong>, <strong>Mean</strong>, <strong>Sum</strong>等, $f$ 对应神经网络, 可以是MLP或者其他结构. 在GCN中, $\mathcal{F}$ 是基于度的加权求和, GAT中是基于Attention的加权求和.</p><h2 id="Graph-Sampling"><a href="#Graph-Sampling" class="headerlink" title="Graph Sampling"></a>Graph Sampling</h2><p>因为节点和节点间存在<strong>依存关系</strong>, 并不像欧式数据那样可以采用MiniBatch的方法训练, 所以在大规模图中, 一般没有办法将算法直接应用于整张图, 例如GCN, 每次更新需要对所有节点依次聚合更新. 故需要一种能够从图中采样, 获取有效<strong>子图</strong>的方法, 在子图上应用我们前面说过的算法, 这种方法就是<strong>图采样</strong>. </p><p>但子图采样并不是随机采样, 我们最起码要保证采样完后的图是<strong>连通</strong>的:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial20.jpg" style="zoom: 33%;" /><h3 id="GraphSAGE"><a href="#GraphSAGE" class="headerlink" title="GraphSAGE"></a>GraphSAGE</h3><p><strong>Graph</strong> <strong>SA</strong>mple &amp; aggre<strong>G</strong>at<strong>E</strong>, <strong>GraphSAGE</strong> 是最简单的图采样算法. 它与GCN其实差别不大. 它分为邻居采样, 邻居聚合, 节点预测三个步骤.</p><p>假设有下面这么一张图, 我们需要求出0号节点的表示, 所以需要从0号节点开始采样:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial21.jpg" style="zoom: 35%;" /><p>我们<strong>从内至外</strong>地<strong>采样</strong>0号节点的第N阶邻居, 假设一阶邻居随机采样到了2, 4, 5号节点, 然后采样二阶邻居8, 9, 11, 12, 13, 15:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial22.jpg" style="zoom: 40%;" /><p>这样就抽出了一张子图, 然后可以<strong>由外至内</strong>地<strong>邻居聚合</strong>更新0号节点的表示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial23.jpg" style="zoom: 40%;" /><p>最后就可以通过采样获得的子图来做节点预测了.</p><p>邻居采样有两个优点:</p><ol><li>极大减少了训练计算量.</li><li>在<strong>推断</strong>时允许新的节点的加入, 增强了<strong>泛化</strong>能力.</li></ol><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial24.jpg" style="zoom: 50%;" /><p>因为做了邻居采样, 所以更新未知节点的表示时<strong>不需要使用整张图的节点信息</strong>, 而是只使用由新加入节点后采样的子图节点信息, 所以说允许泛化到新的节点, 也就是所谓的Inductive能力.</p><blockquote><p>关于GraphSAGE在<strong>Inductive</strong>上的能力讨论可以看<a href="https://www.zhihu.com/question/409415383/answer/1361596817" target="_blank" rel="noopener">这里</a>, 我个人是比较赞同答主的说法. 算法能否Inductive和Transductive仅取决于节点输入是否是One Hot, 以及在更新节点表示时是否只依赖于局部子图.</p></blockquote><h3 id="PinSAGE"><a href="#PinSAGE" class="headerlink" title="PinSAGE"></a>PinSAGE</h3><p>仍然是之前的那副图, 我们希望能够更新0号节点的表示. PinSAGE通过<strong>多次随机游走</strong>, 按照路径中节点出现的<strong>频率</strong>, 将这些节点作为邻居. 假设PinSAGE已经做出了四次随机游走:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial25.jpg" style="zoom: 40%;" /><p>根据四次游走中节点出现的频率排序, 5, 10, 11这三个节点频率较高, 让它们作为0号节点的虚拟邻居:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial26.jpg" style="zoom: 33%;" /><p>所以说, PinSAGE所采样得到的子图不一定选取了真实的邻居节点, 这样做使得节点能够快速的获取到<strong>高阶邻居</strong>的信息, 有点类似于ResNet中的<strong>Residual Connection</strong>的作用, 避免了聚合过程中由于距离过远而损失信息的缺点.</p><h2 id="Neighborhood-Aggregation"><a href="#Neighborhood-Aggregation" class="headerlink" title="Neighborhood Aggregation"></a>Neighborhood Aggregation</h2><p>邻居聚合是在图采样之后做的操作, 不同的聚合方式可以达到不同效果. 经典的聚合函数有:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial27.jpg" style="zoom: 40%;" /><p>评估聚合表达能力的指标是<strong>单射</strong>, 单射能保证对聚合以后的结果<strong>可区分</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial28.jpg" style="zoom: 40%;" /><p>对于不同的子图, SUM也保留了单射能力:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial29.jpg" style="zoom: 40%;" /><p>因此, 就有基于单射的GIN(<strong>G</strong>raph <strong>I</strong>somorphism <strong>N</strong>et)模型:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial30.jpg" style="zoom: 40%;" /><p>它的聚合方式就是具有单射能力的<strong>SUM</strong>, 但是为了区分中心节点与邻居, 特意加上了$\mathcal{E}$ :</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gnntutorial31.jpg" style="zoom: 40%;" /><p>当然, GCN, GAT这类的聚合函数都是相较于经典聚合函数更为复杂的.</p><h2 id="Recommended"><a href="#Recommended" class="headerlink" title="Recommended"></a>Recommended</h2><p>涉及到的原论文和图片出处:</p><ul><li>DeepWalk: <a href="https://dl.acm.org/doi/abs/10.1145/2623330.2623732" target="_blank" rel="noopener">DeepWalk: online learning of social representations</a></li><li>Node2Vec: <a href="https://dl.acm.org/doi/abs/10.1145/2939672.2939754" target="_blank" rel="noopener">node2vec: Scalable Feature Learning for Networks</a></li><li>GCN: <a href="https://arxiv.org/abs/1609.02907" target="_blank" rel="noopener">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li>GAT: <a href="https://arxiv.org/abs/1710.10903" target="_blank" rel="noopener">Graph Attention Networks</a></li><li>MPNN: <a href="https://arxiv.org/abs/1704.01212" target="_blank" rel="noopener">Neural Message Passing for Quantum Chemistry</a></li><li>GraphSAGE: <a href="https://arxiv.org/abs/1706.02216" target="_blank" rel="noopener">Inductive Representation Learning on Large Graphs</a></li><li>PinSAGE: <a href="https://dl.acm.org/doi/abs/10.1145/3219819.3219890" target="_blank" rel="noopener">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</a></li><li>GIN: <a href="https://arxiv.org/abs/1810.00826" target="_blank" rel="noopener">HOW POWERFUL ARE GRAPH NEURAL NETWORKS?</a></li><li>综述类论文: <a href="https://arxiv.org/abs/1901.00596" target="_blank" rel="noopener">A Comprehensive Survey on Graph Neural Networks</a></li></ul><p>除去文中提到的资料, 其他涉及到的参考资料:</p><ul><li><a href="https://wmathor.com/index.php/archives/1531/" target="_blank" rel="noopener">GNN详解</a>, <a href="https://wmathor.com/index.php/archives/1532/" target="_blank" rel="noopener">GCN详解</a>, <a href="https://wmathor.com/index.php/archives/1533/" target="_blank" rel="noopener">GraphSAGE &amp; PinSAGE详解</a></li><li><a href="http://web.stanford.edu/class/cs224w/" target="_blank" rel="noopener">CS224W: Machine Learning with Graphs</a></li><li><a href="https://www.bilibili.com/video/BV1G54y1971S" target="_blank" rel="noopener">台大李宏毅助教讲解GNN图神经网络</a></li><li><a href="https://aistudio.baidu.com/aistudio/education/group/info/1956" target="_blank" rel="noopener">图神经网络7日打卡营</a></li><li><a href="https://www.bilibili.com/video/BV19U4y1s7cv" target="_blank" rel="noopener">深入浅出GCN、GAT、GraphSage，MPNN等图神经网络模型【贪心学院】</a></li><li><a href="https://zhuanlan.zhihu.com/p/76001080" target="_blank" rel="noopener">GNN综述——从入门到入门</a></li><li><a href="https://blog.csdn.net/weixin_45901519/article/details/106492963" target="_blank" rel="noopener">图卷积神经网络笔记——第三章：空域图卷积介绍（1）</a></li><li><a href="https://wangsp.blog.csdn.net/article/details/100709692" target="_blank" rel="noopener">GCN—图卷积神经网络理解</a></li></ul><p>PyTorch代码:</p><ul><li>GCN: <a href="https://github.com/tkipf/pygcn" target="_blank" rel="noopener">Graph Convolutional Networks in PyTorch</a></li><li>GAT: <a href="https://github.com/Diego999/pyGAT" target="_blank" rel="noopener">Pytorch Graph Attention Network</a></li><li>GraphSage: <a href="https://github.com/twjiang/graphSAGE-pytorch" target="_blank" rel="noopener">A PyTorch implementation of GraphSAGE</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ConvBERT: Improving BERT with Span-based Dynamic Convolution</title>
      <link href="/posts/22934.html"/>
      <url>/posts/22934.html</url>
      
        <content type="html"><![CDATA[<h1 id="ConvBERT-Improving-BERT-with-Span-based-Dynamic-Convolution"><a href="#ConvBERT-Improving-BERT-with-Span-based-Dynamic-Convolution" class="headerlink" title="ConvBERT: Improving BERT with Span-based Dynamic Convolution"></a>ConvBERT: Improving BERT with Span-based Dynamic Convolution</h1><blockquote><p>本文前置知识:</p><ul><li>Light Weight Convolution: 详见<a href="https://adaning.github.io/posts/40162.html">基于轻量级卷积和动态卷积替代的注意力机制</a>.</li><li>Depthwise Separable Convolution, Group Convolution: 详见<a href="https://adaning.github.io/posts/13629.html">深度可分离卷积与分组卷积</a>.</li><li>BERT: 详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>.</li></ul></blockquote><p>本文是论文<a href="https://arxiv.org/abs/2008.02496" target="_blank" rel="noopener">ConvBERT: Improving BERT with Span-based Dynamic Convolution</a>的阅读笔记和个人理解. 属于随缘填坑系列.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者发现, BERT的中所使用的Self Attention每次请求都需要全局的信息, 但实际上并不是每次Attention都需要全局信息, 有时只需要<strong>局部信息</strong>即可, 所以Attention总是伴随着<strong>计算冗余</strong>, 总体来看BERT也没有高度局部化的操作, 但很多头部都只学习了自然语言局部信息.</p><p>因此, 作者希望能够使用一种<strong>直接捕捉局部信息</strong>的方法, 从而降低Attention的复杂度. </p><h2 id="ConvBERT"><a href="#ConvBERT" class="headerlink" title="ConvBERT"></a>ConvBERT</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>既然作者是希望能够获得捕捉局部信息的能力, 还对参数压缩有一定的需求, 那么很容易就想到了用<strong>卷积</strong>.</p><p>在经典Transformer中的Attention机制的公式为:</p><p>$$<br>\operatorname{Self}-\operatorname{Attn}(Q, K, V)=\operatorname{softmax}\left(\frac{Q^{\top} K}{\sqrt{d_{k}}}\right) V<br>$$</p><p>即最简单的缩放点积, 在这里就不多做说明了.</p><h4 id="Parameters-Redundancy"><a href="#Parameters-Redundancy" class="headerlink" title="Parameters Redundancy"></a>Parameters Redundancy</h4><p>Self - Attention在处理问题时可能会存在参数冗余的问题:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/convbert2.jpg" style="zoom: 50%;" /><p>从BERT的注意力中能够看到, 相当多的权重被分配在在<strong>对角线邻近</strong>的位置上, 说明了自然语言结构中的邻近信息在很多时候是起作用的, 这也引起了相当多的权重<strong>冗余</strong>.</p><h4 id="Convolutional-based-Attention"><a href="#Convolutional-based-Attention" class="headerlink" title="Convolutional based Attention"></a>Convolutional based Attention</h4><p>在前面的研究工作中介绍过, 使用卷积能够一定程度上来缓解参数冗余的问题, 例如<a href="https://adaning.github.io/posts/40162.html">轻量级卷积</a>:<br>$$<br>\operatorname{LConv}(X, W, i)=\sum_{j=1}^{k} W_{j} \cdot X_{\left(i+j-\left\lceil\frac{k+1}{2}\right\rceil\right)}<br>$$</p><p>以及以轻量级卷积为基础, 做出一定改进的动态卷积:</p><p>$$<br>\operatorname{DConv}\left(X, W_{f}, i\right)=\operatorname{LConv}\left(X, \operatorname{softmax}\left(W_{f} X_{i}\right), i\right)<br>$$</p><h3 id="Span-based-Dynamic-Convolution"><a href="#Span-based-Dynamic-Convolution" class="headerlink" title="Span - based Dynamic Convolution"></a>Span - based Dynamic Convolution</h3><p>在<a href="https://adaning.github.io/posts/40162.html">基于轻量级卷积和动态卷积替代的注意力机制</a>中提到过:</p><blockquote><p>注意力权重的生成只取决于<strong>当前时刻的输入</strong>, 而与前时刻和后时刻输入无关, 这是一个严重缺陷.</p></blockquote><p>基于这个改进点, 作者将当前时刻输入的邻近信息也加入了动态调整当前时刻输出的机制, 并称之为<strong>区间动态卷积</strong>.</p><p>自注意力, 动态卷积, 区间动态卷积这三者之间的差别能够很容易的用如下图示对比, 也可以顺带引出区间动态卷积的本质:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/convbert1.jpg" style="zoom: 50%;" /><p>自注意力与所有基于卷积方法, 特征之间有更加<strong>稠密</strong>的影响, 但在作者的论文中认为许多交互是不必要的, 即存在特征冗余, 导致了模型参数的冗余. 动态卷积的当前时刻输出仅会取决于<strong>当前时刻输入</strong>, 结合<strong>周围语义</strong>动态调整权重的能力比较差, 因为它分配权重时完全没有结合上下文信息. 区间动态卷积算是在二者之间取了个<strong>折中</strong>, 能结合<strong>小区间范围</strong>内的信息对卷积核的参数做更好的权重分配, 即能够契合语言局部性的特点, 也能结合<strong>语境</strong>做出判断.</p><p>因此, 考虑将区间动态卷积和自注意力兼容, 需要考虑一种结合区间信息<strong>动态生成卷积核</strong>的方法. 作者使用输入$X$ 用Self - Attention的方式生成对应的$Q$ 和 $V$, 接着使用<strong>深度可分离卷积</strong>抽取与区间内容相关的$K_s$, 然后动态地生成卷积核权重:</p><p>$$<br>f\left(Q, K_{s}\right)=\operatorname{softmax}\left(W_{f}\left(Q \odot K_{s}\right)\right)<br>$$</p><p>$\odot$ 为逐元素点乘, $W_f$ 为可训练的权重矩阵. </p><p>自注意力, 动态卷积, 区间动态卷积三者的运算流程结构图如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/convbert3.jpg" style="zoom:33%;" /><p>明显的能看出来, 在附加了局部信息后, 区间动态卷积与动态卷积相比更像自注意力, 仅替换了某些线性算子, 相较动态卷积而言去掉了GLU, 不会引入过多的参数. </p><p>将这种生成卷积核的方式融入轻量级卷积, 称为区间动态卷积:<br>$$<br>\operatorname{SDConv}\left(Q, K_{s}, V ; W_{f}, i\right)=\operatorname{LConv}\left(V, \operatorname{softmax}\left(W_{f}\left(Q \odot K_{s}\right)\right), i\right)<br>$$</p><p>与动态卷积的式子相比, 将$\operatorname{LConv}$ 中的第一个参数由输入$X$ 替换为自注意力模式中的$V$, 并将生成卷积核参数的方式替换为结合区间信息的$\operatorname{softmax}\left(W_{f}\left(Q \odot K_{s}\right)\right)$.</p><h3 id="ConvBERT-Architecture"><a href="#ConvBERT-Architecture" class="headerlink" title="ConvBERT Architecture"></a>ConvBERT Architecture</h3><h4 id="Mixed-Attention"><a href="#Mixed-Attention" class="headerlink" title="Mixed Attention"></a>Mixed Attention</h4><p>因为自注意力机制和区间动态卷积是可以兼容的, 所以可以将这二者以某种形式混合起来.</p><p>混合Attention就是将区间动态卷积与Self - Attention做了混合, 这二者之间是没有交互的, 直接通过一个Concat操作将二者拼接起来:</p><p>$$<br>\text {Mixed-Attn}\left(K, Q, K_{s}, V ; W_{f}\right)=\operatorname{Cat}\left(\text {Self-Attn}(Q, K, V), \operatorname{SDConv}\left(Q, K_{s}, V ; W_{f}\right)\right)<br>$$</p><p>这样的设计可以让模型不仅限于局部特征的捕捉, 而是以<strong>多角度</strong>来捕捉整个文本的信息. </p><p>自注意力和区间动态卷积共享相同的$Q, V$, 使用不同的方式来生成$K$.</p><h4 id="Bottleneck-Design-for-Self-Attention"><a href="#Bottleneck-Design-for-Self-Attention" class="headerlink" title="Bottleneck Design for Self - Attention"></a>Bottleneck Design for Self - Attention</h4><p>针对冗余参数, Bottleneck的设计能够有助于模型学习到更<strong>紧凑</strong>的信息:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/convbert4.jpg" style="zoom:33%;" /><p>作者提出的Bottleneck加入<strong>缩放因子</strong>$\gamma$ , 当$\gamma&gt;1$ 时, 将特征维数缩小为$d/\gamma$, 并降低注意力头的数量为原来的$1/\gamma$. </p><p>同时, 这样的设计也可以保证模块的有序堆叠, 输入大小和输出大小一致.</p><h4 id="Grouped-Feed-Forward-Module"><a href="#Grouped-Feed-Forward-Module" class="headerlink" title="Grouped Feed - Forward Module"></a>Grouped Feed - Forward Module</h4><p>因为FFN中占了很多参数, 所以作者希望通过<strong>分组</strong>的方式来减小开销. 与多头注意力类似, 分组卷积分别将特征分组提取后再Concat起来:</p><p>$$<br>\displaylines{<br>M=\Pi_{i=0}^{g}\left[f_{\frac{d}{g} \rightarrow \frac{m}{g}}^{i}\left(H_{\left[:, i-1: i \times \frac{d}{g}\right]}\right)\right], M^{\prime}=\operatorname{GeLU}(M)<br>\\<br>H^{\prime}=\Pi_{i=0}^{g}\left[f_{\frac{m}{g} \rightarrow \frac{d}{g}}^{i}\left(M_{\left[:, i-1: i \times \frac{m}{g}\right]}^{\prime}\right)\right]<br>}<br>$$</p><p>其中$H, H^\prime \in R^{n\times n}$,   $M, M^\prime \in R^{n\times n}$, $f_{d_1 \rightarrow d_2}(\cdot)$ 表示将$d_1$ 维映射到$d_2$ 维的FC层. $g$ 为分组的组数, $\Pi$ 为Concat操作.</p><p>后续的实验表明, 精度下降可以忽略不计.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>在实验中, 作者使用了开源网页数据集OpenWebText(32G)来对标BERT所用的训练数据. 作者采用$\gamma=2$ 来缩小特征维度, 注意力头的数量也为原来的一半. 其余详细的实验设置请参考原论文.</p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>消融实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/convbert5.jpg" style="zoom:33%;" /><p>小Trick对提升模型性能有一些帮助, 但总体来说区间动态卷积的提升比较大.</p><h4 id="Kernel-Size"><a href="#Kernel-Size" class="headerlink" title="Kernel Size"></a>Kernel Size</h4><p>对于Kernel Size实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/convbert6.jpg" style="zoom: 50%;" /><p>模型性能先是随着卷积核的大小提升而提升, 在达到一定阈值后性能反而下降. 很有可能是感受野逐渐扩大, 覆盖到整个输入序列, 此前性能均在提升. 而Kernel Size覆盖整个输入序列时, 模型效果稍有下降.</p><p>这个实验再一次佐证了作者的猜想.</p><h4 id="Ways-to-integrate-convolution"><a href="#Ways-to-integrate-convolution" class="headerlink" title="Ways to integrate convolution"></a>Ways to integrate convolution</h4><p>如下作者探究不同形式的卷积对模型性能的影响:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/convbert7.jpg" style="zoom:33%;" /><p>肯定是区间动态卷积比较好, 因为区间动态卷积本身就是由其他卷积形式变换而来的.</p><h3 id="Comparison-results"><a href="#Comparison-results" class="headerlink" title="Comparison results"></a>Comparison results</h3><h4 id="GLUE"><a href="#GLUE" class="headerlink" title="GLUE"></a>GLUE</h4><p>其中标有十字架标志的模型是基于知识蒸馏的方法, 在GLUE上的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/convbert8.jpg" style="zoom:33%;" /><p>ConvBERT在低资源的情况下结果良好.</p><h4 id="SQuAD"><a href="#SQuAD" class="headerlink" title="SQuAD"></a>SQuAD</h4><p>在问答数据集SQuAD上的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/convbert9.jpg" style="zoom:33%;" /><p>延长训练后, 虽然训练所需的计算量下降了, 但是仍然能够达到近似ELECTRA的效果. 极大地减少了训练成本.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>本文中仍然是一篇NLP领域的论文, 我读它的原因主要是为了获取在Attention上改动的灵感.</p><p>作者在轻量级卷积的基础上进一步的将卷积核的权重生成动态化. 设计了一种新型Bottleneck, 进一步的优化了BERT的结构, 并在其中加入了卷积操作, 大幅的降低了训练成本, 但似乎这种方法<strong>不够简洁</strong>. </p><p>作者没有将自注意力完完全全的去掉, 也说明了NLP中Self - Attention必然是不能被其他方法直接取代的.</p><p>后面实验主要也对比的Baseline主要也是小模型系列, 不知与大模型相比效果如何(虽然有点不公平).</p><p>训练过程中也使用了一些小Trick, 没有这些小Trick可能不会有特别亮眼的效果. </p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> CNN </tag>
            
            <tag> Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PPKE: Knowledge Representation Learning by Path-based Pre-training</title>
      <link href="/posts/11653.html"/>
      <url>/posts/11653.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>BERT(Transformer Encoder).</li></ul></blockquote><h1 id="PPKE-Knowledge-Representation-Learning-by-Path-based-Pre-training"><a href="#PPKE-Knowledge-Representation-Learning-by-Path-based-Pre-training" class="headerlink" title="PPKE: Knowledge Representation Learning by Path-based Pre-training"></a>PPKE: Knowledge Representation Learning by Path-based Pre-training</h1><p>本文是论文<a href="https://arxiv.org/abs/2012.03573" target="_blank" rel="noopener">PPKE: Knowledge Representation Learning by Path-based Pre-training</a>的个人理解和阅读笔记. 论文本身非常短, 公式插图风格完全沿用了<a href="https://adaning.github.io/posts/42304.html">CoKE</a>, 也出自同一研究小组.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>传统的KGE方法将三元组作为一个训练单元, 但忽略了图中存在的<strong>上下文拓扑结构信息</strong>. 与语言模型中的上下文一样, 关系路径能够被认为是KG中的一种上下文关系, 作者称之为”<strong>图上下文信息</strong>“.</p><p>不可靠关系在KG中常见, 但使用可靠的关系做KRL是非常必要的, 尤其是在涉及到<strong>推理</strong>的问题上.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ppke1.jpg" style="zoom:33%;" /><blockquote><p>作者还指出, 前人所发现的基于路径或上下文信息的方法都无法通过基准数据集来证明图上下文信息能改善模型性能.</p></blockquote><p>因此, 作者尝试提出基于<strong>路径</strong>的<strong>预训练</strong>KGE模型PPKE(<strong>P</strong>ath - based <strong>P</strong>re - training model to learn <strong>K</strong>nowledge <strong>E</strong>mbeddings), 目标是将实体间的图上下文信息集成进KGE模型的参数中, 并且它是一种<strong>预训练模型</strong>.</p><h2 id="PPKE"><a href="#PPKE" class="headerlink" title="PPKE"></a>PPKE</h2><p>如果你已经了解了CoKE, 那么</p><p>基于<strong>预训练模型</strong>的思路, 模型的训练分为<strong>Pre - train</strong>和<strong>Tuning</strong>两个部分.</p><h3 id="Path-Based-Pre-training"><a href="#Path-Based-Pre-training" class="headerlink" title="Path - Based Pre - training"></a>Path - Based Pre - training</h3><h4 id="Input-Representation"><a href="#Input-Representation" class="headerlink" title="Input Representation"></a>Input Representation</h4><p>头实体$h$ 到尾实体$t$ 的路径输入被表示为$\left\{h, r_{1}, \ldots, r_{n}, t\right\}$, 其中$r_i$ 代表长度为$n$ 的路径中第$i$ 跳的关系, </p><p>那么三元组$\left\{ h, r, t \right\}$ 可以被认为是路径长度$n=1$ 时的一个特殊情况.</p><p>与CoKE不太一样的是, 为了<strong>避免位置偏差</strong>, 作者将尾实体的位置放到了<strong>头实体后</strong>, <strong>关系路径之前</strong>. 这点也是与<strong>CoKE</strong>的<strong>最大不同点</strong>.</p><p>故, 三元组的输入形式应该为:<br>$$<br>\boldsymbol{x}=\left\{h, t, r_{1}, \ldots, r_{n}\right\}<br>$$<br>为了区分实体和关系的不同角色, 必须对它们加以不同的<strong>位置编码</strong>. </p><blockquote><p>这点与CoKE相同, CoKE也没有对不同的元素加以不同的类型编码, 实体和关系直接不加以区分, 只加位置编码.</p></blockquote><p>如果将输入变为Embedding的形式, 如下所示:<br>$$<br>\mathbf{E} = \left\{\mathbf{E}^{h}, \mathbf{E}^{t}, \mathbf{E}^{r_{1}}, \ldots, \mathbf{E}^{r_{n}}\right\}<br>$$</p><h4 id="Masked-Entity-Predicition"><a href="#Masked-Entity-Predicition" class="headerlink" title="Masked Entity Predicition"></a>Masked Entity Predicition</h4><p>与BERT的食用方法相同, 也是用Masked Language Model的训练方式来训练. 针对Link Prediction任务, 只需要将要预测的实体所在位置打上<code>[Mask]</code>. 例如:<br>$$<br>\begin{aligned}<br>\mathrm{Input} &amp;=[\text {Barack Obama}] [\text{MASK}] [place\  of\  birth] [country]  \\<br>\mathrm{Label} &amp;=[\text{USA}]<br>\end{aligned}<br>$$<br>其中$[\cdot]$ 代表输入的元素.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ppke2.jpg" style="zoom: 33%;" /><p>与上图一致, PPKE使用Transformer Encoder对图上下文信息编码, 假设$e$ 为被Mask的实体, $\mathbf{T}^{[\mathrm{MASK}]}$ 代表$[\text{MASK}]$ 位置上的隐态输出, 那么目标就是最大化$[\text{MASK}]$ 位置的输出分类概率:<br>$$<br>\begin{aligned}<br>\boldsymbol{p}^{[\mathrm{MASK}]} &amp;=\operatorname{softmax}\left(\mathbf{T}^{[\mathrm{MASK}]} \cdot \mathbf{V}^{\mathcal{E}}\right) \\<br>\mathcal{L} &amp;=-\log \boldsymbol{p}_{e}^{[\mathrm{MASK}]}<br>\end{aligned}<br>$$<br>其中$\boldsymbol{p}^{[\mathrm{MASK}]} $ 为实体词表中的候选实体的概率向量, $\mathbf{V}^{\mathcal{E}}$ 为实体的Embedding矩阵.</p><p>然后用极大似然来优化模型参数:<br>$$<br>\hat{\boldsymbol{\theta}}=\underset{\boldsymbol{\theta}}{\operatorname{argmax}} \sum_{\left(\boldsymbol{x}, \boldsymbol{m}_{e}\right) \in \mathcal{X}} \log p\left(\boldsymbol{x}_{e} \mid \boldsymbol{x} \circ \boldsymbol{m}_{e} ; \boldsymbol{\theta}\right)<br>$$</p><h3 id="Fine-tuning"><a href="#Fine-tuning" class="headerlink" title="Fine - tuning"></a>Fine - tuning</h3><h4 id="Link-Prediction"><a href="#Link-Prediction" class="headerlink" title="Link Prediction"></a>Link Prediction</h4><p>链接预测的目标是预测三元组中缺失的头实体$h$ 或者尾实体$t$. 即预测$((h, r, ?) \rightarrow t)$, 或者$((?, r, t) \rightarrow h)$.</p><p>详细的例子如下:<br>$$<br>\begin{aligned}<br>\text { Input }_{\text {head }}&amp;=[\text{MASK}] \text {[USA]}[country\ of\ citizenship] \\<br>\text { Label }_{\text {head }}&amp;=[\text {Barack Obama}] \\<br>\text { Input }_{\text {tail }}&amp;=[\text {Barack Obama}] \text {[MASK]} [country\ of\ citizenship]\\<br>\text { Label }_{\text {tail }}&amp;=[\text{USA}]<br>\end{aligned}<br>$$<br>对于这类任务, 给定训练集$\mathcal{D}_{x e}$, 训练目标是最大化<strong>给定数据集</strong>中由输入预测出实体$e$ 的概率:</p><p>$$<br>\hat{\boldsymbol{\theta}}_{\boldsymbol{x} \rightarrow \boldsymbol{e}}=\underset{\boldsymbol{\theta}_{\boldsymbol{x} \rightarrow e}}{\operatorname{argmax}} \sum_{\boldsymbol{x} \in \mathcal{D}_{\boldsymbol{x} e}} \log p\left(e \mid \boldsymbol{x} ; \boldsymbol{\theta}_{\boldsymbol{x} \rightarrow \boldsymbol{e}}\right)<br>$$</p><h4 id="Relation-Prediction"><a href="#Relation-Prediction" class="headerlink" title="Relation Prediction"></a>Relation Prediction</h4><p>与Link Prediction类似, 任务由预测三元组中的头尾实体变为了预测它们之间的关系$r$,  即$((h, ?, t) \rightarrow r)$. 例如:<br>$$<br>\begin{aligned}<br>\text { Input }_{\text {rel }}&amp;=[\text {Barack Obama}][\text {USA}] \text {[MASK]} \\<br>\text { Label }_{\text {rel }}&amp;=[country\ of\ citizenship]<br>\end{aligned}<br>$$<br>同样的, 给定训练集$\mathcal{D}_{\boldsymbol{x} \boldsymbol{r}}$, 目标是最大化给定数据集中由输入预测出关系$r$ 的概率:<br>$$<br>\hat{\boldsymbol{\theta}}_{\boldsymbol{x} \rightarrow \boldsymbol{r}}=\underset{\boldsymbol{\theta}_{\boldsymbol{x} \rightarrow r}}{\operatorname{argmax}} \sum_{\boldsymbol{x} \in \mathcal{D}_{\boldsymbol{x} r}} \log p\left(r \mid \boldsymbol{x} ; \boldsymbol{\theta}_{\boldsymbol{x} \rightarrow \boldsymbol{r}}\right)<br>$$</p><blockquote><p>所以针对任务和数据集进行Fine - tuning时, 作者使用的全部是<strong>三元组</strong>.</p></blockquote><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>作者主要针对FB15k, FB15k - 237, WN18RR这三个数据集做了实验. </p><h3 id="Settings"><a href="#Settings" class="headerlink" title="Settings"></a>Settings</h3><p>在预训练的阶段, 作者只使用了两跳关系组成的组合$(h, r_1, r_2, t)$, 称为<strong>四元组</strong>进行预训练. 受制于计算资源的限制, 作者在FB15k - 237和FB15k上只随机选取了一部分作为数据集. 其余的参数设置请参见原论文.</p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><h4 id="Link-Prediction-1"><a href="#Link-Prediction-1" class="headerlink" title="Link Prediction"></a>Link Prediction</h4><p>在FB15k - 237和WN18RR上的Link Prediction结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ppke3.jpg" style="zoom:33%;" /><p>使用的Baseline比较有代表性, 也包括了之前的工作CoKE. PPKE取得了不错的效果, 尤其是在WN18RR上表现比较好. </p><blockquote><p>当然我觉得可能还有来自于训练方式上的收益, 预训练可能会带来更高的精度.</p></blockquote><h4 id="Relation-Prediction-1"><a href="#Relation-Prediction-1" class="headerlink" title="Relation Prediction"></a>Relation Prediction</h4><p>作者在FB15k和FB15k - 237上的Relation Prediction结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ppke4.jpg" style="zoom:33%;" /><p>能够看到, 作者给出的Baseline只在FB15k上有结果, 而FB15k - 237上只与CoKE做了对比. 预测数据集中的关系比预测实体要简单的多. 所以CoKE和PPKE的差距并不是很大.</p><h3 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h3><h4 id="Ablation-study"><a href="#Ablation-study" class="headerlink" title="Ablation study"></a>Ablation study</h4><p>为探究PPKE取得好效果的原因, 作者将CoKE, 预训练和未预训练的PPKE做了对比:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ppke5.jpg" style="zoom: 50%;" /><p>如果只看MRR和Hits@10的话, 不使用预训练的PPKE比CoKE的结果还要差一些, 结果大致相同, 作者认为差异来自于参数差异. 但是引入预训练后, PPKE的性能有很大提升.</p><h4 id="Visual-Illustration"><a href="#Visual-Illustration" class="headerlink" title="Visual Illustration"></a>Visual Illustration</h4><p>同样使用了类似CoKE的实验手法, PPKE也是用T - SNE做了可视化:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ppke6.jpg" style="zoom:33%;" /><p>作者将预训练过程中<strong>头尾实体相同</strong>的<strong>三元组</strong>和<strong>四元组</strong>做采样, 将尾实体Mask掉后让<strong>已经预训练好的模型</strong>去预测尾实体, 然后对得到的隐态输出做降维可视化.</p><p>不同的颜色代表不同的采样组, 相同的颜色中的点拥有一致的头尾实体, 圆点代表三元组预测得出的隐态输出, 倒三角代表四元组预测得出的隐态输出.</p><p>在大多数的采样组中, <strong>聚合度</strong>都很高, 意味着PPKE无论对于三元组和四元组的分类结果都比较好, 包含路径的知识已经被注入到参数中.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>PPKE总体上来说是CoKE的<strong>延续</strong>, 与CoKE差距很小. 其主要贡献是将<strong>预训练思想</strong>引入了KGE领域中(其实这点很重要), 分为预训练和Fine - tuning两个阶段.</p><p>总感觉在预训练和精调时候使用的数据输入模式不太一样有点怪怪的, 但BERT也是一个怎么使用都可以的结构, 再加上作者已经对三元组和四元组输入都做了实验, PPKE能起作用也不足为奇了.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Argparse和Logging</title>
      <link href="/posts/27666.html"/>
      <url>/posts/27666.html</url>
      
        <content type="html"><![CDATA[<h1 id="Argparse和Logging"><a href="#Argparse和Logging" class="headerlink" title="Argparse和Logging"></a>Argparse和Logging</h1><p><strong>Argparse</strong>和<strong>Logging</strong>是Python实验中常用的两个模块. 之前没有整理过, 特此整理.</p><h2 id="Argparse"><a href="#Argparse" class="headerlink" title="Argparse"></a>Argparse</h2><p>Argparse是用来<strong>解析Python命令行</strong>的<strong>标准库</strong>. </p><h3 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h3><p>大致使用框架如下:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> argparseparser <span class="token operator">=</span> argparse<span class="token punctuation">.</span>ArgumentParser<span class="token punctuation">(</span>description<span class="token operator">=</span><span class="token string">"This is the description for this python script"</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">'''Add some arugments...'''</span>args <span class="token operator">=</span> parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在命令行中调用脚本时可以<strong>附加参数</strong>:</p><pre class="line-numbers language-sh"><code class="language-sh">python argparse_test.py -n anning<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>总体来说, 流程如下:</p><ol><li>需要导入<code>Argparse</code>.</li><li>创建一个<code>parser</code>, 并对其添加各种参数.</li><li>使用<code>parse_args()</code>解析参数.</li><li>使用Python命令行时可以附加相应的参数.</li></ol><h4 id="添加参数"><a href="#添加参数" class="headerlink" title="添加参数"></a>添加参数</h4><p>添加参数统一使用函数<code>parser.add_argument()</code>, 在该函数中可以设置参数的各种<strong>属性</strong>和<strong>约束</strong>. 在下文会逐一说明. </p><h4 id="获取参数"><a href="#获取参数" class="headerlink" title="获取参数"></a>获取参数</h4><p>设定的argument可以作为属性, 在<strong>解析参数后</strong>, 使用<code>args.argument_name</code>来获取.</p><h3 id="位置参数"><a href="#位置参数" class="headerlink" title="位置参数"></a>位置参数</h3><p>位置参数当然是<strong>必选</strong>的参数, 在启动Python脚本时必须按照<strong>位置</strong>依次输入. 例如:</p><pre class="line-numbers language-python"><code class="language-python">parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'name'</span><span class="token punctuation">,</span> type<span class="token operator">=</span>str<span class="token punctuation">,</span> help<span class="token operator">=</span><span class="token string">'Your name'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>添加了参数<code>name</code>, 并将其设置为<code>str</code>类型的变量.</p><h3 id="可选参数"><a href="#可选参数" class="headerlink" title="可选参数"></a>可选参数</h3><p>顾名思义, 在执行脚本时可选参数可能会被用户附加, 也有可能不附加. 参数的添加方式与位置参数类似, 必须在参数名前加<code>-</code>. 约定上<code>-</code>后应只跟单个字母, 为某个参数的<strong>缩写</strong>, <code>--</code>后跟参数的<strong>全拼</strong>. 例如:</p><pre class="line-numbers language-python"><code class="language-python">parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'-n'</span><span class="token punctuation">,</span> type<span class="token operator">=</span>str<span class="token punctuation">,</span> help<span class="token operator">=</span><span class="token string">'Your name'</span><span class="token punctuation">)</span>parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--age'</span><span class="token punctuation">,</span> type<span class="token operator">=</span>int<span class="token punctuation">,</span> help<span class="token operator">=</span><span class="token string">'Your age'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>添加了两个可选参数<code>n</code>和<code>age</code>, 分别为<code>str</code>和<code>int</code>类型.</p><p>当然, 想要同时将缩写和全拼指向同一个变量也是可以的:</p><pre class="line-numbers language-python"><code class="language-python">parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'-a'</span><span class="token punctuation">,</span> <span class="token string">'--age'</span><span class="token punctuation">,</span> type<span class="token operator">=</span>int<span class="token punctuation">,</span> help<span class="token operator">=</span><span class="token string">'Your age'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>此时, 你既可以通过<code>-a</code>, 也可以通过<code>--age</code>来定义年龄.</p><blockquote><p>如果你同时指定了参数的缩写和全拼, 那么在访问该参数值时, 必须通过<strong>全拼</strong>来访问.</p></blockquote><h3 id="默认值和类型"><a href="#默认值和类型" class="headerlink" title="默认值和类型"></a>默认值和类型</h3><p><code>default</code>设定字段的<strong>默认值</strong>, <code>type</code>可以指定附加参数后将其转化为什么<strong>数据类型</strong>. 例如:</p><pre class="line-numbers language-python"><code class="language-python">parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--name'</span><span class="token punctuation">,</span> type<span class="token operator">=</span>str<span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token string">'anning'</span><span class="token punctuation">,</span> help<span class="token operator">=</span><span class="token string">'Your name'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>添加了一个参数<code>name</code>, 当命令行中不显式声明时缺省值为<code>&#39;anning&#39;</code>, 类型为字符串.</p><h3 id="别名"><a href="#别名" class="headerlink" title="别名"></a>别名</h3><p><code>dest</code>可以指定该参数所对应的<code>args</code>的<strong>别名</strong>, 在Python中获取该参数可以通过指定的别名来获取. 例如:</p><pre class="line-numbers language-python"><code class="language-python">parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--name'</span><span class="token punctuation">,</span> dest<span class="token operator">=</span><span class="token string">'user_name'</span><span class="token punctuation">,</span> type<span class="token operator">=</span>str<span class="token punctuation">,</span> help<span class="token operator">=</span><span class="token string">'Your name'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在没指定别名时, 访问参数<code>name</code>可以通过<code>args.name</code>获取, 而指定别名后, 只能通过<code>args.user_name</code>来获取.</p><h3 id="必须参数"><a href="#必须参数" class="headerlink" title="必须参数"></a>必须参数</h3><p><code>required</code>设定参数是否<strong>必须</strong>填入. 否则会提示该参数没有指定, 因此可以将可选参数转化为必选参数.</p><h3 id="动作"><a href="#动作" class="headerlink" title="动作"></a>动作</h3><p><code>action</code>一般常用于可选参数, 例如有参数<code>--verbose</code>时, 该属性设置为True:</p><pre class="line-numbers language-python"><code class="language-python">parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--verbose'</span><span class="token punctuation">,</span> action<span class="token operator">=</span><span class="token string">'store_true'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>此时, 如果在调用脚本时加上了<code>--verbose</code>, 那么在解析后, 对应的<code>args.verbose</code>就为<code>True</code>.</p><p>当然这只是<code>action</code>其中一种用法, 更多请参见节尾处的链接.</p><h3 id="值约束"><a href="#值约束" class="headerlink" title="值约束"></a>值约束</h3><p><code>choices</code>可以设定用户添加参数时, 参数的<strong>取值范围</strong>. 例如:</p><pre class="line-numbers language-python"><code class="language-python">parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--name'</span><span class="token punctuation">,</span>choices<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'anning'</span><span class="token punctuation">,</span> <span class="token string">'daning'</span><span class="token punctuation">]</span> type<span class="token operator">=</span>str<span class="token punctuation">,</span> help<span class="token operator">=</span><span class="token string">'Your name'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>那么用户只能在<code>&#39;anning&#39;</code>和<code>&#39;daning&#39;</code>之间选择<code>name</code>.</p><h3 id="多参数设定"><a href="#多参数设定" class="headerlink" title="多参数设定"></a>多参数设定</h3><p><code>nargs=&#39;N&#39;</code>可以将命令行的N个参数汇总到一个列表中, <code>nargs=&#39;+&#39;</code>或<code>nargs=&#39;*&#39;</code>能将<strong>所有当前参数汇聚到一个列表中</strong>. 例如:</p><pre class="line-numbers language-python"><code class="language-python">parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--salary'</span><span class="token punctuation">,</span> dest<span class="token operator">=</span><span class="token string">'salary'</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> nargs<span class="token operator">=</span><span class="token string">"+"</span><span class="token punctuation">,</span> type<span class="token operator">=</span>int<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在调用脚本时, 能够传入多个符合条件的值, 它们将以<strong>列表</strong>的形式共同存在.</p><h3 id="互斥参数组"><a href="#互斥参数组" class="headerlink" title="互斥参数组"></a>互斥参数组</h3><p>有的时候不希望用户按照自己想象之外的使用方法传入参数, 就需要用到<strong>互斥参数</strong>. 例如:</p><pre class="line-numbers language-python"><code class="language-python">ab_group <span class="token operator">=</span> parser<span class="token punctuation">.</span>add_mutually_exclusive_group<span class="token punctuation">(</span><span class="token punctuation">)</span>ab_group<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'-a'</span><span class="token punctuation">)</span>ab_group<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'-b'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>那么, 在调用Python脚本时, 两个参数是不能同时启用的, 否则会报错.</p><h3 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h3><p><code>var(args)</code>能将解析好后的内容直接转为<strong>字典</strong>.</p><blockquote><p>在使用Notebook时, 与<code>argparse</code>相关的代码以<code>.py</code>形式存在, 不方便直接从里面把参数扒下来, 而模型必须使用<code>args</code>初始化. </p><p>这时可以使用<code>pickle</code>库将<code>args</code>直接以对象的形式<strong>保存</strong>下来, 然后再使用<code>pickle.load</code>在Notebook中重新读取出来, 再对模型初始化. 示例:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> pickle<span class="token comment" spellcheck="true"># 保存</span><span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">'args.pkl'</span><span class="token punctuation">,</span> <span class="token string">'wb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>   pickle<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>args<span class="token punctuation">,</span> f<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 读取</span><span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">'args.pkl'</span><span class="token punctuation">,</span> <span class="token string">'rb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>   args <span class="token operator">=</span> pickle<span class="token punctuation">.</span>load<span class="token punctuation">(</span>f<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><p>上述列举的只是我遇到的关于Argparse的用法, 可能说的比较碎而且不全面. 更多内容请参见<a href="https://docs.python.org/3/library/argparse.html" target="_blank" rel="noopener">Python官方文档</a>和<a href="http://blog.xiayf.cn/2013/03/30/argparse/" target="_blank" rel="noopener">argparse - 命令行选项与参数解析（译）</a>.</p><h2 id="Logging"><a href="#Logging" class="headerlink" title="Logging"></a>Logging</h2><p>Logging可以记载Python脚本运行的过程, 即记录<strong>日志信息</strong>. 其实实验中用到的Logging非常简单.</p><h3 id="Logger"><a href="#Logger" class="headerlink" title="Logger"></a>Logger</h3><h4 id="简单配置"><a href="#简单配置" class="headerlink" title="简单配置"></a>简单配置</h4><p>如果我们只是想简单的记录日志, 不考虑程序的后续维护问题, 那么我们只需要简单的进行<code>logger</code>的初始化. 例如:</p><pre class="line-numbers language-python"><code class="language-python">logging<span class="token punctuation">.</span>basicConfig<span class="token punctuation">(</span>level<span class="token operator">=</span>logging<span class="token punctuation">.</span>DEBUG<span class="token punctuation">,</span>                    filename<span class="token operator">=</span><span class="token string">'output.log'</span><span class="token punctuation">,</span>                    datefmt<span class="token operator">=</span><span class="token string">'%Y/%m/%d %H:%M:%S'</span><span class="token punctuation">,</span>                    format<span class="token operator">=</span><span class="token string">'%(asctime)s - %(name)s - %(levelname)s - %(lineno)d - %(module)s - %(message)s'</span><span class="token punctuation">)</span>logger <span class="token operator">=</span> logging<span class="token punctuation">.</span>getLogger<span class="token punctuation">(</span><span class="token string">"process_name"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="basicConfig参数"><a href="#basicConfig参数" class="headerlink" title="basicConfig参数"></a>basicConfig参数</h4><ul><li><strong>filename</strong>: 即日志输出的文件名, 如果指定了这个信息之后, 实际上会启用 FileHandler, 而不再是 StreamHandler, 这样日志信息便会输出到文件中了. </li><li><strong>filemode</strong>: 这个是指定日志文件的写入方式, 有两种形式, 一种是<code>w</code>, 一种是<code>a</code>, 分别代表清除后写入和追加写入. </li><li><strong>format</strong>: 指定日志信息的输出格式, 详细格式会在后面补出.</li><li><strong>datefmt</strong>: 指定时间的输出格式. </li><li><strong>style</strong>: 如果 format 参数指定了, 这个参数就可以指定格式化时的占位符风格, 如 %、{、$ 等. </li><li><strong>level</strong>: 指定日志输出的类别, 程序会输出大于等于此级别的信息. </li><li><strong>stream</strong>: 在没有指定 filename 的时候会默认使用 StreamHandler, 这时 stream 可以指定初始化的文件流. </li><li><strong>handlers</strong>: 可以指定日志处理时所使用的 Handlers, 必须是可迭代的. </li></ul><h4 id="配置文件配置"><a href="#配置文件配置" class="headerlink" title="配置文件配置"></a>配置文件配置</h4><p>一般情况下, 为了把配置写活, 人们都会将配置写入<strong>配置文件</strong>, 在记录日志的时候, 读取配置文件中的配置, 方便管理.</p><p>无论以何种方式读取文件, 例如<code>yaml</code>, <code>json</code>, 只要读取成Python的字典, 就可以完成初始化, 例如:</p><pre><code>config_dict = json.load(config_path)logging.config.dictConfig(config_dict)logger = logging.getLogger(&quot;process_name&quot;)</code></pre><p>在这里, 以<code>json</code>作为示范, 在配置文件中写入的内容有:</p><pre class="line-numbers language-json"><code class="language-json"><span class="token punctuation">{</span>    <span class="token property">"version"</span><span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>    <span class="token property">"disable_existing_loggers"</span><span class="token operator">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span>    <span class="token property">"formatters"</span><span class="token operator">:</span> <span class="token punctuation">{</span>        <span class="token property">"simple"</span><span class="token operator">:</span> <span class="token punctuation">{</span>            <span class="token property">"format"</span><span class="token operator">:</span> <span class="token string">"%(asctime)s - %(name)s - [%(levelname)s] - %(message)s"</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">,</span>    <span class="token property">"handlers"</span><span class="token operator">:</span> <span class="token punctuation">{</span>        <span class="token property">"file_handler"</span><span class="token operator">:</span> <span class="token punctuation">{</span>            <span class="token property">"class"</span><span class="token operator">:</span> <span class="token string">"logging.FileHandler"</span><span class="token punctuation">,</span>            <span class="token property">"level"</span><span class="token operator">:</span> <span class="token string">"DEBUG"</span><span class="token punctuation">,</span>            <span class="token property">"formatter"</span><span class="token operator">:</span> <span class="token string">"simple"</span><span class="token punctuation">,</span>            <span class="token property">"filename"</span><span class="token operator">:</span> <span class="token string">"python_logging.log"</span><span class="token punctuation">,</span>            <span class="token property">"encoding"</span><span class="token operator">:</span> <span class="token string">"utf8"</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">,</span>    <span class="token property">"root"</span><span class="token operator">:</span> <span class="token punctuation">{</span>        <span class="token property">"level"</span><span class="token operator">:</span> <span class="token string">"DEBUG"</span><span class="token punctuation">,</span>        <span class="token property">"handlers"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"file_handler"</span><span class="token punctuation">]</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Level"><a href="#Level" class="headerlink" title="Level"></a>Level</h3><p>日志记录分为五个级别, 分别为<strong>CRITICAL &gt; ERROR &gt; WARNING &gt; INFO &gt; DEBUG</strong>.</p><p>它们分别可以通过下述命令来记录在日志中:</p><pre class="line-numbers language-python"><code class="language-python">logger<span class="token punctuation">.</span>debug<span class="token punctuation">(</span><span class="token string">"mess"</span><span class="token punctuation">)</span>logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">"mess"</span><span class="token punctuation">)</span>logger<span class="token punctuation">.</span>warning<span class="token punctuation">(</span><span class="token string">"mess"</span><span class="token punctuation">)</span>logger<span class="token punctuation">.</span>error<span class="token punctuation">(</span><span class="token string">"mess"</span><span class="token string">"mess"</span><span class="token punctuation">)</span>logger<span class="token punctuation">.</span>critical<span class="token punctuation">(</span><span class="token string">"mess"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>logger</code>中的<code>level</code>参数可以指定程序输出的信息级别.</p><h3 id="Handler"><a href="#Handler" class="headerlink" title="Handler"></a>Handler</h3><p><code>Handler</code>是<strong>处理日志</strong>的方法. 我们可以不使用<code>basicConfig</code>来配置<code>logger</code>, 而是单独对<code>logger</code>指定处理方法:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> logginglogger <span class="token operator">=</span> logging<span class="token punctuation">.</span>getLogger<span class="token punctuation">(</span><span class="token string">"process_name"</span><span class="token punctuation">)</span>logger<span class="token punctuation">.</span>setLevel<span class="token punctuation">(</span>level<span class="token operator">=</span>logging<span class="token punctuation">.</span>INFO<span class="token punctuation">)</span>handler <span class="token operator">=</span> logging<span class="token punctuation">.</span>FileHandler<span class="token punctuation">(</span><span class="token string">'output.log'</span><span class="token punctuation">)</span>formatter <span class="token operator">=</span> logging<span class="token punctuation">.</span>Formatter<span class="token punctuation">(</span><span class="token string">'%(asctime)s - %(name)s - %(levelname)s - %(message)s'</span><span class="token punctuation">)</span>handler<span class="token punctuation">.</span>setFormatter<span class="token punctuation">(</span>formatter<span class="token punctuation">)</span>logger<span class="token punctuation">.</span>addHandler<span class="token punctuation">(</span>handler<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我们常用的是<code>FileHandler</code>, 这样能将日志以<strong>文件</strong>形式导出. 除此以外, 还有如下<code>Handler</code>:</p><ul><li>StreamHandler: logging.StreamHandler, 日志输出到流, 可以是 sys.stderr, sys.stdout 或者文件. </li><li>FileHandler: logging.FileHandler, 日志输出到文件. </li><li>BaseRotatingHandler: logging.handlers.BaseRotatingHandler, 基本的日志回滚方式. </li><li>RotatingHandler: logging.handlers.RotatingHandler, 日志回滚方式, 支持日志文件最大数量和日志文件回滚. </li><li>TimeRotatingHandler: logging.handlers.TimeRotatingHandler, 日志回滚方式, 在一定时间区域内回滚日志文件. </li><li>SocketHandler: logging.handlers.SocketHandler, 远程输出日志到 TCP/IP sockets. </li><li>DatagramHandler: logging.handlers.DatagramHandler, 远程输出日志到 UDP sockets. </li><li>SMTPHandler: logging.handlers.SMTPHandler, 远程输出日志到邮件地址. </li><li>SysLogHandler: logging.handlers.SysLogHandler, 日志输出到 syslog. </li><li>NTEventLogHandler: logging.handlers.NTEventLogHandler, 远程输出日志到 Windows NT/2000/XP 的事件日志. </li><li>MemoryHandler: logging.handlers.MemoryHandler, 日志输出到内存中的指定 buffer. </li><li>HTTPHandler: logging.handlers.HTTPHandler, 通过”GET” 或者”POST” 远程输出到 HTTP 服务器. </li></ul><p>实际上, 我们可以给<code>logger</code>添加多个<code>Handler</code>:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> logging<span class="token keyword">from</span> logging<span class="token punctuation">.</span>handlers <span class="token keyword">import</span> HTTPHandler<span class="token keyword">import</span> syslogger <span class="token operator">=</span> logging<span class="token punctuation">.</span>getLogger<span class="token punctuation">(</span><span class="token string">"process_name"</span><span class="token punctuation">)</span>logger<span class="token punctuation">.</span>setLevel<span class="token punctuation">(</span>level<span class="token operator">=</span>logging<span class="token punctuation">.</span>DEBUG<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># StreamHandler</span>stream_handler <span class="token operator">=</span> logging<span class="token punctuation">.</span>StreamHandler<span class="token punctuation">(</span>sys<span class="token punctuation">.</span>stdout<span class="token punctuation">)</span>stream_handler<span class="token punctuation">.</span>setLevel<span class="token punctuation">(</span>level<span class="token operator">=</span>logging<span class="token punctuation">.</span>DEBUG<span class="token punctuation">)</span>logger<span class="token punctuation">.</span>addHandler<span class="token punctuation">(</span>stream_handler<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># FileHandler</span>file_handler <span class="token operator">=</span> logging<span class="token punctuation">.</span>FileHandler<span class="token punctuation">(</span><span class="token string">'output.log'</span><span class="token punctuation">)</span>file_handler<span class="token punctuation">.</span>setLevel<span class="token punctuation">(</span>level<span class="token operator">=</span>logging<span class="token punctuation">.</span>INFO<span class="token punctuation">)</span>formatter <span class="token operator">=</span> logging<span class="token punctuation">.</span>Formatter<span class="token punctuation">(</span><span class="token string">'%(asctime)s - %(name)s - %(levelname)s - %(message)s'</span><span class="token punctuation">)</span>file_handler<span class="token punctuation">.</span>setFormatter<span class="token punctuation">(</span>formatter<span class="token punctuation">)</span>logger<span class="token punctuation">.</span>addHandler<span class="token punctuation">(</span>file_handler<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># HTTPHandler</span>http_handler <span class="token operator">=</span> HTTPHandler<span class="token punctuation">(</span>host<span class="token operator">=</span><span class="token string">'localhost:8001'</span><span class="token punctuation">,</span> url<span class="token operator">=</span><span class="token string">'log'</span><span class="token punctuation">,</span> method<span class="token operator">=</span><span class="token string">'POST'</span><span class="token punctuation">)</span>logger<span class="token punctuation">.</span>addHandler<span class="token punctuation">(</span>http_handler<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Log</span>logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">'This is a log info'</span><span class="token punctuation">)</span>logger<span class="token punctuation">.</span>debug<span class="token punctuation">(</span><span class="token string">'Debugging'</span><span class="token punctuation">)</span>logger<span class="token punctuation">.</span>warning<span class="token punctuation">(</span><span class="token string">'Warning exists'</span><span class="token punctuation">)</span>logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">'Finish'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Formatter"><a href="#Formatter" class="headerlink" title="Formatter"></a>Formatter</h3><p>在对日志格式化输出时, 可以不借助<code>basicConfig</code>来全局化<strong>输出格式</strong>, 使用<code>Formatter</code>灵活单独配置:</p><pre class="line-numbers language-python"><code class="language-python">logger <span class="token operator">=</span> logging<span class="token punctuation">.</span>getLogger<span class="token punctuation">(</span><span class="token string">"process_name"</span><span class="token punctuation">)</span>logger<span class="token punctuation">.</span>setLevel<span class="token punctuation">(</span>level<span class="token operator">=</span>logging<span class="token punctuation">.</span>WARN<span class="token punctuation">)</span>formatter <span class="token operator">=</span> logging<span class="token punctuation">.</span>Formatter<span class="token punctuation">(</span>fmt<span class="token operator">=</span><span class="token string">'%(asctime)s - %(name)s - %(levelname)s - %(message)s'</span><span class="token punctuation">,</span> datefmt<span class="token operator">=</span><span class="token string">'%Y/%m/%d %H:%M:%S'</span><span class="token punctuation">)</span>handler <span class="token operator">=</span> logging<span class="token punctuation">.</span>StreamHandler<span class="token punctuation">(</span><span class="token punctuation">)</span>handler<span class="token punctuation">.</span>setFormatter<span class="token punctuation">(</span>formatter<span class="token punctuation">)</span>logger<span class="token punctuation">.</span>addHandler<span class="token punctuation">(</span>handler<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>更多的信息输出格式如下:</p><ul><li>%(levelno) s : 打印日志级别的数值.</li><li>%(levelname) s : 打印日志级别的名称. </li><li>%(pathname) s : 打印当前执行程序的路径, 其实就是 sys.argv [0]. </li><li>%(filename) s : 打印当前执行程序名. </li><li>%(funcName) s : 打印日志的当前函数. </li><li>%(lineno) d : 打印日志的当前行号. </li><li>%(asctime) s : 打印日志的时间. </li><li>%(thread) d : 打印线程 ID. </li><li>%(threadName) s : 打印线程名称. </li><li>%(process) d : 打印进程 ID. </li><li>%(processName) s : 打印线程名称. </li><li>%(module) s : 打印模块名称. </li><li>%(message) s : 打印日志信息. </li></ul><p>上述内容也并非全部的使用方法, 参考了<a href="https://docs.python.org/3/library/logging.html" target="_blank" rel="noopener">Python官方文档</a>和<a href="https://cuiqingcai.com/6080.html" target="_blank" rel="noopener">Python 中 logging 模块的基本用法</a>.</p>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RREA: Relational Reflection Entity Alignment</title>
      <link href="/posts/51197.html"/>
      <url>/posts/51197.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>GNN</li></ul></blockquote><h1 id="Relational-Reflection-Entity-Alignment"><a href="#Relational-Reflection-Entity-Alignment" class="headerlink" title="Relational Reflection Entity Alignment"></a>Relational Reflection Entity Alignment</h1><p>本文是论文<a href="http://arxiv.org/abs/2008.07962" target="_blank" rel="noopener">Relational Reflection Entity Alignment</a>的阅读笔记和个人理解. 这是我第一次接触关于<strong>实体对齐</strong>领域的文章.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>整篇文章的出发点都源于作者观察到的两个<strong>反直觉</strong>的现象:</p><ol><li>在实体对齐问题中, GNN中使用<strong>标准线性变换</strong>表现得不好.</li><li>那些在<strong>链接预测</strong>上表现很好的Knowledge Embedding Model在<strong>实体对齐</strong>上表现不好.</li></ol><p>针对上述两个现象, 作者说明GNN中约束变换矩阵的重要性, 并提出了<strong>统一</strong>的实体对齐框架, 将现有的两种主流方法相统一.</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>因为是第一次接触实体对齐领域的文章, 所以我在这里将实体对齐相关的背景补充上.</p><p>实体对齐的目标是检测<strong>多源</strong>Knowledge Graph中的<strong>实体对</strong>是否<strong>等价</strong>, 这对多源知识图谱的<strong>合并</strong>是至关重要的. 通俗一点说, 在两个KG中, 对于同一种实体可能有两种不同的表述, 通过实体对齐能够将两种表述相统一. 在下一节, 我会补充上实体对齐问题的数学描述.</p><h3 id="Translation-Based-Methods"><a href="#Translation-Based-Methods" class="headerlink" title="Translation - Based Methods"></a>Translation - Based Methods</h3><p>基于平移的方法灵感来自于Word Embedding中的跨语言表示方法, 例如TransE等, 在实体对齐领域使用时, 有一个非常重要的假设: 在不同的Knowledge Graph中各实体的相对位置是<strong>相似</strong>的.</p><p>一般分为两个模块, <strong>Translation Module</strong>和<strong>Alignment Module</strong>.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/relationalreflectionentityalignment1.jpg" style="zoom:50%;" /><h4 id="Translation-Module"><a href="#Translation-Module" class="headerlink" title="Translation Module"></a>Translation Module</h4><p>平移模块负责学习到两个KG中相似的实体位置分布, 例如最简单的TransE使用$h+r\approx t$ 来学习位置分布.</p><h4 id="Alignment-Module"><a href="#Alignment-Module" class="headerlink" title="Alignment Module"></a>Alignment Module</h4><p>对齐模块负责将两个KG中的实体做对齐. 在Translation Based Model中, 一般有两种方式:</p><ul><li><p><strong>Mapping</strong>: 使用变换矩阵将两个KG中的实体统一到一个空间中最小化它们的距离. 例如$W e_{1} \approx e_{2}$ 或 $W_{1} e_{1} \approx W_{2} e_{2}$.</p></li><li><p><strong>Sharing</strong>: 更为激进, 直接<strong>共享</strong>两个KG中实体的Embedding. 例如在MTransE中通过最小化$||\boldsymbol{e}_{1}-\boldsymbol{e}_{2}||$ 达到共享的效果.</p></li></ul><h3 id="GNNs-Based-Methods"><a href="#GNNs-Based-Methods" class="headerlink" title="GNNs - Based Methods"></a>GNNs - Based Methods</h3><p>受<strong>孪生网络</strong>启发, 大多方法使用两个多层的GNN编码, 再加上额外的损失函数. 与基于平移的方法不同, 基于平移的方法使用独立的三元组, 而缺少了实体和关系的全局信息, GNN摆脱了三元组的束缚, 依靠<strong>聚合</strong>从邻居节点获取实体的Embedding.</p><p>在获取完不同KG中的实体Embedding后, 采用Contrastive Loss或Triplet Loss来对齐实体:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/relationalreflectionentityalignment2.jpg" style="zoom:50%;" /><p>许多基于GNN的方法中经常约束变换矩阵为<strong>对角阵</strong>或<strong>单位阵</strong>, 但从未交代原因.</p><h2 id="Preliminary"><a href="#Preliminary" class="headerlink" title="Preliminary"></a>Preliminary</h2><p>本节描述实体对齐任务的数学描述, 并介绍后续实验中使用的数据集, 为接下来的内容做铺垫.</p><h3 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h3><p>一般的KG都定义为$G=(E,R,T)$, 其中$E,R,T$ 分别代表实体, 关系, 和三元组的集合.</p><p>实体对齐的目标是找到两个不同源的KG中的等价的实体对, 例如$G_1, G_2$ 分别为两个不同源的KG, 对齐的实体对表示为:<br>$$<br>P=\left\{\left(e_{i_{1}}, e_{i_{2}}\right) \mid e_{i_{1}} \in E_{1}, e_{i_{2}} \in E_{2}\right\}_{i=1}^{p}<br>$$<br>模型应该能够利用已知的对齐好的实体来预测新的实体对.</p><h3 id="DataSets"><a href="#DataSets" class="headerlink" title="DataSets"></a>DataSets</h3><p>在后续的实验中, 作者主要使用了两种数据集:</p><ul><li>DBP15K: <strong>跨语言</strong>数据集, 其中含有中文(ZH), 英文(EN), 日语(JA), 法语(FR)的数据. 包括三种跨语言的对齐实体对, 分别是ZH - EN, JA - EN, FR - EN.</li><li>DWY100K: <strong>跨KG</strong>数据集, 它是从DBpedia, Wikidata, YAGO3中抽取出来的. 包含两个不同源KG的对齐实体对集, 分别是DBpedia - Wikidata, DBpedia - YAGO3.</li></ul><p>具体信息如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/relationalreflectionentityalignment3.jpg" style="zoom: 33%;" /><h2 id="Unified-Entity-Alignment-Framework"><a href="#Unified-Entity-Alignment-Framework" class="headerlink" title="Unified Entity Alignment Framework"></a>Unified Entity Alignment Framework</h2><p>在本节中, 作者抽取了基于平移的模型和基于GNN的共性, 将二者统一到一个框架下.</p><h3 id="Shape-Builder-amp-Alignment"><a href="#Shape-Builder-amp-Alignment" class="headerlink" title="Shape - Builder &amp; Alignment"></a>Shape - Builder &amp; Alignment</h3><h4 id="Shape-Builder"><a href="#Shape-Builder" class="headerlink" title="Shape - Builder"></a>Shape - Builder</h4><p>Shape - Builder的主要任务是将随机初始化的实体分布调整到合适形状的分布. 例如上图中, 其实任何Knowledge Embedding Model都可以做为Shape - Builder. 但是仍然要注意<strong>前提条件</strong>, <strong>等价实体在不同空间中具有相似的分布</strong>.</p><h4 id="Alignment"><a href="#Alignment" class="headerlink" title="Alignment"></a>Alignment</h4><p>当<strong>空间相似性</strong>保持时, 就能够使用Mapping之类的手段对齐实体:</p><p>$$<br>\min _{W} \sum_{\left(e_{i}, e_{j}\right) \in P}||\boldsymbol{W} \boldsymbol{h}_{e_{i}}-\boldsymbol{h}_{e_{j}}||<br>$$</p><p>其中$(e_i, e_j)$ 是已知对齐的实体对. $\boldsymbol{h}_{e_i}$ 代表实体$e_i$ 的Embedding.</p><p>但是, 当变换矩阵$\boldsymbol{W}$ 不加以任何约束时, 空间相似性是极其容易被<strong>破坏</strong>的. 除非$\boldsymbol{W}$ 能够满足<strong>正交性</strong>来保证空间中的实体只是经过旋转操作, 而不改变它们之间的相对位置.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/relationalreflectionentityalignment4.jpg" style="zoom:50%;" /><p>在上图中有:</p><ul><li>(a): MTransE没有约束变换矩阵$\boldsymbol{W}$, 所以在对齐后产生了一些混乱.</li><li>(b): OTEA对矩阵加以正交约束, 比较好的对齐了实体.</li><li>(c): JAPE直接对两个Knowledge Graph使用相同的实体嵌入, 最后再做Shape - Building操作, 也取得了比较好的效果.</li></ul><h3 id="GNNs-Based-Methods-Are-Also-Subject-to-Unified-Framework"><a href="#GNNs-Based-Methods-Are-Also-Subject-to-Unified-Framework" class="headerlink" title="GNNs - Based Methods Are Also Subject to Unified Framework"></a>GNNs - Based Methods Are Also Subject to Unified Framework</h3><h4 id="GNN-Based-Methods"><a href="#GNN-Based-Methods" class="headerlink" title="GNN Based Methods"></a>GNN Based Methods</h4><p>GNN的循环迭代由<strong>聚合</strong>和<strong>更新</strong>两部分组成:</p><p><strong>聚合</strong>:<br>$$<br>h^l_{N_{e_{i}}^{e}} \leftarrow \text { Aggregate }\left(\left\{\boldsymbol{h}_{e_{k}}^{l}, \forall e_{k} \in\left\{e_{i}\right\} \cup \mathcal{N}_{e_{i}}^{e}\right\}\right)<br>$$</p><p>聚合时包括了节点自身到自身的特征, 即视为节点有一条自己到自己的<strong>闭环</strong>.</p><p>聚合方式$\text{Aggregate}$有多种, 在GCN中是求平均, 在GAT中是加权求和, 不详细展开说了.</p><p><strong>更新</strong>:<br>$$<br>h_{e_{i}}^{l+1} \leftarrow \sigma\left(\boldsymbol{W}^{l} \cdot h_{\mathcal{N}_{e_{i}}^{e}}^{l}\right)<br>$$<br>对邻居节点信息使用第$l$ 层的变换矩阵$\boldsymbol{W}^l$, 能获得更好的节点表示.</p><p><strong>损失函数</strong>:</p><p>基于GNN实体对齐方法的损失函数能使等价实体更近, 让不相关的实体更远:<br>$$<br>L=\sum_{\left(e_{i}, e_{j}\right) \in P} \mathop{\underline{\lVert\boldsymbol{h}_{e_{i}}-\boldsymbol{h}_{e_{j}}\rVert}}<br>\limits_\text {alignment}<br>+<br>\sum_{\left(e_{i}^{\prime}, e_{j}^{\prime}\right) \in P^{\prime}} \max \left(<br>\mathop{<br>\underline{\lVert\boldsymbol{h}_{e_{i}^{\prime}} - \boldsymbol{h}_{e_{j}^{\prime}}\rVert+\lambda}}\limits_{\text {apart}}, 0<br>\right)<br>$$</p><p>其中$\lambda$ 为间距, $(e_i^\prime, e_j^\prime)$ 代表从$(e_i, e_j)$ 中随机替换一个实体的负样本实体对. </p><p>有了Translation Based Models的基础, 这里能很容易的发现, 损失中的第一项$\lVert\boldsymbol{h}_{e_{i}}-\boldsymbol{h}_{e_{j}}\rVert$ 正巧和作者前面提到的Sharing Aligment一致, 而第二项$\lVert\boldsymbol{h}_{e_{i}^{\prime}} - \boldsymbol{h}_{e_{j}^{\prime}}\rVert+\lambda$恰巧和前面提到的Shape - Builder作用相同. 因此基于GNN的方法也可以视为作者提出的统一框架中的一种. 假设基于GNN的方法也属于作者提出的统一框架的一种, 那么经过GNN进行实体对齐后, 一定能保留量KG中实体对的相似性. 下面作者就要通过实验来证明该猜想.</p><h4 id="Visual-Experiment"><a href="#Visual-Experiment" class="headerlink" title="Visual Experiment"></a>Visual Experiment</h4><p>作者使用最简单的GCN - Align, 保留Triplet Loss, 将GCN从监督模型转化为<strong>自监督</strong>模型:<br>$$<br>L_{\text {apart}}=\sum_{\left(e_{i}^{\prime}, e_{j}^{\prime}\right) \in P^{\prime}} \max \left(\lambda-\lVert\boldsymbol{h}_{e_{i}^{\prime}}-\boldsymbol{h}_{e_{j}^{\prime}}\rVert_{1}, 0\right)<br>$$<br>用T - SNE对两个法语和英语的多个实体对齐后的结果进行<strong>降维可视化</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/relationalreflectionentityalignment5.jpg" style="zoom:50%;" /><p>经过GCN对齐后的实体分布确实有很大的<strong>相似性</strong>.</p><h4 id="Quantitative-Experiment"><a href="#Quantitative-Experiment" class="headerlink" title="Quantitative Experiment"></a>Quantitative Experiment</h4><p>光有可视化分析还不够, 作者希望进行量化实验. 作者提出了Shape Similarity来衡量两个Knowledge Graph中任意两实体构成实体对后的相对距离的<strong>差距</strong>:</p><p>$$<br>SS=\frac{\sum_{\left(e_i, \tilde{e_i}\right) \in P} \sum_{\left(e_j, \tilde{e_j}\right) \in P}dist(e_i, e_j) - dist(\tilde{e_i}, \tilde{e_j})}{\sum_{\left(e_i^{\prime}, \tilde{e_i}^{\prime}\right) \in P^{\prime}} \sum_{\left(e_j^{\prime}, \tilde{e_j}^{\prime}\right) \in P^{\prime}}dist(e_i^{\prime}, e_j^{\prime}) - dist(\tilde{e_i}^{\prime}, \tilde{e_j}^{\prime})}<br>$$</p><p>其中, $e_i, e_j \in G_1, \tilde{e_i}, \tilde{e_j} \in G_2$. $e_i, e_j$ 为$G_1$ 中的任意两实体构成的实体对, $\tilde{e_i}, \tilde{e_j}$ 为$G_2$ 中相应的实体构成的实体对. $({e_i}^{\prime}, \tilde{e_i}^{\prime}, {e_j}^{\prime}, \tilde{e_j}^{\prime})$ 代表$({e_i} , \tilde{e_i} , {e_j} , \tilde{e_j} )$ 中随机替换一个负例实体后的四元组. 故$P$ 代表有效的对齐实体对, $P^{\prime}$ 代表无效的负采样的负例实体对. </p><p>因此, 分子为对齐实体之间的相对距离差, 分母为随机实体对之间的距离差. 如果两个KG中的实体分布相似, 分子应该尽可能的小, 分母尽可能的大.</p><p>作者测试了随机分布, TransE, GCN(未训练), GCN对齐后的结果:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/relationalreflectionentityalignment6.jpg" style="zoom:50%;" /><p>这符合作者的预先设想, 随机和未训练的GCN对齐后的余弦距离都非常大, 接近于1. TransE效果比GCN - Align要稍差一些.</p><h3 id="Why-Linear-Transformation-Not-Work"><a href="#Why-Linear-Transformation-Not-Work" class="headerlink" title="Why Linear Transformation Not Work"></a>Why Linear Transformation Not Work</h3><p>基于前文提到的假设, 作者开始对标准线性变换在GNN中不Work的原因分析. </p><p>对于$\boldsymbol{W}$ 加以正交约束或不加约束实际上对应着Translation Based Model中的两种情况:</p><ul><li>当$\boldsymbol{W}$ 为正交矩阵或者单位矩阵$\boldsymbol{I}$ 时, 本质上就等价于Translation Based Model中的<strong>Sharing</strong> Alignment. </li><li>当$\boldsymbol{W}$ 不加以约束时, 本质上等价于Translation Based Model中的<strong>Mapping</strong> Alignment. 这可能会<strong>破坏</strong>掉数据集中的空间分布相似性.</li></ul><h4 id="Experiment-on-GCN-Align"><a href="#Experiment-on-GCN-Align" class="headerlink" title="Experiment on GCN - Align"></a>Experiment on GCN - Align</h4><p>作者使用最简单的GCN在实体对齐任务上, 对加以各类约束和不加约束的设置做了实验, 采用Loss来保证变换矩阵$\boldsymbol{W}$ 的<strong>正交</strong>性:<br>$$<br>L_{o}=\lVert\boldsymbol{W}^{T} \boldsymbol{W}-\boldsymbol{I}\rVert_{2}^{2}<br>$$</p><p>实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/relationalreflectionentityalignment7.jpg" style="zoom:50%;" /><p>在加以正交约束后, 将近涨了10个点. 这足以说明作者的假设成立. 更一般的, 作者发现, 使用更加简单的<strong>单位阵</strong>比正交矩阵的效果要稍好一些.</p><h4 id="Experiment-on-Complex-GNNs"><a href="#Experiment-on-Complex-GNNs" class="headerlink" title="Experiment on Complex GNNs"></a>Experiment on Complex GNNs</h4><p>作者认为对更加复杂的GNN方法做探究也是有必要的, 所以作者也测试了其他GNN方法在使用单位阵或正交阵作为$\boldsymbol{W}$ 的性能:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/relationalreflectionentityalignment8.jpg" style="zoom:50%;" /><p>对于更加复杂的方法, 加以约束仍然能取得更好的效果, 使用单位阵作为$\boldsymbol{W}$ 的结果总是比单纯正交约束要好一些. 作者猜测复杂的方法中所包含的变换矩阵更多, 使得正交约束的优化有些困难.</p><h3 id="Why-Advanced-KG-Embedding-Not-Work"><a href="#Why-Advanced-KG-Embedding-Not-Work" class="headerlink" title="Why Advanced KG Embedding Not Work"></a>Why Advanced KG Embedding Not Work</h3><p>作者将在链接预测上表现比较好的模型在实体对齐中做了测试:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/relationalreflectionentityalignment9.jpg" style="zoom:50%;" /><p>很多在链接预测中表现很好的KGE模型, 在实体对齐中效果很差, 至少比简单的TransE效果差17%, 而基于GNN的方法比GCN至少差3%. 作者将它们汇总在下表中:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/relationalreflectionentityalignment10.jpg" style="zoom:50%;" /><p>其中$\mid\mid$ 代表Concat, $\ast, \omega$ 分别代表卷积操作和卷积核. $d_i$ 代表实体$e_i$ 的度(GCN中需要用到度矩阵来衡量节点的重要性).</p><p>KGE方法本质上的思想都是<strong>将实体嵌入转换为特定的关系嵌入</strong>.</p><blockquote><p>对于这句话, 我认为可以这样理解, 在列出的模型中, 在训练时必然离不开关系$r$, 其实学习实体嵌入也是为了学习到关系$r$ 的表示, 因为关系的嵌入表示不可能在不依靠实体的情况下学习到. </p><p>在基于平移的模型中, 头实体$h$ 更像是<strong>锚点</strong>, 经过关系$r$ 的作用后能够确定尾实体$t$, 通过调整$r$ 能决定$t$ 的落点. 基于GNN的模型以节点为单位学习, 也将关系隐式表现在节点与邻居节点的聚合之间.</p><p>作者也指出, 尤其是RGCN是GCN和TransR的结合, KBAT是ConvE和GAT的结合.</p></blockquote><p>作者发现, 在最初的设计中, 所有的模型都没有对它们的变换矩阵加以约束, 从而<strong>违背</strong>了变换矩阵需要满足的正交结论. </p><blockquote><p>作者提到, 现在的方法很难在实际中做到正交. 例如TransR和RGCN不可能同时约束KG中非常多的关系. ConvE和KBAT在转换后的嵌入维度必须与输入维度保持一致, 否则会发生尺寸不匹配, 因此ConvE和KBAT的变换矩阵不能是方阵, <strong>更不可能是正交阵</strong>.</p></blockquote><h3 id="Key-Criteria-for-Transformation-Operation"><a href="#Key-Criteria-for-Transformation-Operation" class="headerlink" title="Key Criteria for Transformation Operation"></a>Key Criteria for Transformation Operation</h3><p>根据作者分析的问题, 作者提出了两种<strong>理想状态</strong>下的实体对齐变换标准, 对实体对齐操作加以<strong>约束</strong>.</p><h4 id="Relation-Differentiation"><a href="#Relation-Differentiation" class="headerlink" title="Relation Differentiation"></a>Relation Differentiation</h4><p>对于不同的关系类型, 作者希望将统一实体在不同关系下的表示变得不同:</p><p>$$<br>\varphi\left(\boldsymbol{h}_{e}, \boldsymbol{h}_{r_{1}}\right) \neq \varphi\left(\boldsymbol{h}_{e}, \boldsymbol{h}_{r_{2}}\right), \forall e \in E, \forall r_{1}, r_{2} \in R<br>$$</p><p>这一约束避免了模型学习到同实体在不同关系下的相同表示, 从而不能区分开关系之间的不同.</p><h4 id="Dimensional-Isometry"><a href="#Dimensional-Isometry" class="headerlink" title="Dimensional Isometry"></a>Dimensional Isometry</h4><p>同一KG下的两个实体转换进同一关系空间时, 它们之间的<strong>范数</strong>和<strong>相对位置</strong>应该保持不变:<br>$$<br>\begin{array}{c}<br>\lVert\boldsymbol{h}_{e}\rVert=\lVert\varphi\left(\boldsymbol{h}_{e}, \boldsymbol{h}_{r}\right)\rVert, \forall e \in E, \forall r \in R \\<br>\boldsymbol{h}_{e_{1}}^{T} \boldsymbol{h}_{e_{2}}=\varphi\left(\boldsymbol{h}_{e_{1}}, \boldsymbol{h}_{r}\right)^{T} \varphi\left(\boldsymbol{h}_{e_{2}}, \boldsymbol{h}_{r}\right), \forall e_{1}, e_{2} \in E, \forall r \in R<br>\end{array}<br>$$</p><ul><li><p>第一个约束保证了在经过关系变换后, 实体的Embedding范数与没变化时相同.</p></li><li><p>第二个约束保证了在经过关系变换后, 两实体的相对位置不发生变化(<strong>点积不变</strong>).</p></li></ul><h2 id="The-Proposed-Method"><a href="#The-Proposed-Method" class="headerlink" title="The Proposed Method"></a>The Proposed Method</h2><p>在本节中, 作者依托上节中提出的两个操作, 提出了一种<strong>基于GNN</strong>的新实体对齐方法<strong>RREA</strong>(<strong>R</strong>elational <strong>R</strong>eflection <strong>E</strong>ntity <strong>A</strong>lignment).</p><h3 id="Relational-Reflection-Transformation"><a href="#Relational-Reflection-Transformation" class="headerlink" title="Relational Reflection Transformation"></a>Relational Reflection Transformation</h3><p>令关系嵌入$\boldsymbol{h}_r$ 为<strong>法向量</strong>, 其垂直的超平面为$\boldsymbol{P}_r$, 使用与关系相关的反射矩阵$\boldsymbol{M}_r$ 来描述实体经过的变换:</p><p>$$<br>\boldsymbol{M}_{r}=\boldsymbol{I}-2 \boldsymbol{h}_{r} \boldsymbol{h}_{r}^{T}<br>$$</p><p>其中$\boldsymbol{h}_r$ 应该是归一化的. 并且, 非常容易能够证明反射矩阵$\boldsymbol{M}_r$ 的<strong>正交性</strong>:</p><p>$$<br>\begin{aligned}<br>\boldsymbol{M}_{r}^{T} \boldsymbol{M}_{r} &amp;=\left(\boldsymbol{I}-2 \boldsymbol{h}_{r} \boldsymbol{h}_{r}^{T}\right)^{T}\left(\boldsymbol{I}-2 \boldsymbol{h}_{r} \boldsymbol{h}_{r}^{T}\right) \\<br>&amp;=\boldsymbol{I}-4 \boldsymbol{h}_{r} \boldsymbol{h}_{r}^{T}+4 \boldsymbol{h}_{r} \boldsymbol{h}_{r}^{T} \boldsymbol{h}_{r} \boldsymbol{h}_{r}^{T}=\boldsymbol{I}<br>\end{aligned}<br>$$</p><p>因此, 只要不满足$\{\boldsymbol{h}_{r_i}\neq\boldsymbol{h}_{r_j} \forall{r_i, r_j} \in R \}$, 关系反射变换就能够满足在上一节中提出的两个理想标准.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/relationalreflectionentityalignment11.jpg" style="zoom:50%;" /><h3 id="Relational-Refection-Entity-Alignment"><a href="#Relational-Refection-Entity-Alignment" class="headerlink" title="Relational Refection Entity Alignment"></a>Relational Refection Entity Alignment</h3><p>下面作者正式介绍RREA. 模型需要输入实体嵌入矩阵$\boldsymbol{H}^{e} \in \mathbb{R}^{|E| \times d}$ 和关系嵌入矩阵$\boldsymbol{H}^{r} \in \mathbb{R}^{|R| \times d}$. </p><h4 id="Relational-Reflection-Aggregate-Layer"><a href="#Relational-Reflection-Aggregate-Layer" class="headerlink" title="Relational Reflection Aggregate Layer"></a>Relational Reflection Aggregate Layer</h4><p>节点$e_i$ 第$l$ 层的输出特征如下:</p><p>$$<br>\boldsymbol{h}_{e_{i}}^{l+1}=\operatorname{ReLU}\left(\sum_{e_{j} \in \mathcal{N}_{e_{i}}^{e}} \sum_{r_{k} \in R_{i j}} \alpha_{i j k}^{l} \boldsymbol{M}_{r_{k}} \boldsymbol{h}_{e_{j}}^{l}\right)<br>$$</p><p>其中$\mathcal{N}_{e_i}^e$ 为节点$e_i$ 的邻居集, $R_{ij}$ 为节点$e_i$ 到节点$e_j$ 之间的关系集. $\boldsymbol{M}_{r_{k}} \in \mathbb{R}^{d \times d}$ 为关系$r_k$ 的关系反射矩阵, 能够通过节点$e_i$ 和节点$e_j$ 之间的关系$r_k$ 直接使用关系反射变换得到. $\alpha^l_{ijk}$ 是节点$e_i$ 对其经过关系变换后的邻居节点$e_j$ 的注意力权重. </p><p>因为$\boldsymbol{M}_{r_{k}}$ 是<strong>正交阵</strong>, 所以它不像RGCN中的$\boldsymbol{W}_r$ 中含有$d^2$ 个自由度, 而是只含有$d$ 个自由度.</p><p>类似于<strong>GAT</strong>, 注意力权重$\alpha_{ijk}^l$ 由$\beta_{ijk}^l$ 得来:</p><p>$$<br>\alpha_{i j k}^{l}=\frac{\exp \left(\beta_{i j k}^{l}\right)}{\left.\sum_{e_{j} \in N_{e_{i}}^{e}} \sum_{r_{k} \in R_{i j}} \exp \left(\beta_{i j k}^{l}\right)\right)}<br>$$<br>$$<br>\beta_{i j k}^{l}=\boldsymbol{v}^{T}\left[\boldsymbol{h}_{e_{i}}^{l}||\boldsymbol{M}_{r_{k}} \boldsymbol{h}_{e_{j}}^{l}|| \boldsymbol{h}_{r_{k}}\right]<br>$$</p><p>其中$\boldsymbol{v} \in \mathbb{R}^{2d}$ 是可以学习的参数. </p><blockquote><p>$\mathcal{v}$ 的尺寸存疑.</p></blockquote><p>将所有层的输出Concat起来, 能得到实体$e_i$ 的最终特征输出$\boldsymbol{h}_{e_{i}}^{o u t}$:<br>$$<br>\boldsymbol{h}_{e_{i}}^{o u t}=\left[\boldsymbol{h}_{e_{i}}^{0}||\ldots||\boldsymbol{h}_{e_{i}}^{l}\right]<br>$$</p><p>这样能够保存<strong>全局信息</strong>, 而不是只以最后一层的输出作为特征.</p><h4 id="Dual-Aspect-Embedding"><a href="#Dual-Aspect-Embedding" class="headerlink" title="Dual - Aspect Embedding"></a>Dual - Aspect Embedding</h4><p>基于一些最近的研究, GNN只能包含<strong>拓扑结构信息</strong>, 缺少实体附近的<strong>关系信息</strong>, 因此作者直接将节点附近的关系信息一起<strong>Concat</strong>起来, 作为实体和关系的<strong>共同嵌入</strong>$\boldsymbol{h}_{e_{i}}^{M u l}$:<br>$$<br>\boldsymbol{h}_{e_{i}}^{M u l}=\left[\boldsymbol{h}_{e_{i}}^{o u t} || \frac{1}{\left|\mathcal{N}_{e_{i}}^{r}\right|} \sum_{r_{j} \in N_{e_{i}}^{r}} \boldsymbol{h}_{r_{j}}\right]<br>$$</p><p>其中$\mathcal{N}_{e_i}^r$ 是节点$e_i$ 的关系集. 这样就包含了GNN对节点本身抽取的<strong>节点特征</strong>和节点附近的<strong>关系特征</strong>.</p><h4 id="Alignment-Loss-Function-for-Training"><a href="#Alignment-Loss-Function-for-Training" class="headerlink" title="Alignment Loss Function for Training"></a>Alignment Loss Function for Training</h4><p>为了保证等价的实体在统一的空间中接近, 使用三元组损失:<br>$$<br>L=\sum_{\left(e_{i}, e_{j}\right) \in P} \max \left(\operatorname{dist}\left(e_{i}, e_{j}\right)-\operatorname{dist}\left(e_{i}^{\prime}, e_{j}^{\prime}\right)+\lambda, 0\right)<br>$$<br>其中, $(e_i, e_j) \in P$ 是等价实体对, $e_i^\prime, e_j^\prime$ 是通过最近邻居采样得来的负样本实体对. </p><blockquote><p>注意, 这里的$e_i, e_j$ 和之前描述$SS$ 时的意义是不一样的. </p></blockquote><p>然后采用与GCN - Align相同的距离度量:<br>$$<br>\operatorname{dist}\left(e_{i}, e_{j}\right)=\lVert\boldsymbol{h}_{e_{i}}^{M u l}-\boldsymbol{h}_{e_{j}}^{M u l}\rVert_{1}<br>$$</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细的实验参数设置请参考原论文.</p><h3 id="Baselines"><a href="#Baselines" class="headerlink" title="Baselines"></a>Baselines</h3><p>由于实体对齐现在还处于没有统一的阶段, 有一些研究认为数据集信息不足, 尝试引入<strong>额外信息</strong>, 所以这会导致不公平的比较. 作者基于现在所使用的数据, 将其分为三类:</p><ul><li>Basic: 只使用原始数据.</li><li>Semi - supervised: 引入半监督生成额外的结构化数据.</li><li>Textual: 除了结构化数据, 引入了实体名作为额外的输入特征.</li></ul><p>作者对RREA也制作了符合三类标准的三种版本, 以做公平比较.</p><h3 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h3><h4 id="RREA-vs-Basic-and-Semi-supervised-Methods"><a href="#RREA-vs-Basic-and-Semi-supervised-Methods" class="headerlink" title="RREA vs. Basic and Semi-supervised Methods"></a>RREA vs. Basic and Semi-supervised Methods</h4><p>与前两种模型比较结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/relationalreflectionentityalignment13.jpg" style="zoom:50%;" /><p>在Hits@1上提升非常明显, 可以说作者的方法<strong>有效性</strong>得到了验证. 在Hits@10上提升不是很多, 主要的提升都集中在Hits@1上, 说明RREA能够更<strong>精确</strong>的挑选出合适的实体对. 主要是因为关系反射变换为实体构建了特定关系的嵌入, 能更好的捕获信息.</p><h4 id="RREA-vs-Textual-Methods"><a href="#RREA-vs-Textual-Methods" class="headerlink" title="RREA vs. Textual Methods"></a>RREA vs. Textual Methods</h4><p>与引入实体名的方法相比, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/relationalreflectionentityalignment12.jpg" style="zoom: 33%;" /><p>作者将提升归功于使用了MRAEA提出的无监督文本框架.</p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>消融实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/relationalreflectionentityalignment15.jpg" style="zoom:50%;" /><p>在模型中加入关系反射聚合和多重Embedding对模型提升都比较大, 引入了不同的信息.</p><h3 id="Robustness-Analysis"><a href="#Robustness-Analysis" class="headerlink" title="Robustness Analysis"></a>Robustness Analysis</h3><h4 id="Robustness-on-Pre-aligned-Ratio"><a href="#Robustness-on-Pre-aligned-Ratio" class="headerlink" title="Robustness on Pre - aligned Ratio"></a>Robustness on Pre - aligned Ratio</h4><p>作者希望能在低资源下表现良好, 因此在DBP15K中比较了三种基于GNN模型在不同数据量下的性能:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/relationalreflectionentityalignment14.jpg" style="zoom:50%;" /><p>RREA全面领先.</p><h4 id="Robustness-on-Hyper-parameter"><a href="#Robustness-on-Hyper-parameter" class="headerlink" title="Robustness on Hyper-parameter"></a>Robustness on Hyper-parameter</h4><p>为了探究RREA对超参数的鲁棒性, 作者在DBP15K上, 探究了RREA的超参数设置所带来的影响:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/relationalreflectionentityalignment16.jpg" style="zoom:50%;" /><p>无论是GNN层数还是间隔$\lambda$, 作者认为对模型的影响都比较有限, 模型随超参变化相对比较稳定.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>作者基于两个反直觉的现象, 提出了一种<strong>统一</strong>的实体对齐框架. 并强调了变换矩阵<strong>正交</strong>的重要性. </p><p>依据对两个问题的解释, 作者设计了一种能够解决反直觉问题的<strong>关系反射变换</strong>操作, 并提出了以此为基础的新方法<strong>RREA</strong>, 通过实验证明, 作者提出的方法在多种方法中属于优越的方法, 并且提升非常大.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实体对齐 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Realistic Re-evaluation of KGC Methods: An Experimental Study</title>
      <link href="/posts/53954.html"/>
      <url>/posts/53954.html</url>
      
        <content type="html"><![CDATA[<h1 id="Realistic-Re-evaluation-of-Knowledge-Graph-Completion-Methods-An-Experimental-Study"><a href="#Realistic-Re-evaluation-of-Knowledge-Graph-Completion-Methods-An-Experimental-Study" class="headerlink" title="Realistic Re-evaluation of Knowledge Graph Completion Methods: An Experimental Study"></a>Realistic Re-evaluation of Knowledge Graph Completion Methods: An Experimental Study</h1><p>本文是论文<a href="https://arxiv.org/abs/2003.08001" target="_blank" rel="noopener">Realistic Re-evaluation of Knowledge Graph Completion Methods: An Experimental Study</a> 的阅读笔记和个人理解. </p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者发现, 在FB15k和WN18中存在着大量的<strong>逆关系</strong>和<strong>重复关系</strong>, 表现出高度<strong>冗余</strong>, 从而导致<strong>Data Leakage</strong>. 作者希望能够<strong>系统性</strong>的去评估这些存在问题所带来的<strong>危害</strong>. 这些数据集上含有问题的关系导致了KGE模型在Link Prediction上的表现被高估了.</p><p>例如, 前人已经发现逆关系在FB15k和WN18中大量出现, 在去除逆关系前后对模型性能的影响非常大:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/realisticreevaluationofkge1.jpg" style="zoom: 33%;" /><h2 id="Evaluation-Framework"><a href="#Evaluation-Framework" class="headerlink" title="Evaluation Framework"></a>Evaluation Framework</h2><p>本节是对目前现存的KGE中评估框架的简单的介绍, 为后文作者所进行的所有实验做简单的铺垫, 对于一些KGE领域的常识在此不再赘述, 详细内容请读者自行查阅.</p><h3 id="Evaluation-DataSets"><a href="#Evaluation-DataSets" class="headerlink" title="Evaluation DataSets"></a>Evaluation DataSets</h3><p>现在KGE评估所使用的主流数据集是FB15k, FB15k - 237, WN18, WN18RR. YAGO在最近的论文中似乎用的比较少. 数据集中的实体, 关系个数以及它们所使用的数据划分如下所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/realisticreevaluationofkge2.jpg" style="zoom: 25%;" /><p>FB15k - 237, WN18RR, YAGO3 - 10 - DR是针对逆关系处理过后的数据集, 其实体或关系数据量都有不同程度的下降.</p><h3 id="Evaluation-Method-and-Measures"><a href="#Evaluation-Method-and-Measures" class="headerlink" title="Evaluation Method and Measures"></a>Evaluation Method and Measures</h3><p>现在常用的评估指标主要和<strong>搜索问题</strong>上相关的评估指标相重合.</p><p>首先, 要明确KGE模型主要使用<strong>链接预测</strong>作为评估模型的主要任务. 即对于完整的三元组$(h, r, t)$, 在丢失头实体$h$ 或者尾实体$t$ 的情况下, 需要通过问题$(h, r, ?)$ 或$(?, r, t)$ 信息来预测出丢失的部分. 从这个角度来看, 将Link Prediction用于知识图谱补全, 本身与一个搜索问题无异. </p><p>在搜索问题中常用的指标如下:</p><table><thead><tr><th>指标名</th><th>正方向</th><th>描述</th></tr></thead><tbody><tr><td>Hits@k</td><td>↑</td><td>概率最高的k个结果若正确则为命中</td></tr><tr><td>MR(Mean Rank)</td><td>↓</td><td>正确实体在所有实体中的排名, 并将它们相加求平均</td></tr><tr><td>MRR(Mean Reciprocal Rank)</td><td>↑</td><td>正确实体在所有实体中的排名的倒数, 并将它们相加求平均</td></tr></tbody></table><p>当然, 由于可能存在<strong>多个</strong>正确的头实体或尾实体答案, 上述没有经过任何过滤的排名评估计算方式被称为<strong>Raw</strong>, 如果将其余正确的答案从测试集中<strong>去除</strong>, 仅保留唯一的正确答案, 则称为<strong>Filtered</strong>. 那么上述评估指标也被相应的替换为FHits@k(↑), FMR(↓), FMRR(↑), 方向保持不变.</p><blockquote><p>该方法在TransE中被提出, 详见<a href="https://adaning.github.io/posts/53023.html#Evaluation-protocol">TransE</a>.</p></blockquote><h2 id="Inadequacy-of-Benchmarks-and-Evaluation-Measures"><a href="#Inadequacy-of-Benchmarks-and-Evaluation-Measures" class="headerlink" title="Inadequacy of Benchmarks and Evaluation Measures"></a>Inadequacy of Benchmarks and Evaluation Measures</h2><p>本文的核心工作在本节. 本节作者着重叙述了在FB15K和WN18中出现的种种问题. 主要存在的问题是<strong>逆关系</strong>和<strong>笛卡尔积关系</strong>. 逆关系其实已经被前人所提出并解决, 作者比较新颖的概念是提出了笛卡尔积关系.</p><h3 id="Identifying-the-Most-Probable-Freebase-Snapshot-for-Producing-FB15k"><a href="#Identifying-the-Most-Probable-Freebase-Snapshot-for-Producing-FB15k" class="headerlink" title="Identifying the Most Probable Freebase Snapshot for Producing FB15k"></a>Identifying the Most Probable Freebase Snapshot for Producing FB15k</h3><p>为了探寻FB15k中各类缺陷的<strong>根本原因</strong>, 作者需要观察Freebase原本的样貌. 但Freebase是一直在进行更新的, FB15k并未声明是在哪个时间节点进行创建. Freebase可能会由类似于<strong>快照</strong>的形式存储起来, 过一段周期对Freebase保存一次, 作者尝试搜索了与创建FB15k时相似的Snapshot版本, 有99.54%的三元组都出现在了FB15k中.</p><p>作者列举了一个关于长篇小说《A Room With A View》在Freebase中存储的真实样貌, 以便我们更好的来理解Freebase中的各种结构:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/realisticreevaluationofkge3.jpg" style="zoom: 50%;" /><p>能够看到, 在Freebase中存在着大量的<strong>逆关系</strong>, 几乎每一种关系都能找到它所对应的逆关系.逆关系是导致Data Leakage的主要原因, 模型通过<strong>关联逆关系对</strong>获得提升.</p><blockquote><p>注意, FB15k中来自Freebase的冗余逆关系是<strong>人为创建</strong>的, 它被添加了一个反向三元组, 用关系<code>reverse_property</code>显式表示出来.</p></blockquote><h4 id="CVT-and-Concatenated-Edges"><a href="#CVT-and-Concatenated-Edges" class="headerlink" title="CVT and Concatenated Edges"></a>CVT and Concatenated Edges</h4><p>在Freebase中, 有一类特殊的节点被称为CVT(Compound Value Type), 其发挥的作用更像<strong>中继节点</strong>, 以便能更好的处理<strong>多元关系</strong>. CVT能够作为更<strong>抽象</strong>的节点被多种二元关系所连接, 而三元组则以合并CVT所连接的两条边后的形式被创建. 例如<code>(Bafta Award For Best Film, award_category/nominees, CVT)</code>和<code>(CVT, award_nomination/ nomated_for, A Room With A View)</code>在奖项和作品之间形成三元组, 对这两种关系$r_1, r_2$ 的合并记为$r_1 . r_2$. CVT常用于更简单的标识某个事件所对应的多种属性.</p><p>例如, 关于<code>Obama</code>任职期限的<strong>多元关系</strong><code>government_position_held</code>包含了多个<strong>子二元关系</strong>, <code>offer_holder</code>, <code>office_position</code>, <code>from</code>, <code>to</code>等:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/realisticreevaluationofkge24.jpg" style="zoom: 50%;" /><blockquote><p>该图片取自<a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/44818.pdf" target="_blank" rel="noopener">From Freebase to Wikidata: The Great Migration</a>.</p></blockquote><p><strong>而在FB15k中, 将大多数的CVT节点合并掉了</strong>, 合并的边以<strong>三元组</strong>的形式出现在FB15k中.</p><h3 id="Data-Redundancy"><a href="#Data-Redundancy" class="headerlink" title="Data Redundancy"></a>Data Redundancy</h3><p><strong>Data Redundancy是导致Data Leakage的根本原因</strong>. 冗余的数据使得模型倾向于关联各种逆关系对, 从而获得在真实任务中不能获得的额外信息, 导致过拟合.</p><h4 id="Data-Leakage-due-to-Reverse-Triples"><a href="#Data-Leakage-due-to-Reverse-Triples" class="headerlink" title="Data Leakage due to Reverse Triples"></a>Data Leakage due to Reverse Triples</h4><p>前人已经发现, 在FB15k中含有大量的逆关系. 一般的, 对于逆三元组对$(h, r, t), (t, r^{-1}, h)$, 其中的$r$ 被称为逆关系. 实际上在Freebase中, 逆关系已经使用特殊的关系<code>reverse_property</code>来<strong>显式关联</strong>逆关系对. 例如, <code>(film/directed_by, reverse_property, director/film)</code>意味着<code>film/directed_by</code>和是一对逆关系. 如果是由CVT合并来的两关系$r_1. r_2$, 它们的逆关系理所当然的是$r_2^{-1} . r_1^{-1}$.</p><h5 id="FB15k"><a href="#FB15k" class="headerlink" title="FB15k"></a>FB15k</h5><p>在FB15k中, 训练集中的48w个三元组中, 有将近34w的三元组形成了17w的逆关系对, 在测试集中约有70.3%的三元组的逆关系能够在训练集中被找到. 这是非常严重的泄露了, 模型非常容易的将自己的训练目标转化为如何才能学习到更多的逆关系对, 从表面上优化在FB15k中的效果.</p><h5 id="WN18"><a href="#WN18" class="headerlink" title="WN18"></a>WN18</h5><p>在WN18中, 一共有18个关系, 其中有14个关系构成了7对逆关系对, 除此外还有三种<strong>对称关系</strong>. 在测试集和训练集中包含逆关系对的三元组非常多, 也导致了非常严重的泄露.</p><h4 id="Other-Redundant-Triples"><a href="#Other-Redundant-Triples" class="headerlink" title="Other Redundant Triples"></a>Other Redundant Triples</h4><p>作者指出, 除去表面上已经明确指出的逆关系, 在FB15k中还存在其他类型的<strong>语义冗余关系</strong>. 作者通过一种简单的衡量标准来判断关系对$(r_1, r_2)$ 中出现的两种关系$r_1, r_2$ 是否是冗余的. </p><p>假设$\left| r \right|$ 是关系$r$ 所对应的三元组实例个数, $T_r$ 代表关系$r$ 对应三元组实例中的头实体$h$ 和尾实体$t$ 对的集合, 即$T_r=\{(h, t) \mid r(h, t) \in \mathcal{G} \}$. 如果满足以下条件, 那么称$r_1$ 和$r_2$ 为<strong>重复关系</strong>:<br>$$<br>\frac{\left|T_{r_{1}} \cap T_{r_{2}}\right|}{\left|r_{1}\right|}&gt;\theta_{1} \text { and } \frac{\left|T_{r_{1}} \cap T_{r_{2}}\right|}{\left|r_{2}\right|}&gt;\theta_{2}<br>$$<br>其实就是非常简单的去看两种关系所对应的头尾实体对的<strong>重合度</strong>. 如果两种关系所对应的头尾实体对交集越多, 则证明其对应头尾实体的重合区域就越大, 两种关系也越有可能具有相似的语义, 冗余的可能性也就越大.</p><p>更多的, 对于关系$r_1$ 和逆关系$r_2^{-1}$ 也能由之推广而来. $T_r^{-1}$ 代表$T_r$ 的逆实体对, 即$T_r^{-1}=\{(t, h) \mid (h, t) \in T_r \}$. 同样的, 如果满足以下条件, 则称$r_1$ 和$r_2$ 为<strong>逆重复关系</strong>:<br>$$<br>\frac{\left|T_{r_{1}} \cap T_{r_{2}}^{-1}\right|}{\left|r_{1}\right|}&gt;\theta_{1} \text { and } \frac{\left|T_{r_{1}} \cap T_{r_{2}}^{-1}\right|}{\left|r_{2}\right|}&gt;\theta_{2}<br>$$<br>作者将$\theta_1, \theta_2$ 在FB15k中设置为0.8.</p><p>例如, 关系$r_1$ <code>football_position/players</code> 和关系$r_2$ <code>sports_position/players.football_roster_position/player</code>, 就是一对重复关系, 因为计算得出$\frac{\left|T_{r_{1}} \cap T_{r_{2}}\right|}{\left|r_{1}\right|}=0.87 , \frac{\left|T_{r_{1}} \cap T_{r_{2}}\right|}{\left|r_{2}\right|}=0.97$. 上述两种关系在下图中分别用红色, 绿色标记出:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/realisticreevaluationofkge4.jpg" style="zoom: 50%;" /><p>红色的关系记载了每个足球运动员整体的职业生涯所踢的位置, 绿色关系是通过CVT连接而来的, 代表特定时间球员在球队中所处的位置, 属于多元关系. 但实际上, 绝大多数的球员在职业生涯中只踢某一个特定的位置, 所以这两种关系大多数情况下是冗余的. 另一个类似的例子是$r_1$ 和关系$r_3$ <code>football_player/current_team . sports_team_roster/position</code>, 在图中用蓝色标出. $r_1, r_3$ 是逆重复的, 因为计算得出$\frac{\left|T_{r_{1}} \cap T_{r_{2}}^{-1}\right|}{\left|r_{1}\right|}=0.88 , \frac{\left|T_{r_{1}} \cap T_{r_{2}}^{-1}\right|}{\left|r_{2}\right|}=0.97$.</p><p>在上述讨论下, 作者将其他的冗余情况分为4种:</p><ol><li><strong>训练集</strong>中存在<strong>逆三元组</strong>.</li><li><strong>训练集</strong>中存在<strong>重复三元组</strong>或<strong>逆重复三元组</strong>.</li><li><strong>测试集</strong>中存在<strong>逆三元组</strong>.</li><li><strong>测试集</strong>中存在<strong>重复三元组</strong>或<strong>逆重复三元组</strong>.</li></ol><p>作者将上述四种情况用四位<strong>二进制码</strong>来表示, 并统计了FB15k中的测试集冗余情况:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/realisticreevaluationofkge5.jpg" style="zoom: 50%;" /><p>从中能够看出, 68%的三元组集中在<code>1000</code>, 即训练集中存在逆三元组. 没有出现作者所归纳出的四种问题的三元组<code>0000</code>也占了18%, 只在测试集中存在逆三元组的<code>0010</code>占了8%. 其余情况占少部分.</p><h3 id="Cartesian-Product-Relations"><a href="#Cartesian-Product-Relations" class="headerlink" title="Cartesian Product Relations"></a>Cartesian Product Relations</h3><p>笛卡尔积关系在之前的研究中并没有被提出过, 作者第一次发现了这种特殊的关系. 我们首先来回顾一下笛卡尔积. 集合$A=\{a_1, a_2\}, B=\{b_1, b_2, b_3\}$, 假设用$\times$ 来表示笛卡尔积, 那么有:<br>$$<br>\begin{aligned}<br>A\times B &amp;= \{(a_x, b_y)\mid x\ in (1, 2), y \in (1, 2, 3)\}\\<br>&amp;=\{(a_1, b_1), (a_1, b_2), (a_1, b_3), (a_2, b_1), (a_2, b_2), (a_2, b_3)\}<br>\end{aligned}<br>$$<br>如果将笛卡尔积用于<strong>三元组</strong>中, 那么对于笛卡尔积关系$r$, 其对应的头实体$h$ 和尾实体$t$ 总是符合笛卡尔积的运算关系, 即$h \in \mathcal{H}, t \in \mathcal{T}$, 关系$r$ 中的头尾关系实体对应该为$\mathcal{H} \times \mathcal{T}$, $\mathcal{H}$ 到$\mathcal{T}$ 中的每个实体都存在关系$r$.</p><p>例如, 关系<code>climate</code>, 通常以<code>(a, climate, b)</code> 的形式出现在现实生活中, 其中<code>a</code>为城市, <code>b</code>为月份. 但实际上, Link Prediction在这种三元组上做的预测<strong>毫无意义</strong>. 例如链接预测在​<code>climate</code>上的意义会转化为某城市在几月份是否有气候. </p><p>再假设一个场景, <code>Tokyo</code>与<code>CVT</code>相连, 边上的Label为<code>climate</code>, <code>January</code>与<code>CVT</code>相连, 边上的Label为<code>month</code>,  <code>34</code>与<code>CVT</code>相连, 边上Label为<code>average_min_temp_c</code>. 这个关系代表东京一月份的平均低温为34度, 但人们实际上在现实世界中更关心平均气温, 而不是将链接预测用于此预测一月是否有温度. 根本原因是现实世界的多重关系与链接预测任务将其<strong>简化</strong>为多重二元关系, 而<strong>多重二元关系在拆解开后不能恢复成多元关系</strong>.</p><p>此外, 负采样获得的三元组也有可能是正确的, 而笛卡尔积关系在这个问题上尤为突出.</p><p>在笛卡尔积关系大量出现在数据集中时, 会拉高整体模型的评估精度. 这种<strong>特殊性</strong>使得笛卡尔积与其他关系放在一起比较不太公平, 作者在论文中给出的建议是, <strong>将笛卡尔积关系和非笛卡尔积关系分别比较</strong>.</p><p>作者仍然尝试使用简单的方法来检测笛卡尔积关系, 对于关系$r$, 其头实体集$S_{r}=\{\mathrm{h} \mid \exists \ r(\mathrm{h}, \mathrm{t}) \in \mathcal{G}\}$, 尾实体集 $O_r = \{\mathrm{t} \mid \exists \ r(\mathrm{h}, \mathrm{t}) \in \mathcal{G}\}$, 若$|r| /\left(\left|S_{r}\right| \times\left|O_{r}\right|\right)$ 高于了某个阈值(之前设置的为0.8), 则认为该关系是笛卡尔积关系.</p><p>尽管笛卡尔积关系不像逆关系数量那么多, 但是笛卡尔积关系非常容易被模型学习到, 下图为FB15k - 237中部分笛卡尔积关系的FMRR:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/realisticreevaluationofkge6.jpg" style="zoom: 50%;" /><p>作者尝试证明自己的假设, 观察笛卡尔积关系所带来的影响. 如果笛卡尔积关系能被检测到, 则在链接预测时直接从已知的笛卡尔积头尾实体对的候选集中<strong>随机排序</strong>, 挑选合适的头尾实体.</p><blockquote><p>随机排序是我在<a href="https://github.com/idirlab/kgcompletion/blob/master/Cartesian-product/fb15k_cartesian_product.py" target="_blank" rel="noopener">代码</a>中看到的, 并不是很确定正确与否, 作者在论文中并没有明确指出.</p></blockquote><p>下图为单纯使用TransE在笛卡尔积关系上的表现和利用笛卡尔积特性后的模型表现:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/realisticreevaluationofkge7.jpg" style="zoom: 50%;" /><p>下图为上图中所有关系的具体含义:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/realisticreevaluationofkge8.jpg" style="zoom: 50%;" /><p>能从结果中观察到, 利用笛卡尔积特性的效果比单纯使用TransE要更好. 使用Freebase效果要比它的子集FB15k更强一些.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>FB15k - 237, WN18RR, YAGO3 - 10 - DR是三种去除逆关系后数据集, 基本上消除了论文中提到的逆关系带来Leakage的问题. </p><ul><li><p><strong>FB15k - 237</strong>中, 删除逆关系的过程与作者提出的过程本质上一致, 这就有可能存在误伤, 即删除了一些在语义上不冗余的关系, 没有考虑笛卡尔积关系.</p></li><li><p><strong>WN18RR</strong>中, 保留了<strong>对称关系</strong>(也产生了逆关系对), 数据集中的11种关系中, 有3种是自反关系.</p></li><li><p><strong>YAGO3 - 10 - DR</strong>是作者<strong>自制</strong>的数据集, 也做了移除逆关系的处理.</p></li></ul><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>作者将诸多流行的Baseline在多种数据集上用<strong>链接预测</strong>详细测试. 我在写的时候对原文中陈述的顺序进行了<strong>调整</strong>, 按照数据集将各种结果整合到一起, 方便阅读.</p><h4 id="FB15k-and-FB15k-237"><a href="#FB15k-and-FB15k-237" class="headerlink" title="FB15k and FB15k - 237"></a>FB15k and FB15k - 237</h4><p>FB15k, FB15k - 237上, 各类模型结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/realisticreevaluationofkge9.jpg" style="zoom: 50%;" /><p>不同颜色的结果代表不同的实验结果来源, 其中AMIE是<strong>基于规则</strong>的模型.</p><p>在将逆关系移除后, 所有的模型结果都发生了明显的<strong>退化</strong>.</p><p>作者进一步精确的分析了模型在数据集中各种关系的性能(以<strong>FMRR</strong>为指标), 将每个模型在测试集中相较于其他模型性能<strong>最好</strong>的<strong>关系所占百分比</strong>. 下图是FB15k - 237的热力图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/realisticreevaluationofkge13.jpg" style="zoom: 50%;" /><p>为了更好地理解模型优缺点, 作者将上表中的关系<strong>分类</strong>对待, 根据前面得出的<strong>FMRR</strong>做出柱状图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/realisticreevaluationofkge15.jpg" style="zoom: 50%;" /><p>下图为模型在每类别关系表现最优的占比:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/realisticreevaluationofkge16.jpg" style="zoom: 50%;" /><p>各模型在FB15k - 237区分类别的关系下的FHits@10:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/realisticreevaluationofkge17.jpg" style="zoom: 50%;" /><h4 id="WN18-WN18RR"><a href="#WN18-WN18RR" class="headerlink" title="WN18, WN18RR"></a>WN18, WN18RR</h4><p>本小节结构与上小节完全一致. WN18, WN18RR上, 各类模型结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/realisticreevaluationofkge10.jpg" style="zoom: 50%;" /><p>下图是WN18RR的热力图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/realisticreevaluationofkge14.jpg" style="zoom: 50%;" /><p>因为WN18RR中含有的关系数量过少, 为了维持实验的<strong>稳健性</strong>, 所以作者并没有做出与FB15k - 237中类似的柱状图分析. 各模型在WN18RR区分类别的关系下的FHits@10:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/realisticreevaluationofkge18.jpg" style="zoom: 50%;" /><h4 id="YAGO3-10-YAGO3-10-DR"><a href="#YAGO3-10-YAGO3-10-DR" class="headerlink" title="YAGO3 - 10, YAGO3 - 10 - DR"></a>YAGO3 - 10, YAGO3 - 10 - DR</h4><p>本小节结构与上上小节也完全一致. YAGO3 - 10, YAGO3 - 10 - DR上, 各类模型结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/realisticreevaluationofkge19.jpg" style="zoom: 50%;" /><p>同样的, 在YAGO 3 - 10上作者也做了同样的可视化:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/realisticreevaluationofkge20.jpg" style="zoom: 50%;" /><p>下图为模型在每类别关系表现最优的占比:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/realisticreevaluationofkge21.jpg" style="zoom: 50%;" /><p>各模型在YAGO3 - 10区分类别的关系下的FHits@10:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/realisticreevaluationofkge22.jpg" style="zoom: 50%;" /><h4 id="Together"><a href="#Together" class="headerlink" title="Together"></a>Together</h4><p>作者假设删除了逆三元组和重复三元组后, 优于TransE的模型并没有体现出很好优势, </p><p>下图是在测试集的三元组某指标上优于TransE的模型中, 在训练集中含有逆或冗余三元组所占的百分比:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/realisticreevaluationofkge11.jpg" style="zoom: 50%;" /><p>这证实了在FB15k和WN18上, 大多数比TransE效果好的方法与逆关系或冗余三元组是强相关的, 并且在去掉它们之后, 会有相当大的性能损失.</p><p>作者进一步的分析了在<strong>改进后</strong>的数据集上, 不同模型在不同指标中取得最优的关系数量:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/realisticreevaluationofkge12.jpg" style="zoom: 50%;" /><p>从中能观察到, 在移除了包含逆关系的三元组后, 很多模型并不能取得比TransE好太多的效果.</p><p>最后, 作者汇总了各模型在各类数据集下的<strong>FHits@1</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/realisticreevaluationofkge23.jpg" style="zoom: 50%;" /><p>其中, Simple Model是作者提出的一个简单的基于统计信息的模型, 能够自动推断出两个关系能否形成逆关系对, 从而简单的推断出答案. 仅仅使用统计信息在FB15k和WN18上取得了相当好的成绩, 但在FB15k - 237和WN18RR上的成绩非常差, 进一步证明了之前数据集问题的存在性和危害.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>本篇论文是比较扎实的工作, 对现有常用的KGE数据集中存在的缺陷进行了系统性而全面的分析, 并测试了多种Baseline在数据集上的表现, 做了大量的可视化工作. 此外, 也证明了现有评估KGE的方法存在诸多缺陷.</p><p>对于逆关系, 作者仍然采取移除的方式处理. 对于比较新颖的笛卡尔积关系, 在其他的工作中并没有被提出.</p><p>如何合理的处理这些含有问题的关系是非常值得思考的事情, 有没有移除之外更好的办法? 这可能要追本溯源到KG中三元组的表示方法的诟病上.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Can We Predict New Facts with Open Knowledge Graph Embeddings?</title>
      <link href="/posts/57546.html"/>
      <url>/posts/57546.html</url>
      
        <content type="html"><![CDATA[<h1 id="Can-We-Predict-New-Facts-with-Open-Knowledge-Graph-Embeddings-A-Benchmark-for-Open-Link-Prediction"><a href="#Can-We-Predict-New-Facts-with-Open-Knowledge-Graph-Embeddings-A-Benchmark-for-Open-Link-Prediction" class="headerlink" title="Can We Predict New Facts with Open Knowledge Graph Embeddings? A Benchmark for Open Link Prediction"></a>Can We Predict New Facts with Open Knowledge Graph Embeddings? A Benchmark for Open Link Prediction</h1><p>本文是论文<a href="https://www.aclweb.org/anthology/2020.acl-main.209/" target="_blank" rel="noopener">Can We Predict New Facts with Open Knowledge Graph Embeddings? A Benchmark for Open Link Prediction</a>的阅读笔记和个人理解. </p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者指出, 虽然人们已经在<strong>特定领域</strong>的知识图谱上取得了成功, 但先前的方法都不是基于<strong>开放域</strong>的.</p><p>作者希望探讨是否有可能从<strong>开放知识图谱</strong>中通过<strong>非规范化的文本</strong>直接推断出新的事实, 即能否依据原始文本完成开放域的<strong>Link Prediction</strong>.</p><h2 id="Open-Knowledge-Graph"><a href="#Open-Knowledge-Graph" class="headerlink" title="Open Knowledge Graph"></a>Open Knowledge Graph</h2><p>开放知识图谱的三元组经常由<strong>文本三元组</strong>组成, 即$(\text { subject text, relation text, object text) }$, 由于文本信息非常嘈杂, 其中含有很多<strong>噪声</strong>, 因此同一种关系或者实体对应着非常多种不同的文本表述:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/predictnewfactfromopenkg1.jpg" style="zoom:33%;" /><p>例如, <code>NBC</code>这一实体可能对应着<code>{NBC - TV, NBC, NBC Television}</code>这一实体集合.</p><blockquote><p>对于同一实体的不同自然语言表述在原文中似乎被称为是Entity Mention, 但我没想好到底翻译成什么, 暂称实体集合或<strong>实体提及</strong>.</p></blockquote><p>能看到, 因为使用<strong>自然语言文本</strong>来表示三元组, 导致实体和关系非常<strong>不规范</strong>. 但是Open KG中通常包括着更多的文字表述, 也可以被捕获更多的信息.</p><p>Open KG<strong>不直接编码知识</strong>, 即使在文字描述中已经给出了实体本身, 在其中还可能含有大量的概念化知识.</p><p>在OKG中, 其结构应该是能<strong>自动构建</strong>的, 因为其涉及到自然语言的复杂性, 应该不需要实体和关系的词表.</p><h2 id="Open-Link-Prediction"><a href="#Open-Link-Prediction" class="headerlink" title="Open Link Prediction"></a>Open Link Prediction</h2><h3 id="Link-Prediction"><a href="#Link-Prediction" class="headerlink" title="Link Prediction"></a>Link Prediction</h3><p>Link Prediction在一般的KG中经常被用于测试模型的<strong>推断</strong>能力. </p><p>假设$\mathcal{E}$ 是实体$i, j$ 的集合, $\mathcal{R}$ 是关系$k$ 的集合, 那么知识图谱$\mathcal{T} \subseteq \mathcal{E} \times \mathcal{R} \times \mathcal{E}$. 如果从QA的角度来考虑, 查询头实体$i$ 和尾实体$j$ 的问题就分别被描述为$q_{h}=(?, k, j)$ 和 $q_{t}=(i, k, ?)$. 并且, 它们可能分别在训练集中<strong>单独</strong>出现过, 但并未以三元组的的形式一起出现.</p><p>例如<code>(NBC, headquarterIn, ?)</code>, 就是在询问NBC在哪个地方设立了总部.</p><h3 id="Open-Link-Prediction-1"><a href="#Open-Link-Prediction-1" class="headerlink" title="Open Link Prediction"></a>Open Link Prediction</h3><h4 id="Difference-Between-LP-and-OLP"><a href="#Difference-Between-LP-and-OLP" class="headerlink" title="Difference Between LP and OLP"></a>Difference Between LP and OLP</h4><p>在开放域知识图谱中, Link Prediction相应的被转化为Open Link Prediction:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/predictnewfactfromopenkg2.jpg" style="zoom:35%;" /><p>由于OKG自身的特性, 实体和关系被转化为实体提及(Entity Mention)和开放关系(Open Relation). 如果对应到QA中, 问题和答案可能都会有多种不同的表述. 甚至对于<strong>不同时间点</strong>, 对于同一实体都会产生不同的表述.</p><p>作者将每个实体提及和开放关系以非空的Token序列对待, 记Token的词表为$\mathcal{V}$. 记开放集$\mathcal{M}=\mathcal{V^+}$, 代表OKG中所观察到的元素集合, 即实体提及$i,j \in \mathcal{M}(\mathcal{E})$ 和开放关系$k \in \mathcal{M}(\mathcal{R})$. 故$\mathcal{T} \subset \mathcal{M} \times \mathcal{M} \times \mathcal{M}$.</p><p>在OLP中, 其任务目标是预测<strong>新且正确</strong>的问题$(i, k, ?)$ 或 $(?, k, j)$ 的答案. 答案来自于$\mathcal{M}(\mathcal{E})$, 但问题可能来自于$\mathcal{M}$ 中的任意的实体提及和开放关系.</p><p>例如, 对于问题<code>(&quot;NBC - TV&quot;, &quot;has office in &quot;, ?)</code>, 我们想要的答案是的<code>NewYorkCity</code>的实体提及<code>{&quot;New York&quot;, &quot;NYC&quot;, ...}</code>中的任意一条表述.</p><h4 id="Evaluation-Protocol"><a href="#Evaluation-Protocol" class="headerlink" title="Evaluation Protocol"></a>Evaluation Protocol</h4><h5 id="KGs-and-Entity-Ranking"><a href="#KGs-and-Entity-Ranking" class="headerlink" title="KGs and Entity Ranking"></a>KGs and Entity Ranking</h5><p>先回顾一下在普通KG模型在Link Prediction中的实体排名规则. </p><p>对于三元组$z=(i, j, k)$, 模型对问题$q_{h}(z)=(?, k, j)$ 或$q_{t}(z)=(i, k, ?)$ 进行排名时, 有两种设置:</p><ul><li><strong>Raw</strong>: 直接依据于正确的实体$j$ 或 $i$ 的排名.</li><li><strong>Filtered</strong>: 依据于$q_t(z)$ 或 $q_h(z)$ 只保留正确答案$j$ 或 $i$ 的排名. 即滤去了除$j$ 和$i$ 外的其他所有正确答案后的排名. </li></ul><h5 id="OKGs-and-Mention-Ranking"><a href="#OKGs-and-Mention-Ranking" class="headerlink" title="OKGs and Mention Ranking"></a>OKGs and Mention Ranking</h5><p>出于在OKG中所出现的种种的问题, 作者将一般KG中评估用的协议改进到了OKG中.</p><p>在OKG中, 问题可能会有多个等价的正确答案, 所以一定要考虑答案的实体提及. 相同的, 作者对OLP也提出两种设置:</p><ul><li><strong>Raw</strong>: 依据于正确答案提及的最高排名.</li><li><strong>Filtered</strong>: 滤去评估实体外其余正确答案后的所有实体提及, 然后再在评估实体的提及中选择最高排名.</li></ul><p>作者将上述流程用下图来概括:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/predictnewfactfromopenkg3.jpg" style="zoom: 40%;" /><ol><li>对于给定的问题, 对所有答案的实体提及进行排名.</li><li>将除评估实体外的其余正确答案的所有实体提及从排名中过滤掉, 并将其标记为”filtered”.</li><li>选择评估实体的提及中排名最高的作为其排名.</li></ol><p>综上, 根据作者提出的协议, 我们在评估时需要以下两方面数据准备:</p><ol><li>问题的正确实体提及的标注数据.</li><li>同一个实体的所有实体提及.</li></ol><p>根据这两个要求, 作者接下来自制了所需的数据集.</p><h2 id="Creating-the-Open-Link-Prediction-DataSet"><a href="#Creating-the-Open-Link-Prediction-DataSet" class="headerlink" title="Creating the Open Link Prediction DataSet"></a>Creating the Open Link Prediction DataSet</h2><p>在OKG中, 经常伴随着Test Leakage, 因此模型能够毫不费力的推断出真正的事实. 因此, 验证集和测试集的数据必须精心准备, 否则将出现模型能力的误判. 所以作者需要自己按照需求制作OLP的数据集.</p><h3 id="Source-DataSet"><a href="#Source-DataSet" class="headerlink" title="Source DataSet"></a>Source DataSet</h3><p>OLPBENCH是基于OPIEC的创建的.</p><p>三元组从维基百科抽取的过程如下所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/predictnewfactfromopenkg4.jpg" style="zoom:33%;" /><p>借助维基百科的<strong>超链接</strong>, 直接就完成了<strong>实体消歧</strong>, 即直接生成实体的所有实体提及.</p><p>作者将实体链接前后的数据展示在表中, 以方便大家了解这个过程:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/predictnewfactfromopenkg5.jpg" style="zoom: 33%;" /><p>在没有实体链接前, 实体的表述是不规范的, 关系表述也是不规范的. 在链接后, 生成了实体到提及之间的映射, 实体被唯一的确定了下来.</p><h3 id="Evaluation-data"><a href="#Evaluation-data" class="headerlink" title="Evaluation data"></a>Evaluation data</h3><p>作者认为, 验证集和测试集的数据需要满足很多严格的要求, 以此来解决OKG中出现的问题.</p><h4 id="Data-Quality"><a href="#Data-Quality" class="headerlink" title="Data Quality"></a>Data Quality</h4><p>评估数据可能是非常具有挑战性的, 因为其中含有非常多的<strong>噪声</strong>.</p><p>作者非常简单的通过规则来筛选关系. 不考虑长度小于3个Token的关系, 理由如下:</p><ol><li>短关系通常是长关系的一部分.</li><li>长关系更难以被普通的KG构建方法所捕捉</li><li>使用短关系抽取的实体注释噪声经常更多.</li></ol><h4 id="Human-Effort-for-Data-Creation"><a href="#Human-Effort-for-Data-Creation" class="headerlink" title="Human Effort for Data Creation"></a>Human Effort for Data Creation</h4><p>在Mention Ranking Protocol中, 作者引入了实体相关的知识来完成实体消歧的工作, 作者希望评估引入实体知识对<strong>模型选择</strong>的作用, 因此作者按照人工干预的程度对测试集和验证集进行划分, 并探究在它们基础上选择模型后的模型表现. 人工干预越多, 引入的实体知识就越多.</p><p>作者将验证集划分为以下三种:</p><ul><li><strong>Test and Valid - Linked</strong> data: 绝大多数都是人工制造的, 能根据实体提及找到正确的实体. 并可以使用Mention Ranking Protocol.</li><li><strong>Valid - Mention</strong> data: 引入了一部分人工. 没有使用实体链接, 但保证了数据与测试数据的相同分布. 如果目标领域可以使用NER的话, 验证集可能会自动通过NER生成.</li><li><strong>Valid - All</strong> data: 没有人工干预. 基本维持了原始文本的样子.</li></ul><h4 id="Training-Data"><a href="#Training-Data" class="headerlink" title="Training Data"></a>Training Data</h4><p>在普通的KG中, 人们经常通过删除逆关系来解决Test Leakage. 但在Open KG中, 因为自然文本的嘈杂, 更有可能通过<strong>文本</strong>中所包含的内容导致Test Leakage.</p><p>这种Leakage会导致数据集的制作和划分更加困难, 在评估模型时, 还需要某种方法判断模型是否在真正推断信息. 因此, 作者想更深入的量化在验证集中到底有多少问题能够不通过整个问题的信息来回答, 即只给出头实体或者关系是否能成功预测尾实体. 在不借助整个问题信息来回答问题时, 往往是借助<strong>统计信息</strong>来回答的, 即只回答<strong>最常共现</strong>的答案, 而不是真正在做推断.</p><p>例如:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/predictnewfactfromopenkg6.jpg" style="zoom: 40%;" /><p>图中左侧为传统KG中的Test Leakage, 逆关系②<code>hasCompany</code>, 模型通常能够很容易的利用其信息建立与①<code>headquaterIn</code> 的联系, 从而不加推断的给出答案.</p><p>图中右侧为OKG中的Test Leakage, 图中①和②直接就是相同的关系描述, 当然会导致Leakage. 而最上方③中的尾实体描述<code>New York&#39;s NBC</code> 也会导致①的Leakage. 最下方③的开放关系描述也会导致相同的Leakage.</p><p>作者尝试对OKG上的训练集移除Leakage划分了三种等级:</p><ul><li><p><strong>Simple Removal</strong>: 仅仅删除三元组$(i, k, j)$, 而三元组中的$i$ 和 $j$ 的提及全部被保留.</p></li><li><p><strong>Basic Removal</strong>: 删除三元组$(i, k, j)$ 和三元组$(j, k ,i)$, 并删除$i$ 和$j$ 的所有提及.</p></li><li><p><strong>Thorough Removal</strong>: 在BASIC Removal的基础上, 额外按照以下规则移除三元组:</p><ol><li>$(i, \ast, j)$ 和 $(j, \ast, i)$.</li><li>$(i, k +j, \ast)$ 和 $(\ast, k+i, j)$.</li><li>$(i+k+j, \ast, \ast)$ 和 $(\ast, \ast, i+k+j)$.</li></ol><p>基于规则移除更多的从自然语言的角度考虑了更严格的移除方式.</p></li></ul><h2 id="Open-Knowledge-Graph-Embeddings"><a href="#Open-Knowledge-Graph-Embeddings" class="headerlink" title="Open Knowledge Graph Embeddings"></a>Open Knowledge Graph Embeddings</h2><p>作者提出了一种用于开放知识图谱的KGE方法, 以便评估不同设定对模型所带来的影响.</p><p>因为开放知识图谱中实体和关系的<strong>多重表述性</strong>, 必须对实体和关系进行<strong>Token Level Modeling</strong>, 这样才能保证模型能处理任何长度的实体和关系输入. 我们默认Token以Token Embedding作为输入.</p><p>作者将模型分为<strong>关系模型</strong>和<strong>组合函数</strong>两部分:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/predictnewfactfromopenkg7.jpg" style="zoom: 40%;" /><p>实体和关系都是由多个Token组成的, $\mathcal{V}(\mathcal{E})^{+}$ 是从词表$\mathcal{V}(\mathcal{E})$ 中提取的非空序列, 若用$d, o \in \mathbb{N}_+$ 分别代表实体和关系的嵌入维度, 则实体嵌入函数$f$ 为$f: \mathcal{V}(\mathcal{E})^{+} \rightarrow \mathbb{R}^{d}$, 同理关系嵌入函数$g$ 为$g: \mathcal{V}(\mathcal{R})^{+} \rightarrow \mathbb{R}^{o}$. 那么在实体和关系分别嵌入完后, 使用关系打分函数$\mathcal{RM}$ 对这对开放三元组打分, $\mathcal{R} \mathcal{M}: \mathbb{R}^{d} \times \mathbb{R}^{o} \times \mathbb{R}^{d} \rightarrow \mathbb{R}$.</p><p>总结下, 对于给定的三元组$(i, k, j)$, $i, j \in \mathcal{V}(\mathcal{E})^{+}$, $k \in \mathcal{V}(\mathcal{R})^{+}$, 其得分可以被计算为:<br>$$<br>s(i, k, j)=\mathcal{R} \mathcal{M}(f(i), g(k), f(j))<br>$$</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Models-and-Training"><a href="#Models-and-Training" class="headerlink" title="Models and Training"></a>Models and Training</h3><h4 id="Prototpical-Model"><a href="#Prototpical-Model" class="headerlink" title="Prototpical Model"></a>Prototpical Model</h4><p>作者将COMPLEX作为最后的关系打分函数, 将LSTM作为组合函数$f$ 和$g$, COMPLEX + LSTM作为评估方法好坏的基准模型.</p><h4 id="Diagnostic-Models"><a href="#Diagnostic-Models" class="headerlink" title="Diagnostic Models"></a>Diagnostic Models</h4><p>作者希望能量化有多少问题是无需问题的全部信息就能够给出答案, 作者提出两个用于对比的模型:</p><ul><li><strong>Predict - With - Rel</strong>: 对于问题只需要根据关系就能给出答案. 例如对于问题$(i, k, ?)$ 只需要根据$(r, ?)$ 就能够进行作答. 考虑到有头实体和尾实体的区别, 使用两个打分函数对其进行建模, 将其打分函数记为:<br>$$<br>s_{t}(k, e)=g(k)^{T} f(j), \quad s_{h}(i, k)=f(i)^{T} g(k)<br>$$</li></ul><p>  其中$s_{t}: \mathbb{R}^{o} \times \mathbb{R}^{d} \rightarrow \mathbb{R}$, $s_{h}: \mathbb{R}^{d} \times \mathbb{R}^{o} \rightarrow \mathbb{R}$, 分别为$(i, k, ?)$ 和$(?, k, j)$ 所设计.</p><ul><li><strong>Predict - With - Ent</strong>: 与前者类似, 直接忽略关系, 只计算实体对$(i, j)$ 的分数, 打分函数被设计为:</li></ul><p>$$<br>s_{e}(i, j)=f(i)^{T} f(j)<br>$$</p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>作者在<strong>测试集</strong>上分别用三种验证集选择模型, 并使用了不同的模型和不同的Leakage Removal方式, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/predictnewfactfromopenkg8.jpg" style="zoom: 50%;" /><p>作者在<strong>验证集</strong>上比较了不同验证集对模型性能的影响:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/predictnewfactfromopenkg9.jpg" style="zoom:50%;" /><h4 id="Influence-of-Leakage"><a href="#Influence-of-Leakage" class="headerlink" title="Influence of Leakage"></a>Influence of Leakage</h4><p>从测试集结果能看出, 不同泄露移除方式下的COMPLEX - LSTM表现不同. Leakage移除越严格, 模型的性能就越差. 说明在训练集中确实存在着大量的Test Leakage, 在开放知识图谱中, 该现象十分严重.</p><h4 id="Influence-of-Non-Relational-Information"><a href="#Influence-of-Non-Relational-Information" class="headerlink" title="Influence of Non - Relational Information"></a>Influence of Non - Relational Information</h4><p>从测试集结果中看出, 所有的只使用Entity信息预测结果均为0, 而只使用关系进行预测, 仍然能够达到全部信息预测性能的20% ~ 25%.</p><h4 id="Effectiveness-of-Mention-Ranking"><a href="#Effectiveness-of-Mention-Ranking" class="headerlink" title="Effectiveness of Mention - Ranking"></a>Effectiveness of Mention - Ranking</h4><p>从验证集结果中能看出, 使用LINKED验证集的效果是最好的. 因为在三种验证集中, 只有LINKED能够使用实体到实体提及的映射, 即Mention Ranking Protocol, 这确实证明了OKG中引入实体知识消歧的重要性.</p><h4 id="Influence-of-Model-Selection"><a href="#Influence-of-Model-Selection" class="headerlink" title="Influence of Model Selection"></a>Influence of Model Selection</h4><p>从测试集结果中看出, 在THOROUGH设定下的COMPLEX + LSTM对于不同的验证集表现不同. 人工干预最多的LINKED性能最好, 但MENTION和ALL的表现其实相差无几, 没有人工干预的ALL的表现最差. 作者认为只使用包含实体提及的验证数据就足够了, 这样可以避免昂贵的实体消歧. 此外, 即使在验证集上LINKED和MENTION表现差距很大, 但实际上在测试集中并不能很好的体现出它们二者的差异, 作者认为使用自己所提出的协议来计算MRR并不能有益于模型选择.</p><h4 id="Overall-Performance"><a href="#Overall-Performance" class="headerlink" title="Overall Performance"></a>Overall Performance</h4><p>与COMPLEX在普通KG上的性能进行对比, 在OKG上的性能下降十分明显. 作者认为有以下四个原因;</p><ol><li>OKG中的文本噪声过多.</li><li>评估数据的难度非常大.</li><li>OKG非常的零散, 导致信息流被抑制.</li><li>问题不一定知道所有的正确答案.</li></ol><h4 id="Model-and-Data-Errors"><a href="#Model-and-Data-Errors" class="headerlink" title="Model and Data Errors"></a>Model and Data Errors</h4><p>作者将错误的Link Prediction分为三类:</p><ul><li>correct sense / wrong entity: 排名最高的提及在语义上是可以理解的, 但是不正确.</li><li>wrong sense: 预测的结果小方向上是不对的, 但大方向上正确, 还靠点边.</li><li>noise: 与预测结果完全不一致, 压根不沾边.</li></ul><p>100个采样收集的错误案例分类如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/predictnewfactfromopenkg10.jpg" style="zoom: 50%;" /><p>下面一栏中, 作者评估了在OPEIC中发生的提取错误三元组和概念性的事实. 从中看到, 模型的低性能大多并不是噪声所致. 有74%的预测都是正确的但是不常见的事实.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>作者根据开放知识图谱和普通知识图谱之间的特性差异提出了Open Link Prediction, 并提出了OKG中的评估协议. 作者建立了新的数据集OLPBENCH, 包含了实体到实体提及之间的映射.</p><p>最后, 作者全面的评估了OKG中模型选择, Leakage, 非关系信息所带来的影响. </p><p>本文仍然有诸多不理解的地方, 对OKG接触的还比较少, 等有新理解后会后续补充.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> OKG </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Generative Adversarial Zero-Shot Relational Learning for KGs</title>
      <link href="/posts/62547.html"/>
      <url>/posts/62547.html</url>
      
        <content type="html"><![CDATA[<h1 id="Generative-Adversarial-Zero-Shot-Relational-Learning-for-Knowledge-Graphs"><a href="#Generative-Adversarial-Zero-Shot-Relational-Learning-for-Knowledge-Graphs" class="headerlink" title="Generative Adversarial Zero-Shot Relational Learning for Knowledge Graphs"></a>Generative Adversarial Zero-Shot Relational Learning for Knowledge Graphs</h1><p>本文是论文<a href="https://arxiv.org/abs/2001.02332" target="_blank" rel="noopener">Generative Adversarial Zero-Shot Relational Learning for Knowledge Graphs</a>的阅读笔记和个人理解. 本论文涉及到大量关于GAN的内容, 我对GAN还不是很熟悉, 在论文中具体的内容也不展开讲了, 我会放在推荐阅读中. 其中涉及到的地方如果有错误欢迎指出.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者指出, 即使是现在的大规模知识图谱, 仍然有可能无法满足日益增长的<strong>扩展</strong>需求. </p><p>对于新加入的关系, 经典的KGE算法无法应对新加入的关系. 而获取人为标注的数据是非常困难的, 因此作者希望能通过<strong>Zero - Shot Learning</strong>来缓解数据缺失的问题. 更具体点, 从关系的<strong>文本</strong>中学习它们的语义特征, 来增强在没有见过任何样本的情况下对没见过的关系的认知能力. </p><p>作者提出了一种KGE<strong>范式</strong>, 能够把<strong>任意</strong>的KGE方法应用于此.</p><h2 id="Convert-into-Knowledge-Transfer-Problem"><a href="#Convert-into-Knowledge-Transfer-Problem" class="headerlink" title="Convert into Knowledge Transfer Problem"></a>Convert into Knowledge Transfer Problem</h2><p>作者将Zero Shot Learning转化为一个<strong>知识迁移</strong>问题, 作者将关注如何只用文本描述生成没见过的关系嵌入, 这样, 只要经过训练, 模型能对任意关系在不进行Fine Tune的情况下生成嵌入. 通过关系嵌入, 能够对没有见过的关系通过<strong>余弦相似度</strong>简单的识别.</p><p>最首要的问题就是将文本语义空间的信息<strong>迁移</strong>到KG语义空间, 对此作者采用GAN来做知识迁移, 作者提出的架构如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ganzslforkg1.jpg" style="zoom: 33%;" /><p>对于模型已经见过的文本, 训练一个生成器(Generator), 能够从文本中生成相应的<strong>Fake</strong> Relation Embedding, 再针对该关系中所涉及到的头尾实体使用某种方法编码成<strong>Real</strong> Relation Embedding. 将真与假的Relation Embedding<strong>交替</strong>输入判别器(Discriminator), 让它来判断Relation Embedding到底是真的还是假的. </p><p>当Generator生成的数据能够<strong>以假乱真</strong>时, 生成器所生成的Relation Embedding就能直接<strong>近似</strong>的当做是未见过的关系的Relation Embedding, 在Link Prediction任务中, 便能轻松应对未见过的关系.</p><h2 id="Zero-Shot-Learning-in-KG"><a href="#Zero-Shot-Learning-in-KG" class="headerlink" title="Zero - Shot Learning in KG"></a>Zero - Shot Learning in KG</h2><p>有必要稍微说一下Zero - Shot Learning在Link Prediction的设定.</p><p>对于一个查询关系三元组$\left\{\left(e_{1}, r, e_{2}\right)\right\}$, 给定头实体和关系元组$(e_1, r)$, 假设其应该对应的候选尾实体$e_2^\prime \in C_{\left(e_{1}, r\right)}$, 其真正的尾实体是$e_2$. 模型应该能将真正尾实体$e_2$ 排在最高, 而其余候选集合的实体$e_2^{\prime}$ 应该排在后面. </p><p>在Zero - Shot Learning的设置下, 还应该有见过的关系集$R_{s}=\left\{r_{s}\right\}$ 和没有见过的关系集$R_{u}=\left\{r_{u}\right\}$, 显然$R_{s} \cap R_{u}=\emptyset$.</p><p>对于训练集, 所有关系都是<strong>见过</strong>的, 即:<br>$$<br>D_{s}=\left\{\left(e_{1}, r_{s}, e_{2}, C_{\left(e_{1}, r_{s}\right)}\right) \mid e_{1} \in E, r_{s} \in R_{s}, e_{2} \in E\right\}<br>$$<br>在测试集中, 所有关系都是<strong>没有见过</strong>的, 即:<br>$$<br>D_{u}=\left\{\left(e_{1}, r_{u}, e_{2}, C_{\left(e_{1}, r_{u}\right)}\right) \mid e_{1} \in E, r_{u} \in R_{u}, e_{2} \in E\right\}<br>$$<br>出于可行性, 作者将所有实体设置为闭集, 即测试集中出现的所有<strong>实体</strong>均在训练集中<strong>见过</strong>.</p><h2 id="Model-for-Zero-Shot-KG-Relational-Learning"><a href="#Model-for-Zero-Shot-KG-Relational-Learning" class="headerlink" title="Model for Zero-Shot KG Relational Learning"></a>Model for Zero-Shot KG Relational Learning</h2><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ganzslforkg3.jpg" style="zoom: 33%;" /><p>该图在后文中讲解关于GAN的部分会再次出现.</p><p>作者提出的方法中, 核心问题是如何设计一种<strong>条件生成模型</strong>, 去学习从文本中生成高质量的关系嵌入. 在作者的框架中, 主要设计了三个组件:</p><ul><li><strong>Feature Encoder</strong>: 仅通过实体生成真实的Relation Embedding.</li><li><strong>Generator</strong>: 在文本表示下生成合理的Relation Embedding.</li><li><strong>Discriminator</strong>: 判断数据的来源真假, 并识别<strong>关系类别</strong>.</li></ul><h3 id="Feature-Encoder"><a href="#Feature-Encoder" class="headerlink" title="Feature Encoder"></a>Feature Encoder</h3><p>对于没见过的关系, 普通KGE方法是无法取得其Embedding的.</p><p>Feature Encoder使用的Embedding可以是任意KGE模型得来的Embedding, 这也就是为什么作者说这是一种Zero - Shot的使用范式, 能够应用于任何的KGE方法.</p><p>Feature Encoder应该是在GAN训练前<strong>预先训练好</strong>的, 训练生成器和判别器时, 其参数应该不变.</p><h4 id="Network-Architecture"><a href="#Network-Architecture" class="headerlink" title="Network Architecture"></a>Network Architecture</h4><p>出于大规模KG的复杂度, 考虑到实验的可行性, 对于每个实体对中的$e$, 作者只考虑到它们的一阶邻居$\mathcal{N}_{e}$:<br>$$<br>\mathcal{N}_{e}=\left\{\left(r^{n}, e^{n}\right) \mid\left(e, r^{n}, e^{n}\right) \in \mathcal{G}\right\}<br>$$<br><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ganzslforkg2.jpg" style="zoom: 25%;" /></p><p>假设在原来的KGE模型中, 给出的邻居关系和实体Look Up Table Embedding分别为$v_{r^{n}}, v_{e^{n}}$, 其嵌入维度为$d$, 那么该实体$e$ 一阶邻居的所有信息$u_e$ 能被表示为:<br>$$<br>\begin{array}{l}<br>f_{1}\left(v_{r^{n}}, v_{e^{n}}\right)=W_{1}\left(v_{r^{n}} \oplus v_{e^{n}}\right)+b_{1} \\<br>u_{e}=\sigma\left(\frac{1}{\left|\mathcal{N}_{e}\right|} \sum_{\left(r^{n}, e^{n}\right) \in \mathcal{N}_{e}} f_{1}\left(v_{r^{n}}, v_{e^{n}}\right)\right)<br>\end{array}<br>$$<br>其中的$\sigma$ 是$\operatorname{tanh}$. $\oplus$ 代表Concat操作. 出于可行性, 作者<strong>限制</strong>了邻居的采样个数.</p><blockquote><p>将该实体对应在图中, 这个操作其实本质上就是<strong>图聚合</strong>.</p></blockquote><p>对于实体对$(e_1, e_2)$ 中的两个实体本身也需要被编码, 只考虑用前馈神经网络简单的进行编码即可:<br>$$<br>\begin{array}{l}<br>f_{2}\left(v_{e}\right)=W_{2}\left(v_{e}\right)+b_{2} \\<br>u_{e p}=\sigma\left(f_{2}\left(v_{e_{1}}\right) \oplus f_{2}\left(v_{e_{2}}\right)\right)<br>\end{array}<br>$$<br>最后将计算出的实体对编码和实体对中实体的邻居编码Concat起来, 得到$e_1, e_2$ 之间的关系表示$x_{(e_1, e_2)}$:<br>$$<br>x_{\left(e_{1}, e_{2}\right)}=u_{e_{1}} \oplus u_{e p} \oplus u_{e_{2}}<br>$$<br>上述过程中$W_1 \in R^{d \times 2d}, W_2 \in R^{d \times d}, b_1,b_2 \in R^d$ 均为可以学习的参数.</p><h4 id="Pretrained-Strategy"><a href="#Pretrained-Strategy" class="headerlink" title="Pretrained Strategy"></a>Pretrained Strategy</h4><p>作者指出, 预训练的关键是学习到了簇状结构数据的分布, 一般具有簇内的高相似度和簇外的低相似度. 对于每个关系$r_s$, 在每个Training Step中, 采样以下三种三元组:</p><ul><li>$\left\{e_{1}^{\star}, r_{s}, e_{2}^{\star}\right\}$: 直接<strong>无差别</strong>的随机从KG中选择与$r_s$ 相关的三元组, 称为<strong>参考三元组</strong>.</li><li>$\left\{e_{1}^{+}, r_{s}, e_{2}^{+}\right\}$: 从训练集中, 采样包含关系$r_s$ 的<strong>正例三元组</strong>.</li><li>$\left\{e_{1}^{+}, r_{s}, e_{2}^{-}\right\}$: 从其余的训练集中, 做替换尾实体的<strong>负采样</strong>, 称为<strong>负例三元组</strong>.</li></ul><p>通过Feature Encoder能生成参考三元组的真实关系表示$x_{(e_1^{\star}, e_2^{\star})}$, 然后分别计算参考三元组与正例负例三元组之间的<strong>余弦相似度</strong>:<br>$$<br>\begin{aligned}<br>score^+_\omega &amp;=\operatorname{cosine}(x_{(e_1^{\star}, e_2^{\star})}, x_{(e_1^{+}, e_2^{+})}) \\<br>score^-_\omega &amp;=\operatorname{cosine}(x_{(e_1^{\star}, e_2^{\star})}, x_{(e_1^{+}, e_2^{-})})<br>\end{aligned}<br>$$<br>那么最终目标就是<strong>最大化间隔</strong>:<br>$$<br>L_{\omega}=\max \left(0, \gamma+\operatorname{score}_{\omega}^{+}-\operatorname{score}_{\omega}^{-}\right)<br>$$</p><p>其中$\gamma$ 是间隔, $\omega$ 是模型中涉及到的所有可学习参数. 通过计算余弦相似度, Feature Encoder能尽可能的将实体之间的关系<strong>聚类</strong>, 从而生成到作者所说的<strong>簇状</strong>结构数据.</p><h3 id="Generative-Adversarial-Model"><a href="#Generative-Adversarial-Model" class="headerlink" title="Generative Adversarial Model"></a>Generative Adversarial Model</h3><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ganzslforkg3.jpg" style="zoom: 50%;" /><h4 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h4><p>生成器的作用是从<strong>嘈杂</strong>的文本$T_r$ 中生成伪造的Relation Embedding. 但因为文本中经常伴随着非常多的停止词之类的<strong>无意义词语</strong>, 所以作者简单的<strong>去除停止词和标点</strong>, 并使用<strong>TF - IDF</strong>来分配词语的权重. 词向量直接使用<strong>Word2Vec</strong>将词语转化为稠密向量. 在Sentence Level Modeling上, 作者使用无视语序的<strong>词袋模型</strong>对句子建模. </p><blockquote><p>至于为什么作者没有在词向量上使用BERT, 在后文的实验中作者做了探究.</p></blockquote><p>Generator在生成数据时, 一般都需要加入一些<strong>噪声</strong>. 在这里, 作者加入高斯随机噪声$z \in R^Z$, $z$ 是从高斯分布$N(0, 1)$ 中采样的来的向量.</p><p>作者将高斯噪声$z$ 和TF - IDF后的句向量Concat起来, 经过两个FC层和一个Layer Norm, 最后得到生成器$G$ 通过文本生成的Relation Embedding $\tilde{x}_{r}$, 即$\tilde{x}_{r} \leftarrow G_{\theta}\left(T_{r}, z\right)$, 其中$\theta$ 是参数.</p><p> 对于GAN的训练方面, 为了避免<strong>模型崩溃</strong>并<strong>增强多样性</strong>(在推荐阅读中, 有解释GAN训练上出现的常见问题), 作者使用了WGAN中的Wasserstein Loss.</p><p>作者继续添加了分类损失, 其形式与Feature Encoder中使用的最大化间隔损失类似, 计算它分别与正例(在GAN训练时正例指$\tilde{x}_{r}$)和负例三元组之间的相似度, 但作者将<strong>簇中心</strong>$x_c^r$ 作为真实的关系表示:<br>$$<br>x_{c}^{r}=\frac{1}{N_{r}} \sum_{i=1}^{N_{r}} x_{\left(e_{1}, e_{2}\right)}^{i}<br>$$<br>其中$N_r$ 为涉及到关系$r_s$ 所有的三元组个数.</p><p>那么分类损失可以被写成:<br>$$<br>\begin{aligned}<br>score^+_\omega &amp;=\operatorname{cosine}(x_{c}^r, \tilde x_r) \\<br>score^-_\omega &amp;=\operatorname{cosine}(x_{c}^r, x_{(e_1^{+}, e_2^{-})}) \\<br>L_{cls}\left(G_{\theta}\left(T_{r}, z\right)\right)&amp;=\max \left(0, \gamma+\operatorname{score}_{\omega}^{+}-\operatorname{score}_{\omega}^{-}\right)<br>\end{aligned}<br>$$<br>最后, 作者还添加了Visual Pivot Regularization, 用于增加类内的区别. </p><p>生成器的损失函数如下:</p><p>$$<br>L_{G_{\theta}}=-\mathbb{E}_{z \sim p_{z}}\left[D_{\phi}\left(G_{\theta}\left(T_{r}, z\right)\right)\right] +L_{c l s}\left(G_{\theta}\left(T_{r}, z\right)\right)+L_{P}<br>$$<br>第一项是Wasserstein Loss, 第二项是Classification Loss, 第三项是Visual Pivot Regularization.</p><h4 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h4><p>判别器的作用是用于识别输入是否是伪造的. 生成器的结构比较简单, 只由一层使用Leaky ReLU的FC层, 接上Layer Norm组成, 最后结果分别用于判别真假和对关系分类, 损失函数如下:</p><p>$$<br>L_{D_{\phi}}=\mathbb{E}_{z \sim p_{z}}\left[D_{\phi}\left(G_{\theta}\left(T_{r}, z\right)\right)\right]-\mathbb{E}_{x \sim p_{\text {data}}}\left[D_{\phi}(x)\right]<br>+\frac{1}{2} L_{c l s}\left(G_{\theta}\left(T_{r}, z\right)\right)+\frac{1}{2} L_{c l s}(x)+L_{G P}<br>$$<br>第一二项仍然来自于Wasserstein Loss, 第三四项来源于分类, 分别对应着生成器的假数据$\tilde{x}_{r}$ 和Feature Encoder生成的真实数据$x_{(e_1, e_2)}$, 最后一项是梯度惩罚项.</p><blockquote><p>GAN相关的细节还不太懂, 不瞎解释了.</p></blockquote><h4 id="GAN-Training-Process"><a href="#GAN-Training-Process" class="headerlink" title="GAN Training Process"></a>GAN Training Process</h4><p>在讲解完生成器和判别器后, 作者将GAN训练流程总结如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ganzslforkg4.jpg" style="zoom: 50%;" /><p>在GAN中, 通常采用多次更新判别器参数后只更新一次生成器的策略.</p><h3 id="Predicting-Unseen-Relations"><a href="#Predicting-Unseen-Relations" class="headerlink" title="Predicting Unseen Relations"></a>Predicting Unseen Relations</h3><p>在训练完GAN后, 生成器对于一个没有见过的关系描述文本$T_{r_u}$ 应该能给出合理的Relation Embedding, 即$\tilde{x}_{r_u} \leftarrow G_{\theta}\left(T_{r_u}, z\right)$. 在已知$(e_1, r_u)$ 的情况下, 我们应该根据生成器所生成的$\tilde{x}_{r_u}$ 与Feature Encoder产生的$x_{(e_1, e_2)}$ 的余弦相似度来评估计排名:<br>$$<br>\operatorname{score}_{\left(e_{1}, r_{u}, e_{2}\right)}=\operatorname{cosine}\left(\tilde{x}_{r_u}, x_{(e_1, e_2)}\right)<br>$$<br>但生成器会为生成的数据添加<strong>噪声</strong>, 为了尽可能弱化随机造成的影响, 作者采用生成多组数据最后取平均的方法计算得分: </p><p>$$<br>\operatorname{score}_{\left(e_{1}, r_{u}, e_{2}\right)}=\frac{1}{N_{\text {test}}} \sum_{i=1}^{N_{\text {test}}} \text {score}_{\left(e_{1}, r_{u}, e_{2}\right)}^{i}<br>$$</p><p>即生成任意数量$N_{test}$ 的Relation Embedding$\left\{\tilde{x}_{r_{u}}^{i}\right\}_{i=1,2, \dots, N_{\text {test}}}$, 最后取平均余弦相似度作为真正的得分.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>在本节中, 详细的参数设置请参考原论文.</p><h3 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h3><p>作者发现没有可用的ZS数据集, 所以作者自己根据需要制作了两个数据集:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ganzslforkg5.jpg" style="zoom: 50%;" /><p>作者选取数据集的标准是, 满足<strong>大规模</strong>, 并含有<strong>关系文本描述</strong>.</p><h3 id="Baselines"><a href="#Baselines" class="headerlink" title="Baselines"></a>Baselines</h3><p>在非常普遍使用的KGE方法中, 它们基本<strong>不具备</strong>Zero - Shot的能力, 因为对于没有见过的关系, 在它们所存储的Embedding矩阵中是查询不到的, 所以作者用与Generator类似的方法为它们添加了Zero - Shot的能力: 使用一个与Generator类似的神经网络结构, 而不是直接为未知的关系生成随机的Relation Embedding. 在这个条件下, 模型能够直接根据文本信息生成未见过关系的Embedding, 然后继续结合它们原来的目标函数来调整Entity Embedding和与Generator类似的结构的参数.</p><p>例如, 在TransE中, Entity Embedding和Relation Embedding都是使用一个Look Up Table存储的:<br>$$<br>f_{\text {Trans} E}(\boldsymbol{h}, \boldsymbol{r}, \boldsymbol{t})=\left|v_{h}+v_{r}-v_{t}\right|_{1 / 2}<br>$$<br>在经过改造后, 它们的Relation Embedding不再通过Look Up Table给出, 而是通过类似生成器的结构$g$ 从文本$T_r$ 中生成:<br>$$<br>v_{r}=g\left(T_{r}\right)<br>$$<br>在对实体排名时, 也是按照它们原本的打分函数进行排名.</p><p>对于DisMult, ComplEx亦是如此. RESCAL都没有替换Relation Embedding的位置, 所以就不对它进行比较了.</p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>作者将改进后的Zero - Shot模型在自己制作的数据集NELL - ZS和Wiki - ZS上测试了它们Link Prediction的能力, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ganzslforkg6.jpg" style="zoom: 33%;" /><p>其中ZSGAN代表使用了GAN训练框架, 即在之前的基础上加入了判别器, 并引入GAN相关的训练损失.下划线代表该结果在ZSGAN中是最棒的.</p><p>能从结果中观察到, 所有使用ZSGAN的模型都比未使用GAN的Baseline效果要好, 并且在这些模型中, DisMult体现出更好的能力.</p><h3 id="Analysis-of-Textual-Representations"><a href="#Analysis-of-Textual-Representations" class="headerlink" title="Analysis of Textual Representations"></a>Analysis of Textual Representations</h3><h4 id="Text-Descriptions-Analysis"><a href="#Text-Descriptions-Analysis" class="headerlink" title="Text Descriptions Analysis"></a>Text Descriptions Analysis</h4><p>作者将自制的两个数据集中的关系文本描述词频统计了出来:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ganzslforkg7.jpg" style="zoom: 50%;" /><p>NELL - ZS的文本长度明显没有Wiki - ZS高.</p><p>在经过TF - IDF过滤后, 作者统计了TF - IDF &gt; 0.3的词语个数:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ganzslforkg8.jpg" style="zoom: 50%;" /><p>在使用了TF - IDF后, 大多数重要的词语都落在$[2, 5]$ 区间内, 说明TF - IDF能够比较有效的降噪.</p><h4 id="Word-Representations"><a href="#Word-Representations" class="headerlink" title="Word Representations"></a>Word Representations</h4><p>作者其实尝试过现在非常流行的词向量表示BERT(Transformer Encoder):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ganzslforkg10.jpg" style="zoom: 50%;" /><p>但实际上效果不理想, 作者认为其中与以下原因有关:</p><ol><li>BERT引入了更多的<strong>句子级噪声</strong>, 在这个任务上似乎句子中的信息是无效的.</li><li>BERT产生的Word Embedding<strong>维度过高</strong>, 不利于GAN的训练.</li></ol><blockquote><p>我认为跟BERT的打开方式有一定的关联, Self - Attention本身就会根据句子中的其他内容来调整自身的表达, 跟TF - IDF的功能有一部分重复的地方.</p></blockquote><h3 id="Quality-of-Generated-Data"><a href="#Quality-of-Generated-Data" class="headerlink" title="Quality of Generated Data"></a>Quality of Generated Data</h3><p>作者为了评估生成器所生成的Relation Embedding的质量, 作者计算Feature Encoder生成的簇中心$x_r^c$ 和生成器生成的关系嵌入$\tilde x_r$ 之间的<strong>余弦相似度</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ganzslforkg9.jpg" style="zoom: 50%;" /><p>能看到, 生成器所生成的关系嵌入的余弦相似度基本与模型的MRR和Hits@10表现成正比.</p><h3 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h3><p>作者尝试过类似CNN, LSTM对Sentence建模的方法, 但都失败了. 作者认为这些方法都引入了更多的参数, 从而不利于GAN的学习, 会导致GAN的过拟合. 所以对句子建模只是使用简单的使用词袋模型, 结合TF - IDF. 此外, 现在作者方法中的实体均为闭集, 将来会考虑到实体在开集中的处理.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>本文非常巧妙的想到用GAN结合KGE处理在未见过的关系上的问题. 作者通过使用Feature Encoder来构造真实的数据分布, 并通过一个简单的生成器来伪造数据, 当生成器能够很好的骗过判别器后, 直接使用生成器生成的Relation Embedding近似当做真实的Relation Embedding.</p><p>作者声称, 这是在KG领域的首个Zero - Shot关系学习方法.</p><p>抛开作者提出的方法本身不谈, 就作者在论文中提到关于GAN的内容来说, 有很大一部分是关于GAN的<strong>训练Trick</strong>. GAN虽然是一个非常好的idea. 但<strong>GAN的训练</strong>可能是一个很大的问题, 这点在实验结果中也多次提到, 在模型中引入的其他部分都有可能会对GAN的训练产生很大影响, 导致训练的不稳定.</p><h2 id="Recommended"><a href="#Recommended" class="headerlink" title="Recommended"></a>Recommended</h2><p>关于本文的:</p><ul><li><a href="https://www.bilibili.com/video/av541082248/" target="_blank" rel="noopener">AAAI2020丨知识图的生成性对抗式零样本关系学习</a></li><li><a href="https://zhuanlan.zhihu.com/p/112908641" target="_blank" rel="noopener">《基于对抗生成的Zero-Shot知识图谱关系学习》阅读笔记</a></li></ul><p>关于文中涉及到的GAN:</p><ul><li><a href="https://zhuanlan.zhihu.com/p/25071913" target="_blank" rel="noopener">令人拍案叫绝的Wasserstein GAN</a></li><li><a href="https://zhuanlan.zhihu.com/p/68120231" target="_blank" rel="noopener">提高GAN训练稳定性的9大tricks</a></li><li><a href="https://zhuanlan.zhihu.com/p/84617531" target="_blank" rel="noopener">Wasserstein距离学习笔记</a></li><li><a href="https://zhuanlan.zhihu.com/p/33752313" target="_blank" rel="noopener">通俗理解生成对抗网络GAN</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> GAN </tag>
            
            <tag> ZSL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>R-MeN: A Relational Memory-based Embedding Model</title>
      <link href="/posts/2954.html"/>
      <url>/posts/2954.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>Self - Attention: 详见<a href="https://adaning.github.io/posts/6744.html#Self-Attention">Transformer精讲</a>.</li></ul><p><strong>2020.12.14</strong>: 修正错误.</p></blockquote><h1 id="A-Relational-Memory-based-Embedding-Model-for-Triple-Classification-and-Search-Personalization"><a href="#A-Relational-Memory-based-Embedding-Model-for-Triple-Classification-and-Search-Personalization" class="headerlink" title="A Relational Memory-based Embedding Model for Triple Classification and Search Personalization"></a>A Relational Memory-based Embedding Model for Triple Classification and Search Personalization</h1><p>本文是论文<a href="https://arxiv.org/abs/1907.06080" target="_blank" rel="noopener">A Relational Memory-based Embedding Model for Triple Classification and Search Personalization</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者认为, 现在的KGE方法普遍在<strong>记忆有效的三元组问题</strong>上受限, 并不能<strong>有效的捕捉三元组之间的潜在依赖关系</strong>.</p><p>现在的KGE普遍都用Link Prediction评估模型的性能, 但在<strong>实际应用</strong>中, 经常以<strong>三元组分类</strong>和<strong>个性化搜索</strong>为主要目标. 所以作者希望通过记忆提升这两个场景中的KGE性能, 关系记忆能够编码关系三元组之间的潜在依赖关系.</p><blockquote><p>作者给出以下问题的定义:</p><ul><li>三元组分类: 判断给定的三元组是否<strong>有效</strong>.</li><li>个性化搜索: 针对用户给出的<strong>请求</strong>, 对系统给出的搜索结果进行<strong>重排</strong>.</li></ul></blockquote><h2 id="R-MeN"><a href="#R-MeN" class="headerlink" title="R - MeN"></a>R - MeN</h2><p>在R - MeN中, 作者考虑将三元组视为一个长度为3的<strong>时序序列</strong>看待, 通过<strong>Self - Attention</strong>对记忆<strong>循环交互</strong>.</p><p>整体流程是Embedding -&gt; Memory Attention interact -&gt; CNN decode:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rmen1.jpg" style="zoom: 50%;" /><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>作者假设三元组$(s, r, o)$ 的<strong>相对位置关系</strong>对于推断三元组是否有效是十分必要的, 因此需要在三元组输入之前对它们都加以位置编码, 用$\mathbf{v}_{s}, \mathbf{v}_{r}, \mathbf{v}_{o} \in \mathbb{R}^{d}$ 分别代表$s,r,o$ 的嵌入, 加上位置编码后, 再经过一次线性变换的三元组向量$\left\{\mathbf{x}_{1}, \mathbf{x}_{2}, \mathbf{x}_{3}\right\}$ 为:<br>$$<br>\begin{aligned}<br>\mathbf{x}_{1} &amp;=\mathbf{W}\left(\mathbf{v}_{s}+\mathbf{p}_{1}\right)+\mathbf{b} \\<br>\mathbf{x}_{2} &amp;=\mathbf{W}\left(\mathbf{v}_{r}+\mathbf{p}_{2}\right)+\mathbf{b} \\<br>\mathbf{x}_{3} &amp;=\mathbf{W}\left(\mathbf{v}_{o}+\mathbf{p}_{3}\right)+\mathbf{b}<br>\end{aligned}<br>$$<br>其中$\mathbf{W} \in \mathbb{R}^{k\times d}$ 为权重矩阵, $\mathbf{p}_{1}, \mathbf{p}_{2} \mathbf{p}_{3} \in \mathbb{R}^{d}$  分别它们的位置编码, 其中的$k$ 代表每个Memory Slot的维度大小(下文会提到). 在文中, 并没有提及位置编码的实现方式. </p><h3 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h3><p>作者假设有一个记忆$M \in \mathbb{R}^{N \times k}$, 它是由$N$ 行Memory Slot组成的, 每个Slot的大小为$k$. 用$M^{(t)}$ 代表$t$ 时刻的$M$ 矩阵, 那么$M_{i,:}^{(t)} \in \mathbb{R}^{k}$ 就代表其中第$i$ 个Memory Slot在$t$ 时刻存储的所有Memory.</p><p>那么每个Memory Slot中的记忆就一定会根据在$t$ 时刻的新输入$\mathbf{x}_t$ 来<strong>更新</strong>当前时刻的记忆$\hat{M}_{i,:}^{(t+1)}$.</p><p>如果使用跟Transformer中的<strong>多头</strong>类似的机制, 对每个Slot中的Memory进行分割:<br>$$<br>\hat{M}_{i,:}^{(t+1)}=\left[\hat{M}_{i,:}^{(t+1), 1} \oplus \hat{M}_{i,:}^{(t+1), 2} \oplus\right.\left.\ldots \oplus \hat{M}_{i,:}^{(t+1), H}\right]<br>$$<br>其中$\oplus$ 代表Concat操作, $H$ 代表总头数.</p><p>每个头的记忆$\hat{M}_{i,:}^{(t+1), h}$ 使用Self - Attention将过去时刻的记忆与当前时刻输入<strong>加权交互</strong>:<br>$$<br>\hat{M}_{i,:}^{(t+1), h}=\alpha_{i, N+1, h}\left(\mathbf{W}^{h, V} \mathbf{x}_{t}\right)+\sum_{j=1}^{N} \alpha_{i, j, h}\left(\mathbf{W}^{h, V} M_{j,:}^{(t)}\right)<br>$$<br>其中$\mathbf{W}^{h, V} \in \mathbb{R}^{n \times k}$ 是Value的投影矩阵, $n$ 是每个头的大小, 满足$k = nH$, 这样能保证Concat后的记忆矩阵中每个Slot的大小仍然是$k$ 维. $\left\{\alpha_{i, j, h}\right\}_{j=1}^{N}, \alpha_{i, N+1, h}$ 是注意力权重. 具体细节在下一小节介绍.</p><blockquote><p>三元组中的三个元素被分别输入后, 记忆将被<strong>清除</strong>.</p></blockquote><h3 id="Self-Attention-Interaction"><a href="#Self-Attention-Interaction" class="headerlink" title="Self - Attention Interaction"></a>Self - Attention Interaction</h3><p>与Self - Attention大同小异, 但在<strong>记忆存储机制</strong>的影响下, 归一化一定要包含<strong>过去记忆</strong>和<strong>当前时刻新输入</strong>两个部分:<br>$$<br>\begin{aligned}<br>\alpha_{i, j, h} &amp;=\frac{\exp \left(\beta_{i, j, h}\right)}{\sum_{m=1}^{N+1} \exp \left(\beta_{i, m, h}\right)} \\<br>\alpha_{i, N+1, h} &amp;=\frac{\exp \left(\beta_{i, N+1, h}\right)}{\sum_{m=1}^{N+1} \exp \left(\beta_{i, m, h}\right)}<br>\end{aligned}<br>$$<br>$\beta_{i, j, h}$ 是Attention得分, 也分别对应着过去记忆和当前时刻新输入两部分:<br>$$<br>\begin{aligned}<br>\beta_{i, j, h} &amp;=\frac{\left(\mathbf{W}^{h, Q} M_{i,:}^{(t)}\right)^{\top}\left(\mathbf{W}^{h, K} M_{j,:}^{(t)}\right)}{\sqrt{n}} \\<br>\beta_{i, N+1, h} &amp;=\frac{\left(\mathbf{W}^{h, Q} M_{i,:}^{(t)}\right)^{\top}\left(\mathbf{W}^{h, K} \mathbf{x}_{t}\right)}{\sqrt{n}}<br>\end{aligned}<br>$$<br>其中$\mathbf{W}^{h, Q} \in \mathbb{R}^{n \times k}$ , $\mathbf{W}^{h, K} \in \mathbb{R}^{n \times k}$ 分别是Query和Key的投影矩阵. 此外, 还在$\mathbf{x}_{t}$ 和$\hat{M}_{i,:}^{(t+1), h}$ 之间添加了Residual Connection和MLP, Gating, 具体添加方式作者未说明, 应该和关系记忆网络一致:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rmen7.jpg" style="zoom:33%;" /><p>图片来自于关系记忆网络的论文<a href="https://arxiv.org/abs/1806.01822" target="_blank" rel="noopener">Relational recurrent neural networks</a>. </p><p>对于输入$\mathbf{x}_t$ 的最后编码产生的结果记为$\mathbf{y}_t \in \mathbb{R}^k$.</p><h3 id="Score-Function"><a href="#Score-Function" class="headerlink" title="Score Function"></a>Score Function</h3><p>将之前得出的$\left\{\mathbf{y}_{1}, \mathbf{y}_{2}, \mathbf{y}_{3}\right\}$ Concat起来, 接着用一个基于CNN的Decoder来解码:<br>$$<br>f(s, r, o)=\max \left(\operatorname{ReLU}\left(\left[\mathbf{y}_{1}, \mathbf{y}_{2}, \mathbf{y}_{3}\right] \ast \mathbf{\Omega}\right)\right)^{\top} \mathbf{w}<br>$$<br>$\left[\mathbf{y}_{1}, \mathbf{y}_{2}, \mathbf{y}_{3}\right] \in \mathbb{R} ^ {k \times 3}$, $\Omega$ 代表大小为$\mathbb{R} ^ {m \times 3}$ 的卷积核集合, 其中$m$ 为卷积核的窗口大小, $\ast$ 代表卷积操作. $\mathbf{w} \in \mathbb{R}^{\left| \Omega \right|}$ 是一个权重向量, 能将所有卷积核卷积得到的结果变成标量(其实就是得分). $\operatorname{max}$ 在这里代表<strong>最大池化</strong>. 作者认为这样能捕捉Feature map中的最重要特征, 并减小参数量. </p><h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><p>采用Adam优化, 损失函数如下:<br>$$<br>\mathcal{L}=\sum_{(s, r, o) \in\left\{\mathcal{G} \cup \mathcal{G}^{\prime}\right\}} \log \left(1+\exp \left(-t_{(s, r, o)} \cdot f(s, r, o)\right)\right)<br>$$<br>其中$t_{(s, r, o)}$ 是一个符号函数:<br>$$<br>t_{(s, r, o)}=\left\{\begin{array}{l}<br>1 \text { for }(s, r, o) \in \mathcal{G} \<br>-1 \text { for }(s, r, o) \in \mathcal{G}^{\prime}<br>\end{array}\right.<br>$$<br>其中$\mathcal{G}$ 代表正确的知识图谱, $\mathcal{G}^{\prime}$ 代表被替换后污染的知识图谱(负例的知识图谱, 其中包含负例三元组).</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>作者主要面向实际应用中比较多的三元组分类和个性化搜索两个任务进行实验. 详细的模型参数设置请参考原论文.</p><h3 id="Triple-Classification"><a href="#Triple-Classification" class="headerlink" title="Triple Classification"></a>Triple Classification</h3><p>三元组分类的问题是给定一个三元组, 通过模型来判断它们是否有效. 作者在WN11和FB13数据集上测试了R - MeN的三元组分类性能. 结果取决于一个阈值$\theta_r$, 如果高于阈值则视为三元组有效, 否则无效. </p><p>虽然前面介绍的Memory可以包含多个Slots, 但通过实验后, 作者发现对于所有的数据集, Memory Slot为1的时候效果都是最好的. 所以在之后的实验中只考虑使用单一的Memory Slot. </p><blockquote><p>如果Memory Slot = 1, 关系记忆网络应该类似于GRU, 感觉有点退化的意思. 我始终没有想明白Single slot能work的理由.</p></blockquote><p>实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rmen2.jpg" style="zoom: 50%;" /><p>其中下划线代表效果次高的模型, 最下面一栏是借助了关系路径推理的模型.</p><p>R - MeN在这两个数据集上平均表现最好, 并且比TransE要好许多. </p><p>作者将TransE和R - MeN在WN11和FB13上针对关系的准确率进行了对比:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rmen3.jpg" style="zoom: 50%;" /><p>在WN11中, 对于一对一的关系<code>similar_to</code>, R - MeN比TransE有非常强的学习能力. 在<strong>作者给出的</strong>FB13关系准确率中, R - MeN比TransE要好.</p><h3 id="Search-Personalization"><a href="#Search-Personalization" class="headerlink" title="Search Personalization"></a>Search Personalization</h3><p>个性化搜索的问题被定义为根据用户给出的搜索请求, 目标是根据搜索系统给出的搜索结果进行重排. 这个搜索场景能被视为是三元组$(query, user, document)$, 所以R - MeN也能用于个性化搜索任务上.<br>作者在SEARCH17数据集上进行了实验, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rmen4.jpg" style="zoom: 25%;" /><p>KGE方法在个性化搜索任务上比传统方法要好一些, R - MeN显示出更好的性能.</p><h3 id="Effects-of-Hyper-Parameters"><a href="#Effects-of-Hyper-Parameters" class="headerlink" title="Effects of Hyper-Parameters"></a>Effects of Hyper-Parameters</h3><p>作者将多头的头大小$n$, 头的个数$H$ 在各个数据集上的性能进行对比:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rmen5.jpg" style="zoom: 80%;" /><p>在三个数据集上, 使用更大的头有利于性能提升. 而在头的个数上, WN11和FB13中使用多头效果要更好, 在SEARCH17中单头效果更好, 作者认为SEARCH17是单意图的, 所以通常用1个头的效果会比较好. 因为头越多, 注意力就越分散, 就越容易Over Fitting. </p><h3 id="Ablation-Analysis"><a href="#Ablation-Analysis" class="headerlink" title="Ablation Analysis"></a>Ablation Analysis</h3><p>在消融实验中, 作者尝试去掉R - MeN的位置编码, 不使用关系记忆网络:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rmen6.jpg" style="zoom: 50%;" /><p>在不使用关系记忆网络时, 打分函数直接就变为了接收最原始的实体和关系Embedding $\mathbf{v}_{s}, \mathbf{v}_{r}, \mathbf{v}_{o}$ 作为输入:<br>$$<br>f(s, r, o)=\max \left(\operatorname{ReLU}\left(\left[\mathbf{v}_{s}, \mathbf{v}_{r}, \mathbf{v}_{o}\right] \ast \mathbf{\Omega}\right)\right)^{\top} \mathbf{w}<br>$$<br>去掉位置编码后, 在SEARCH17上的表现退化比较大, WN11没有变化, FB13有一点点下降. 在不使用关系记忆网络时, 所有性能都有退化, 关系记忆网络体现出比较大的作用.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>作者将<strong>关系记忆网络</strong>用于KGE中, 并将三元组视为<strong>时序序列输入</strong>来看待. 并同时指出了KGE模型普遍采用Link Prediction来评估KGE模型的问题.</p><p>普通的KGE方法不能间接捕捉三元组内的联系, 只能用知识图谱中给出的实体间关系<strong>显式</strong>的学习它们之间的联系. 但通过关系记忆网络, 能够从三元组内的其他元素遗留下来的信息中学到一些潜在的联系.</p><p>循环记忆机制一直都有一个诟病: 自回归. 也就是当前时刻输入必须使用必须以上一个时刻的输出, 导致整个过程只能<strong>串行</strong>而不能并行. </p><blockquote><p>另外, 在使用Single Slot的时候, Memory可能就退化成类似GRU的结构了, 有点像Triple Level的RNN… 暂时还不知道这个看法的对错.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> Attention </tag>
            
            <tag> 记忆网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于轻量级卷积和动态卷积替代的注意力机制</title>
      <link href="/posts/40162.html"/>
      <url>/posts/40162.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>Depthwise Convolution: 详见<a href="https://adaning.github.io/posts/13629.html">深度可分离卷积与分组卷积</a>.</li><li>Attention: 详见<a href="https://adaning.github.io/posts/40071.html">Seq2Seq和Attention</a>.</li><li>Transformer: 详见<a href="https://adaning.github.io/posts/6744.html">Transformer精讲</a>.</li></ul></blockquote><p>本文是论文<a href="http://arxiv.org/abs/1901.10430" target="_blank" rel="noopener">PAY LESS ATTENTION WITH LIGHTWEIGHT AND DYNAMIC CONVOLUTIONS</a>的阅读笔记和个人理解. 因为本文的图片均来自于原论文或Reference中的文章, 我觉得列出的几篇文章都很好, 图片特别有助于讲解. 论文中还有大量我不了解的知识, 再有相关的东西再进行补充.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>Self - Attention虽然解决了长距离依赖问题, 但因为计算量大, 必须对文本长度进行限制. 牵扯到计算效率的问题, 自然而然的就想到高效而体积小的<strong>卷积</strong>. 作者希望用轻量级卷积实现类似Self - Attention的效果.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lightweightconv7.jpg" style="zoom: 67%;" /><p>左侧为Self - Attention的权重生成方式, 右侧为Dynamic Convolution的权重生成方式.</p><p>作者希望能通过仅通过每个时刻的输入就能生成注意力权重, 而非像Self - Attention一样依赖全局输入生成.</p><h2 id="Depthwise-Convolution-and-Convolution-1D"><a href="#Depthwise-Convolution-and-Convolution-1D" class="headerlink" title="Depthwise Convolution and Convolution 1D"></a>Depthwise Convolution and Convolution 1D</h2><p>Lightweight Convolution的核心是<strong>单个维度上的深度卷积</strong>(Depthwise Convolution). </p><h3 id="Convolution-in-1-Dimension"><a href="#Convolution-in-1-Dimension" class="headerlink" title="Convolution in 1 Dimension"></a>Convolution in 1 Dimension</h3><p>无论在多少Dimension的卷积中, Channel维对应的是输入数据相独立的”<strong>厚度</strong>“这个维度, 它必须能保留输入单元的原始信息, 以保证不同输入元素之间的交互.</p><p>例如Conv2d在CV中, 为了保留二维图片的<strong>平面位置信息</strong>, Channel维对应的是Depth维, 而Conv1d在NLP中, 为了保留一维序列的<strong>先后顺序信息</strong>, <strong>Channel维所对应的是词向量的Hidden维</strong>. 这里先入为主, </p><blockquote><p>我开始是Channel维没找对, 卡了很长时间, 希望大家在看的时候从这个角度先入为主.</p></blockquote><h3 id="Depthwise-Convolution"><a href="#Depthwise-Convolution" class="headerlink" title="Depthwise Convolution"></a>Depthwise Convolution</h3><p>深度卷积是一种对每个Channel分别卷积的卷积方式:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/dsconv3.jpg" style="zoom: 33%;" /><p>假设你已经具备了深度卷积的基础, 其数学表达如下:<br>$$<br>O_{i, c}=\operatorname{DepthwiseConv}\left(X, W_{c,:}, i, c\right)=\sum_{j=1}^{k} W_{c, j} \cdot X_{\left(i+j-\left\lceil\frac{k+1}{2}\right\rceil\right), c}<br>$$<br>其中$c$ 代表Channel, $k$ 为卷积核宽度, $i$ 为特征图中Token的位置. 那么$W_{c, :}$ 就代表指定Channel $c$ 的卷积核权重. </p><p>现在我们对已经嵌入好的序列输入做卷积. 如果我们使用的是<strong>标准卷积</strong>, 那么每次卷积的部分都必须<strong>贯穿Channel维</strong>, 即对每个Channel使用不同的参数:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lightweightconv1.jpg" style="zoom: 50%;" /><p>因为不同Channel上不共享相同的权重, 所以此时卷积核的参数量为$d_{in} \cdot d_{out} \cdot k$.</p><p>如果使用<strong>深度卷积</strong>, 那么每次卷积就只在每个Channel上单独进行:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lightweightconv2.jpg" style="zoom:50%;" /><p>如果$d_{in} = d_{out}=d$, 即<strong>维持卷积前后的Channel数不变</strong>, 这样参数一下就从$d^2k$ 降到了$dk$, 因为我们不用再做贯穿整个Channel的运算了.</p><p>如果我们拿深度卷积的式子举个例子, 当$i=2, c=5, k=3$ 时(即图中所对应的绿色卷积过程), 那么在特征图上第2行第5列的输出$O_{2, 5}$ 则为$O_{2, 5} =W_{5, 1:3}X_{1:3, 5}$.</p><blockquote><p>我开始不能理解为什么参数能下降$d$ 倍, 后来发现我一直都忽视了”<strong>代替Self - Attention</strong>“这个目的, 在Self - Attention前后, Channel是不变的, 即$d_{in}=d_{out}$. 因此作者说法无误.</p></blockquote><h2 id="Lightweight-Convolution"><a href="#Lightweight-Convolution" class="headerlink" title="Lightweight Convolution"></a>Lightweight Convolution</h2><p>轻量级卷积在深度卷积的基础上进一步改进, 它引入了<strong>多头共享权重</strong>机制, 使得参数进一步减少.</p><h3 id="Weight-sharing"><a href="#Weight-sharing" class="headerlink" title="Weight sharing"></a>Weight sharing</h3><p>为了进一步减少参数量, 作者将Channel维拆成$H$ 个头, 在同一个头覆盖的区域内<strong>仅使用一个头</strong>(或者说在同一个头内的每个Channel维上的卷积核参数相同):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lightweightconv3.jpg" style="zoom: 50%;" /><p>例如上图中, 橙色区域的两个Channel共享同一个卷积核.</p><p>经过权重共享, 每个头所占有的Channel数为$\frac{d}{H}$, 这样参数量就缩小为了原来的$\frac{d}{H}$ 倍. 若有$H$ 个头, 参数量进一步从$d \cdot k$ 缩减到$H \cdot k$. </p><p>轻量级卷积的数学表达如下:</p><p>$$<br>\operatorname{LightConv}\left(X, W_{\lceil\frac{c H}{d}\rceil,:}, i, c\right)=\operatorname{ DepthwiseConv }\left(X, \operatorname{softmax}\left(W_{\lceil\frac{c H}{d}\rceil,:}\right), i, c\right)<br>$$</p><p>其中$\lceil\frac{c H}{d}\rceil$ 指的是$c$ 属于哪个头, $\frac{c}{d}$ 指的是Channel $c$ 在Channel维总深度$d$ 中所处的位置百分比. 因为一共就有$H$ 个头, 所以每个头占总Channel的$\frac{1}{H}$. 那么$\lceil\frac{c}{d} \div \frac{1}{H}\rceil$ 就能算出$c$ 的位置前面到底有几个头, 向上取整就是$c$ 所属的头位置.</p><p>输入时对卷积核Softmax归一化详见下一小节.</p><h3 id="Softmax-Normalization"><a href="#Softmax-Normalization" class="headerlink" title="Softmax Normalization"></a>Softmax Normalization</h3><p>我们最初的目标是通过卷积来代替自注意力机制, 所以仅仅通过卷积得到还不足够, 我们必须将其归一化形成<strong>权重</strong>, 才能够符合注意力权重的标准. 由于权重是对不同时间步分配的, 所以在卷积核大小$k$ 对卷积核<strong>内部进行</strong>权重归一化:</p><p>$$<br>\operatorname{softmax}(W)_{h, j}=\frac{\exp W_{h, j}}{\sum_{j^{\prime}=1}^{k} \exp W_{h, j^{\prime}}}<br>$$</p><p>其中$W \in \mathbb{R}^{H \times k}$, 为多头卷积核的权重矩阵. 通过Softmax归一化, 同一个卷积核中的参数只能得到<strong>固定</strong>的注意力权重, 因为我们目前仅仅是对卷积核权重这个固定参数上归一化, 而没有结合当前时刻的输入信息.</p><h3 id="Gated-Linear-Units"><a href="#Gated-Linear-Units" class="headerlink" title="Gated Linear Units"></a>Gated Linear Units</h3><p>GLU是一种应用在CNN上的一种<strong>门控机制</strong>, 于<a href="http://proceedings.mlr.press/v70/dauphin17a" target="_blank" rel="noopener">Language Modeling with Gated Convolutional Networks</a>中提出, 该结构能够提升CNN抽取高层抽象特征的能力, 其核心思想如下:</p><p>$$<br>h_{l}(\mathbf{X})=(\mathbf{X} \ast \mathbf{W}+\mathbf{b}) \otimes \sigma(\mathbf{X} \ast \mathbf{V}+\mathbf{c})<br>$$</p><p>其中$\ast$ 代表卷积操作, $\mathbf{X}$ 代表输入矩阵, $\mathbf{W}, \mathbf{V}$ 代表两卷积核, $\mathbf{b}, \mathbf{c}$ 代表两卷积偏置, $\otimes$ 代表逐元素点乘.</p><p> 在GLU中, 最基本的Block被定义为:<br>$$<br>\operatorname{GLU}(X) = X + \operatorname{CNN}(X) \otimes \operatorname{CNN}(X)<br>$$<br>残差连接和门控CNN就组成了最小的GLU单元.</p><blockquote><p>推荐阅读<a href="https://leimao.github.io/blog/Gated-Linear-Units/" target="_blank" rel="noopener">Gated Linear Units (GLU) and Gated CNN</a>.</p></blockquote><h3 id="Module"><a href="#Module" class="headerlink" title="Module"></a>Module</h3><p>轻量级卷积模块由<strong>Linear</strong>, <strong>GLU</strong>, <strong>LConv</strong>, <strong>Linear</strong>依次组成. 第一个Linear先将Token的Embedding维度从$d$ 投影映射到$2d$, 接着通过一个门控来调控输入的信息量, 再通过轻量级卷积, 最后再接一个Linear将维度调整回$d$:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lightweightconv8.jpg" style="zoom: 33%;" /><p>左图为Transformer中使用的点击缩放注意力, 右图为作者目前提出的轻量级卷积模块, 输入维度和输出维度是一致的.</p><h2 id="Dynamic-Convolution"><a href="#Dynamic-Convolution" class="headerlink" title="Dynamic Convolution"></a>Dynamic Convolution</h2><p>目前的轻量级卷积在不同时间步的权重都是<strong>固定</strong>的, 根本没有达到动态生成权重的效果, 基于轻量级卷积, 作者进一步提出<strong>动态卷积</strong>, 动态卷积对每个位置的权重都是<strong>动态</strong>的.</p><p>$$<br>\begin{aligned}<br>\operatorname{DynamicConv}(X, i, c)&amp;=\operatorname{LightConv}\left(X, f(X_{i})_{h,:}, i, c\right) \\<br>&amp;=\operatorname{ DepthwiseConv }\left(X, \operatorname{softmax}\left(f(X_{i})_{h,:}\right), i, c\right)<br>\end{aligned}<br>$$</p><p>其中$f$ 是一个简单的可学习的线性变换$W^Q$, 例如$f(X_{i})=\sum_{c=1}^{d} W_{h, j, c}^{Q} X_{i, c}$. 它能将当前时刻的$d$ 维的输入转化成$H\times k$ 维的注意力权重.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lightweightconv4.jpg" style="zoom: 50%;" /><p>简而言之, 就是利用某个时刻(实际上是Token)的全部Channel信息为当前时刻窗口内所有头的卷积核参数赋予权重, 而当前时刻Token的内容就相当于Self - Attention中的<strong>Query</strong>.</p><blockquote><p>注意力权重的生成只取决于<strong>当前时刻的输入</strong>, 而与前时刻和后时刻输入无关, 这是一个严重缺陷.</p></blockquote><p>下图依次为点积缩放注意力, 轻量级卷积模块, 动态卷积模块.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lightweightconv6.jpg" style="zoom: 33%;" /><p>动态卷积模块与轻量级卷积相比, 只是多增加了一个动态分配权重的Linear层.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>作者在机器翻译, 语言建模, 文本摘要任务上将Transformer中的自注意力机制换成轻量级卷积或动态卷积测试本方法的性能. 在Encoder中, 将自注意力模块替换, 在Decoder中将Masked自注意力模块替换.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lightweightconv5.jpg" style="zoom:50%;" /><p>针对<strong>具体任务</strong>作者进行了不同的调整, 详细配置请参照原论文.</p><p>作者提出, 参数量近似的情况下对比性能. 所以并没有采用6层的Block堆叠, 而是使用了7层. 这七层中, 卷积核大小依次为3, 7, 15, 31, 31, 31, 31.</p><blockquote><p>高层卷积核窗口设置的比较大, 我猜还是因为高层特征抽取中卷积的<strong>局部性限制</strong>问题. 在BERT的Attention可视化对高层能观察到, 除了一些特定层能很明显的体现出注意力差异, 其他高层基本上是均摊注意力权重, 所以卷积核需要更大范围的捕捉上下文特征相关性.</p></blockquote><h3 id="Machine-Translation"><a href="#Machine-Translation" class="headerlink" title="Machine Translation"></a>Machine Translation</h3><p>作者分别在newstest2014上测试了BLEU准确率:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lightweightconv9.jpg" style="zoom: 33%;" /><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lightweightconv10.jpg" style="zoom: 33%;" /><p>动态卷积与原生Transformer在参数量相同的情况下有较大提升.</p><h3 id="Model-Ablation"><a href="#Model-Ablation" class="headerlink" title="Model Ablation"></a>Model Ablation</h3><p>消融实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lightweightconv11.jpg" style="zoom: 33%;" /><p>感觉小Trick颇多, 权重共享在提高句子推断速度上有较大贡献. 在性能提升上每种Trick的贡献都差不多.</p><h3 id="Language-Modeling"><a href="#Language-Modeling" class="headerlink" title="Language Modeling"></a>Language Modeling</h3><p>在Google Billion Word上以困惑度为指标结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lightweightconv12.jpg" style="zoom: 33%;" /><p>动态卷积的参数比自注意力还稍多, 在测试集上取得了略胜自注意力一丢丢的成绩…</p><h3 id="Abstractive-Summarization"><a href="#Abstractive-Summarization" class="headerlink" title="Abstractive Summarization"></a>Abstractive Summarization</h3><p>作者在CNN - DailyMail中的文本摘要实验结果:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lightweightconv13.jpg" style="zoom: 33%;" /><p>轻量级卷积略胜一筹, 但其实也还是没节省多少参数.</p><p>从实验结果来看, 轻量级卷积在相同参数的情况下给出了一定的性能提升, 但并没有比较更少参数和其他模型的对比. 此外, 实验中的结果跟具体任务下的<strong>参数微调</strong>绝对是分不开的.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://qiita.com/koreyou/items/328fa92a1d3a7e680376" target="_blank" rel="noopener">論文紹介: Pay Less Attention with Lightweight and Dynamic Convolutions</a></li><li><a href="https://zhuanlan.zhihu.com/p/60482693" target="_blank" rel="noopener">Pay less attention with light-weight &amp;dynamic CNN</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg4MTEyMDk4Mg==&mid=2247484363&idx=1&sn=513a3f43902fc676f0e82f7136a94047&chksm=cf6b8072f81c096461d774eea90708121be418c53489b57e1951486b61f0c935bd69e948d482&mpshare=1&scene=1&srcid=1126GOjiRcL3TMhItmW6o196&sharer_sharetime=1606391708291&sharer_shareid=fd56e6671880039dc74b6cae5739dbd6&key=9520b16d9caa09008014ec82a8170b261af98843c13ece553a632234270e3c33bb2e29b1363f68b0b27df64710bdeadc29771bfb85bf331dff2c338dc3adb8a1794ae07bc06ea53a6088117fea040ab7da42919076085bc9bc06bb605be58f8aed3db8ef8b05c566bf1212d529e7685c07ef59850f6e244f20a2e229ba6b1b29&ascene=1&uin=MzExMTYwMjkzNw%3D%3D&devicetype=Windows+10+x64&version=6300002f&lang=zh_CN&exportkey=A%2B4JQqyuu2BEHy%2Fe%2B%2BFCG00%3D&pass_ticket=cOZYWOTfohJ2mVibAtIZzeO5jFOFGWxvNcr1jum%2BAYkYBwgP0NMOV0reW3wv3lkV&wx_header=0" target="_blank" rel="noopener">ICLR 2019 | 采用轻量化及动态卷积替代注意力机制</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
            <tag> Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KG-BERT: BERT for Knowledge Graph Completion</title>
      <link href="/posts/18273.html"/>
      <url>/posts/18273.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>BERT: 详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>.</li></ul></blockquote><p>本文是论文<a href="https://arxiv.org/abs/1909.03193" target="_blank" rel="noopener">KG-BERT: BERT for Knowledge Graph Completion</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>在先前的KGE方法中, 虽然它们能学到独特的实体关系表示, 但是却忽略了<strong>上下文</strong>. <strong>句法</strong>和<strong>语义</strong>信息在大规模文本数据中没有得到很好的利用, 它们仅仅使用了实体描述, 关系提及或者实体共现.</p><p>因为BERT在NLP中作为PLM取得的成果非常亮眼, 所以作者希望将它迁移到知识图谱补全任务中, 测试其在KGC中的性能. BERT是针对自然语言进行处理的, 作者简单的将实体和关系描述放入BERT, 使BERT能够获取KGC的能力, 称之为KG - BERT. 作者称, 这是<strong>第一项</strong>使用PLM对三元组进行建模的研究.</p><h2 id="KG-BERT"><a href="#KG-BERT" class="headerlink" title="KG - BERT"></a>KG - BERT</h2><p>既然BERT是基于自然语言的, 那么很容易就想到用实体和关系的<strong>描述</strong>或者它们的<strong>名字</strong>放入BERT, 然后获得通过某种训练方式得到三元组的表示. 作者设计了两种训练方式的KG - BERT, 这样能使它被运用到不同的知识图谱补全任务当中.</p><h3 id="KG-BERT-a"><a href="#KG-BERT-a" class="headerlink" title="KG - BERT(a)"></a>KG - BERT(a)</h3><p>在第一种方式中, 作者非常朴素的完全沿用了BERT的方法, 将实体, 关系<strong>描述</strong>或名字全部放入BERT中, 并用<code>[CLS]</code>处的隐态输出$C$ 来预测三元组是否正确, 该方式与BERT中的<strong>NSP任务</strong>完全一致. 第一种方式是针对<strong>三元组建模</strong>的.</p><p>例如, 三元组$(\text{SteveJobs}, \text{founded}, \text{AppleInc})$ 中的头实体$\text{SteveJobs}$ 可以表示为它的描述<code>Steven Paul Jobs was an American business magnate, entrepreneur and investor</code> 或者它的名字<code>Steve Jobs</code>, 而尾实体$\text{AppleInc}$ 可以表示为<code>Apple Inc. is an American multinational technology company headquartered in Cupertino, California</code>或者它的名字<code>Apple Inc</code>.</p><blockquote><p>在后续的研究中, NSP已经被证实会在NLP任务中带来<strong>副作用</strong>.</p></blockquote><p>在不同实体和关系之间用<code>[SEP]</code> 进行分隔, 并且每个Token的描述分别由Token本身的Embedding, Segment Embedding, Position Embedding组成. Segment Embedding因<strong>元素类型</strong>不同而不同, 头实体和尾实体都使用$e_A$ 作为Segment Embedding, 而关系采用$e_B$.</p><p>我们把<code>[CLS]</code>处的隐态输出$C$ 用来计算三元组的分类, 对于三元组$(h, r, t)$, 其打分函数为:<br>$$<br>\mathbf{s}_{\tau}=f(h, r, t)=\operatorname{sigmoid}\left(C W^{T}\right)<br>$$<br>其中$W$ 是变换矩阵, 和$C$ 乘完后可以获得三元组是否正确的概率$s_\tau$. </p><blockquote><p>在文中写到, $s_\tau$ 是一个二维向量, 包含$s_{\tau0}, s_{\tau1}$. $s_{\tau0} + s_{\tau1}=1$. 这是不是有点多余了… 其实只需要一个一维的$s_{\tau}$ 就足够了, 因为另一半可以用概率和为1算出来. </p></blockquote><p>现在对前面的模型描述进行总结, 整体结构如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kgbert1.jpg" style="zoom:33%;" /><p>其中优化用的损失函数为BCE:<br>$$<br>\mathcal{L}=-\sum_{\tau \in \mathbb{D}+\cup \mathbb{D}^{-}}\left(y_{\tau} \log \left(s_{\tau 0}\right)+\left(1-y_{\tau}\right) \log \left(s_{\tau 1}\right)\right)<br>$$<br>其中$y_\tau$ 是三元组是正例还是负例的标签, 在0和1之间取. $\mathbb{D}^{-}$ 代表负例, $\mathbb{D}^{+}$ 代表正例.</p><p>负样本仍然是<strong>负采样</strong>, 仅替换头实体和尾实体得来的:<br>$$<br>\mathbb{D}^{-}=\left\{\left(h^{\prime}, r, t\right) \mid h^{\prime} \in \mathbb{E} \wedge h^{\prime} \neq h \wedge\left(h^{\prime}, r, t\right) \notin \mathbb{D}^{+}\right\}<br>\cup\left\{\left(h, r, t^{\prime}\right) \mid t^{\prime} \in \mathbb{E} \wedge t^{\prime} \neq t \wedge\left(h, r, t^{\prime}\right) \notin \mathbb{D}^{+}\right\}<br>$$</p><h3 id="KG-BERT-b"><a href="#KG-BERT-b" class="headerlink" title="KG - BERT(b)"></a>KG - BERT(b)</h3><p>在第二种方式中, 作者只使用两个实体$h, t$ 的描述, 来预测它们之间的关系$r$. 在实验中, 作者发现这种结构在预测关系时效果要优于KG - BERT(a).</p><p>KG - BERT(b)采用<code>[CLS]</code> 处的隐态输出$C$ 后接一个分类矩阵来预测两实体之间的关系:<br>$$<br>\mathbf{s}_{\tau}^{\prime}=f(h, r, t)=\operatorname{softmax}\left(C W^{\prime T}\right)<br>$$<br>其中$W$ 为关系的分类矩阵, 多分类也将$\operatorname{sigmoid}$ 换成了$\operatorname{softmax}$. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kgbert2.jpg" style="zoom:33%;" /><p>负样本仍然来源于负采样, 只是对正例三元组的关系进行替换即可.</p><p>因为变更了任务, 损失函数不再使用BCE, 而是采用CE进行多分类:<br>$$<br>\mathcal{L}^{\prime}=-\sum_{\tau \in \mathbb{D}^{+}} \sum_{i=1}^{R} y_{\tau i}^{\prime} \log \left(s_{\tau i}^{\prime}\right)<br>$$<br>其中$y_{\tau i}^{\prime}$ 是关系的独热向量.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>在实验中, 作者希望探究KG - BERT的下述能力:</p><ul><li>模型能不能判断没见过的三元组的正确与否?(KG - BERT(a))</li><li>模型能不能根据给出的单个实体和关系描述预测出另一个实体?(Link Prediction)</li><li>模型能不能预测两个实体之间的关系?(KG - BERT(b))</li></ul><p>作者使用BERT - BASE初始化权重, 因为BASE比LARGE版本所受超参影响更小, 可选择的超参也很少. 其余参数设置详见原论文.</p><h3 id="Knowledge-Graph-Compeltion-Tasks"><a href="#Knowledge-Graph-Compeltion-Tasks" class="headerlink" title="Knowledge  Graph Compeltion Tasks"></a>Knowledge  Graph Compeltion Tasks</h3><h4 id="Triple-Classification"><a href="#Triple-Classification" class="headerlink" title="Triple Classification"></a>Triple Classification</h4><p>在三元组分类问题上, 作者在WN11和FB13做了实验:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kgbert3.jpg" style="zoom:33%;" /><p>KG - BERT效果非常明显, 该任务目标与其训练目标是一致的. 作者将其优秀的表现总结如下:</p><ol><li>输入中含有实体和关系的单词序列(使用了文本描述).</li><li>三元组分类与BERT训练时的NSP任务类似.</li><li>Token Vector结合了上下文, 在不同的三元组中描述往往是不同的, 因此不同三元组中的相同元素能获得不同表示.</li><li>Self - Attention很强.</li></ol><p>作者绘制了测试集准确率随训练集数据量提升的变化曲线:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kgbert4.jpg" style="zoom:33%;" /><p>WN11(左), FB13(右). KG - BERT从开始就优于其他模型, 得益于BERT强大的特征抽取能力.</p><h4 id="Link-Prediction"><a href="#Link-Prediction" class="headerlink" title="Link Prediction"></a>Link Prediction</h4><p>在链接预测上, 作者对多种模型在WN18RR和FB15k - 237上, 以及UMLS上做了测试:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kgbert5.jpg" style="zoom:33%;" /><p>KG - BERT(a)在MR上取得了很好的效果, 但是Hits@10的表现总比MR差很多, 作者对其的解释是KG - BERT虽然能避免实体和关系相关性很强的相似三元组, 但是没有对<strong>三元组本身</strong>进行<strong>明确</strong>的建模, 因此不好给定它们的准确排名.</p><blockquote><p>虽然作者在论文中没有明确写出模型输入数据的方法, 但大致能够猜到是对每个实体挨个替换实体描述.</p></blockquote><h4 id="Relation-Prediction"><a href="#Relation-Prediction" class="headerlink" title="Relation Prediction"></a>Relation Prediction</h4><p>作者在FB15k上测试了KG - BERT(b)关系预测的性能:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kgbert6.jpg" style="zoom: 50%;" /><p>碍于打分函数和模型方法的限制, 没有Conv系列的模型登场. KG - BERT在诸多模型中取得了最好的成绩.</p><h3 id="Attention-Visualization"><a href="#Attention-Visualization" class="headerlink" title="Attention Visualization"></a>Attention Visualization</h3><p>对<strong>注意力可视化</strong>能增强模型的可解释性, 也能一定程度上观察模型所学到的东西是否有效. 我认为这部分可以算作是本文的亮点之一.</p><p>作者从KG - BERT(a)取出第11层, 从WN18RR中取出的正例三元组<code>(twenty dollar bill NN 1,hypernym,note NN 6)</code>作为例子, 绘制Attention的可视化情况. 以头实体描述为<code>a United States bill worth 20 dollars</code>, 关系名<code>hypernym</code>, 尾实体描述<code>a piece of paper money</code>, 作为输入序列:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kgbert7.jpg" style="zoom: 50%;" /><p>第11层中<code>paper</code>, <code>money</code>具有很高的权重, <code>worth</code>, <code>20</code> 具有很低的权重. 抛开句子不谈, 模型很好的学到了<code>[SEP]</code>的作用, 因为它在不同的头之间多多少少分配了一些权重.</p><p>在KG - BERT(b)中, 以三元组<code>(20th century, /time/event/includes event, World War II)</code>为例:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kgbert8.jpg" style="zoom: 50%;" /><p>能看到b学到了类似a的模式. 但在这个例子中似乎每个头的注意力更为分散. KG - BERT(b)的目标是对实体进行关系预测, 所以对<code>[CLS]</code> 分配了更高的权重.</p><h3 id="Discussions"><a href="#Discussions" class="headerlink" title="Discussions"></a>Discussions</h3><p>作者提到, KG - BERT现在最大的问题还是<strong>计算成本</strong>太过高昂. 尤其是在链接预测的Evaluation时, 因为轮流替换实体描述花费了大量的时间. 作者认为可行的方法是使用像ConvE那样的<strong>1 - N Scoring</strong>或者采用更加轻量级的语言模型.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>KG - BERT是BERT最早在KG上的应用. 输入数据的方式也非常的简单, 符合我们的直觉, 效果也还不错. 从现在的眼光来看, 与KG相结合的BERT最好不要再使用之前的训练方式.</p><p>缺点是没有对三元组进行直接的建模, 计算成本比较高.</p><p>只是NSP任务在BERT上已经被证明会给BERT带来副作用, 如果要沿用KG - BERT的训练方式, 就需要对NSP任务在KG - BERT上的效果进行研究, 如果去掉NSP任务需要用什么任务来代替?</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BERT </tag>
            
            <tag> KGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ConvE: Convolutional 2D Knowledge Graph Embeddings</title>
      <link href="/posts/42031.html"/>
      <url>/posts/42031.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>CNN</li></ul></blockquote><p>本文是论文<a href="https://arxiv.org/abs/1707.01476" target="_blank" rel="noopener">Convolutional 2D Knowledge Graph Embeddings</a>的阅读笔记和个人理解. 与之前在<a href="https://adaning.github.io/posts/59193.html">AcrE</a>中提到的ConvE不同, 本文重新对整篇论文进行叙述, 而非仅介绍论文中建模的部分.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>其实ConvE的出发点非常的简单, 就是之前的模型不够<strong>深</strong>, 有些简单. 因为之前使用的模型大多数采用矩阵映射, 内积等方式, 可能简单的模型能够处理小规模的KG, 想要提升性能就只能通过增大Embedding Dimension. 它们在<strong>大规模KG</strong>上不一定能获得良好的效果. 由于深度学习的兴起, 作者尝试将<strong>卷积</strong>引入到KGE领域, 使它能够在保证深度和模型复杂度的情况下能够处理大规模KG.</p><h2 id="ConvE"><a href="#ConvE" class="headerlink" title="ConvE"></a>ConvE</h2><p>ConvE其实就是将CNN移植到了KGE领域, CNN能在不引入过多参数的情况下, 高效而简单的提供多次交互.</p><h3 id="1D-Convolution-vs-2D-Convolution"><a href="#1D-Convolution-vs-2D-Convolution" class="headerlink" title="1D Convolution vs 2D Convolution"></a>1D Convolution vs 2D Convolution</h3><p>作者指出, 相较于1维卷积, 2维卷积有更强的<strong>表达能力</strong>(其实从直觉来说也是这样).</p><p>在做1维卷积时, 卷积核最多只能与左侧或右侧离得比较近的元素<strong>交互</strong>:<br>$$<br>\left(\begin{array}{lll}<br>\left.\left[\begin{array}{lll}<br>a &amp; a &amp; a<br>\end{array}\right] ;\left[\begin{array}{lll}<br>b &amp; b &amp; b<br>\end{array}\right]\right)=\left[\begin{array}{llllll}<br>a &amp; a &amp; a &amp; b &amp; b &amp; b<br>\end{array}\right]<br>\end{array}\right.<br>$$<br>但2维卷积不一样, 除了能够与邻近的左右元素交互, 还能与上下元素进行交互:<br>$$<br>\left(\left[\begin{array}{lll}<br>a &amp; a &amp; a \\<br>a &amp; a &amp; a<br>\end{array}\right] ;\left[\begin{array}{lll}<br>b &amp; b &amp; b \\<br>b &amp; b &amp; b<br>\end{array}\right]\right)=\left[\begin{array}{lll}<br>a &amp; a &amp; a \\<br>a &amp; a &amp; a \\<br>b &amp; b &amp; b \\<br>b &amp; b &amp; b<br>\end{array}\right]<br>$$<br>如果两种元素代表的意义不同, 那么交换它们的拼接方式还能进一步的<strong>提升</strong>交互次数:<br>$$<br>\left[\begin{array}{lll}<br>a &amp; a &amp; a \\<br>b &amp; b &amp; b \\<br>a &amp; a &amp; a \\<br>b &amp; b &amp; b<br>\end{array}\right]<br>$$<br>由于交换了Concat方式, a和b交错, 能够实现更多次交互. 所以作者认为, 2D卷积的表达能力比1D卷积要强, 因此不是单纯在1D的Embedding数据上做卷积, 而是尝试扩展到2D上做卷积.</p><blockquote><p>在<a href="https://adaning.github.io/posts/30287.html">InteractE</a>中, 使用了交错程度更大的<strong>棋盘式布局</strong>, 进一步提升了交互次数.</p></blockquote><h3 id="ConvE-Architecture"><a href="#ConvE-Architecture" class="headerlink" title="ConvE Architecture"></a>ConvE Architecture</h3><p>我们先看ConvE的整体流程的<strong>概括</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/conve1.jpg" style="zoom:67%;" /><p>然后再给出ConvE的<strong>打分函数</strong>:<br>$$<br>\psi_{r}\left(\mathbf{e}_{s}, \mathbf{e}_{o}\right)=f\left(\operatorname{vec}\left(f\left(\left[\overline{\mathbf{e}_{s}} ; \overline{\mathbf{r}_{r}}\right] \ast \omega\right)\right) \mathbf{W}\right) \mathbf{e}_{o}<br>$$<br>其中$\mathbf{e}_{s}, \mathbf{e}_{o}$ 分别代表头实体和尾实体的Embedding, $\overline{\mathbf{e}_{s}}, \overline{\mathbf{r}_{r}}$ 分别代表Reshape后的头实体和关系向量. $\omega$代表卷积核, $\mathbf{W}$ 代表投影矩阵. </p><p>接下来对ConvE的打分函数进行讲解, 请结合概括图来看. ConvE先通过Embedding的方式分别获得头实体表示$\mathbf{e}_{s}$ 和关系表示$\mathbf{r}_{r}$. 将头实体和关系表示先Concat起来, 然后将其Reshape到某一种尺寸, 此时头实体和关系的表示记为$\left[\overline{\mathbf{e}_{s}} ; \overline{\mathbf{r}_{r}}\right]$. 接着利用卷积抽取Reshape后的二维向量, 也就是对头实体和关系进行交互. 利用卷积抽取完信息后, 将所有的特征打平成一个一维向量, 通过投影矩阵$\mathbf{W}$ 投影到隐空间中, 然后与为尾实体表示$\mathbf{e}_{o}$ 做内积, 获得相似度, 即Logits. 这种方式通过内积来比较所获向量与尾实体的<strong>相似度</strong>, 越相似得分越高.</p><p>然后将Logits经过$\sigma$ 函数, 得到每个实体的概率:<br>$$<br>p=\sigma(\psi_{r}\left(\mathbf{e}_{s}, \mathbf{e}_{o}\right))<br>$$<br>优化时的损失函数采用BCE:<br>$$<br>\mathcal{L}(p, t)=-\frac{1}{N} \sum_{i}\left(t_{i} \cdot \log \left(p_{i}\right)+\left(1-t_{i}\right) \cdot \log \left(1-p_{i}\right)\right)<br>$$<br>$t$ 是尾实体的独热编码向量. 除此外还加入了Dropout, BatchNorm, 标签平滑等防止过拟合的手段.</p><p>这个损失函数很有说法, 因为在ConvE提取过Feature后, 能获得对所有实体相关的Logits, 这样就能<strong>对所有的尾实体同时打分</strong>, <strong>而不用考虑采样的问题</strong>. 在原文中这种打分方式被称为<strong>1 - N Scoring</strong>. </p><p>这种方式能极大地加快Evaluation的速度, 因为负采样只能对单一的三元组打分, 而这种方式能同时对所有的尾实体同时打分. 作者还测试了不对所有实体同时打分的情况, 例如只对10%的实体打分, 这种情况记为1 - 0.1 N Scoring, 虽然在正向传播和反向传播的速度减少了25%, 但训练速度慢了许多.</p><p>作者还指出, 这种思想能够应用于所有的1 - 1 Scoring Model.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="WN18RR"><a href="#WN18RR" class="headerlink" title="WN18RR"></a>WN18RR</h3><p>作者指出, 最常用的几个数据集例如FB15k, WN18都存在大量的<strong>互逆关系</strong>, 模型能从中掌握关系之间的对应情况, 而轻而易举的推出结果, 模型通过互逆关系<strong>偷懒</strong>, 而并非掌握推断能力. 已经有人对FB15k进行了修正, 推出FB15k -237, 作者也相应的对WN18进行了调整, 推出了<strong>WN18RR</strong>.</p><h3 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h3><p>作者对比了DistMult和ConvE在FB15k - 237上不同Embedding Size的参数量:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/conve2.jpg" style="zoom: 25%;" /><p>在相同参数量的情况下, ConvE能够腾出更多参数在Embedding Size上, 并且拥有比DistMult好的性能. </p><p>对于其他的超参设置, 作者在论文中指出, 大的卷积核效果不如多个小卷积核效果好(这点与CV中结论一致, 基本已经成常识了).</p><h3 id="Inverse-Model"><a href="#Inverse-Model" class="headerlink" title="Inverse Model"></a>Inverse Model</h3><p>虽然大家都已经知道在FB15k和WN18的测试集中存在大量的互逆关系, 但是从来没有人<strong>定量</strong>的对这个问题的<strong>严重性</strong>进行调查. 作者随意构造了一个<strong>基于规则的简单模型</strong>, 发现这个模型能随随便便在存在问题的数据集上取得非常棒的性能.</p><p>这个<strong>Inverse Model</strong>能在训练时自动从给定的两个关系对$r_1, r_2$ 中提取出来, 并检查它们是否是互逆关系. 在测试时, 模型能够自动的看三元组是否含有逆关系, 如果有的话则对最为匹配的$k$ 个逆关系进行排名, 然后从前$k$ 个排名中进行选择, 如果没有逆关系则随机从所有排名中选.</p><p>在后面的链接预测任务中, Inverse Model能够非常好的检查数据集是否存在大量的逆关系, 并评估这种情况的危害.</p><h3 id="Link-Prediction"><a href="#Link-Prediction" class="headerlink" title="Link Prediction"></a>Link Prediction</h3><p>在链接预测任务中, 作者分别在WN18, FB15k, WN18RR, FB15k - 237, YAGO3 - 10, Countries上做了实验.</p><p>WN18, FB15k:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/conve3.jpg" style="zoom: 33%;" /><p>Inverse Model轻而易举的超过了许多模型. </p><p>下面是去除了互逆关系的WN18RR和FB15k - 237:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/conve4.jpg" style="zoom: 33%;" /><p>Inverse Model的性能一落千丈, 同时ConvE在这两个数据集上表现都非常不错, FB15k - 237上表现超过所有Baseline, WN18RR和其他模型也没有差太多.</p><p>YAGO3 - 10, Countries:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/conve5.jpg" style="zoom: 33%;" /><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>作者对ConvE的组成因素挨个做了消融实验:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/conve7.jpg" style="zoom: 33%;" /><p>各种Dropout似乎对ConvE的影响很大, 1 - N Scoring的增益也不小, 标签平滑倒是不怎么重要.</p><h3 id="Indegree-and-PageRank"><a href="#Indegree-and-PageRank" class="headerlink" title="Indegree and PageRank"></a>Indegree and PageRank</h3><h4 id="Indegree"><a href="#Indegree" class="headerlink" title="Indegree"></a>Indegree</h4><p>作者指出, YAGO3 - 10, FB15k - 237与WN18RR相比都具有非常高的关系<strong>入度</strong>, 并且ConvE在它们上面表现非常好. 作者假设, 像ConvE这样更加<strong>深层次</strong>的模型能更好的对高入度的关系建模.</p><p>基于这个假设, 作者将低入度的WN18(low - WN18)和高入度的(high - FB15k)进行转换, 变为高入度的WN18(high - WN18)和低入度的(low - FB15k), 然后观察ConvE在上面的表现. 作者发现在low - FB15k上, ConvE的表现逊于DistMult, 在high - WN18上表现强于DistMult. 这支持了作者的假设.</p><h4 id="PageRank"><a href="#PageRank" class="headerlink" title="PageRank"></a>PageRank</h4><p><strong>PageRank</strong>是节点中心性的度量, PageRank越高, 节点的<strong>影响力</strong>就越大. 作者假设更深的模型在捕获约束上更有优势, 但更加难以优化.</p><p>作者仍然将ConvE和DistMult进行比对, 比较将DistMult换成ConvE后在不同数据集上Hits@10之间的差距:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/conve6.jpg" style="zoom: 33%;" /><p>基本规律是PageRank越高, 错误减少的就越多, ConvE就越强大. 这也佐证了作者的猜想.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>我认为作者在本文中的贡献如下:</p><ol><li>ConvE将卷积的方法引入了KGE中.</li><li>提供了去除互逆关系的WN18RR数据集.</li><li>提出了1 - N Score的思想, 大大加速Evaluation的过程. 而且它还能移植到其他1 - 1 Score模型上.</li><li>指出了卷积的高参数利用率, 为后续多种基于卷积的KGE方法打开了新的大门.</li></ol><p>本文不仅仅是简单的将卷积引入, 在实验的证明思路也比较值得学习.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度可分离卷积与分组卷积</title>
      <link href="/posts/13629.html"/>
      <url>/posts/13629.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>CNN: 详见<a href="https://adaning.github.io/posts/28799.html">卷积神经网络小结</a>.</li></ul></blockquote><p>本文着重介绍<strong>深度可分离卷积</strong>和<strong>分组卷积</strong>两种操作.</p><h2 id="深度可分离卷积"><a href="#深度可分离卷积" class="headerlink" title="深度可分离卷积"></a>深度可分离卷积</h2><p><strong>深度可分离卷积</strong>(Depthwise Separable Convolution)应用在MobileNet和Xception中. 似乎这二者的实现略有不同, 但二者的出发点都是通过深度可分离卷积来<strong>减少参数量</strong>.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/dsconv1.jpg" style="zoom: 33%;" /><blockquote><p>该图片出自<a href="https://zhuanlan.zhihu.com/p/28749411" target="_blank" rel="noopener">zhihu</a>(找不到最开始的原出处了).</p></blockquote><p>我们来举一个例子说明深度可分离卷积是怎样运算的.</p><p>如果我们使用四个标准的卷积对下面的RGB三通道图像数据$C \times H \times W$ 进行卷积操作, 卷积后的通道数(或者说特征图个数)取决于卷积核的个数, 在这里生成了四张特征图. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/dsconv2.jpg" style="zoom: 33%;" /><blockquote><p>本小节其余图片出自<a href="https://yinguobing.com/separable-convolution/" target="_blank" rel="noopener">卷积神经网络中的Separable Convolution</a>.</p></blockquote><p>卷积核在不同的通道上使用了不同的权重, 卷积核的大小可以看成是三维的$d_c \times k_h \times k_w$. 还有没有其他方法能达到等效或<strong>近似</strong>的效果呢? 在VGG中出现过类似的方法, 一个大卷积核可以被多个小卷积核<strong>近似表达</strong>. </p><p>$1\times 1$ 卷积能<strong>不改变特征图的大小</strong>, 利用这个特性, 深度可分离卷积将卷积拆成两步来做, 分别是<strong>深度卷积</strong>和<strong>分离卷积</strong>. 即使拆成两步, 最后能拿到的特征图大小也与标准卷积相同.</p><h3 id="深度卷积"><a href="#深度卷积" class="headerlink" title="深度卷积"></a>深度卷积</h3><p><strong>深度卷积</strong>(Depthwise Convolution)也可以称为<strong>逐通道卷积</strong>. 假设用$3\times 3 $ 大小的卷积核去卷积, 我们只对每个通道分别进行卷积, 而不是一起进行卷积:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/dsconv3.jpg" style="zoom: 33%;" /><p>逐通道卷积这一步中, 每个卷积核只有<strong>二维</strong>参数, 即$k_h \times k_w$ 个参数. 生成并能生成$C$ 张宽高变化后的特征图. 深度卷积只是单纯的<strong>分别运用</strong>了每个平面空间上的信息, 还没有将它们整合到一起, 即<strong>没有进行Channel维运算</strong>.</p><h3 id="分离卷积"><a href="#分离卷积" class="headerlink" title="分离卷积"></a>分离卷积</h3><p><strong>分离卷积</strong>(Separable Convolution)也称为<strong>逐点卷积</strong>. 其核心就是利用$1\times 1$卷积在不改变特征图大小情况下<strong>任意更改Channel数量</strong>的特性, 对Channel信息进行<strong>整合</strong>. $1\times1$ 卷积核有多少个, 就有多少特征图产生. 假设有$M$ 个$1\times1$ 卷积核, 就能生成$M$ 张宽高不变的特征图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/dsconv4.jpg" style="zoom: 33%;" /><p>此时的$1 \times 1$ 卷积与标准卷积无异, 将<strong>贯穿</strong>于每一个Channel, 这样就完成了Channel维度上的<strong>信息整合</strong>. 经过深度卷积和分离卷积两步后, 其产生的特征图大小和使用四个$3\times3\times3$ 大小的标准卷积产生的结果无异.</p><h3 id="参数量分析"><a href="#参数量分析" class="headerlink" title="参数量分析"></a>参数量分析</h3><p>更一般的, 假设图像大小为$C \times H \times W$, 使用$N$ 个$K \times K$ 的卷积核. 那么标准卷积所使用的参数量$P_{std}$ 为:<br>$$<br>P_{std} = K \times K \times C \times N = K^2\times C \times N<br>$$<br>而深度可分离卷积的参数量$P_{ds}$ 由两部分组成:</p><ul><li><strong>Depthwise Convolution</strong>: 由于卷积核没有在Channel上的维度, 所以只使用了$K \times K \times N$ 个参数.</li><li><strong>Separable Convolution</strong>: 该部分是$1\times1$ 卷积的参数, 使用了$1\times1\times C \times N$ 个参数.</li></ul><p>综上, 深度可分离卷积参数量为:<br>$$<br>P_{ds} = K \times K \times N + 1\times1\times C \times N = (K^2+C) \times N<br>$$<br>故深度可分离卷积与标准卷积的<strong>参数比</strong>为:<br>$$<br>\frac{P_{ds}}{P_{std}} = \frac{(K^2+C)\times N}{K^2 \times C \times N} = \frac{1}{C} + \frac{1}{K^2}<br>$$<br>参数量大量的减少了, 效率也提高了(详见<a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="noopener">Mobilenet</a>).</p><h2 id="分组卷积"><a href="#分组卷积" class="headerlink" title="分组卷积"></a>分组卷积</h2><p><strong>分组卷积</strong>(Group Convolution)早在AlexNet中就出现了, AlexNet当时受算力的限制, 不能将整张图片直接读入GPU中, 所以对一张图片<strong>分割</strong>处理, 最后再整合起来:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/alexnet.jpg" style="zoom:67%;" /><blockquote><p>本节其余图片来源于<a href="https://blog.yani.ai/filter-group-tutorial/" target="_blank" rel="noopener">A Tutorial on Filter Groups (Grouped Convolution)</a>.</p></blockquote><p>假设我们要使用$c_2$ 个卷积核对$c_1\times H \times W$ 的图片进行卷积.</p><p>对于标准卷积, 使用的卷积核大小为$c_1\times h_1 \times w_1$:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/groupconv1.png" style="zoom:67%;" /><p>所产生的总参数量为$ c_1 \times h_1 \times w_1 \times c_2$.</p><p>如果将图片在Channel上分为$g$ 组, 分别只在$g$ 组上使用深度更小的卷积核分别进行卷积, 最后<strong>拼凑</strong>起来, 就称为了分组卷积:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/groupconv2.png" style="zoom:67%;" /><p>如图所示, 分组卷积所产生的总参数量为$g \times h_1 \times w_1 \times \frac{c_2}{g}$. 即标准卷积参数量的$\frac{1}{g}$ 倍.</p><blockquote><p>我能想到一个极端情况, 当分组数量$g$ 与Channel数相同, 即$g = c_1$ 时, 分组卷积就等价于前面说过的<strong>深度卷积</strong>(Depthwise Convolution). 因为只是最后将Channel维上的特征图Concat起来, 而没有进行<strong>交互</strong>, 所以在不同的Group之间, Channel维上的信息是没有得到整合的. 如果分组卷积只作为CNN的<strong>中间部件</strong>, 在后续的结构中可以使用其他整合的方式.</p></blockquote><h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/65377955" target="_blank" rel="noopener">理解分组卷积和深度可分离卷积如何降低参数量</a></li><li><a href="https://zhuanlan.zhihu.com/p/28749411" target="_blank" rel="noopener">变形卷积核、可分离卷积？卷积神经网络中十大拍案叫绝的操作。</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch实现: Transformer</title>
      <link href="/posts/63679.html"/>
      <url>/posts/63679.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>Pytorch基本操作</li><li>Transformer: 详见<a href="https://adaning.github.io/posts/6744.html">Transformer精讲</a></li></ul><p><strong>2022.04.03</strong>: 去掉了Pre Norm比Post Norm效果好的表述.</p></blockquote><h1 id="Pytorch实现-Transformer"><a href="#Pytorch实现-Transformer" class="headerlink" title="Pytorch实现: Transformer"></a>Pytorch实现: Transformer</h1><p>本文是Transfomrer的Pytorch版本实现. 实现的过程中非常考验<strong>维度控制</strong>的功底. 本文实现参考<a href="https://wmathor.com/index.php/archives/1455/" target="_blank" rel="noopener">Transformer 的 PyTorch 实现</a>, 我对其在个别地方进行了修改, 并对所有的数据<strong>全部</strong>加上了维度注释.</p><p>本文的代码已经放到了Colab上, 打开设置GPU就可以复现(需要科学上网). </p><p><a href="https://colab.research.google.com/drive/1OAXor9Io7pk64ilo-HoFS6RnH9ImbaaF?usp=sharing" target="_blank" rel="noopener"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="右键我在COLAB中打开!"></a></p><p>如果你不能科学上网, 应该看不到<code>Open in Colab</code>的图标.</p><p>在开头需要说明的是:</p><ul><li>网上的所有流传的代码, 一般都会把<code>batch_size</code>放在第0维. 因为我们基本上不对batch维做操作, 放在最前面来防止影响后面总需要使用<code>transpose</code>移动.</li><li>如果对Transformer不熟悉, 最好熟悉后再来看这篇文章.</li><li>注意<code>view</code>和<code>transpose</code>拆维度时不要乱了.</li></ul><h2 id="Preparing"><a href="#Preparing" class="headerlink" title="Preparing"></a>Preparing</h2><p>按照惯例, 先导包:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch <span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> torch <span class="token keyword">import</span> optim<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils <span class="token keyword">import</span> data <span class="token keyword">as</span> Data<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>因为后面需要用到一些关于Transformer的超参数, 所以在开头就先全部定义出来:</p><pre class="line-numbers language-python"><code class="language-python">d_model <span class="token operator">=</span> <span class="token number">512</span> <span class="token comment" spellcheck="true"># embedding size </span>max_len <span class="token operator">=</span> <span class="token number">1024</span> <span class="token comment" spellcheck="true"># max length of sequence</span>d_ff <span class="token operator">=</span> <span class="token number">2048</span> <span class="token comment" spellcheck="true"># feedforward nerual network  dimension</span>d_k <span class="token operator">=</span> d_v <span class="token operator">=</span> <span class="token number">64</span> <span class="token comment" spellcheck="true"># dimension of k(same as q) and v</span>n_layers <span class="token operator">=</span> <span class="token number">6</span> <span class="token comment" spellcheck="true"># number of encoder and decoder layers</span>n_heads <span class="token operator">=</span> <span class="token number">8</span> <span class="token comment" spellcheck="true"># number of heads in multihead attention</span>p_drop <span class="token operator">=</span> <span class="token number">0.1</span> <span class="token comment" spellcheck="true"># propability of dropout</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果你对Transformer足够熟悉, 看变量名和注释一定能看出来它们的含义, 它们依次是:</p><ul><li>d_model: Embedding的大小.</li><li>max_len: 输入序列的最长大小.</li><li>d_ff: 前馈神经网络的隐藏层大小, 一般是d_model的四倍.</li><li>d_k, d_v: 自注意力中K和V的维度, Q的维度直接用K的维度代替, 因为这二者必须始终相等.</li><li>n_layers: Encoder和Decoder的层数.</li><li>n_heads: 自注意力多头的头数.</li><li>p_drop: Dropout的概率.</li></ul><h2 id="Mask"><a href="#Mask" class="headerlink" title="Mask"></a>Mask</h2><p>Mask分为两种, 一种是因为在数据中使用了padding, 不希望pad被加入到注意力中进行计算的Pad Mask for Attention, 还有一种是保证Decoder自回归信息不泄露的Subsequent Mask for Decoder.</p><h3 id="Pad-Mask-for-Attention"><a href="#Pad-Mask-for-Attention" class="headerlink" title="Pad Mask for Attention"></a>Pad Mask for Attention</h3><p>为了方便, 假设<code>&lt;PAD&gt;</code>在字典中的Index是0, 遇到输入为0直接将其标为True.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_attn_pad_mask</span><span class="token punctuation">(</span>seq_q<span class="token punctuation">,</span> seq_k<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token triple-quoted-string string">'''  Padding, because of unequal in source_len and target_len.  parameters:  seq_q: [batch, seq_len]  seq_k: [batch, seq_len]  return:  mask: [batch, len_q, len_k]  '''</span>  batch<span class="token punctuation">,</span> len_q <span class="token operator">=</span> seq_q<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>  batch<span class="token punctuation">,</span> len_k <span class="token operator">=</span> seq_k<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># we define index of PAD is 0, if tensor equals (zero) PAD tokens</span>  pad_attn_mask <span class="token operator">=</span> seq_k<span class="token punctuation">.</span>data<span class="token punctuation">.</span>eq<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, 1, len_k]</span>  <span class="token keyword">return</span> pad_attn_mask<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> len_q<span class="token punctuation">,</span> len_k<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, len_q, len_k]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在Encoder和Decoder中使用Mask的情况可能各有不同:</p><ul><li><p>在Encoder中使用Mask, 是为了将<code>encoder_input</code>中没有内容而打上PAD的部分进行Mask, 方便矩阵运算.</p></li><li><p>在Decoder中使用Mask, 可能是在Decoder的自注意力对<code>decoder_input</code> 的PAD进行Mask, 也有可能是对Encoder - Decoder自注意力时对<code>encoder_input</code>和<code>decoder_input</code>的PAD进行Mask.</p></li></ul><h3 id="Subsequent-Mask-for-Decoder"><a href="#Subsequent-Mask-for-Decoder" class="headerlink" title="Subsequent Mask for Decoder"></a>Subsequent Mask for Decoder</h3><p>该Mask是为了防止Decoder的自回归信息泄露而生的Mask, 直接生成一个上三角矩阵即可:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_attn_subsequent_mask</span><span class="token punctuation">(</span>seq<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token triple-quoted-string string">'''  Build attention mask matrix for decoder when it autoregressing.  parameters:  seq: [batch, target_len]  return:  subsequent_mask: [batch, target_len, target_len]   '''</span>  attn_shape <span class="token operator">=</span> <span class="token punctuation">[</span>seq<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> seq<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> seq<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># [batch, target_len, target_len]</span>  subsequent_mask <span class="token operator">=</span> np<span class="token punctuation">.</span>triu<span class="token punctuation">(</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>attn_shape<span class="token punctuation">)</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, target_len, target_len] </span>  subsequent_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>subsequent_mask<span class="token punctuation">)</span>  <span class="token keyword">return</span> subsequent_mask <span class="token comment" spellcheck="true"># [batch, target_len, target_len] </span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其中, 用到了生成上三角的函数<code>np.triu</code>, 其用法为:</p><pre class="line-numbers language-python"><code class="language-python">np<span class="token punctuation">.</span>triu<span class="token punctuation">(</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">'''array([[0., 1., 1., 1.],       [0., 0., 1., 1.],       [0., 0., 0., 1.]])'''</span>np<span class="token punctuation">.</span>triu<span class="token punctuation">(</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">'''array([[1., 1., 1., 1.],       [0., 1., 1., 1.],       [0., 0., 1., 1.]])'''</span>np<span class="token punctuation">.</span>triu<span class="token punctuation">(</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">'''array([[1., 1., 1., 1.],       [1., 1., 1., 1.],       [0., 1., 1., 1.]])'''</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其中<code>k</code>能控制上三角的大小, 越大则上三角范围越小. 与之完全<strong>相反</strong>的函数是<code>np.tril</code>, 能够生成下三角矩阵.</p><h2 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h2><p>在Transformer中, 使用的是绝对位置编码, 用于传输给模型Self - Attention所不能传输的位置信息, 编码使用正余弦公式实现:<br>$$<br>\begin{aligned}<br>PE(pos, 2i)&amp; = \sin(pos / 10000^{\frac{2i}{d_{model}}}) \\<br>PE(pos, 2i+1)&amp; = \cos(pos / 10000^{\frac{2i}{d_{model}}})<br>\end{aligned}<br>$$<br>基于上述公式, 我们把它实现出来:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">PositionalEncoding</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">,</span> max_len<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    super<span class="token punctuation">(</span>PositionalEncoding<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span>p_drop<span class="token punctuation">)</span>    positional_encoding <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>max_len<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [max_len, d_model]</span>    position <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> max_len<span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [max_len, 1]</span>    div_term <span class="token operator">=</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span>                          <span class="token punctuation">(</span><span class="token operator">-</span>torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">10000</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> d_model<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [max_len / 2]</span>    positional_encoding<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>position <span class="token operator">*</span> div_term<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># even</span>    positional_encoding<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>position <span class="token operator">*</span> div_term<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># odd</span>    <span class="token comment" spellcheck="true"># [max_len, d_model] -> [1, max_len, d_model] -> [max_len, 1, d_model]</span>    positional_encoding <span class="token operator">=</span> positional_encoding<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># register pe to buffer and require no grads</span>    self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">'pe'</span><span class="token punctuation">,</span> positional_encoding<span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># x: [seq_len, batch, d_model]</span>    <span class="token comment" spellcheck="true"># we can add positional encoding to x directly, and ignore other dimension</span>    x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>pe<span class="token punctuation">[</span><span class="token punctuation">:</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>实现$1/ 10000^{\frac{2i}{d_{model}}}$ 时既可以像我写出的那样使用幂指运算, 也可以直接写出.</p><p><code>register_buffer</code>能够申请一个缓冲区中的<strong>常量</strong>, 并且它不会被加入到计算图中, 也就不会参与反向传播.</p><p>更多关于<code>register</code>在<code>parameter</code>和<code>buffer</code>上的区别请见<a href="https://zhuanlan.zhihu.com/p/89442276" target="_blank" rel="noopener">Pytorch模型中的parameter与buffer</a>.</p><h2 id="Feed-Forward-Neural-Network"><a href="#Feed-Forward-Neural-Network" class="headerlink" title="Feed Forward Neural Network"></a>Feed Forward Neural Network</h2><p>在Transformer中, Encoder或者Decoder每个Block都需要用一个前馈神经网络来添加<strong>非线性</strong>:<br>$$<br>\operatorname{FFN}(x)=\operatorname{ReLU}(xW_1+b_1)W_2 + b_2<br>$$<br>注意, 这里它们都是有偏置的, 而且这两个Linear可以用两个$1\times1$ 的卷积来实现:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">FeedForwardNetwork</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token triple-quoted-string string">'''  Using nn.Conv1d replace nn.Linear to implements FFN.  '''</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    super<span class="token punctuation">(</span>FeedForwardNetwork<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># self.ff1 = nn.Linear(d_model, d_ff)</span>    <span class="token comment" spellcheck="true"># self.ff2 = nn.Linear(d_ff, d_model)</span>    self<span class="token punctuation">.</span>ff1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_ff<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>ff2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>d_ff<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span>p_drop<span class="token punctuation">)</span>    self<span class="token punctuation">.</span>layer_norm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># x: [batch, seq_len, d_model]</span>    residual <span class="token operator">=</span> x    x <span class="token operator">=</span> x<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, d_model, seq_len]</span>    x <span class="token operator">=</span> self<span class="token punctuation">.</span>ff1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> self<span class="token punctuation">.</span>ff2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    x <span class="token operator">=</span> x<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, seq_len, d_model]</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>layer_norm<span class="token punctuation">(</span>residual <span class="token operator">+</span> x<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>作为一个子层, 不要忘记Transformer中提到的Residual Connection和Layer Norm.</p><p>我选择用两个卷积代替Linear. 在<code>nn.Conv1d</code>中, 要求数据的规格为<code>[batch, x, ...]</code>, 我们是要对<code>d_model</code> 上的数据进行卷积, 所以还是需要<code>transpose</code>一下.</p><h2 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi - Head Attention"></a>Multi - Head Attention</h2><p>先说多头注意力, 因为多头注意力能够决定缩放点积注意力的输入大小. 作为一个子层, 其中的Residual Connection和Layer Norm是必须的.</p><p>多头注意力是多个不同的头来获取不同的特征, 类似于多个<strong>卷积核</strong>所达到的效果. 在计算完后通过一个Linear调整大小:<br>$$<br>\begin{aligned}<br>\operatorname{MultiHead}(Q, K, V) &amp;= \operatorname{Concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_h)W^O \\<br>\text{where } \text{head}_i &amp;= \operatorname{Attention}(QW^Q_i, KW^K_i, VW^V_i)<br>\end{aligned}<br>$$<br>多头注意力在Encoder和Decoder中的使用略有区别, 主要区别在于Mask的不同. 我们前面已经实现了两种Mask函数, 在这里会用到.</p><p>多头注意力实际上不是通过弄出很多大小相同的矩阵然后相乘来实现的, 只需要合并到一个矩阵进行计算:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n_heads<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    super<span class="token punctuation">(</span>MultiHeadAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># do not use more instance to implement multihead attention</span>    <span class="token comment" spellcheck="true"># it can be complete in one matrix</span>    self<span class="token punctuation">.</span>n_heads <span class="token operator">=</span> n_heads    <span class="token comment" spellcheck="true"># we can't use bias because there is no bias term in formular</span>    self<span class="token punctuation">.</span>W_Q <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_k <span class="token operator">*</span> n_heads<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>W_K <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_k <span class="token operator">*</span> n_heads<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>W_V <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_v <span class="token operator">*</span> n_heads<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_v <span class="token operator">*</span> n_heads<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>layer_norm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_Q<span class="token punctuation">,</span> input_K<span class="token punctuation">,</span> input_V<span class="token punctuation">,</span> attn_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''    To make sure multihead attention can be used both in encoder and decoder,     we use Q, K, V respectively.    input_Q: [batch, len_q, d_model]    input_K: [batch, len_k, d_model]    input_V: [batch, len_v, d_model]    '''</span>    residual<span class="token punctuation">,</span> batch <span class="token operator">=</span> input_Q<span class="token punctuation">,</span> input_Q<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># [batch, len_q, d_model] -- matmul W_Q --> [batch, len_q, d_q * n_heads] -- view --> </span>    <span class="token comment" spellcheck="true"># [batch, len_q, n_heads, d_k,] -- transpose --> [batch, n_heads, len_q, d_k]</span>    Q <span class="token operator">=</span> self<span class="token punctuation">.</span>W_Q<span class="token punctuation">(</span>input_Q<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> d_k<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, n_heads, len_q, d_k]</span>    K <span class="token operator">=</span> self<span class="token punctuation">.</span>W_K<span class="token punctuation">(</span>input_K<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> d_k<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, n_heads, len_k, d_k]</span>    V <span class="token operator">=</span> self<span class="token punctuation">.</span>W_V<span class="token punctuation">(</span>input_V<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> d_v<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, n_heads, len_v, d_v]</span>    attn_mask <span class="token operator">=</span> attn_mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, n_heads, seq_len, seq_len]</span>    <span class="token comment" spellcheck="true"># prob: [batch, n_heads, len_q, d_v] attn: [batch, n_heads, len_q, len_k]</span>    prob<span class="token punctuation">,</span> attn <span class="token operator">=</span> ScaledDotProductAttention<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>Q<span class="token punctuation">,</span> K<span class="token punctuation">,</span> V<span class="token punctuation">,</span> attn_mask<span class="token punctuation">)</span>    prob <span class="token operator">=</span> prob<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, len_q, n_heads, d_v]</span>    prob <span class="token operator">=</span> prob<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_heads <span class="token operator">*</span> d_v<span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, len_q, n_heads * d_v]</span>    output <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>prob<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, len_q, d_model]</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>layer_norm<span class="token punctuation">(</span>residual <span class="token operator">+</span> output<span class="token punctuation">)</span><span class="token punctuation">,</span> attn<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>提两个非常重要的点:</p><ol><li>在拆维度时不要破坏维度原来本身的意义.</li><li>虽然新版本已经有<code>reshape</code>函数可以用了, 但是仍然不要忘记, <code>transpose</code>后如果接<code>permute</code>或者<code>view</code>必须要加<code>contiguous</code>, 这是<strong>数据真实存储连续与否</strong>的问题, 请参见<a href="https://adaning.github.io/posts/42255.html">Pytorch之张量基础操作</a>中的<strong>维度变换</strong>部分.</li></ol><h2 id="Scaled-DotProduct-Attention"><a href="#Scaled-DotProduct-Attention" class="headerlink" title="Scaled DotProduct Attention"></a>Scaled DotProduct Attention</h2><p>Tranformer中非常重要的概念, 缩放点积注意力, 公式如下:<br>$$<br>\operatorname{Attention}(Q, K, V) = \operatorname{softmax}(\frac{QK^T}{\sqrt{d_k}})V<br>$$<br>实现起来非常简单, 只需要把Q, K两个矩阵一乘, 然后再缩放, 过一次Softmax, 再和V乘下:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">ScaledDotProductAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    super<span class="token punctuation">(</span>ScaledDotProductAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> Q<span class="token punctuation">,</span> K<span class="token punctuation">,</span> V<span class="token punctuation">,</span> attn_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''    Q: [batch, n_heads, len_q, d_k]    K: [batch, n_heads, len_k, d_k]    V: [batch, n_heads, len_v, d_v]    attn_mask: [batch, n_heads, seq_len, seq_len]    '''</span>    scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>Q<span class="token punctuation">,</span> K<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>d_k<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, n_heads, len_q, len_k]</span>    scores<span class="token punctuation">.</span>masked_fill_<span class="token punctuation">(</span>attn_mask<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1e9</span><span class="token punctuation">)</span>    attn <span class="token operator">=</span> nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span>scores<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, n_heads, len_q, len_k]</span>    prob <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>attn<span class="token punctuation">,</span> V<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, n_heads, len_q, d_v]</span>    <span class="token keyword">return</span> prob<span class="token punctuation">,</span> attn<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>masked_fill_</code>能把传进来的Mask为True的地方全都填充上某个值, 这里需要用一个很大的负数来保证$e^x \rightarrow 0$, 使得其在Softmax​ 中可以被忽略.</p><h2 id="Encoder-and-Decoder"><a href="#Encoder-and-Decoder" class="headerlink" title="Encoder and Decoder"></a>Encoder and Decoder</h2><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>先写出Encoder的每个Layer, 由多头注意力和FFN组成:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">EncoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    super<span class="token punctuation">(</span>EncoderLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>encoder_self_attn <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>ffn <span class="token operator">=</span> FeedForwardNetwork<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder_input<span class="token punctuation">,</span> encoder_pad_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''    encoder_input: [batch, source_len, d_model]    encoder_pad_mask: [batch, n_heads, source_len, source_len]    encoder_output: [batch, source_len, d_model]    attn: [batch, n_heads, source_len, source_len]    '''</span>    encoder_output<span class="token punctuation">,</span> attn <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder_self_attn<span class="token punctuation">(</span>encoder_input<span class="token punctuation">,</span> encoder_input<span class="token punctuation">,</span> encoder_input<span class="token punctuation">,</span> encoder_pad_mask<span class="token punctuation">)</span>    encoder_output <span class="token operator">=</span> self<span class="token punctuation">.</span>ffn<span class="token punctuation">(</span>encoder_output<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, source_len, d_model]</span>    <span class="token keyword">return</span> encoder_output<span class="token punctuation">,</span> attn<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>对于给定的<code>encoder_input</code>和<code>encoder_pad_pask</code>, Encoder应该能够完成整个Block(Layer)的计算流程. 然后实现整个Encoder:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Encoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    super<span class="token punctuation">(</span>Encoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>source_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>source_vocab_size<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>    self<span class="token punctuation">.</span>positional_embedding <span class="token operator">=</span> PositionalEncoding<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>    self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>EncoderLayer<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> layer <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_layers<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder_input<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># encoder_input: [batch, source_len]</span>    encoder_output <span class="token operator">=</span> self<span class="token punctuation">.</span>source_embedding<span class="token punctuation">(</span>encoder_input<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, source_len, d_model]</span>    encoder_output <span class="token operator">=</span> self<span class="token punctuation">.</span>positional_embedding<span class="token punctuation">(</span>encoder_output<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, source_len, d_model]</span>    encoder_self_attn_mask <span class="token operator">=</span> get_attn_pad_mask<span class="token punctuation">(</span>encoder_input<span class="token punctuation">,</span> encoder_input<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, source_len, source_len]</span>    encoder_self_attns <span class="token operator">=</span> list<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>      <span class="token comment" spellcheck="true"># encoder_output: [batch, source_len, d_model]</span>      <span class="token comment" spellcheck="true"># encoder_self_attn: [batch, n_heads, source_len, source_len]</span>      encoder_output<span class="token punctuation">,</span> encoder_self_attn <span class="token operator">=</span> layer<span class="token punctuation">(</span>encoder_output<span class="token punctuation">,</span> encoder_self_attn_mask<span class="token punctuation">)</span>      encoder_self_attns<span class="token punctuation">.</span>append<span class="token punctuation">(</span>encoder_self_attn<span class="token punctuation">)</span>    <span class="token keyword">return</span> encoder_output<span class="token punctuation">,</span> encoder_self_attns<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>对于整个Encoder, 直接将Token的Index传入Embedding中, 再添入位置编码, 之后就经过多层Transformer Encoder. 在传入Block前, 先需要计算Padding的Mask, 再将上层的输出作为下层输入依次迭代.</p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>其实实现了Encoder, Decoder的实现部分都是对应的. 先实现Decoder的Block:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">DecoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    super<span class="token punctuation">(</span>DecoderLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>decoder_self_attn <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>encoder_decoder_attn <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>ffn <span class="token operator">=</span> FeedForwardNetwork<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> decoder_input<span class="token punctuation">,</span> encoder_output<span class="token punctuation">,</span> decoder_self_mask<span class="token punctuation">,</span> decoder_encoder_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''    decoder_input: [batch, target_len, d_mdoel]    encoder_output: [batch, source_len, d_model]    decoder_self_mask: [batch, target_len, target_len]    decoder_encoder_mask: [batch, target_len, source_len]    '''</span>    <span class="token comment" spellcheck="true"># masked mutlihead attention</span>    <span class="token comment" spellcheck="true"># Q, K, V all from decoder it self</span>    <span class="token comment" spellcheck="true"># decoder_output: [batch, target_len, d_model]</span>    <span class="token comment" spellcheck="true"># decoder_self_attn: [batch, n_heads, target_len, target_len]</span>    decoder_output<span class="token punctuation">,</span> decoder_self_attn <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder_self_attn<span class="token punctuation">(</span>decoder_input<span class="token punctuation">,</span> decoder_input<span class="token punctuation">,</span> decoder_input<span class="token punctuation">,</span> decoder_self_mask<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Q from decoder, K, V from encoder</span>    <span class="token comment" spellcheck="true"># decoder_output: [batch, target_len, d_model]</span>    <span class="token comment" spellcheck="true"># decoder_encoder_attn: [batch, n_heads, target_len, source_len]</span>    decoder_output<span class="token punctuation">,</span> decoder_encoder_attn <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder_decoder_attn<span class="token punctuation">(</span>decoder_output<span class="token punctuation">,</span> encoder_output<span class="token punctuation">,</span> encoder_output<span class="token punctuation">,</span> decoder_encoder_mask<span class="token punctuation">)</span>    decoder_output <span class="token operator">=</span> self<span class="token punctuation">.</span>ffn<span class="token punctuation">(</span>decoder_output<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, target_len, d_model]</span>    <span class="token keyword">return</span> decoder_output<span class="token punctuation">,</span> decoder_self_attn<span class="token punctuation">,</span> decoder_encoder_attn<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>与Encoder相对应, 只不过因为多了一个Encoder - Decoder自注意力, 所以需要额外计算一个Encoder - Decoder的Mask. 然后写出整个Decoder:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Decoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    super<span class="token punctuation">(</span>Decoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>target_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>target_vocab_size<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>    self<span class="token punctuation">.</span>positional_embedding <span class="token operator">=</span> PositionalEncoding<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>    self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>DecoderLayer<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> layer <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_layers<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> decoder_input<span class="token punctuation">,</span> encoder_input<span class="token punctuation">,</span> encoder_output<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''    decoder_input: [batch, target_len]    encoder_input: [batch, source_len]    encoder_output: [batch, source_len, d_model]    '''</span>    decoder_output <span class="token operator">=</span> self<span class="token punctuation">.</span>target_embedding<span class="token punctuation">(</span>decoder_input<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, target_len, d_model]</span>    decoder_output <span class="token operator">=</span> self<span class="token punctuation">.</span>positional_embedding<span class="token punctuation">(</span>decoder_output<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, target_len, d_model]</span>    decoder_self_attn_mask <span class="token operator">=</span> get_attn_pad_mask<span class="token punctuation">(</span>decoder_input<span class="token punctuation">,</span> decoder_input<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, target_len, target_len]</span>    decoder_subsequent_mask <span class="token operator">=</span> get_attn_subsequent_mask<span class="token punctuation">(</span>decoder_input<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, target_len, target_len]</span>    decoder_encoder_attn_mask <span class="token operator">=</span> get_attn_pad_mask<span class="token punctuation">(</span>decoder_input<span class="token punctuation">,</span> encoder_input<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, target_len, source_len]</span>    decoder_self_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>gt<span class="token punctuation">(</span>decoder_self_attn_mask <span class="token operator">+</span> decoder_subsequent_mask<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>    decoder_self_attns<span class="token punctuation">,</span> decoder_encoder_attns <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>      <span class="token comment" spellcheck="true"># decoder_output: [batch, target_len, d_model]</span>      <span class="token comment" spellcheck="true"># decoder_self_attn: [batch, n_heads, target_len, target_len]</span>      <span class="token comment" spellcheck="true"># decoder_encoder_attn: [batch, n_heads, target_len, source_len]</span>      decoder_output<span class="token punctuation">,</span> decoder_self_attn<span class="token punctuation">,</span> decoder_encoder_attn <span class="token operator">=</span> layer<span class="token punctuation">(</span>decoder_output<span class="token punctuation">,</span> encoder_output<span class="token punctuation">,</span> decoder_self_mask<span class="token punctuation">,</span> decoder_encoder_attn_mask<span class="token punctuation">)</span>      decoder_self_attns<span class="token punctuation">.</span>append<span class="token punctuation">(</span>decoder_self_attn<span class="token punctuation">)</span>      decoder_encoder_attns<span class="token punctuation">.</span>append<span class="token punctuation">(</span>decoder_encoder_attn<span class="token punctuation">)</span>    <span class="token keyword">return</span> decoder_output<span class="token punctuation">,</span> decoder_self_attns<span class="token punctuation">,</span> decoder_encoder_attns<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>和Encoder相对应, 但Decoder和Encoder使用了两个不同的Embedding. 对于Mask, 可以把自回归Mask和Padding Mask用<code>torch.gt</code>整合成一个Mask, 送入其中.</p><h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>终于到了这一步, 虽然后面还有一些小小的工作, 但现在终于能看到Transformer的<strong>全貌</strong>了:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer.jpg" style="zoom: 50%;" /><p>里面有一个Encoder, 一个Decoder, 在Decoder端还需要加上投影层来分类:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Transformer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    super<span class="token punctuation">(</span>Transformer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> Encoder<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> Decoder<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>projection <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> target_vocab_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder_input<span class="token punctuation">,</span> decoder_input<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''    encoder_input: [batch, source_len]    decoder_input: [batch, target_len]    '''</span>    <span class="token comment" spellcheck="true"># encoder_output: [batch, source_len, d_model]</span>    <span class="token comment" spellcheck="true"># encoder_attns: [n_layers, batch, n_heads, source_len, source_len]</span>    encoder_output<span class="token punctuation">,</span> encoder_attns <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>encoder_input<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># decoder_output: [batch, target_len, d_model]</span>    <span class="token comment" spellcheck="true"># decoder_self_attns: [n_layers, batch, n_heads, target_len, target_len]</span>    <span class="token comment" spellcheck="true"># decoder_encoder_attns: [n_layers, batch, n_heads, target_len, source_len]</span>    decoder_output<span class="token punctuation">,</span> decoder_self_attns<span class="token punctuation">,</span> decoder_encoder_attns <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>decoder_input<span class="token punctuation">,</span> encoder_input<span class="token punctuation">,</span> encoder_output<span class="token punctuation">)</span>    decoder_logits <span class="token operator">=</span> self<span class="token punctuation">.</span>projection<span class="token punctuation">(</span>decoder_output<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, target_len, target_vocab_size]</span>    <span class="token comment" spellcheck="true"># decoder_logits: [batch * target_len, target_vocab_size]</span>    <span class="token keyword">return</span> decoder_logits<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> decoder_logits<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> encoder_attns<span class="token punctuation">,</span> decoder_self_attns<span class="token punctuation">,</span> decoder_encoder_attns<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>最后对logits的处理是<code>view</code>成了<code>[batch * target_len, target_vocab_size]</code>, 前面的大小并不影响我们一会用交叉熵计算损失.</p><h2 id="Input-Data"><a href="#Input-Data" class="headerlink" title="Input Data"></a>Input Data</h2><p>输入数据没什么好说的, 为了方便直接采用了硬编码的方式构造<code>word2index</code>, 这样我们的输入序列都被转换为了Token的index输入到Embedding层中, 自动转化为嵌入在低维空间的稠密向量:</p><p>Decoder的输入构造过程采用了<strong>Teaching Forcing</strong>, 保证了训练过程是可以保持<strong>并行</strong>的.</p><pre class="line-numbers language-python"><code class="language-python">sentences <span class="token operator">=</span> <span class="token punctuation">[</span>        <span class="token comment" spellcheck="true"># enc_input           dec_input         dec_output</span>        <span class="token punctuation">[</span><span class="token string">'ich mochte ein bier P'</span><span class="token punctuation">,</span> <span class="token string">'S i want a beer .'</span><span class="token punctuation">,</span> <span class="token string">'i want a beer . E'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token string">'ich mochte ein cola P'</span><span class="token punctuation">,</span> <span class="token string">'S i want a coke .'</span><span class="token punctuation">,</span> <span class="token string">'i want a coke . E'</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># Padding Should be Zero</span>source_vocab <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'P'</span> <span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'ich'</span> <span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'mochte'</span> <span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'ein'</span> <span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'bier'</span> <span class="token punctuation">:</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token string">'cola'</span> <span class="token punctuation">:</span> <span class="token number">5</span><span class="token punctuation">}</span>source_vocab_size <span class="token operator">=</span> len<span class="token punctuation">(</span>source_vocab<span class="token punctuation">)</span>target_vocab <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'P'</span> <span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'i'</span> <span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'want'</span> <span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'a'</span> <span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'beer'</span> <span class="token punctuation">:</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token string">'coke'</span> <span class="token punctuation">:</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token string">'S'</span> <span class="token punctuation">:</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token string">'E'</span> <span class="token punctuation">:</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token string">'.'</span> <span class="token punctuation">:</span> <span class="token number">8</span><span class="token punctuation">}</span>idx2word <span class="token operator">=</span> <span class="token punctuation">{</span>i<span class="token punctuation">:</span> w <span class="token keyword">for</span> i<span class="token punctuation">,</span> w <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>target_vocab<span class="token punctuation">)</span><span class="token punctuation">}</span>target_vocab_size <span class="token operator">=</span> len<span class="token punctuation">(</span>target_vocab<span class="token punctuation">)</span>source_len <span class="token operator">=</span> <span class="token number">5</span> <span class="token comment" spellcheck="true"># max length of input sequence</span>target_len <span class="token operator">=</span> <span class="token number">6</span><span class="token keyword">def</span> <span class="token function">make_data</span><span class="token punctuation">(</span>sentences<span class="token punctuation">)</span><span class="token punctuation">:</span>  encoder_inputs<span class="token punctuation">,</span> decoder_inputs<span class="token punctuation">,</span> decoder_outputs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    encoder_input <span class="token operator">=</span> <span class="token punctuation">[</span>source_vocab<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token keyword">for</span> word <span class="token keyword">in</span> sentences<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>    decoder_input <span class="token operator">=</span> <span class="token punctuation">[</span>target_vocab<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token keyword">for</span> word <span class="token keyword">in</span> sentences<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>    decoder_output <span class="token operator">=</span> <span class="token punctuation">[</span>target_vocab<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token keyword">for</span> word <span class="token keyword">in</span> sentences<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>    encoder_inputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>encoder_input<span class="token punctuation">)</span>    decoder_inputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>decoder_input<span class="token punctuation">)</span>    decoder_outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>decoder_output<span class="token punctuation">)</span>  <span class="token keyword">return</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>encoder_inputs<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>decoder_inputs<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>decoder_outputs<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>数据量非常的少, 所以等会的训练会根本不充分.</p><h2 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h2><p>制作一个Seq2Seq的数据集, 只需要按照Index返回Encoder的输出, Decoder的输入, Decoder的输出(label)就好:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Seq2SeqDataset</span><span class="token punctuation">(</span>Data<span class="token punctuation">.</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder_input<span class="token punctuation">,</span> decoder_input<span class="token punctuation">,</span> decoder_output<span class="token punctuation">)</span><span class="token punctuation">:</span>    super<span class="token punctuation">(</span>Seq2SeqDataset<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>encoder_input <span class="token operator">=</span> encoder_input    self<span class="token punctuation">.</span>decoder_input <span class="token operator">=</span> decoder_input    self<span class="token punctuation">.</span>decoder_output <span class="token operator">=</span> decoder_output  <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>encoder_input<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>  <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>encoder_input<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>decoder_input<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>decoder_output<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>对训练所需的所有东西进行定义:</p><pre class="line-numbers language-python"><code class="language-python">batch_size <span class="token operator">=</span> <span class="token number">64</span>epochs <span class="token operator">=</span> <span class="token number">64</span>lr <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">'cpu'</span><span class="token punctuation">)</span>model <span class="token operator">=</span> Transformer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span>ignore_index<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>encoder_inputs<span class="token punctuation">,</span> decoder_inputs<span class="token punctuation">,</span> decoder_outputs <span class="token operator">=</span> make_data<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span>dataset <span class="token operator">=</span> Seq2SeqDataset<span class="token punctuation">(</span>encoder_inputs<span class="token punctuation">,</span> decoder_inputs<span class="token punctuation">,</span> decoder_outputs<span class="token punctuation">)</span>data_loader <span class="token operator">=</span> Data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里有个<code>criterion = nn.CrossEntropyLoss(ignore_index=0)</code>, 其中<code>ignore_index=0</code>指的是PAD在计算交叉熵时不应该被包括进去(前面提到过PAD所对应的Index是0).</p><p>我们从定义好的数据集中取出数据到<code>device</code>, 然后用torch三件套:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token triple-quoted-string string">'''  encoder_input: [batch, source_len]  decoder_input: [batch, target_len]  decoder_ouput: [batch, target_len]  '''</span>  <span class="token keyword">for</span> encoder_input<span class="token punctuation">,</span> decoder_input<span class="token punctuation">,</span> decoder_output <span class="token keyword">in</span> data_loader<span class="token punctuation">:</span>    encoder_input <span class="token operator">=</span> encoder_input<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    decoder_input <span class="token operator">=</span> decoder_input<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    decoder_output <span class="token operator">=</span> decoder_output<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    output<span class="token punctuation">,</span> encoder_attns<span class="token punctuation">,</span> decoder_attns<span class="token punctuation">,</span> decoder_encoder_attns <span class="token operator">=</span> model<span class="token punctuation">(</span>encoder_input<span class="token punctuation">,</span> decoder_input<span class="token punctuation">)</span>    loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output<span class="token punctuation">,</span> decoder_output<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Epoch:'</span><span class="token punctuation">,</span> <span class="token string">'%04d'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'loss ='</span><span class="token punctuation">,</span> <span class="token string">'{:.6f}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>loss<span class="token punctuation">)</span><span class="token punctuation">)</span>    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Attention-Visualization"><a href="#Attention-Visualization" class="headerlink" title="Attention Visualization"></a>Attention Visualization</h2><p>这回有了自己造的Transformer, 经过了<strong>根本不完全的训练: )</strong>, 我们可以把它的Attention矩阵画出来看看:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> seaborn <span class="token keyword">as</span> sns<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token triple-quoted-string string">'''batch 1:[[1, 2, 3, 5, 0],[1, 2, 3, 4, 0]]'''</span>temp_batch <span class="token operator">=</span> <span class="token number">0</span>n_layers <span class="token operator">=</span> <span class="token number">4</span>plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span>n_heads <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">,</span> n_layers <span class="token operator">*</span> <span class="token number">3</span> <span class="token operator">+</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># encoder_attns: [n_layers, batch, n_heads, source_len, source_len]</span>i <span class="token operator">=</span> <span class="token number">0</span>tokens <span class="token operator">=</span> sentences<span class="token punctuation">[</span>temp_batch<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">for</span> layer <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">for</span> head <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_heads<span class="token punctuation">)</span><span class="token punctuation">:</span>    i <span class="token operator">+=</span> <span class="token number">1</span>    plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span>n_layers<span class="token punctuation">,</span> n_heads<span class="token punctuation">,</span> i<span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Layer:{}, Head:{}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>layer<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> head<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> i <span class="token operator">%</span> n_heads <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>      cbar<span class="token operator">=</span><span class="token boolean">True</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>      cbar<span class="token operator">=</span><span class="token boolean">False</span>    sns<span class="token punctuation">.</span>heatmap<span class="token punctuation">(</span>encoder_attns<span class="token punctuation">[</span>layer<span class="token punctuation">]</span><span class="token punctuation">[</span>temp_batch<span class="token punctuation">]</span><span class="token punctuation">[</span>head<span class="token punctuation">]</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cmap<span class="token operator">=</span><span class="token string">'YlGnBu'</span><span class="token punctuation">,</span>             xticklabels<span class="token operator">=</span>tokens<span class="token punctuation">,</span> yticklabels<span class="token operator">=</span>tokens<span class="token punctuation">,</span> cbar<span class="token operator">=</span>cbar<span class="token punctuation">,</span> vmin<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> vmax<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    plt<span class="token punctuation">.</span>xticks<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>yticks<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>最后两行<code>plt.xticks</code>和<code>plt.yticks</code>纯粹是为了<strong>方便注释掉</strong>, 才又写在了外面.</p><p><strong>不要对结果太在意</strong>, 因为<strong>训练是根本不完整的</strong>, 数据也才只有两条. 我只是想画出来看看每个头都大致学到了什么:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/pytorchtransformer1.jpg" alt=""></p><p>最右侧是Padding, 这一列的权重都被当做是0来计算. 在浅一些的层确实学到了不同Token对不同部分的权重. 再深一些的层基本都没有得到训练, 因为数据实在太少了. </p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Transformer </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KEPLER: Knowledge Embedding and Pre-trained Language Representation</title>
      <link href="/posts/52897.html"/>
      <url>/posts/52897.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>BERT(详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>)</li></ul></blockquote><h1 id="KEPLER-A-Unified-Model-for-Knowledge-Embedding-and-Pre-trained-Language-Representation"><a href="#KEPLER-A-Unified-Model-for-Knowledge-Embedding-and-Pre-trained-Language-Representation" class="headerlink" title="KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation"></a>KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation</h1><p>本文是论文<a href="http://arxiv.org/abs/1911.06136" target="_blank" rel="noopener">KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation</a> 的阅读笔记和个人理解.</p><blockquote><p>顺带吐槽一下这论文第一版和第二版差的也太大了…</p></blockquote><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者指出, PLM不能直接从文本中<strong>获取常识</strong>. 相反, KGE经常能获得知识图谱中实体和关系的有效表示, 却不能<strong>捕捉上下文</strong>. 基于这个简单的观察, 作者希望将常识注入PLM, 这样PLM不但能学到常识, 还能学到有效而丰富的信息<strong>表示</strong>. </p><p>并且在已经注入知识的PLM中, 作者还观察到以下问题:</p><ul><li>实体嵌入和语言分离, 不方便表示空间的对齐. KGE模型很少将KG结构作为输入, 并很少结合文本信息, 因此无法帮助PLM. </li><li>需要实体链接器, 在传播时容易出现错误.</li><li>与普通PLM相比, 查找实体的表示会带来额外的开销.</li></ul><p>而文本描述有与实体相关的丰富信息, 能够帮助文本语义空间与KG的符号空间对齐: </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler1.jpg" style="zoom: 25%;" /><p>作者将实体结合其<strong>描述</strong>用PLM编码, 对KRL和PLM的目标进行<strong>联合优化</strong>. </p><h2 id="KEPLER"><a href="#KEPLER" class="headerlink" title="KEPLER"></a>KEPLER</h2><p>KEPLER(<strong>K</strong>nowledge <strong>E</strong>mbedding and <strong>P</strong>re - trained <strong>L</strong>anguag<strong>E</strong> <strong>R</strong>epresentation)是一个KGE和PLM表示<strong>统一</strong>的模型. 所以它包含了PLM和KE的联合优化目标. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler2.jpg" style="zoom: 50%;" /><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>作者使用了Transformer Encoder(其实就是BERT)作为<strong>文本编码器</strong>. 即Transformer Encoder将$N$ 个Token的序列$(x_1,\dots, x_N)$作为输入, 然后经过$L$ 层Transformer Encoder的堆叠计算, 得到$d$ 维的上下文表示$\mathbf{H}_i, 1\leq i \leq L$. 每层编码器$\mathrm{E}_i$由多头自注意力和前馈神经网络组成, 每层的Encoder表示记为:<br>$$<br>\mathbf{H}_i=\mathrm{E}_i(\mathbf{H}_{i-1})<br>$$<br>对于任意文本$\text{text}$, 作者希望将经过编码后的$\mathrm{[CLS]}$ 处的输出$\mathrm{E}_{[\mathrm{CLS}]} $ 做为文本的表示. </p><p>在KEPLER中, 作者和RoBERTa一样使用了<strong>BPE</strong>, 与之前出现的Knowledge - Enhanced Model相比较, 这里没有使用额外的<strong>实体连接器</strong>或者知识集成层. </p><h3 id="Knowledge-Embedding"><a href="#Knowledge-Embedding" class="headerlink" title="Knowledge Embedding"></a>Knowledge Embedding</h3><p>与其他KGE方法一样, KEPLER将实体和关系映射进一个$d$ 维的空间中, 并且使用打分函数训练. </p><p>但是KEPLER和普通的KGE方法又不一样, 它不再<strong>存储</strong>Embedding, 而是将实体结合它们本身的描述<strong>编码</strong>做为Embedding. 作者设计了两种结合实体描述的方法: </p><ul><li>只用<strong>实体描述</strong>. </li><li>使用<strong>实体描述</strong>和<strong>关系描述</strong>. </li></ul><h4 id="Using-Entity-Descriptions"><a href="#Using-Entity-Descriptions" class="headerlink" title="Using Entity Descriptions"></a>Using Entity Descriptions</h4><p>对于三元组$(h, r, t)$, 只使用三元组就是对头实体$h$ 的描述$\text{text}_h$和尾实体$t$ 的描述$\text{text}_t$ 分别进行编码, 然后再将关系$r$ 单独嵌入: </p><p>$$<br>\begin{aligned}<br>\mathbf{h} &amp;=\mathrm{E}_{[\mathrm{CLS}]}\left(\operatorname{text}_{h}\right) \\<br>\mathbf{t} &amp;=\mathrm{E}_{[\mathrm{CLS}]}\left(\operatorname{text}_{t}\right) \\<br>\mathbf{r} &amp;=\mathbf{T}_{r}<br>\end{aligned}<br>$$</p><p>其中$\mathbf{T}_r$ 代表关系$r$ 的Embedding权重. </p><h4 id="Using-Entity-and-Relation-Descriptions"><a href="#Using-Entity-and-Relation-Descriptions" class="headerlink" title="Using Entity and Relation Descriptions"></a>Using Entity and Relation Descriptions</h4><p>与只使用实体描述不一样, 因为BERT是可以对两段文字联合编码的, 所以这种方法可以将头实体描述和关系放在一起使用: </p><p>$$<br>\mathbf{h}_{r}=\mathrm{E}_{[\mathrm{CLS}]}\left(\operatorname{text}_{h, r}\right)<br>$$</p><p>具体一点, $\mathrm{text}_{h,r}$ 代表头实体$h$ 和其三元组关系$r$的描述, 在二者之间像BERT输入两段信息一样加上特殊的分隔符$[\mathrm{SEP}]$ 做为区分. 尾实体仍然单独输入进BERT. </p><blockquote><p>我认为该方法最少有两个问题: </p><ol><li>头实体获得关系描述加成(也有可能是污染), 尾实体没有被考虑到. </li><li>BERT捕捉上下文的能力被<strong>局限</strong>了, 仅能体现在<strong>描述</strong>的上下文当中, 并不能根据语境调整实体的表示. </li></ol></blockquote><h4 id="Konwledge-Embedding-Loss-and-Score-Function"><a href="#Konwledge-Embedding-Loss-and-Score-Function" class="headerlink" title="Konwledge Embedding Loss and Score Function"></a>Konwledge Embedding Loss and Score Function</h4><p>KE部分的损失如下: </p><p>$$<br>\mathcal{L}_{\mathrm{KE}} =-\log \sigma\left(\gamma-d_{r}(\mathbf{h}, \mathbf{t})\right)<br>-\sum_{i=1}^{n} \frac{1}{n} \log \sigma\left(d_{r}\left(\mathbf{h}_{\mathbf{i}}^{\prime}, \mathbf{t}_{\mathbf{i}}^{\prime}\right)-\gamma\right)<br>$$</p><p>其中$(h_i^\prime, r, t_i^\prime)$是负采样得到的样本, $\sigma$ 是Sigmoid函数, $\gamma$ 是间隔, $d_r$ 是打分函数, KEPLER沿用TransE的打分函数: </p><p>$$<br>d_{r}(\mathbf{h}, \mathbf{t})=\lVert\mathbf{h}+\mathbf{r}-\mathbf{t}\rVert_{p}<br>$$</p><p>作者使用的是一阶范数, 即$p=1$. </p><h3 id="Masked-Language-Modeling"><a href="#Masked-Language-Modeling" class="headerlink" title="Masked Language Modeling"></a>Masked Language Modeling</h3><p>在Masked Language Model上, KEPLER沿用了BERT和RoBERTa的<strong>MLM</strong>训练损失, 结构和Mask方式都<strong>没有发生任何变化</strong>. 在分类时仍然是在Encoder的最后一层输出$\mathbf{H}_{L,j}$ 后接上和<strong>字典</strong>等同大小的$W$ 路分类器. </p><p>因为RoBERTa的效果比较好, 所以直接用$\text{RoBERTa}_{\mathrm{BASE}}$ 的参数初始化. </p><h3 id="Training-Objectives"><a href="#Training-Objectives" class="headerlink" title="Training Objectives"></a>Training Objectives</h3><p>训练目标就是之前提到过的KGE部分和MLM部分之和: </p><p>$$<br>\mathcal{L}=\mathcal{L}_{\mathrm{KE}}+\mathcal{L}_{\mathrm{MLM}}<br>$$</p><p>这两部分目标是<strong>共享Encoder</strong>的, 在训练时可以采样<strong>不同类型</strong>的文本(倾向于优化KE或者MLM)作为训练数据. </p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h3><p>作者需要使用不同数据优化KE和MLM. </p><h4 id="KE-Objective-Wikidata5M"><a href="#KE-Objective-Wikidata5M" class="headerlink" title="KE Objective: Wikidata5M"></a>KE Objective: Wikidata5M</h4><p>因为作者需要大规模的KG, 并且必须包含对应的实体和关系<strong>描述</strong>, 最好还要支持Inductive Setting, 这样的数据集基本不存在, 所以作者自己根据<strong>Wikidata</strong>和<strong>Wikipedia</strong>构建了一个新的包含实体关系描述文本的大规模KG数据集<strong>Wikidata5M</strong>. </p><p>Wikidata5M比现在的常用数据集大得多, 并几乎涵盖了所有领域: </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler3.jpg" style="zoom: 25%;" /><p>Wikidata5M中实体类型统计:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler4.jpg" style="zoom: 20%;" /><h5 id="DataSplit"><a href="#DataSplit" class="headerlink" title="DataSplit"></a>DataSplit</h5><p>数据可以按照两种设置进行划分: </p><ul><li><p><strong>Transductive Setting</strong>: 在绝大多数KG数据集中使用, 对于训练集, 验证集, 测试集<strong>共享所有实体</strong>, 但<strong>并不知道完整三元组</strong>.</p></li><li><p><strong>Inductive Setting</strong>: 训练集, 验证集, 测试集中, <strong>实体和三元组都不共享</strong>. 这更考验模型的<strong>推断</strong>能力, 也更困难. 但它更符合<strong>现实世界</strong>的应用情况. 具体数据集划分情况如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler5.jpg" style="zoom: 25%;" /></li></ul><h5 id="Benchmark"><a href="#Benchmark" class="headerlink" title="Benchmark"></a>Benchmark</h5><p>作者用之前常用的KGE模型测试了Wikidata5M的难度, 分别比较了它们在Wikidata5M上的MRR, MR, HITS@1, 3, 10.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler6.jpg" style="zoom: 33%;" /><p>确实非常难, 很多流行的数据集都没取得太好的效果. 因为Wikidata包括了各种不同类型的实体和关系, 作者也建议大家使用<strong>大规模数据集</strong>以确保模型能够被充分测试. </p><h4 id="MLM-Objective"><a href="#MLM-Objective" class="headerlink" title="MLM Objective"></a>MLM Objective</h4><p>作者使用优化MLM的数据集有: </p><ul><li>BookCorpus. </li><li>English WIkipedia. </li></ul><h3 id="Pre-Training-Settings"><a href="#Pre-Training-Settings" class="headerlink" title="Pre - Training Settings"></a>Pre - Training Settings</h3><p>关于参数设置我就不再提了, 详见原文. 主要叙述一下在后面实验经常比较的内容. </p><h4 id="KE-Settings"><a href="#KE-Settings" class="headerlink" title="KE Settings"></a>KE Settings</h4><p>对于KE的优化, 作者设计了三种设置: </p><ol><li><strong>KEPLER - Wiki</strong>: 用<strong>Wikidata5M</strong>训练KEPLER, 总使用描述的前512个Token. 当使用实体和关系一起作为输入时(Using Entity and Relation Descriptions), 模型被称为KEPLER - Wiki - rel. </li><li><strong>KEPLER - WordNet</strong>: 用<strong>WordNet</strong>训练KEPLER, 作者尝试将更多的语言知识融入进去, 或许会有益于NLP任务. WordNet中的关系数量相对来说非常少, 所以只采用实体描述. </li><li><strong>KEPLER - W + W</strong>: 联合训练Wikidata5M和WordNet. 损失函数相应的发生变化:<br>$$<br>\mathcal{L}=\mathcal{L}_{\mathrm{Wiki}}+ \mathcal{L}_{\mathrm{WordNet}}+\mathcal{L}_{\mathrm{MLM}}<br>$$</li></ol><h4 id="RoBERTa"><a href="#RoBERTa" class="headerlink" title="RoBERTa"></a>RoBERTa</h4><p>KEPLER是基于RoBERTa的, RoBERTa采用了更大的语料库进行训练. 为公平起见, 作者训练了<strong>RoBERTa*</strong>, 也是用RoBERTa的权重进行初始化, 但用相同的语料库, 并只对它的MLM目标继续优化. </p><blockquote><p>这样设计实验应该是为了凸显出加入KE Loss带来的变化. </p></blockquote><h3 id="NLP-Tasks"><a href="#NLP-Tasks" class="headerlink" title="NLP Tasks"></a>NLP Tasks</h3><p>在NLP任务中, 作者主要和其他Knowledge Enhanced Model在NLP任务上<strong>横向对比</strong>. </p><h4 id="Relation-Classification"><a href="#Relation-Classification" class="headerlink" title="Relation Classification"></a>Relation Classification</h4><h5 id="TACRED"><a href="#TACRED" class="headerlink" title="TACRED"></a>TACRED</h5><p>TACRED是人为标注用于关系分类的数据集. 在TACRED上表现如下: </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler7.jpg" style="zoom: 25%;" /><p>KEPLER - Wiki表现很棒, 与RoBERTa*相比有很大进步, 但KEPLER - WordNet表现只比RoBERTa*好一点点. KEPLER - W + W和KEPLER - Wiki表现相当, 作者认为是WordNet限制了性能. </p><h4 id="FewRel"><a href="#FewRel" class="headerlink" title="FewRel"></a>FewRel</h4><p>FewRel是用于体现Few - Shot能力的关系分类数据集, 其2.0版本添加了更多的领域. 在FewRel上表现如下: </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler8.jpg" style="zoom: 33%;" /><p>就作者给出的结果来看, KEPLER - Wiki表现很不错. 作者认为在FewRel1.0和2.0上的差异是因为2.0版本加入了医疗类的数据. </p><h4 id="Entity-Typing"><a href="#Entity-Typing" class="headerlink" title="Entity Typing"></a>Entity Typing</h4><p>KEPLER在OpenEntity上的表现如下: </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler11.jpg" style="zoom: 20%;" /><p>从F1 Score上来看, KEPLER表现不错. </p><h4 id="GLUE"><a href="#GLUE" class="headerlink" title="GLUE"></a>GLUE</h4><p>GLUE(General Language Understanding Evaluation)是用来评判<strong>语言理解能力</strong>的测试, 这种任务不需要知识, 但需要<strong>理解能力</strong>. 作者通过GLUE尝试证明KEPLER对NLU任务表现没有退化. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler9.jpg" style="zoom: 33%;" /><p>在比较大的数据集上KEPLER表现相对正常, 在比较小的数据集上例如RTE, KEPLER退化的比较严重. 一般来说 , KEPLER对NLU没有明显副作用. </p><blockquote><p>即然RoBERTa*训练与KEPLER使用的是相同数据集, 那么按理说会因描述类的文本数据扩充而提升文本理解能力, 实际上却没有. 从这个角度来看, PTM和KE并不能给NLU任务带来增益, 甚至还会有<strong>减益</strong>. 应该还有更深层的原因. </p></blockquote><h3 id="KG-Tasks"><a href="#KG-Tasks" class="headerlink" title="KG Tasks"></a>KG Tasks</h3><p>作者在KG的Task上不能使用常用数据集, 因为它们都没有高质量的实体描述, 并且不支持Inductive Setting. </p><h4 id="Transductive-Setting"><a href="#Transductive-Setting" class="headerlink" title="Transductive Setting"></a>Transductive Setting</h4><p>在该设置中, 所有实体在所有阶段均是可见的, 但三元组不可见. 作者将其与TransE性能进行比较: </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler10.jpg" style="zoom: 33%;" /><blockquote><p>用TransE当Baseline是不是有点太不公平了？TransE没有使用任何额外辅助信息, 实体关系描述肯定算引入辅助信息了. 作者在文中列出三条理由, 但我不是很认同. </p></blockquote><h4 id="Inductive-Setting"><a href="#Inductive-Setting" class="headerlink" title="Inductive Setting"></a>Inductive Setting</h4><p>在该设置中, 所有实体和三元组在各阶段都不共享. 作者将也引入实体描述的DKRL(也是作者提出的模型)作为Baseline进行比较: </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler14.jpg" style="zoom: 33%;" /><p>KEPLER - Wiki - rel比KEPLER - Wiki要强大许多, 并且比DKRL提升巨大. </p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>KEPLER是一个<strong>多任务学习</strong>的模型, 作者希望探究对KEPLER性能提升的因素.<br>作者先将RoBERTa, RoBERTa*(仅使用MLM Loss), KEPLER - KE(仅使用KE Loss), KEPLER - Wiki在TACRED上进行测试: </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler12.jpg" style="zoom: 25%;" /><p>相比于RoBERTa, RoBERTa*和KEPLER - KE性能下降了, 作者说这证明了KE Loss和MLM Loss都是必不可缺的.</p><p>作者希望进一步量化的去看看KEPLER到底学到了多少知识, 在TACRED中在<strong>对实体进行Mask</strong>(ME, Masked Entity)和<strong>只保留实体</strong>(Only Entity)的情况下, 重新对关系进行分类, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kepler13.jpg" style="zoom: 25%;" /><p>KEPLER - Wiki学习到一些知识, 比用同等数据训练出来的RoBERTa*效果要更好一些.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>作者分别从PLM和KGE两个方面总结了KEPLER的优点:</p><ul><li>KEPLER作为PLM, 将常识集成进了语言表示, 并且具有强大的语言理解能力, 并且增强了抽取知识的能力, 能够直接适应很多NLP任务. </li><li>KEPLER作为KEM, 能够使用丰富的文本信息, 能在文本描述的引导下预测从没见过的实体.</li></ul><p>在我看来, KEPLER其实很一般. 而且它有一些很明显的<strong>缺陷</strong>, 它没有很好地将实体描述中的相关做进一步<strong>扩展</strong>, 例如它没有很好地利用KG作为<strong>图</strong>的优势. 而且它的上下文提取能力获取的实体和关系表示是<strong>静态</strong>的, 并不能根据上下文改变实体的表达. </p><p>除去KEPLER本身外, 作者贡献了一个大规模附带文本描述的数据集. </p><p>最后, 就注入知识是否有益于NLU这个问题来说, 答案还是不明确. 从直觉的角度来说, KEPLER本身就能从上下文中利用BERT的结构学到一些语法知识, 在注入知识的情况下应该进一步提升NLU能力, 而现在很多工作实验结果则不然.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> BERT </tag>
            
            <tag> KGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KGE预警论文两则</title>
      <link href="/posts/19912.html"/>
      <url>/posts/19912.html</url>
      
        <content type="html"><![CDATA[<p>本文是两篇KGE方向的预警论文的阅读笔记和个人理解. 预警类的工作其实是比较少见的, 对领域的发展也非常有指导意义.</p><blockquote><p><strong>2020.11.22</strong>: 更新Reciprocal Relation.</p><p><strong>2021.05.13</strong>: 修正Reciprocal Relation描述.</p></blockquote><h2 id="A-Re-evaluation-of-Knowledge-Graph-Completion-Methods"><a href="#A-Re-evaluation-of-Knowledge-Graph-Completion-Methods" class="headerlink" title="A Re - evaluation of Knowledge Graph Completion Methods"></a>A Re - evaluation of Knowledge Graph Completion Methods</h2><p>首先来看第一篇论文<a href="http://arxiv.org/abs/1911.03903" target="_blank" rel="noopener">A Re-evaluation of Knowledge Graph Completion Methods</a>.</p><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>这篇论文是在深度学习参与时代背景下提出的. 有相当多的深度学习方法被当做<strong>黑箱</strong>使用在Knowledge Embedding中. 例如普通的CNN, RNN, GNN, 到现在的Attention, 甚至是胶囊网络也有被用于KGE的研究. </p><p>作者敏锐的观察到, 虽然基于DL的方法有非常明显的提升, 但有些方法呈现出在<strong>不同数据集</strong>上的<strong>不一致性</strong>. 作者基于这一个问题, 深度剖析了<strong>基于卷积神经网络</strong>的几种方法呈现错误实验结果的原因.</p><h3 id="Observations"><a href="#Observations" class="headerlink" title="Observations"></a>Observations</h3><h4 id="Inconsistent-Improvements-on-Different-DataSet"><a href="#Inconsistent-Improvements-on-Different-DataSet" class="headerlink" title="Inconsistent Improvements on Different DataSet"></a>Inconsistent Improvements on Different DataSet</h4><p>基于DL的方法在FB15k - 237上的MRR表现十分良好, 但到WN18RR上就出现了问题:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120154015009.png" style="zoom:33%;" /><p>仔细看实验结果, 传统的建模方法例如RotatE, TuckER在两个数据集上相比ConvE是都有提升的, 只是提升的幅度不同. 而有些基于DL的方法在WN18RR上居然出现了<strong>退化</strong>的现象, 并且ConvKB的退化居然这么明显. 即使WN18RR是比较<strong>困难</strong>的数据集, 也不应该呈现<strong>大幅度</strong>的表现不一致.</p><h4 id="Score-Functions"><a href="#Score-Functions" class="headerlink" title="Score Functions"></a>Score Functions</h4><p>作者在FB15k - 237上做了个测试, 作者发现很多正确三元组和负采样得来的三元组居然具有<strong>相同Score</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120154042981.png" style="zoom:33%;" /><p>这意味着<strong>无论输入是什么</strong>, <strong>输出始终恒定</strong>(这不是离谱吗), 作者进一步在三种基于CNN的方法上做了对比:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120153725829.png" style="zoom: 33%;" /><p>从图中可以看到, 同是基于CNN的KGE方法, 刚才呈现退化的ConvKB和CapsE出现异常的次数非常多, 而一致提升的ConvE异常次数却非常少.</p><h3 id="Root-of-the-Problem"><a href="#Root-of-the-Problem" class="headerlink" title="Root of the Problem"></a>Root of the Problem</h3><p>经过作者的深入研究后, 发现居然是<strong>ReLU</strong>的锅:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120153853322.png" style="zoom: 33%;" /><p>可能模型输出的结果, 但最后通过ReLU激活的时候, 这些负数全都被<strong>过滤成零</strong>了, 这也就导致了许多三元组的得分完全一致. 该实验曲线与上一个实验趋势是保持一致的.</p><blockquote><p>其实这个现象被称为<strong>Dead Neuron</strong>, 因为ReLU将数据滤掉后, 会导致在BP时一整条链上的导数全为0, 也就不再更新梯度, 神经元的权重将得不到更新. 我想出的解决方案是使用Leaky ReLU等在负值域上有<strong>斜率</strong>的函数, 但同时激活函数作为NN中很重要的部分, 在更换后可能会对模型产生不可知的影响.</p></blockquote><h3 id="New-Evaluation-Protocol"><a href="#New-Evaluation-Protocol" class="headerlink" title="New Evaluation Protocol"></a>New Evaluation Protocol</h3><p>因为很多三元组的Score都相同, 导致最后在排名进行挑选时会出现<strong>不公平</strong>的现象. 假设正确的三元组是在相同得分的候选三元组中<strong>稳定分布</strong>的, 作者提出了三种选择策略:</p><ul><li>TOP: 将正确的三元组插入待预测插入到分数相同的候选三元组<strong>前</strong>.</li><li>BOTTOM: 将正确的三元组插入待预测插入到分数相同的候选三元组<strong>后</strong>.</li><li>RANDOM: 将正确的三元组<strong>随机</strong>插入待预测插入到分数相同的候选三元组中.</li></ul><blockquote><ol><li>作者认为这样做有效的原因是, 在Link Prediction中, 我们先计算所有负采样三元组的得分, 并将其排列, 最后再计算正确三元组得分, 将其放到合适的位置. 在选择三元组时, 选择排行<strong>最靠前</strong>的三元组. 如果调整了正确三元组的插入位置, 就能从一定程度上解决CNN模型性能虚高的问题.</li><li>在论文中这部分的观点我不是很认同, 我更倾向于是<strong>模型本身</strong>的问题而非<strong>评估协议</strong>的问题. 当然在Evaluation Protocol上动手也可以矫正实验结果, 但不能从根本上解决问题.</li></ol></blockquote><p>在FB15k -237上实验结果如下(作者也在WN18RR上做了实验, 结果一致):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120153927434.png" style="zoom:33%;" /><p>作者提到, 在原来的论文中, ConvE, RotatE, TuckER使用的协议是类似于RANDOM的方法, 而ConvKB, CapsE, KBAT使用的是TOP.</p><p>通过观察, 基于TOP的结果都显示出<strong>虚高</strong>的性能, 而且刚才问题最严重的的CapsE性能虚高最为明显. 基于BOTTOM的结果都<strong>虚低</strong>. 而RANDOM的结果相对来说要<strong>公平</strong>一些.</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>经过研究发现基于DL的KGE方法仍然存在一些问题, 在某些数据集上呈现出性能虚高的问题, 这些方法都具有<strong>误导性</strong>, 作者鼓励用文中的方法进行评估, 相对来说更加<strong>公平</strong>. 质疑类的论文价值很高, 但我认为<strong>作者提出的问题并没有从根本上解决</strong>.</p><h2 id="YOU-CAN-TEACH-AN-OLD-DOG-NEW-TRICKS-ON-TRAINING-KNOWLEDGE-GRAPH-EMBEDDINGS"><a href="#YOU-CAN-TEACH-AN-OLD-DOG-NEW-TRICKS-ON-TRAINING-KNOWLEDGE-GRAPH-EMBEDDINGS" class="headerlink" title="YOU CAN TEACH AN OLD DOG NEW TRICKS! ON TRAINING KNOWLEDGE GRAPH EMBEDDINGS"></a>YOU CAN TEACH AN OLD DOG NEW TRICKS! ON TRAINING KNOWLEDGE GRAPH EMBEDDINGS</h2><p>然后再来看第二篇论文<a href="https://openreview.net/forum?id=BkxSmlBFvr" target="_blank" rel="noopener">YOU CAN TEACH AN OLD DOG NEW TRICKS! ON TRAINING KNOWLEDGE GRAPH EMBEDDINGS</a>.</p><h3 id="Background-1"><a href="#Background-1" class="headerlink" title="Background"></a>Background</h3><p>随着KGE受到大家的重视, 越来越多的方法涌现出来, 并且在实验中SOTA. 但实际上大家击败的Baseline可能并不是该模型发挥的真正水平, 因为模型的性能是一定会与<strong>超参选择</strong>和<strong>训练</strong>有关. 许多研究人员将优秀的模型参数设置<strong>移植</strong>过来, 事实上该参数设置可能并不能在自己的模型上达到良好的效果. 除此外, 模型之间采用不同的方法导致<strong>很难横向对比</strong>也是一个很大的问题. </p><p>因此, 作者希望能够采用更大的超参搜索范围和更多种训练技巧, 来对这些模型的性能<strong>量化和总结</strong>.</p><p>以下是在论文中将要比较的模型, 粗体表示首次出现:</p><p>作者将对以下模型进行汇总, 其中粗体代表该方法首次在各类模型中使用:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120165535529.png" style="zoom:33%;" /><p>作者的研究只关注纯KGE模型, 不包括引入辅助信息的KGE模型.</p><h3 id="Models-Traning-Evaluation"><a href="#Models-Traning-Evaluation" class="headerlink" title="Models, Traning, Evaluation"></a>Models, Traning, Evaluation</h3><p>该部分中, 对于常见的领域类问题被我跳过了, 即论文中的前四点:</p><ul><li>Multi - relational link prediction.</li><li>Knowledge graph embeddings (KGE).</li><li>Evaluation.</li><li>KGE models.</li></ul><h4 id="Training-Type"><a href="#Training-Type" class="headerlink" title="Training Type"></a>Training Type</h4><p>现在常用的有三种训练类型, 分别是:</p><ul><li><strong>负采样</strong>: 将正样本中的任一元素随机替换作为负样本(有些模型中只替换头实体和尾实体).</li><li><strong>1 vs ALL</strong>: 打乱头和尾实体的位置, 由单个三元组生成全部负例(略存疑).</li><li><strong>K vs ALL</strong>: 批量构建头实体或尾实体非空的一个Batch, 如果在训练集中出现则为正例, 否则为负例. 这种方法在ConvE中首次出现, 在其中被作者称为1 - N Score.</li></ul><h4 id="Loss-Functions"><a href="#Loss-Functions" class="headerlink" title="Loss Functions"></a>Loss Functions</h4><p>一般也只有下列四种Loss:</p><ul><li>MSE(Mean Square Error).</li><li>MR(Margin Ranking), 也称为Hinge Loss.</li><li>BCE(Binary Cross Entropy).</li><li>CE(Cross Entropy).</li></ul><h4 id="Reciprocal-Relations"><a href="#Reciprocal-Relations" class="headerlink" title="Reciprocal Relations"></a>Reciprocal Relations</h4><p>对于Link Prediction任务来说, 对于同一关系, 分别预测头实体和尾实体的<strong>打分函数</strong>应该是不同的. </p><p>但实际上我们不用规定不同的头尾实体打分函数, 我们可以给同种关系以<strong>顺逆区分</strong>, 即每种关系使用两种不同的Embedding, 将所有问题都转化为预测尾实体, 然后打分函数共享相同的实体Embedding, 这样也能最大限度的节省计算开销.</p><p>该方法现在已经广泛的应用于各类KGE方法中, 可能会带来性能提升.</p><h4 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h4><p>现在一般使用的是L2正则化, 个别研究人员推荐使用L3正则. TransE使用归一化. 关于DL的模型可以使用Dropout, 例如ConvE. 在作者的研究中, 还考虑了L1正则.</p><h4 id="Hyperparameters"><a href="#Hyperparameters" class="headerlink" title="Hyperparameters"></a>Hyperparameters</h4><p>现在要考虑的超参有Batch Size, Learning Rate, 负采样个数, 实体和关系的正则权重等.</p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><h4 id="Experiments-Setup"><a href="#Experiments-Setup" class="headerlink" title="Experiments Setup"></a>Experiments Setup</h4><p>实验的设置非常琐碎, 但这是作者的主要工作.</p><ul><li><strong>Dataset</strong>: 现在最常用的两个数据集FB15k - 237和WN18RR.</li><li><strong>Models</strong>: 选用RESCAL, TransE, DistMult, ComplEx, ConvE.</li><li><strong>Evaluation</strong>: MRR和HITS@10.</li><li><strong>Hyperparameters</strong>: 考虑前面提到的三种Training Type(负采样, 1 vs ALL, K vs ALL), 提到的正则(None, L1, L2, L3, Dropout), 优化器(Adam, Adagrad), Embedding Size(128, 256, 512), 并分别对实体和关系建立权重用于Dropout和正则. 这是作者已知的最大范围的超参搜索空间.</li><li><strong>Training</strong>: 最大400Epochs. 每5个Epoch算一次MRR, 并且在有50个Epoch中模型MRR没有5%以上的提升, 则触发早停.</li><li><strong>Model Selection</strong>: 作者通过一个框架, 对于每个模型和数据集<strong>随机生成</strong>(即<strong>随机超参搜索</strong>)了30中不同的配置, 每种配置都包含不同的Training Type和Loss Function. 在随机搜索后用<strong>贝叶斯优化</strong>对参数进一步调优.</li></ul><p>作者给出了一张汇总表(摘自附录):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120193313355.png" style="zoom: 50%;" /><blockquote><p>Table 6 是随机搜索在FB15k - 237上的详细最优配置图, 是同级标题下的Impact of Hyperparameters -&gt; Best configurations (quasi-random search)中的第二幅图.</p></blockquote><h4 id="Comparison-of-Model-Performance"><a href="#Comparison-of-Model-Performance" class="headerlink" title="Comparison of Model Performance"></a>Comparison of Model Performance</h4><p>作者将搜索过后的模型(Ours)性能与之前发布的最初版模型(First), 以及最近的模型(Recent)和更大号的模型(Large)进行了比较:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120183435443.png" style="zoom:33%;" /><p>根据实验结果, 得到以下结论:</p><ol><li>经过重新超参搜索后的模型相比于原作者首次发布有了<strong>巨大提升</strong>.</li><li>在作者重新训练的这些模型之间, 性能差距逐渐<strong>缩小</strong>, 甚至发生<strong>逆转</strong>.</li><li>有些模型例如RESCAL, 相比于最近<strong>新发布</strong>的模型也<strong>没差多少</strong>.</li></ol><h4 id="Impact-of-Hyperparameters"><a href="#Impact-of-Hyperparameters" class="headerlink" title="Impact of Hyperparameters"></a>Impact of Hyperparameters</h4><h5 id="Anatomy-of-Search-Space"><a href="#Anatomy-of-Search-Space" class="headerlink" title="Anatomy of Search Space"></a>Anatomy of Search Space</h5><p>作者将搜索空间所有的模型的所有结果做了一张<strong>箱型图</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120183501722.png" style="zoom:33%;" /><p>从箱型图得到以下结论:</p><ol><li>不同类型的超参和训练方式给模型带来的影响巨大, 大多的模型性能<strong>均值</strong>和<strong>上限</strong>基本一致, 下限差的有点多. </li><li>TransE的结果不太好, 这是因为其他模型都有大概200种配置, 而TransE只有60种.</li><li>各个模型的表现在FB15k - 237上似乎要稳定一些, WN18RR上最优和最差差距很大.</li></ol><h5 id="Best-configurations-quasi-random-search"><a href="#Best-configurations-quasi-random-search" class="headerlink" title="Best configurations (quasi-random search)"></a>Best configurations (quasi-random search)</h5><p>作者给出了在随机搜索后得到最佳结果的配置(简略), 括号内是不使用该参数导致MRR减少的值:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120183522075.png" style="zoom:33%;" /><p>从表中发现不了什么规律, 每个模型所使用的最佳配置基本不同. 但<strong>随机搜索时CE性能似乎比较好</strong>.</p><p>各模型经过随机搜索在<strong>FB15k - 237</strong>上的详细最优配置(详细):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120193607208.png" style="zoom: 50%;" /><p>各模型经过随机搜索在<strong>WN18RR</strong>上的最优配置(详细):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120193632189.png" style="zoom:50%;" /><h5 id="Best-configurations-Bayesian-optimization"><a href="#Best-configurations-Bayesian-optimization" class="headerlink" title="Best configurations (Bayesian optimization)"></a>Best configurations (Bayesian optimization)</h5><p>各模型经过贝叶斯优化在各数据集上的最优配置:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120200139031.png" style="zoom: 33%;" /><p>贝叶斯优化后的效果比随机搜索的结果又稍稍<strong>提升</strong>了一点.</p><h5 id="Impact-of-Training-Type-and-Loss-Functions"><a href="#Impact-of-Training-Type-and-Loss-Functions" class="headerlink" title="Impact of Training Type and Loss Functions"></a>Impact of Training Type and Loss Functions</h5><p>作者在随机搜索上给出了不同训练技巧和不同损失函数的箱型图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201120183542089.png" style="zoom: 50%;" /><p>除了能体现出不同训练技巧和不同损失函数对不同模型的影响之外, 再次说明<strong>数据集</strong>也是一个非常重要的原因.</p><p>在图中还能看出来咖啡色组(<strong>1 vs ALL + CE</strong>)和粉色组(<strong>K vs ALL + CE</strong>)的上限非常高, 平均水平也不错. 这说明<strong>CE</strong>可能比其他损失函数要稍好些.</p><h3 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h3><p>这篇论文显示了参数配置在KGE模型上的重要性. 作者用了非常大的超参数搜索空间, 来说明有些模型的性能不一定像它在初始论文中看起来的表现一样. 在作者的更大范围的超参搜索和修改训练方式后, 模型之间的差距<strong>逐渐缩小</strong>, 甚至<strong>结果反转</strong>(有点NFL的意思).</p><p>本论文还有更多的结果在附录中, 主要是对更细粒度的组合做的箱型图, 其实都说明不了什么规律性的问题, 本来就没有规律可言. 给我们最大的启发就是KGE有些模型的性能需要<strong>重新审视</strong>, 对于不同的参数可能会出现截然不同的性能, 作者也鼓励使用<strong>更大参数搜索范围</strong>尽可能的将模型真实的性能展现出来, 方便大家做比较.</p><p>最后, 对作者的<strong>钻研精神</strong>和<strong>科研毅力</strong>致敬.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch实现: Skip-Gram</title>
      <link href="/posts/60645.html"/>
      <url>/posts/60645.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>Pytorch基本操作</li><li>Word2Vec</li></ul></blockquote><h1 id="Pytorch实现-Skip-Gram"><a href="#Pytorch实现-Skip-Gram" class="headerlink" title="Pytorch实现: Skip-Gram"></a>Pytorch实现: Skip-Gram</h1><p>本文用<strong>Pytorch</strong>实现了Skip - Gram, 它是Word2Vec的其中一种. 本文实现参考<a href="https://wmathor.com/index.php/archives/1435/" target="_blank" rel="noopener">PyTorch 实现 Word2Vec</a>, 如果理解上有困难, 另外推荐该博主更简单的实现版本<a href="https://wmathor.com/index.php/archives/1443/" target="_blank" rel="noopener">Word2Vec 的 PyTorch 实现(乞丐版)</a>, 以及其Word2Vec讲解<a href="https://wmathor.com/index.php/archives/1430/" target="_blank" rel="noopener">Word2Vec</a>.</p><p>本文的代码已经放到了Colab上, 打开设置GPU就可以复现(需要科学上网). </p><p><a href="https://colab.research.google.com/drive/1q2ne4eKD2ZZr7doxU1vyDUtfYI3WJVld?usp=sharing" target="_blank" rel="noopener"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="右键我在COLAB中打开!"></a></p><p>如果你不能科学上网, 应该看不到<code>Open in Colab</code>的图标.</p><h2 id="Preparing"><a href="#Preparing" class="headerlink" title="Preparing"></a>Preparing</h2><p>首先我们先导包:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils <span class="token keyword">import</span> data <span class="token keyword">as</span> tud<span class="token keyword">from</span> torch <span class="token keyword">import</span> optim<span class="token keyword">from</span> collections <span class="token keyword">import</span> Counter<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>Counter</code>会应用于等会为字典计数.</p><p>设置GPU:</p><pre class="line-numbers language-python"><code class="language-python">device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">'cpu'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Current Device:"</span><span class="token punctuation">,</span> device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>如果在GPU可用的情况下, <code>device = &#39;cuda&#39;</code>, 否则<code>device = cpu</code>.</p><p>定义其他参数:</p><pre class="line-numbers language-python"><code class="language-python">MAX_VOCAB <span class="token operator">=</span> <span class="token number">10000</span>window <span class="token operator">=</span> <span class="token number">3</span>negative_sample <span class="token operator">=</span> <span class="token number">15</span>hidden <span class="token operator">=</span> <span class="token number">128</span>batch_size <span class="token operator">=</span> <span class="token number">256</span>epochs <span class="token operator">=</span> <span class="token number">2</span>lr <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token comment" spellcheck="true"># set random seed to ensure result is reproducible</span><span class="token keyword">def</span> <span class="token function">set_random</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">import</span> random  np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">1116</span><span class="token punctuation">)</span>  torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">1116</span><span class="token punctuation">)</span>  random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">1116</span><span class="token punctuation">)</span>set_random<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>MAX_VOCAB</code> 是最大词表中的单词. <code>window</code>指的是除去中心词后, 窗口中<strong>单侧</strong>的词数. <code>negative_sample</code>指的是对于每个窗口中除去中心词的其他词, 进行多少次负采样, 即总共负采样<code>window * 2 * negative_sample</code>个单词.</p><h2 id="Getting-Information-from-Text"><a href="#Getting-Information-from-Text" class="headerlink" title="Getting Information from Text"></a>Getting Information from Text</h2><p>导入文件, 并初始化词表, 以及后续需要用到的参数.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">with</span> open <span class="token punctuation">(</span><span class="token string">'./drive/My Drive/Colab Notebooks/text8.train.txt'</span><span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>  text <span class="token operator">=</span> f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>text <span class="token operator">=</span> text<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># We can only use MAX_VOCAB - 1 words we use &lt;UNK> as a word.</span>vocab <span class="token operator">=</span> dict<span class="token punctuation">(</span>Counter<span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">.</span>most_common<span class="token punctuation">(</span>MAX_VOCAB <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># the count of &lt;UNK> is text length - other words' count</span>vocab<span class="token punctuation">[</span><span class="token string">'&lt;UNK>'</span><span class="token punctuation">]</span> <span class="token operator">=</span> len<span class="token punctuation">(</span>text<span class="token punctuation">)</span> <span class="token operator">-</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>list<span class="token punctuation">(</span>vocab<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># save the mapping pair of word to index</span>word2idx <span class="token operator">=</span> <span class="token punctuation">{</span>word<span class="token punctuation">:</span> i <span class="token keyword">for</span> i<span class="token punctuation">,</span> word <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>vocab<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">}</span>idx2word <span class="token operator">=</span> <span class="token punctuation">{</span>i<span class="token punctuation">:</span> word <span class="token keyword">for</span> i<span class="token punctuation">,</span> word <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>vocab<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">}</span>word_count <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>count <span class="token keyword">for</span> count <span class="token keyword">in</span> vocab<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>word_freqs <span class="token operator">=</span> word_count <span class="token operator">/</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>word_count<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># refer to original paper</span>word_freqs <span class="token operator">=</span> word_freqs <span class="token operator">**</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token operator">/</span><span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>数据集下载地址: 链接: <a href="https://pan.baidu.com/s/1j52-cQiIvHpbTGW312f4aw" target="_blank" rel="noopener">https://pan.baidu.com/s/1j52-cQiIvHpbTGW312f4aw</a> 提取码: af3p</p></blockquote><h2 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h2><p>创建一个专门给Embedding用的数据集:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">EmbeddingDataset</span><span class="token punctuation">(</span>tud<span class="token punctuation">.</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">,</span> word2idx<span class="token punctuation">,</span> word_freqs<span class="token punctuation">)</span><span class="token punctuation">:</span>    super<span class="token punctuation">(</span>EmbeddingDataset<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>text_encoded <span class="token operator">=</span> <span class="token punctuation">[</span>word2idx<span class="token punctuation">.</span>get<span class="token punctuation">(</span>word<span class="token punctuation">,</span> word2idx<span class="token punctuation">[</span><span class="token string">'&lt;UNK>'</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> word <span class="token keyword">in</span> text<span class="token punctuation">]</span>    self<span class="token punctuation">.</span>text_encoded <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>self<span class="token punctuation">.</span>text_encoded<span class="token punctuation">)</span>    self<span class="token punctuation">.</span>word2idx <span class="token operator">=</span> word2idx    self<span class="token punctuation">.</span>word_freqs <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>word_freqs<span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>text_encoded<span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>    center_word <span class="token operator">=</span> self<span class="token punctuation">.</span>text_encoded<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># get words in window exception center word</span>    pos_idx <span class="token operator">=</span> <span class="token punctuation">[</span>i <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>idx <span class="token operator">-</span> window<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span>i <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>idx<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> idx <span class="token operator">+</span> window <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>    pos_idx <span class="token operator">=</span> <span class="token punctuation">[</span>i <span class="token operator">%</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>text_encoded<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> pos_idx<span class="token punctuation">]</span>    pos_words <span class="token operator">=</span> self<span class="token punctuation">.</span>text_encoded<span class="token punctuation">[</span>pos_idx<span class="token punctuation">]</span>    neg_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>self<span class="token punctuation">.</span>word_freqs<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    neg_mask<span class="token punctuation">[</span>pos_words<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>    neg_words <span class="token operator">=</span> torch<span class="token punctuation">.</span>multinomial<span class="token punctuation">(</span>neg_mask<span class="token punctuation">,</span> negative_sample <span class="token operator">*</span> pos_words<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># check if negative sample failure exists</span>    <span class="token keyword">if</span> len<span class="token punctuation">(</span>set<span class="token punctuation">(</span>pos_words<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span> set<span class="token punctuation">(</span>neg_words<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Need to resample.'</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> center_word<span class="token punctuation">,</span> pos_words<span class="token punctuation">,</span> neg_words<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我们需要自行实现的函数包括<code>__init__</code>, <code>__len__</code>, <code>__getitem__</code>. </p><p>这个数据集创建好后, 等会就可以用<code>torch.utils.data</code>中的<code>Dataloader</code>进行加载了.</p><blockquote><p>注意, 如果设定的负采样数比较大, 千万不要采用如下代码:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">while</span> len<span class="token punctuation">(</span>set<span class="token punctuation">(</span>pos_words<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span> set<span class="token punctuation">(</span>neg_words<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span> neg_words <span class="token operator">=</span> torch<span class="token punctuation">.</span>multinomial<span class="token punctuation">(</span>self<span class="token punctuation">.</span>word_freqs<span class="token punctuation">,</span> negative_sample <span class="token operator">*</span> pos_words<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Negative sample false"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>这样会导致负采样次数大量增加. 我开始就纳闷为什么训练速度这么慢, 后来发现是采样会重复很多次.</p><p>因为对于我们的训练来说, 负采样时根据单词出现的概率采样, <strong>有非常大概率采样到窗口中已经出现的词</strong>, 这样在计算Loss时会出现问题. 正确的做法应该是像我写的一样, 将单词的概率做一份<strong>拷贝</strong>, 然后将窗口词和中心词在拷贝中的概率<strong>全部置零</strong>, 然后再采样, 这样即使是使用<code>torch.multionmial</code>, 也不会采样到出现在窗口内的词.</p></blockquote><h2 id="Skip-Gram"><a href="#Skip-Gram" class="headerlink" title="Skip - Gram"></a>Skip - Gram</h2><p>然后定义Word2Vec的模型, 直接把损失函数放到里面了:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Word2Vec</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> hidden<span class="token punctuation">)</span><span class="token punctuation">:</span>    super<span class="token punctuation">(</span>Word2Vec<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>vocab_size <span class="token operator">=</span> vocab_size    self<span class="token punctuation">.</span>hidden <span class="token operator">=</span> hidden    <span class="token comment" spellcheck="true"># we use two embedding between input word and other words in window</span>    self<span class="token punctuation">.</span>in_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>self<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden<span class="token punctuation">)</span>    self<span class="token punctuation">.</span>out_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>self<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden<span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_labels<span class="token punctuation">,</span> pos_labels<span class="token punctuation">,</span> neg_labels<span class="token punctuation">)</span><span class="token punctuation">:</span>    input_embedding <span class="token operator">=</span> self<span class="token punctuation">.</span>in_embedding<span class="token punctuation">(</span>input_labels<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, hidden]</span>    pos_embedding <span class="token operator">=</span> self<span class="token punctuation">.</span>out_embedding<span class="token punctuation">(</span>pos_labels<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, window * 2, hidden]</span>    neg_embedding <span class="token operator">=</span> self<span class="token punctuation">.</span>out_embedding<span class="token punctuation">(</span>neg_labels<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, window * 2 * k, hidden]</span>    input_embedding <span class="token operator">=</span> input_embedding<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, hidden, 1] must be the same dimension when use torch.bmm</span>    pos_dot <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>pos_embedding<span class="token punctuation">,</span> input_embedding<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, window * 2, 1]</span>    neg_dot <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>neg_embedding<span class="token punctuation">,</span> <span class="token operator">-</span>input_embedding<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, window * 2 * k, 1]</span>    pos_dot <span class="token operator">=</span> pos_dot<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, window * 2]</span>    neg_dot <span class="token operator">=</span> neg_dot<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch, window * 2 * k]</span>    pos_loss <span class="token operator">=</span> F<span class="token punctuation">.</span>logsigmoid<span class="token punctuation">(</span>pos_dot<span class="token punctuation">)</span><span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    neg_loss <span class="token operator">=</span> F<span class="token punctuation">.</span>logsigmoid<span class="token punctuation">(</span>neg_dot<span class="token punctuation">)</span><span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    loss <span class="token operator">=</span> neg_loss <span class="token operator">+</span> pos_loss    <span class="token keyword">return</span> <span class="token operator">-</span>loss  <span class="token keyword">def</span> <span class="token function">get_input_embedding</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># get weights to build an application for evaluation</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>in_embedding<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在对Tensor进行操作时, 一定要<strong>时刻追踪Tensor维度的变换意义</strong>.</p><p>在我参考的博客中, 直接将Loss写到模型中了, 虽然结果都一样, 但我个人不建议这样做, 如果有一份可以复用的代码模板, 还是需要进行一些改动.</p><h2 id="Training-and-Save"><a href="#Training-and-Save" class="headerlink" title="Training and Save"></a>Training and Save</h2><p>对我们前面定义的类进行<strong>实例化</strong>, 同时定义优化器:</p><pre class="line-numbers language-python"><code class="language-python">dataset <span class="token operator">=</span> EmbeddingDataset<span class="token punctuation">(</span>text<span class="token punctuation">,</span> word2idx<span class="token operator">=</span>word2idx<span class="token punctuation">,</span> word_freqs<span class="token operator">=</span>word_freqs<span class="token punctuation">)</span>dataloader <span class="token operator">=</span> tud<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span>word2vec <span class="token operator">=</span> Word2Vec<span class="token punctuation">(</span>MAX_VOCAB<span class="token punctuation">,</span> hidden<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>word2vec<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Step in one epoch:{}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>len<span class="token punctuation">(</span>dataloader<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>训练时可以用<strong>tqdm</strong>来对剩余时间进行评估:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> time <span class="token keyword">import</span> time<span class="token keyword">from</span> tqdm<span class="token punctuation">.</span>notebook <span class="token keyword">import</span> tqdmstart <span class="token operator">=</span> time<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">for</span> step<span class="token punctuation">,</span> <span class="token punctuation">(</span>input_label<span class="token punctuation">,</span> pos_label<span class="token punctuation">,</span> neg_label<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>tqdm<span class="token punctuation">(</span>dataloader<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    input_label <span class="token operator">=</span> input_label<span class="token punctuation">.</span>long<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    pos_label <span class="token operator">=</span> pos_label<span class="token punctuation">.</span>long<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    neg_label <span class="token operator">=</span> neg_label<span class="token punctuation">.</span>long<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 3 step in torch</span>    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>    loss <span class="token operator">=</span> word2vec<span class="token punctuation">(</span>input_label<span class="token punctuation">,</span> pos_label<span class="token punctuation">,</span> neg_label<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> step <span class="token operator">%</span> <span class="token number">1000</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">and</span> step <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>        end <span class="token operator">=</span> time<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"epoch:{}, step:{}, loss:{}, in time:{:.2f}s"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>epoch<span class="token punctuation">,</span> step<span class="token punctuation">,</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> end <span class="token operator">-</span> start<span class="token punctuation">)</span><span class="token punctuation">)</span>        start <span class="token operator">=</span> time<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我最终的训练Loss是在18 ~ 19左右波动. 训练总时长在COLAB一小时左右.</p><p>保存一下模型:</p><pre class="line-numbers language-python"><code class="language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>word2vec<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'./drive/My Drive/embedding-{}.th'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>hidden<span class="token punctuation">)</span><span class="token punctuation">)</span>embedding_weights <span class="token operator">=</span> word2vec<span class="token punctuation">.</span>get_input_embedding<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>注意, 如果对模型进行保存, 也同时要保存<code>word2idx</code>. 因为Word2Vec本质上是<strong>查表</strong>, 除了存储模型权重, 还需要存储单词到表(权重)索引的映射关系<code>word2idx</code>.</p><p>这里的<code>embbeding_weights</code>从GPU上拿下来, 等会做一个小检测, 看看我们训练的Word2Vec效果怎么样.</p><h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><p>选取与其相似度最高的十个词来检测一下Word2Vec的训练效果:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> scipy<span class="token punctuation">.</span>spatial<span class="token punctuation">.</span>distance <span class="token keyword">import</span> cosine<span class="token keyword">def</span> <span class="token function">find_nearest</span><span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">:</span>    index <span class="token operator">=</span> word2idx<span class="token punctuation">[</span>word<span class="token punctuation">]</span>    embedding <span class="token operator">=</span> embedding_weights<span class="token punctuation">[</span>index<span class="token punctuation">]</span>    cos_dis <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>cosine<span class="token punctuation">(</span>e<span class="token punctuation">,</span> embedding<span class="token punctuation">)</span> <span class="token keyword">for</span> e <span class="token keyword">in</span> embedding_weights<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> <span class="token punctuation">[</span>idx2word<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> cos_dis<span class="token punctuation">.</span>argsort<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token keyword">for</span> ie_words <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'two'</span><span class="token punctuation">,</span> <span class="token string">'man'</span><span class="token punctuation">,</span> <span class="token string">'computers'</span><span class="token punctuation">,</span> <span class="token string">'machine'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'word:{} is similar to {}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>ie_words<span class="token punctuation">,</span> find_nearest<span class="token punctuation">(</span>ie_words<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>控制台输出:</p><pre><code>word:two is similar to [&#39;two&#39;, &#39;three&#39;, &#39;four&#39;, &#39;five&#39;, &#39;zero&#39;, &#39;six&#39;, &#39;seven&#39;, &#39;one&#39;, &#39;eight&#39;, &#39;nine&#39;]word:man is similar to [&#39;man&#39;, &#39;woman&#39;, &#39;young&#39;, &#39;god&#39;, &#39;men&#39;, &#39;person&#39;, &#39;girl&#39;, &#39;soul&#39;, &#39;goddess&#39;, &#39;son&#39;]word:computers is similar to [&#39;computers&#39;, &#39;computer&#39;, &#39;devices&#39;, &#39;hardware&#39;, &#39;machines&#39;, &#39;applications&#39;, &#39;systems&#39;, &#39;components&#39;, &#39;electronic&#39;, &#39;computing&#39;]word:machine is similar to [&#39;machine&#39;, &#39;machines&#39;, &#39;device&#39;, &#39;program&#39;, &#39;memory&#39;, &#39;computer&#39;, &#39;engine&#39;, &#39;ibm&#39;, &#39;computers&#39;, &#39;programming&#39;]</code></pre><p>效果其实还不错, 基本上基于平移的规则, 我们给定一个词, 都能找到与其表面语义近似的词.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Word2Vec </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
      <link href="/posts/24649.html"/>
      <url>/posts/24649.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>BERT(详见<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>)</li></ul></blockquote><h1 id="RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach"><a href="#RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach" class="headerlink" title="RoBERTa: A Robustly Optimized BERT Pretraining Approach"></a>RoBERTa: A Robustly Optimized BERT Pretraining Approach</h1><p>本文是论文<a href="https://arxiv.org/abs/1907.11692" target="_blank" rel="noopener">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a>的阅读笔记和个人理解. RoBERTa已经被广泛的应用于各类由BERT衍生的模型参数初始化, 可以视为是完全体形态的BERT.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者在文章中指出, 训练是一个非常重要的过程, 但BERT在发布时并没有得到很好的训练, 导致其性能看起来比现在的<strong>自回归语言</strong>模型性能要略差(例如XLNet). 但实际上, 对BERT应用一些<strong>训练技巧</strong>对提升BERT性能影响是非常大的. 因此, 作者重新对BERT施加了一些训练技巧, 使得BERT的<strong>性能</strong>得到了进一步提升, 并且具有更强的<strong>鲁棒性</strong>.</p><h2 id="RoBERTa"><a href="#RoBERTa" class="headerlink" title="RoBERTa"></a>RoBERTa</h2><p>RoBERTa(<strong>R</strong>obustly <strong>o</strong>ptimized <strong>BERT</strong> <strong>a</strong>pproach). RoBERTa只是应用了更好的训练技巧, 因此整体结构是没有发生任何变化的. 如果对BERT的结构不熟悉, 建议回顾BERT的知识.</p><h3 id="Dynamic-Masking"><a href="#Dynamic-Masking" class="headerlink" title="Dynamic Masking"></a>Dynamic Masking</h3><p>作者总结出三种Mask的方法:</p><ul><li><strong>纯静态Mask</strong>: 就是BERT中使用的Mask, 在<strong>数据预处理</strong>阶段就进行, 每个Epoch所Mask同一句中的Token位置都是相同的.</li><li><strong>改进一点的静态Mask</strong>: 将每个Sentence都<strong>重复</strong>N次, 这样可能在预处理阶段能得到N种不同的Mask. 因为扩大了每个Epoch的数据量, 训练的Epoch要是原来的1/N倍.</li><li><strong>动态Mask</strong>: 每个Sentence给BERT之前<strong>动态</strong>Mask, 即生成一种新的Mask方式. 这样每个Epoch拿到的Mask基本上是不同的.</li></ul><p>从Mask的方法上来看, 动态Mask并没有引入太多的计算花费, 但是却大大提升了训练时句子的<strong>多样性</strong>. 为了证明其有效性, 作者将上述三种Mask方式的性能将BERT(Base)在SQuAD上的F1 Score, MNLI - m和SST - 2上的ACC做了比较:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/roberta1.jpg" style="zoom: 33%;" /><p>其中reference来自XLNet中给出的BERT(Base)数据. 每种方式都采用了5轮的随机初始化.</p><p>从中能看出, 改进后的静态Mask与原版Mask性能相仿, 动态Mask要比改进后的静态Mask稍好一点. 但动态Mask又不会引入太高的时间开销, 这样的增益还是很划算的.</p><h3 id="Training-without-NSP"><a href="#Training-without-NSP" class="headerlink" title="Training without NSP"></a>Training without NSP</h3><p>在BERT中, 训练时候采用了预测Mask和NSP两种训练任务. NSP(Next Sentence Prediction)任务是它随机的将两段连续或毫不相关的文档拼在一起, 然后用<code>[CLS]</code>位置的输出预测两段文字是否来自于统一文章(或者说连续不连续).</p><p>其实在早一点的多篇论文中指出, NSP任务虽然在BERT中被假设非常重要, 但实际上NSP任务会导致BERT的<strong>退化</strong>. 越来越多的人开始<strong>质疑</strong>NSP任务的必要性. 作者为了观察各种训练方式之间的差异, 设计了如下四种训练方式:</p><ol><li><strong>Segment - Pair + NSP</strong>: 与训练BERT时的方案无异. <strong>每次输入两段来自同一文档或多个文档的内容</strong>. 内容总长度必须少于512个Token.</li><li><strong>Sentence - Pair + NSP</strong>: <strong>每次只输入两个来自同一文档或多个文档的句子</strong>. 每次输入的序列长度肯定小于512个Token, 所以用<strong>增大Batch Size</strong>的方式来让这种方式的总Token数与Segment - Pair + NSP总Token数相近.</li><li><strong>Full - Sentences</strong>: <strong>全部输入可能来自于同一文档或多个文档的连续句子</strong>, 直到填满为止. 序列长度最多512个Token. 在切换不同文档时, 在之间加上特殊的分隔符. 不采用NSP任务.</li><li><strong>Doc - Sentences</strong>: <strong>全部输入来自同一文档的句子</strong>, 即只从一篇文档中对连续句子采样, 如果文档的内容少于512个Token, 则动态增大Batch Size使得其与Full - Sentences总Token数量相近.</li></ol><p>实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/roberta2.jpg" style="zoom: 33%;" /><p>作者得出了以下结论:</p><ol><li>观察方式1和方式2, 使用单个句子明显的损失了下游任务的性能. 可能模型并不能从句子中学习到长范围的依赖.</li><li>观察方式1和方式4, 即BERT的原始训练方式与不适用NSP的训练方式, 移除NSP能稍微增强下游任务的性能.</li></ol><p>但是由于表现最好的Doc - Sentences需要动态调整Batch Size, 作者还是采用了Full - Sentences作为后文实验方式.</p><h3 id="Bigger-bigger-and-bigger"><a href="#Bigger-bigger-and-bigger" class="headerlink" title="Bigger, bigger, and bigger"></a>Bigger, bigger, and bigger</h3><h4 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h4><p>XLNet用了126G的数据, 当时BERT训练只用了十几个G的数据, 所以对比是很不公平的. RoBERTa在数据上不能落后, 一口气用了160G的数据. 分别由以下部分组成:</p><table><thead><tr><th align="left">数据集名称</th><th align="center">大小(GB)</th><th>说明</th></tr></thead><tbody><tr><td align="left">BookCorpus</td><td align="center">16</td><td>BERT训练时候用的数据</td></tr><tr><td align="left">CC - NEWS</td><td align="center">76</td><td>过滤后的新闻类数据</td></tr><tr><td align="left">OpenWebText</td><td align="center">38</td><td>根据网友点赞数从URL中提取的帖子文本</td></tr><tr><td align="left">Stories</td><td align="center">31</td><td>故事类数据</td></tr><tr><td align="left"><strong>总计</strong></td><td align="center"><strong>161</strong></td><td></td></tr></tbody></table><p>更大量的数据对BERT提升是巨大的, 使之能够与XLNet相对公平的进行比较.</p><blockquote><p>从<strong>辩证</strong>的角度来说, 更大量的数据也会使模型的<strong>偏见</strong>和<strong>歧视性</strong>更强. 在数据上的偏见消除是非常重要的, 目前来说没有太好的解决方案.</p></blockquote><h4 id="Batch-Size"><a href="#Batch-Size" class="headerlink" title="Batch Size"></a>Batch Size</h4><p>先前有大量实验表明, 适量增大Batch Size有益于模型的收敛, 更能使训练稳定, 并有助于提高模型的性能. 更大的Batch Size也能帮助快速训练. 作者做了增大Batch Size对性能影响的实验, 保证Batch Size和Step的积不变, 即<strong>维持相同的计算开销</strong>, 实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/roberta3.jpg" style="zoom: 33%;" /><p>适量增大Batch Size确实有助于提高模型的性能, 但考虑到更大Batch Size在<strong>训练速度</strong>上带来的优势, 作者只采用了8K的Batch Size, 而非效果最好的2K Batch Size.</p><p>本小节叙述一些无关紧要的参数调整. 除去峰值Learning Rate, 和Warmup的次数, RoBERTa延续了BERT的<strong>原始参数</strong>. 考虑到RoBERTa采用了更大的Batch Size, 所以将Adam中的Beta2从0.999 换为了0.98.</p><p>RoBERTa不会随机的将短句注入, 并且前90%的训练中不会使用缩短的序列, 只使用全长序列.</p><p>RoBERTa还采用了<strong>BPE</strong>缩小词表, 进一步提升了性能.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>作者从增大数据量和增长训练时间两个角度做了实验:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/roberta4.jpg" style="zoom: 50%;" /><p>从逐步添加训练技巧的流程来看, RoBERTa在和BERT使用十几G数据的情况下提升非常大, 也和XLNet在用13G时的性能不分高下, 证明了RoBERTa改进的<strong>正确性</strong>. 逐渐增大训练数据量和训练时长(这里是通过Step调整), RoBERTa<strong>逐渐碾压</strong>了XLNet.</p><p>然后将RoBERta与GLUE排行榜中其他的模型也进行了实验:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/roberta5.jpg" style="zoom: 50%;" /><p>RoBERTa作者提到, 对于GLUE有两种Fine - Tune的方式:</p><ul><li>单任务, 对每个GLUE任务分别进行Fine - Tune, 并且只用相应任务的训练数据.</li><li>多任务, 在测试集上进行比较. 但与排行榜上其他的模型不同, 其他模型对多任务进行Fine - Tune, RoBERTa只对单任务进行Fine - Tune.</li></ul><p>从实验结果中来看, RoBERTa比没训练好的BERT提升相当的大.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>作者实际上在RoBERTa中主要做了四件事:</p><ol><li>用更大的Batch Size, 更多的Data, 更长的训练时间. 就是更大.</li><li>废除NSP的训练目标, 这个非常重要.</li><li>将静态Mask换为动态Mask.</li><li>用更长序列训练(不怎么重要).</li></ol><p>严格意义上来说, RoBERTa才是BERT的完全体. 这提供给大家一个非常好的预训练基准, 而且在其他论文中也鼓励用RoBERTa而不是BERT进行比较, 因为BERT的训练是不够充分的.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> BERT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Integrating Image-Based and Knowledge-Based Representation Learning</title>
      <link href="/posts/13721.html"/>
      <url>/posts/13721.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>AlexNet(详见<a href="https://adaning.github.io/posts/38085.html">卷积神经网络发展史</a>)</li><li>Attention(详见<a href="https://adaning.github.io/posts/40071.html">Seq2Seq和Attention</a>)</li><li>TransE(详见<a href="https://adaning.github.io/posts/53023.html">TransE: Translating Embeddings for Modeling Multi-relational Data</a>)</li></ul></blockquote><h1 id="Integrating-Image-Based-and-Knowledge-Based-Representation-Learning"><a href="#Integrating-Image-Based-and-Knowledge-Based-Representation-Learning" class="headerlink" title="Integrating Image-Based and Knowledge-Based Representation Learning"></a>Integrating Image-Based and Knowledge-Based Representation Learning</h1><p>本文是论文<a href="https://ieeexplore.ieee.org/abstract/document/8689107/" target="_blank" rel="noopener">Integrating Image-Based and Knowledge-Based Representation Learning</a>的阅读笔记和个人理解. 这篇论文是刘志远老师&lt;知识图谱与深度学习&gt;中2.8节提到的模型.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>研究人员发现, 语言的理解和生成是由大脑<strong>不同位置</strong>的区域负责的, 这些区域对应了许多现实生活中的事物.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113210117233.png"  style="zoom: 25%;" /><p>有时, 图片之间也能暗含关系:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113210138922.png" style="zoom: 25%;" /><p>即若A是B的一部分, 我们的视觉也认为A的周围应该有B.</p><p>作者认为, 我们理解世界是通过<strong>Knowledge Base</strong>(实体和关系的结构化三元组), 和<strong>Image Representation</strong>(通过Deep Convolutional Networks). 所以作者研究了基于<strong>图片</strong>的知识表示模型IKRL Model(<strong>I</strong>mage - <strong>B</strong>ased Knowledge <strong>R</strong>epresentation <strong>L</strong>earning Model). </p><p>在先前的KRL方法中, 只使用了KG中的关系信息. 然而KG中的结构化信息经常<strong>过于简单</strong>, 或者<strong>不完整</strong>, 会限制知识表示在下游任务中的表现. KRL是允许向表示中添加实体图像信息的, 而基于结构和图片的表示能够从<strong>多方面</strong>表示实体.</p><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>受到大脑的启发, 通过编码和图像两种手段来对实体进行表示, $\mathbf{h}_S, \mathbf{t}_S$ 是<strong>基于结构化的表示(SBR)</strong>的头实体和尾实体, $\mathbf{h}_I, \mathbf{t}_I$ 是<strong>基于图片的表示(IBR)</strong>的头实体和尾实体.</p><h3 id="Joint-Energy-Function"><a href="#Joint-Energy-Function" class="headerlink" title="Joint Energy Function"></a>Joint Energy Function</h3><p>我们将基于结构化的表示(SBR)和基于图片的表示(IBR), 融合起来, 形成一个IKRL Model, <strong>联合能量函数</strong>如下:<br>$$<br>E(h, r, t)=E_{S S}+E_{S I}+E_{I S}+E_{I I}<br>$$</p><p>联合能量函数中有四项, 这四项分别是:</p><ul><li>$E_{S S}=\lVert\mathbf{h}_{S}+\mathbf{r}-\mathbf{t}_{S}\rVert$: 和<strong>TransE</strong>的能量函数一模一样.</li><li>$E_{II}=\lVert\mathbf{h}_{I}+\mathbf{r}-\mathbf{t}_{I}\rVert$: 与TransE的能量函数也一样, 但是是<strong>图片版本</strong>的.</li><li>$E_{SI}=\lVert\mathbf{h}_{S}+\mathbf{r}-\mathbf{t}_{I}\rVert$, $E_{IS}=\lVert\mathbf{h}_{I}+\mathbf{r}-\mathbf{t}_{S}\rVert$: 这两项希望能将SBR和IBR投入<strong>相同语义空间</strong>.</li></ul><p>其中实体向量$\mathbf{h}_S, \mathbf{h}_I, \mathbf{t}_S, \mathbf{t}_I$ 都是<strong>归一化</strong>过的, 关系$\mathbf{r}$ 不是, TransE论文中曾提到关系的归一化对学习到关系没太大影响. 但是请注意, 在这里SBR和IBR共用同一个关系向量$r$, 关系的表示是<strong>不能从图像中直接学到</strong>的. 共享关系向量能够作为两种实体表示之间的<strong>转换</strong>, 也能方便它们嵌入到同一语义空间中.</p><p>模型结构的概览图如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113210155434.png" style="zoom: 50%;" /><p>因为考虑到融入了IBR, 所以每个实体的多个图片都被送入Image Encoder中, 然后在通过注意力机制让模型考虑每张图片的重要性, 然后SBR和IBR联合学习整个能量函数.</p><h3 id="Image-Encoder"><a href="#Image-Encoder" class="headerlink" title="Image Encoder"></a>Image Encoder</h3><p>图像对IKRL来说是非常重要的, 因为图像能够从外观等多个方面刻画实体. 此外, <strong>多张图片</strong>可能从不同角度提供同一个实体的不同特性, 我们用$I_{k}=\left\{\mathrm{img}_{1}^{(k)}, \mathrm{img}_{2}^{(k)}, \ldots, \mathrm{img}_{n}^{(k)}\right\}$ 来表示多张图片.</p><p>既然涉及到图像, 那么比较成熟的方案肯定是用<strong>CNN</strong>提取视觉特征了, 用CNN对每张图像构建特征表示. </p><p>Image Encoder由<strong>图像表示模块</strong>和<strong>图像投影模块</strong>组成:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113210231408.png" style="zoom: 25%;" /><h4 id="Image-Representation-Module"><a href="#Image-Representation-Module" class="headerlink" title="Image Representation Module"></a>Image Representation Module</h4><p>图像表示模块主要依赖于CNN对特征进行抽取, 将实体在<strong>图像空间</strong>中进行表示. 作者在这里只使用ALexNet(确实比较老), AlexNet是一个只由5层Conv层和2层全连接层组成的神经网络:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/alexnet.jpg" style="zoom: 50%;" /><p>将图像Reshape成$224\times 224$ 的大小, 然后接上5层Conv层, 2层全连接层就得到了实体的图像表示.</p><blockquote><p>这里使用AlexNet肯定不是最好的选择, 我猜测作者出于<strong>实验性目的</strong>才使用了AlexNet.</p></blockquote><h4 id="Image-Projection-Module"><a href="#Image-Projection-Module" class="headerlink" title="Image Projection Module"></a>Image Projection Module</h4><p>因为IBR和SBR不在同一个语义空间中, 所以需要用一次变换将它们投入相同的实体空间中:<br>$$<br>\mathbf{p}_{i}=\mathbf{M} \cdot f\left(\mathrm{img}_{i}\right)<br>$$<br>$\mathbf{p}_i$ 指图片表示, $f(\cdot)$ 表示神经网络的输出. 当然, 这个投影矩阵$\mathbf{M}$ 在每个实例之间<strong>共享</strong>.</p><h3 id="Attention-Based-Multi-Instance-Learning"><a href="#Attention-Based-Multi-Instance-Learning" class="headerlink" title="Attention Based Multi - Instance Learning"></a>Attention Based Multi - Instance Learning</h3><p>Attention在IKRL中起到了非常大的作用. 因为Attention让更多信息性的图片对IKRL有更多的贡献. Attention在IKRL中被用于<strong>多实例学习</strong>, 因为绝大多数实体都有不止一张不同的图片, 但视觉信息经常伴随着<strong>噪声</strong>, 所以相当有必要对实体所对应的图片进行<strong>选择</strong>. 论文后面实验会多次说明这一点.</p><p>细想一下, 其实我们也是这样的, 我们善用注意力去选择表征实例, 而滤掉不相关的实例.</p><p>在IKRL中, 实例级别的注意力能将每个实例与实体进行<strong>匹配</strong>, 得到实例对实体的<strong>权重</strong>:<br>$$<br>\operatorname{att}\left(\mathbf{p}_{i}^{(k)}, \mathbf{e}_{S}^{(k)}\right)=\frac{\exp \left(\mathbf{p}_{i}^{(k)} \cdot \mathbf{e}_{S}^{(k)}\right)}{\sum_{j=1}^{n} \exp \left(\mathbf{p}_{j}^{(k)} \cdot \mathbf{e}_{S}^{(k)}\right)}<br>$$</p><p>其中$\mathbf{e}_{S}^{(k)}$ 代表第$k$ 个实体的SBR.</p><p>在获取了权重后, 对图像表示<strong>加权求和</strong>, 得到IBR:<br>$$<br>\mathbf{e}_{I}^{(k)}=\sum_{i=1}^{n} \frac{\operatorname{att}\left(\mathbf{p}_{i}^{(k)}, \mathbf{e}_{S}^{(k)}\right) \cdot \mathbf{p}_{i}^{(k)}}{\sum_{j=1}^{n} \operatorname{att}\left(\mathbf{p}_{j}^{(k)}, \mathbf{e}_{S}^{(k)}\right)}<br>$$<br>除了上述普通的Attention外, 作者还用两种方法与之进行比较:</p><ul><li>如果对每张实例分配相同权重, 称为$\text{AVG}$.</li><li>如果只选权重最大的实例作为$\mathbf{p}_{i}^{(k)}$, 称为$\text{MAX}$.</li></ul><p>在后续的实验中, 会比较这三种方法之间的性能.</p><h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><p>作者使用最大化间隔的Hinge Loss来训练:</p><p>$$<br>L=\sum_{(h, r, t) \in T} \sum_{\left(h^{\prime}, r^{\prime}, t^{\prime}\right) \in T^{\prime}} \max \left(\gamma+E(h, r, t)-E\left(h^{\prime}, r^{\prime}, t^{\prime}\right), 0\right)<br>$$</p><p>其中$\gamma$ 代表间隔. 与现在的其他KRL模型训练一样, 都有<strong>负采样</strong>:<br>$$<br>T^{\prime}=\left\{\left(h^{\prime}, r, t\right) \mid h^{\prime} \in E\right\} \cup\left\{\left(h, r, t^{\prime}\right) \mid t^{\prime} \in E\right\}<br>\cup\left\{\left(h, r^{\prime}, t\right) \mid r^{\prime} \in R\right\}, \quad(h, r, t) \in T<br>$$<br>但这里的负采样不光替换实体, 而是随机替换三元组中的<strong>实体</strong>和<strong>关系</strong>.</p><h3 id="Optimization-and-Implementation-Details"><a href="#Optimization-and-Implementation-Details" class="headerlink" title="Optimization and Implementation Details"></a>Optimization and Implementation Details</h3><p>IKRL模型中, 所有的参数为$\theta=(\mathbf{E}, \mathbf{R}, \mathbf{W}, \mathbf{M})$, $\mathbf{E}$ 代表SBR的Embedding, 包括$\mathbf{h}_s, \mathbf{t}_s$. $\mathbf{R}$ 代表关系嵌入. $\mathbf{W}$ 代表神经网络的参数, $\mathbf{M}$ 是投影矩阵的参数. 作者使用SGD来优化模型, $\mathbf{E}, \mathbf{R}$ 直接用TransE的参数初始化, $\mathbf{M}$ 随机初始化.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详细试验参数设置请参考原论文, 不过我认为本论文是一篇<strong>实验性</strong>的工作, 参数设置上没有太多意义. SBR的实体和关系Embedding维度$d_s=50$, 并且最多为每个实体使用10张图片.</p><h3 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h3><p>因为没有现成的Image Based Knowledge Dataset, 所以作者团队自己搞了一个WN9 - IMG. 这个数据集先从WN18中抽出一部分三元组, 然后从ImageNet中抽出来一部分图片, 组成了基于图片的数据集. 这个数据集中只有9种关系, 6555个实体. 不同关系的数据集分布如下所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113210532097.png" style="zoom: 25%;" /><p>8类实体数量如下所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113210614662.png"  style="zoom: 25%;" /><h3 id="Entity-Prediction"><a href="#Entity-Prediction" class="headerlink" title="Entity Prediction"></a>Entity Prediction</h3><p>在训练时, 仍然使用IBR和SBR<strong>混合</strong>的方式进行训练. 只是在<strong>测试</strong>时对使用的信息进行改动:</p><ul><li>SBR: 在测试时只使用基于结构的表示.</li><li>IBR: 在测试时只使用基于图像的表示.</li><li>UNION: 二者都可以使用.</li></ul><p>在与TransE和TransR的比较中, IKRL的表现非常好:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113210715845.png" style="zoom: 25%;" /><p>从结果中得到以下结论:</p><ul><li>无论是哪种IKRL, 全面碾压TransE和TransR, 证明了实体图像中丰富的视觉信息能帮助更深入的理解实体.</li><li>相比于只使用SBR训练(TransE), 融合了IBR的表示训练有极大的提升. 因为IKRL能通过能量函数中的两种表示进行混合项训练, 也间接地学习到了一部分图像信息, 这也是为什么测试时只使用SBR就能得到很大提升.</li><li>在MR上显示出的效果提升比较大. 作者认为MR是更多关注Embedding在空间上的<strong>整体效果</strong>, 而Hits@10对错误比例更加敏感. IKRL因为融入了图片信息, 能从图片中间接的发现KG中没有体现的直接关系, 可以利用图片的中发现的<strong>潜在关系</strong>.</li><li>IKRL是基于TransE进行训练的, 但仍然比TransR效果好, 说明IKRL有更好的<strong>鲁棒性</strong>, 能更好的用在基于平移模型的改进模型上.</li></ul><blockquote><p>我有些惊讶, 仅仅只使用IBR带来的效果居然这么好. 是不是说明了模型在训练阶段使用了SBR后, 在不同任务上SBR测试阶段的意义是不同的? 可能有些任务不需要SBR?</p></blockquote><p>作者还对Attention的三种方式的效果进行了比较:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113210928871.png" style="zoom: 33%;" /><p>MAX最差, 这是因为单一图片提供的视觉信息有限. AVG其次, 虽然考虑到了所有实例, 但不可避免的引入了噪声. 普通的Attention是最好的, 它能够根据图片的质量来对实例进行<strong>筛选</strong>.</p><h3 id="Triple-Classification"><a href="#Triple-Classification" class="headerlink" title="Triple Classification"></a>Triple Classification</h3><p>在三元组分类任务上, 作者对每种关系依据验证集设置一个阈值$\delta_r$, 用阈值来对三元组分类是否正确进行分类. 例如当$\lVert\mathbf{h}+\mathbf{r}-\mathbf{t}\rVert&gt;\delta_r$ 时, 分类错误, 反之分类正确. 其他模型同样按照自己的评分函数分类判断.</p><p>在不同的Attention种类下, 结果如下所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113211050295.png" style="zoom: 50%;" /><p>仍然是与Entity Prediction相似的结论. 融入视觉信息的表示是一件非常重要的事情, 并同时证明了Attention保证了图像的质量, 充分利用实体的多样性, 增强了模型的<strong>鲁棒性</strong>.</p><h3 id="Representation-Analysis"><a href="#Representation-Analysis" class="headerlink" title="Representation Analysis"></a>Representation Analysis</h3><h4 id="SBR-and-IBR"><a href="#SBR-and-IBR" class="headerlink" title="SBR and IBR"></a>SBR and IBR</h4><p>作者分别基于SBR和IBR计算了数据集中所有类别的实体之间的<strong>协方差</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113211018302.png" style="zoom: 67%;" /><p>从中不难发现, IBR能对不同类别的实体相<strong>区分</strong>(不同类之间相关性差), SBR将少数类别的实体相<strong>连接</strong>(有些类相关性强). 但在IBR中, Plant和Object似乎具有很高的相关性.</p><p>IBR更容易<strong>基于外观</strong>区分实体, SBR更善于<strong>基于功能</strong>区分实体, 如下图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113211141244.png" style="zoom: 50%;" /><p>左侧”sport”从图像上来看并不相似, 但功能相似. 右侧”aritifact”从图像上看上去非常相似, 但功能不相似.</p><p>作者分别对SBR和IBR进行了PCA降维成2维后绘制出实体在坐标系中的位置, 并用RSA做可解释性分析:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113211120889.png" style="zoom:67%;" /><p>SBR(左), IBR(中), 主成分可解释性(右). IBR能够明显的将各关系的实体分开, SBR效果就差一些. 从数据可解释性来看, 除了随维度增大可解释性增强, 但从表示类型来看, IBR也是始终要强于IBR的. </p><h4 id="Relation-Analysis"><a href="#Relation-Analysis" class="headerlink" title="Relation Analysis"></a>Relation Analysis</h4><p>作者将关系也做PCA和RSA:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113211207797.png" style="zoom: 50%;" /><p>能很清楚的看到, 反义关系往往处于某个主成分正交轴的<strong>对立面</strong>. RSA图中看到, 在主成分没有达到8时可解释性已经收敛, 说明目前学到的关系复杂度是比需求要大的.</p><h3 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h3><p>作者在这里对案例进行了分析.</p><h4 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h4><p>首先是对Attention对<strong>实例筛选</strong>的结果进行了可视化:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113211241891.png" style="zoom: 50%;" /><p>例如”cycling”这个实体, 真正的骑行图像被赋予了高注意力, 而没有自行车的图像被赋予低注意力. “typewirter”中, 整体打印机的图片被赋予高注意力, 而打印机局部细节的图片被赋予低注意力. 在”riding”中, 人类骑马的图片被赋予高注意力, 马群自己走的照片被赋予低注意力.</p><p>这证明了注意力能自动从图像中学习知识表示, 减少低质量图片的噪声.</p><h4 id="Semantic-Translation-in-Image-Representation-Space"><a href="#Semantic-Translation-in-Image-Representation-Space" class="headerlink" title="Semantic Translation in Image Representation Space"></a>Semantic Translation in Image Representation Space</h4><p>然后, 作者发现, 在跨模态的图像 - 知识空间中, 像Word2Vec一样, 也含有<strong>语义平移</strong>规则:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/image-20201113211321972.png" style="zoom: 33%;" /><p>在图像的表示中, 柜子和抽屉的差, 与钢琴和琴键之间的差大致相等, 表示出”属于”的关系. 这体现了编码的<strong>语义规则</strong>性.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>IKRL介绍了一种<strong>基于图像</strong>的知识表示方法. 将<strong>基于结构化的表示(SBR)</strong>和<strong>基于图片的表示(IBR)</strong>融合在了一起, 并用<strong>Attention</strong>做图片的自动过滤, 提高模型的<strong>性能</strong>和<strong>鲁棒性</strong>.</p><p>我认为, 这篇文章比较有价值的有以下几个部分:</p><ul><li>论文中提出的<strong>多实例学习</strong>思路确实不错, 或许多模态都可以用多实例学习作为接口, 将实体与不同模态之间的数据进行自动对齐和筛选.</li><li>做了很多的<strong>分析类实验</strong>, 虽然使用的模型都是最原始最简单的, 但这些<strong>可视化探究</strong>都是非常有价值的, 我认为相当多的可视化都<strong>得益于图像</strong>.</li><li>提供了基于图像的知识数据集<strong>WN9 - IMG</strong>.</li></ul><p>我认为的缺点有:</p><ul><li>在处理SBR时只用了TransE, 在处理IBR时只用了非常早的AlexNet. 那么在对关系建模时就肯定会遇到对<strong>多关系</strong>建模的痛点. 但想必这篇论文也只是一次<strong>尝试和探索</strong>, 而非追求性能.</li><li>没有与更多的KRL方法进行对比, 但想必在后续肯定会有相关工作.</li></ul><p>我认为实验还揭示了另一个点, 或许在某些任务上<strong>融入语言特征</strong>并不能起到很大的作用. 比如本论文提到的模型, 如果只用IBR, 也能取得相当好的效果. 当然不排除作者提出的KRL模型没有对三元组使用其他方法优化的因素. SBR确实能够在某些任务上使模型完成<strong>更复杂</strong>的任务, 并与IBR<strong>互补</strong>. 但从本论文的结果来看, <strong>并不能很大幅度的提升性能</strong>.</p><p>希望<strong>脑科学和神经科学</strong>能快快进步, 给深度学习发展带来更多动力和想法.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> 多模态 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TransE: Translating Embeddings for Modeling Multi-relational Data</title>
      <link href="/posts/53023.html"/>
      <url>/posts/53023.html</url>
      
        <content type="html"><![CDATA[<h1 id="TransE-Translating-Embeddings-for-Modeling-Multi-relational-Data"><a href="#TransE-Translating-Embeddings-for-Modeling-Multi-relational-Data" class="headerlink" title="TransE: Translating Embeddings for Modeling Multi-relational Data"></a>TransE: Translating Embeddings for Modeling Multi-relational Data</h1><p>本文是论文<a href="https://proceedings.neurips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html" target="_blank" rel="noopener">Translating Embeddings for Modeling Multi-relational Data</a>的阅读笔记和个人理解. 这篇论文是一篇比较早的论文了, 2012年Knowledge graph这个概念被谷歌提出, 2013年这篇论文就发表了, 并且大家也对它认可度很高, 几乎之后的所有关于KGE的论文中都会出现以它为Baseline的实验.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>在有向图中, 可以允许有多元关系数据的存在, 例如图中的边可以被表示为$ (head, label, tail) $, 即$(h, \ell, t)$. 这点经常体现在各种知识库中, 比如Google的知识图谱.</p><p>在先前的Knowledge Embedding方法中, 要么具有很<strong>高复杂度</strong>, 要么有<strong>高计算成本</strong>.<br>作者阐述, 基于平移的模型主要来源于下列两个Motivation:</p><ul><li>最主要的Motivation是在很多KBs中, <strong>层次化表示</strong>都非常的常见, 那么平移一种很自然的转换表示法. 比如树的自然表示就可以用兄弟关系和父子关系来投射到一个二维坐标系中表示. 所以可以尝试可以把三元组<strong>嵌入到低维空间</strong>中.</li><li>类似与<strong>Word2vec</strong>, 作者观察到Word Embedding能够捕获一些文本中的一对一关系, 例如国家和城市之间的”省会”关系, 在嵌入空间上就是平移.</li></ul><h2 id="Translation-Based-Model"><a href="#Translation-Based-Model" class="headerlink" title="Translation - Based Model"></a>Translation - Based Model</h2><p>TransE的思想其实是非常简单的. 对于三元组$(h, \ell, t)$, TransE希望能够在空间中有$h+\ell \approx h$, 即$h+\ell$ 离$t$ 最近, 离其他的尾实体$\ell^{\prime}$非常远. 用$d(h+\ell^{\prime}, t)$ 代表二者间不相似的程度, 其中使用的不相似度度量$d$ 可以是L1范数, 也可以是L2范数等等.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transe2.jpg" style="zoom: 33%;" /><h3 id="TransE-Algorithm"><a href="#TransE-Algorithm" class="headerlink" title="TransE Algorithm"></a>TransE Algorithm</h3><p>其实明白了TransE的思想, 算法的流程也就不难明白. 算法伪代码如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transe1.jpg" style="zoom:50%;" /><p>基本分为如下几步:</p><ol><li>用均匀分布初始化关系和实体矩阵.</li><li>进入循环, 每次循环开始都对实体$e$ <strong>归一化</strong>.</li><li>从三元组中采样出一个Minibatch来.</li><li>对每个Batch中的三元组进行替换, 主要是替换头实体或者尾实体生成负样本, 将打乱后的数据(负样本)和原来正确的三元组(正样本)和放到一个集合中.</li><li>对集合中的样本按照损失函数梯度对Embedding进行更新.</li></ol><p>与SVD一样, 采用<strong>最大化间隔</strong>作为损失函数:<br>$$<br>\mathcal{L}=\sum_{(h, \ell, t) \in S} \sum_{\left(h^{\prime}, \ell, t^{\prime}\right) \in S_{(h, \ell, t)}^{\prime}}\left[\gamma+d(\boldsymbol{h}+\boldsymbol{\ell}, \boldsymbol{t})-d\left(\boldsymbol{h}^{\prime}+\boldsymbol{\ell}, \boldsymbol{t}^{\prime}\right)\right]_{+}<br>$$<br>其中$\gamma$ 是超参数, 代表<strong>间隔</strong>. $[x]_+$ 代表取$x$ 的正的部分, 其实就是ReLU. </p><p>这个损失函数的意义是, 希望能够最小化正例三元组在空间中的距离, 最大化负例三元组在空间中的距离.</p><p>其中$S^{\prime}$ 来自正确三元组替换头实体或尾实体的集合:<br>$$<br>S_{(h, \ell, t)}^{\prime}=\left\{\left(h^{\prime}, \ell, t\right) \mid h^{\prime} \in E\right\} \cup\left\{\left(h, \ell, t^{\prime}\right) \mid t^{\prime} \in E\right\}<br>$$</p><h3 id="Dissimilarity-Function"><a href="#Dissimilarity-Function" class="headerlink" title="Dissimilarity Function"></a>Dissimilarity Function</h3><p>正如我们之前所说, 不相似度度量可以使用L1或L2范数, 这里作者直接使用L2范数:<br>$$<br>d(\boldsymbol{h}+\boldsymbol{\ell}, \boldsymbol{t})=    \lVert\boldsymbol{h}    \rVert_{2}^{2}+<br>\lVert\ell\rVert_{2}^{2}+<br>\lVert\boldsymbol{t}\rVert_{2}^{2}-2\left(\boldsymbol{h}^{T} \boldsymbol{t}+\boldsymbol{\ell}^{T}(\boldsymbol{t}-\boldsymbol{h})\right)<br>$$<br>那么$h+\ell$ 与$t$ 越近, 不相似度就越低(越相似), 反之不相似度就越高(越不相似). 因为每个Batch都对实体做归一化, 所以还有$\lVert\boldsymbol{h}\rVert_{2}^{2}=\lVert\boldsymbol{t}\rVert_{2}^{2}=1$.</p><h3 id="Some-Explain"><a href="#Some-Explain" class="headerlink" title="Some Explain"></a>Some Explain</h3><p>在优化时使用SGD, 并且在训练过程中, 作者提出的算法只对每个Batch更新时的实体进行了归一化, 关系没有做要求, 在此作者没有说明原因.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="DataSets"><a href="#DataSets" class="headerlink" title="DataSets"></a>DataSets</h3><p>因为是比较早的KGE方法, 作者只在WN, FB15k, FB1M上进行了对比.</p><h3 id="Evaluation-protocol"><a href="#Evaluation-protocol" class="headerlink" title="Evaluation protocol"></a>Evaluation protocol</h3><p>评估阶段的协议非常重要, 直到现在, 很多模型也都遵循着TransE的协议. </p><p>对每个测试三元组移除头实体, 并用其他实体轮流替换, 按照Loss升序排列, 最后再计算正确的三元组Loss. 然后对每个尾实体做相同的操作. </p><p>可能在替换后, 三元组是<strong>仍然正确</strong>的. 如果不做任何<strong>过滤</strong>的操作, 这个被替换后仍然正确的三元组将被记为错误, 这样就可能会<strong>低估</strong>模型能力. 所以应该将已经在训练集, 验证集, 测试集中出现的三元组从替换后的三元组中剔除, 从而保证在替换头实体或尾实体后三元组一定是错误的. 这种方法被称为<strong>Filtered setting</strong>, 没有经过过滤的称为raw, 该方法在其他很多论文中都出现过.</p><h3 id="Link-Prediction"><a href="#Link-Prediction" class="headerlink" title="Link Prediction"></a>Link Prediction</h3><p>在实体链接上的结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transe3.jpg" style="zoom:50%;" /><p>结果分为了两类, 有些指标在Raw和Filtered上表现差距非常大, 说明Filtered Setting还是非常有必要的.</p><p>在和Baseline模型相比较时都显示出TransE的强大.</p><p>之后作者又在FB15k上做了区分类别的实体链接实验:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transe4.jpg" style="zoom:50%;" /><p>能够很清楚的看出, TransE在一对一的建模能力上要好于其他Baseline模型, 并且在预测头实体时一对多, 预测尾实体时多对一时本身的效果比较好, 但不及于其他模型.</p><h3 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h3><p>作者将预测尾实体时的一些案例放了出来, 其中粗体表示测试集中的正确元组, 斜体表示训练集中预测正确的元组.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transe5.jpg" style="zoom:50%;" /><p>虽然在最高位的不是最佳答案, 但也说明了TransE能够表示一些<strong>常识</strong>.</p><h3 id="Generalization-Ability"><a href="#Generalization-Ability" class="headerlink" title="Generalization Ability"></a>Generalization Ability</h3><p>在FB15k上, 作者随机选择了40种关系作为FB15k - 40rel, 并将其于三元组全部作为FB15k - rest. 作者只让所有Embedding模型学习这四十个关系所对应的数据, 即只学习FB15k - 40rel, 并对FB15k - rest进行连接预测, 尝试观察TransE的<strong>泛化能力</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transe6.jpg" style="zoom:50%;" /><p>左图是模型在MR上的表现, 右图是Hit@10的表现.</p><p>结果表明, TransE是<strong>学习速度最快</strong>的方法, 即使只有少量三元组, TransE表现仍然不错. 随着知识的数量增大, TransE仍然有<strong>进步空间</strong>.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Trans它为KGE世界打开了新的大门, 之后衍生了例如TransR, TransH等不少基于平移的模型. 并且参数量和计算量都不算特别多. 但TransE也有非常多的<strong>缺点</strong>, 这些问题包括但不限于:</p><ul><li>一对多建模存在<strong>竞争</strong>, 即比如$(Writer, Compose, Novel)$这个三元组, 一名作者可以写多部小说, 作品之间是不同的实体. 在TransE中, $Writer + Compose$只能尽可能指向某一部作品, 如果有多部作品就会导致竞争. 多对多同理, 肯定更不擅长了.</li><li>不能完成<strong>对称关系</strong>的建模, 例如”朋友”这种关系. 在TransE中, 假设小明和小李是朋友, 用$(Ming, Friend, Li)$ 来表示, 小李被表示为$Ming + Friend$. 但是小明却不能被表示为$Li + Friend$.</li><li>不能完成<strong>自反关系</strong>的建模, 例如$(h, r, h)$ 的关系在TransE中完全没有办法表示.</li><li>没有考虑<strong>语义关系</strong>.</li></ul>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CoKE: Contextualized Knowledge Graph Embedding</title>
      <link href="/posts/42304.html"/>
      <url>/posts/42304.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>Self - Attention</li><li>BERT</li></ul><p><strong>2020.11.17</strong>: 解决了标签泄露的疑惑.</p><p><strong>2021.04.09</strong>: 修正可视化实验的描述.</p></blockquote><h1 id="CoKE-Contextualized-Knowledge-Graph-Embedding"><a href="#CoKE-Contextualized-Knowledge-Graph-Embedding" class="headerlink" title="CoKE: Contextualized Knowledge Graph Embedding"></a>CoKE: Contextualized Knowledge Graph Embedding</h1><p>本文是论文<a href="https://arxiv.org/abs/1911.02168" target="_blank" rel="noopener">CoKE: Contextualized Knowledge Graph Embedding</a> 的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者指出, 在先前的Embedding方法都是<strong>静态</strong>的, <strong>忽略了实体和关系在不同图之间的上下文所对应的真正含义</strong>. 在不同的上下文中, 实体和关系的含义经常是不同的, 例如下图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/coke1.jpg" style="zoom: 33%;" /><p>假设有两个子图, 代表政治关系的子图(左侧蓝色)和代表家庭关系子图(右侧橘色), 它们都指向同一个实体<code>Barack Obama</code>, 在不同子图中它们所代表的表示应该是不同的, 因为在政治和家庭领域中, <code>Barack Obama</code>应该具有不同的含义, 因此Embedding需要在不同语境中被<strong>动态</strong>表示. 必须根据<strong>上下文语义信息</strong>来判断应该采取怎样的表示. 相比于静态表示法, 结合上下文语义信息的表示法有更丰富而灵活的Embedding.</p><p>除此外还有一个重要原因, 作者发现<strong>实体和关系很少孤立出现</strong>, 它们更多的出现伴随丰富的上下文, 甚至以边, 路径, 子图的方式出现, 这就为利用上下文提供了极大地便利.</p><p><strong>CoKE</strong>(<strong>Co</strong>ntextualized <strong>K</strong>nowledge Graph <strong>E</strong>mbedding)的设计初衷便是一种结合语境的动态知识表示法.</p><h2 id="CoKE"><a href="#CoKE" class="headerlink" title="CoKE"></a>CoKE</h2><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/coke2.jpg" style="zoom: 33%;" /><h3 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h3><p>对于Knowledge Graph中给定的三元组结构$(s, r, o)$, 两个实体之间可能在<strong>给定的上下文</strong>中对应着两种情况:</p><ul><li><strong>Edge</strong>:$s \rightarrow r \rightarrow o$, 即实体到实体只经过一跳, 用一种关系的<strong>边</strong>就能表示, 这是一种在知识图谱中最基本的方式. 例如$\text{BarackObama}\rightarrow\text{HasChild}\rightarrow\text{SashaObama}$.</li><li><strong>Path</strong>: $s \rightarrow r_{1} \rightarrow \cdots \rightarrow r_{k} \rightarrow o$, 即实体到实体需要用一系列关系构成的<strong>路径</strong>来表示, 由于包含了多跳信息, 这种表示往往伴随着更强的<strong>推理性</strong>. 例如$\text{BarackObama}\rightarrow\text{HasChild}\stackrel { (\text {Sasha}) }\rightarrow\text {LivesIn} \stackrel { \text {(US)} } \rightarrow\text{OffcialLanguage}\rightarrow\text{English}$.</li></ul><p>CoKE的目标就是根据丰富的图结构上下文学习到实体和关系的<strong>动态自适应性</strong>表示.</p><h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p>CoKE也采用了现在大家一致认为表现最好的<strong>Transformer Encoder(BERT)</strong> 架构完成Embedding. 其实模型的结构非常简单, 就是Transformer Encoder.</p><p>对于给定的输入序列$X=\left(x_{1}, x_{2}, \cdots, x_{n}\right)$,  在输入到Transformer Encoder前, 只需要对Token Emedding额外加上<strong>位置编码</strong>:<br>$$<br>\mathbf{h}_{i}^{0}=\mathbf{x}_{i}^{\mathrm{ele}}+\mathbf{x}_{i}^{\mathrm{pos}}<br>$$<br>需要注意的是, 这里没有对Entity和Relation的Embedding加以区分, 直接统一使用Word Embedding.</p><p>然后就对Transformer Encoder进行<strong>堆叠</strong>:<br>$$<br>\mathbf{h}_{i}^{\ell}=\text { Transformer }\left(\mathbf{h}_{i}^{\ell-1}\right), \quad \ell=1,2, \cdots ,L<br>$$<br>其中$\ell$ 代表Transformer的堆叠层数. 论文这里还吹了一波Transformer的双向捕捉上下文能力和其他优点等等. 通过多层堆叠处理后, 得到的表示$\left\{\mathbf{h}_{i}^{L}\right\}_{i=1}^{n}$ 是<strong>对输入自适应</strong>的.</p><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>CoKE也用给Token打<code>[Mask]</code>的方式训练模型, 作者设计了一种给出子图上下文情况下的实体预测任务.</p><p>但是与Masked Language Model的Mask方式不同, 并不是对Sequence 随机Mask, 而是只对边(路径)中的<strong>实体</strong>进行Mask, 这样每次预测的任务类似于<strong>问答</strong>的任务. 并且这样还有一个好处, 在执行很多下游任务(例如实体链接, 路径查询问答)时, 这种方式能够完全的避免BERT遇到的<strong>Train - Test discripency</strong>.</p><h4 id="Double-Entity-Mask"><a href="#Double-Entity-Mask" class="headerlink" title="Double Entity Mask"></a>Double Entity Mask</h4><p>Double Entity Mask这个名字是我自己起的, 因为它分别Mask了两个不同的实体并创建了两个不同的实例.</p><p>针对我们在Problem Formulation中提到的, 在KG中对于给定的两个实体可能有两种情况, 分别是边和路径. 边可以看做是一种特殊的路径, 所以作者采用<strong>相同策略</strong>对它们进行Mask, 虽然形式上没有区别, 但实际上对模型有着<strong>不同的影响</strong>.</p><ul><li>对于边$s \rightarrow r \rightarrow o$, 将产生两个实例, 分别是$? \rightarrow r \rightarrow o$ 和 $s \rightarrow r \rightarrow ?$. 要把实体预测出来, 这样的问题是在问答任务中的<strong>单跳</strong>问题. 例如$\text{BarackObama}\rightarrow\text{HasChild}\rightarrow?$, 就是在询问”<em>Who is the child of Barack Obama?</em>“.</li><li>对于路径$s \rightarrow r_{1} \rightarrow \cdots \rightarrow r_{k} \rightarrow o$, 也映射成两个实例, 分别预测头实体$s$ 和尾实体$o$. 它可以被看做是问答任务中的<strong>多跳</strong>问题. 例如$\text{BarackObama}\rightarrow\text{HasChild}\rightarrow\text {LivesIn} \rightarrow\text{OffcialLanguage}\rightarrow?$, 就是在询问”<em>What is the official language of the country where Barack Obama’s child lives in?</em>“.</li></ul><p>将上面二者统一, 对于给定的输入序列$X=\left(x_{1}, x_{2}, \cdots, x_{n}\right)$, 创建两个训练实例, 一个用<code>[MASK]</code> 替换掉$x_1$ 来让模型预测头实体$s$, 与之相对的另一个用<code>[MASK]</code> 替换$x_n$, 让模型预测尾实体$o$. 然后将替换后的序列加上位置编码, 一股脑放进Transformer Encoder, 最后得到最终隐态$\mathbf{h}_{1}^{L}$ 和$\mathbf{h}_{n}^{L}$, 然后用它来预测被Mask的实体.</p><blockquote><p>我之前以为这样会产生标签泄露, 实际上并<strong>不会产生标签泄露</strong>. 假设原三元组是$(h, r, t)$, 那么CoKE产生的实例是$(?, r, t)$ 和$(h, r, ?)$. 这时模型预测的分别是头实体和尾实体在相同关系下的概率分布. </p><p>与WN18和WN18RR的关系不一样, WN18RR是在WN18的基础上, 在训练集去除类似于$(t, r^{-1}, h)$ 这样的三元组. 也就是说在WN18中存在$(h, r, ?)$ 和$(t, r^{-1}, ?)$ 如果模型对<strong>顺逆关系</strong>做关联, 逆关系能直接泄露出另一端的实体嵌入.</p></blockquote><h4 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h4><p>和BERT一致, 在最后分类的时候用前馈神经网络和Softmax来做实体的预测:<br>$$<br>\begin{array}{l}<br>\mathbf{z}_{1}=\text { Feedforward }\left(\mathbf{h}_{1}^{L}\right), \mathbf{z}_{n}=\text { Feedforward }\left(\mathbf{h}_{n}^{L}\right) \\<br>\mathbf{p}_{1}=\operatorname{softmax}\left(\mathbf{E}^{\mathrm{ele}} \mathbf{z}_{1}\right), \mathbf{p}_{n}=\operatorname{softmax}\left(\mathbf{E}^{\mathrm{ele}} \mathbf{z}_{n}\right)<br>\end{array}<br>$$<br>其中$\mathbf{E}^{\mathrm{ele}}$ 是和实体Embedding<strong>共享权重</strong>的分类权重矩阵.</p><p>模型的概览图如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/coke2.jpg" style="zoom: 50%;" /><p>左图是对边的Mask, 右图是对关系的Mask. 它们都是通过Transformer Encoder最后时刻的隐态来确定被Mask掉的目标实体.</p><h4 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h4><p>因为以分类为目标, 所以用<strong>交叉熵</strong>为损失函数;<br>$$<br>\mathcal{L}(X)=-\sum_{t} y_{t} \log p_{t}<br>$$<br>训练时还使用了<strong>标签平滑</strong>, 设定实体标签所对应的$y_t=\epsilon$, 其他实体$y_{t}=\frac{1-\epsilon}{V-1}$.</p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Link-Prediction"><a href="#Link-Prediction" class="headerlink" title="Link Prediction"></a>Link Prediction</h3><p>链接预测上使用了FB15k, WN18和剔除包含反关系的FB15k-237, WN18RR.</p><p>CoKE只允许最大输入序列长度为3, 即<strong>只使用三元组</strong>, 不使用上下文. 结果直接使用了<strong>filtered setting</strong>, 对头实体和尾实体分别Mask再预测, 用MRR, H@n来测试性能. 详细的参数设置和训练Trick请见原论文.</p><blockquote><p>Filtered setting最早出现在TransE的论文中, 为了避免在替换实体后三元组仍然正确, 从而将正确的答案判为错误, 看低模型性能. 避免的方法是从替换后的实体中剔除在训练集, 验证集, 测试集中已经出现过的三元组.</p></blockquote><h4 id="Main-Result"><a href="#Main-Result" class="headerlink" title="Main Result"></a>Main Result</h4><p>FB15k, WN18:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/coke3.jpg" style="zoom: 33%;" /><p>上面一组是没有使用上下文的Embedding方法, 下面一组是结合上下文或路径的方法. CoKE在这里只使用了三元组, 应该从属于第一组. 在FB15k上表现良好, WN18上与最佳结果差距不大.</p><p>FB15k-237, WN18RR:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/coke4.jpg" style="zoom: 33%;" /><p>仍然表现不错, 除了WN18RR上与RotatE差距较大.</p><p>这组只使用三元组的CoKE证明它在<strong>单跳推理</strong>上的表现很不错.</p><h4 id="Parameter-Efficiency"><a href="#Parameter-Efficiency" class="headerlink" title="Parameter Efficiency"></a>Parameter Efficiency</h4><p>作者还在这比较了一下CoKE的参数效率, 当然这是<strong>建立在预测效果之上</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/coke5.jpg" style="zoom: 33%;" /><p>我觉得都已经上Transformer了, 就别再考虑啥参数效率了吧, 很大一个原因是它只使用了<strong>256</strong>维的Embedding, 并且每层只用了4个头.</p><h3 id="Path-Query-Answering"><a href="#Path-Query-Answering" class="headerlink" title="Path Query Answering"></a>Path Query Answering</h3><p>这个任务与两实体间需要多跳, 且有一个实体被Mask掉情况相同. 作者使用Random Walk从WordNet和FreeBase生成数据. 最多通过连续$k$ 跳从头实体$s$ 到达尾实体$o$. 在本任务中, 限制输入序列长为7, 即两实体间最多有5跳.</p><h4 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h4><p>作者将正确的答案$o$ 和不正确的答案$\mathcal{N}$ 的概率分布按降序排序, 并计算在$o$ 后不正确的答案占结果的总比例, 记为MQ, 范围是0到1, 1为最佳. 是按结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/coke6.jpg" style="zoom: 33%;" /><p>关于问答相关的内容还不是很了解. 但能看到随着Path长度的增加, 性能逐渐提升. 到$k\leq3$ 时, 性能就已经全面领先于作者列出的方法了. 在本任务中CoKE作者证明了具有多跳推理能力.</p><h4 id="Further-Analysis"><a href="#Further-Analysis" class="headerlink" title="Further Analysis"></a>Further Analysis</h4><p>作者想检测CoKE是否不但拥有多跳推理能力, 还同时经过多跳推理而强化了单跳推理能力. 作者设计了一个实验, 在训练CoKE时使用不同路径长的数据, 但在测试时仅让CoKE预测三元组(长度为1). 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/coke7.jpg" style="zoom: 33%;" /><p>结果显示, CoKE随着训练时可使用的数据路径长的增加, 单跳推理能力也会增加. 证明了<strong>多跳推理的训练有利于单跳推理能力的提升</strong>.</p><h3 id="Visual-Illustration"><a href="#Visual-Illustration" class="headerlink" title="Visual Illustration"></a>Visual Illustration</h3><p>作者通过T - SNE探究了CoKE对上下文语境利用的情况.</p><p>从FB15k中以实体<code>TheKingsSpeech</code>为例, 将所有三元组收集起来. 将在不同语境下的<code>TheKingsSpeech</code> Mask掉(无论是头还是尾), 然后用已经训练好的CoKE获取它的Final Hidden State $\mathbf{h}_n^L$, 再用<strong>T - SNE</strong>降维可视化.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/coke8.jpg" style="zoom: 50%;" /><p>先看左图, <code>TheKingsSpeech</code>的表示形式在不同的关系之间表示是不同的, 大概有几种不同的关系. 作者不同关系使用不同颜色区分. 例如<code>award winning work/award winner</code> 和<code>award nominated work/award nominee</code>具有<strong>几乎相同的表示</strong>, 都聚在左上角. 这说明CoKE具有了结合上下文信息的能力.</p><p>另外, 对于顺关系$(s, r, o)$的逆关系$\left(o, r^{-1}, s\right)$, 例如<code>film/genre</code>和<code>film genre/films in this genre</code>, 因为<code>TheKingsSpeech</code>在<strong>顺逆关系</strong>中从属于<strong>不同的头尾位置</strong>, 但是它们居然获得了相同的表示, 说明CoKE对<strong>逆关系</strong>能够很好的识别.</p><p>右图也是CoKE对顺逆关系学习的探究, 这回作者直接将每组互逆关系对用相同的颜色和正三角倒三角表示, 结果显示几乎所有相同颜色的三角都聚到了一起. 但右侧有两堆不同颜色的三角聚到了一起, 分别是<code>JoelCoen</code>和<code>EthanCoen</code>, 这两人被称为科恩兄弟, 共同创作. 这表明了CoKE对<strong>关系的区分粒度</strong>.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>CoKE的思路其实很不错, 考虑到了已出现两个实体之间的单跳和多跳关系, 采用了目前效果最好的Transformer Encoder(BERT)作为特征抽取器.</p><p>同时该思考, 如果有某些暗含的背景因素没有出现在两实体中, 是否有可能对这两实体或关系的表示产生影响? 比如我们得到的路径是$A \rightarrow r_{1} \rightarrow \cdots \rightarrow r_{k} \rightarrow B$, 但是有某个隐含的背景条件或是实体有$C\rightarrow A$, 能对$A, B$的表示产生影响, 这种情况应该如何处理? 其实已经有方法给出了这个问题的解决方案.</p><p>值得一提的是, 作者在<strong>探究实验</strong>上的设计是很有意思的, 首先是想到了证明多跳能强化单跳的推理能力, 其次就是使用T - SNE来可视化探究CoKE对上下文的利用和识别能力.</p><p>最后, 如果对我提出的思考感兴趣, 或是对结合上下文Embedding的相关内容感兴趣, 欢迎阅读我写的<a href="https://adaning.github.io/posts/28100.html">CoLAKE: Contextualized Language and Knowledge Embedding</a>, 我认为CoLAKE比CoKE要进步了不少.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ConvKB: A Novel Embedding Model for KB Completion Based on CNN</title>
      <link href="/posts/52280.html"/>
      <url>/posts/52280.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>ConvE</li><li>Conv1d</li></ul><p><strong>2021.03.15</strong>: 指出权重共享并没有出现在源码中.</p></blockquote><h1 id="A-Novel-Embedding-Model-for-Knowledge-Base-Completion-Based-on-Convolutional-Neural-Network"><a href="#A-Novel-Embedding-Model-for-Knowledge-Base-Completion-Based-on-Convolutional-Neural-Network" class="headerlink" title="A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network"></a>A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network</h1><p>本文是论文<a href="https://www.aclweb.org/anthology/N18-2053/" target="_blank" rel="noopener">A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network</a>的阅读笔记和个人理解. 本文篇幅比较短, 模型比较简单.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>作者认为ConvE只考虑了局部不同维度的关系, 而没有考虑全局相同维度的关系, 这就需要一种方式在实体和关系之间捕获全局关系和过渡特性.</p><p>我个人的理解就是, 一般KGE都是用Link Prediction去训练模型, 只使用了头实体和关系的信息, 去预测尾实体. 但ConvKB能够同时使用头实体, 尾实体, 关系的信息.</p><h2 id="ConvKB"><a href="#ConvKB" class="headerlink" title="ConvKB"></a>ConvKB</h2><p>ConvKB的方法是非常简单的.</p><p>假设头实体$h$, 关系$r$, 尾实体$t$, 先将$(h, r, t)$ 通过$k$ 维的Embedding, 得到$\left(\boldsymbol{v}_{h}, \boldsymbol{v}_{r}, \boldsymbol{v}_{t}\right)$, 并将其转为矩阵$\boldsymbol{A}=\left[\boldsymbol{v}_{h}, \boldsymbol{v}_{r}, \boldsymbol{v}_{t}\right] \in \mathbb{R}^{k\times 3}$. 作者使用多个$1\times 3$ 的Conv1d来捕捉三者<strong>同个维度上的全局信息</strong>, 其卷积核为$\omega$, 通过卷积运算能产生特征图$\boldsymbol{v}=\left[v_{1}, v_{2}, \ldots, v_{k}\right] \in \mathbb{R}^{k}$:<br>$$<br>v_{i}=g\left(\boldsymbol{\omega} \cdot \boldsymbol{A}_{i,:}+b\right)<br>$$<br>其中$b$ 为偏置项, $g$ 为激活函数, 这里用的是ReLU.</p><p>设$\Omega$ 为所有卷积核的集合, $\tau$ 为卷积核的数量, 则有$\tau =|\Omega|$. 那么将所有的卷积核扫描完后, 能产生$\tau$ 个大小为$k \times 1$ 的向量, 将他们Concat起来, 大小为$\mathbb{R}^{\tau k \times 1}$. 然后用一个权重向量$\mathbf{w}$ 来和Concat后的向量做点积, 得到分数.</p><p>看图总结一下, 图中取的$\tau=3$, $k=4$:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/convkb1.jpg" style="zoom: 50%;" /><h3 id="Score-Function"><a href="#Score-Function" class="headerlink" title="Score Function"></a>Score Function</h3><p>打分函数为:<br>$$<br>f(h, r, t)=\operatorname{concat}\left(g\left(\left[\boldsymbol{v}_{h}, \boldsymbol{v}_{r}, \boldsymbol{v}_{t}\right] \ast \boldsymbol{\Omega}\right)\right) \cdot \mathbf{w}<br>$$<br>其中$\ast$ 代表卷积操作, $\Omega$ 和$\mathbf{w}$ 是<strong>参数共享</strong>, $\mathbf{w} \in \mathbb{R}^{\tau k \times 1}$. <strong>打分函数的输出越小证明三元组越可靠</strong>.</p><blockquote><p>虽然作者说可以”权重共享”, 但思考一下维度变换其实根本没法这么做(除非$k=3$).</p><p>并且作者<a href="https://github.com/daiquocnguyen/ConvKB" target="_blank" rel="noopener">开源代码</a>也没有关于权重共享的任何内容.</p></blockquote><p>作者还提到, 如果只使用一个卷积核$\omega=[1, 1, -1]$, 固定$b=0$, 令激活函数$g(x)=|x|$ 或 $g(x)=x^2$, 并使$\mathbf{w} = 1$, <strong>ConvKB将退化为TransE</strong>, 即$|h+r-t|$.</p><h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><p>损失函数为:<br>$$<br>\begin{array}{c}<br>\mathcal{L}=\sum_{(h, r, t) \in\left\{\mathcal{G} \cup \mathcal{G}^{\prime}\right\}} \log \left(1+\exp \left(l_{(h, r, t)} \cdot f(h, r, t)\right)\right)<br>+\frac{\lambda}{2}||\mathbf{w}||_{2}^{2}<br>\end{array}<br>$$<br>$l_{(h,r,t)}$ 是一个符号:<br>$$<br>\ l_{(h, r, t)}=\left\{\begin{array}{l}<br>1 \text { for }(h, r, t) \in \mathcal{G} \\<br>-1 \text { for }(h, r, t) \in \mathcal{G}^{\prime}<br>\end{array}\right.<br>$$<br>$\mathcal{G}$ 代表知识图谱, $\mathcal{G’}$ 代表从知识图谱中生成的<strong>无效</strong>三元组集合.</p><p>损失函数大致的意思是, 模型能否根据ConvKB来区分<strong>三元组是否正确</strong>. 所以其实它的本质是一个<strong>二分类交叉熵</strong>.</p><p>后面加了正则项防止过拟合.</p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>作者把ConvKB在WN18RR和FB15k-237上做了<strong>链接预测</strong>的实验, 结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/convkb2.jpg" style="zoom: 50%;" /><p>能看到, 相比于ConvE, ConvKB有相当大的提升, 作者还吐槽了ConvE在某些指标上甚至还没有TransE好.</p><p>一些超参数的设置细节不再列举了, 原文说的比较详细.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>ConvKB没有使用Reshape, 不像ConvE那样提取Embedding间的局部信息, 而是通过Conv1d来提取包含尾实体的全局信息. 但实验列出的指标没有Hit@1和Hit@3, 不知道ConvKB在这两个指标上是否表现得也像Hit@10一样好. </p><blockquote><p>另外, 我认为在$(h, r, t)$ 的每个相同Dimension上不一定有实质的联系, 可能是交错的, 所以可以考虑用Permutation之类的方式来尽可能消除这种可能性. 或者通过某些方式让CNN提取交错Dimension位置上的特征.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>InteractE: Improving Convolution-based KGE by Increasing Feature Interactions</title>
      <link href="/posts/30287.html"/>
      <url>/posts/30287.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>ConvE</li><li>Depth - wise Convolution</li></ul><p><strong>2020.11.14</strong>: 对实验进行部分补充.</p></blockquote><h1 id="InteractE-Improving-Convolution-based-Knowledge-Graph-Embeddings-by-Increasing-Feature-Interactions"><a href="#InteractE-Improving-Convolution-based-Knowledge-Graph-Embeddings-by-Increasing-Feature-Interactions" class="headerlink" title="InteractE: Improving Convolution-based Knowledge Graph Embeddings by Increasing Feature Interactions"></a>InteractE: Improving Convolution-based Knowledge Graph Embeddings by Increasing Feature Interactions</h1><p>本文章是论文<a href="http://arxiv.org/abs/1911.00219" target="_blank" rel="noopener">InteractE: Improving Convolution-based Knowledge Graph Embeddings by Increasing Feature Interactions</a>的阅读笔记和个人理解.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>Link Prediction能够根据已有的事实对缺失的链接进行推断, 依此缓解<strong>KG不完整</strong>的问题.</p><p>虽然已经有基于卷积的Embedding方法, 例如ConvE, ConvKB等, 但作者认为虽然卷积增加了头实体和关系之间的Embedding交互, 但<strong>交互次数</strong>仍然不够, 因此抽取特征的能力被<strong>限制</strong>了.</p><p>从名字上就能看出来, InteractE的主要思路就是通过一切手段让<strong>交互最大化</strong>, 从而实现更好的表达.</p><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><h3 id="Reshape-Function"><a href="#Reshape-Function" class="headerlink" title="Reshape Function"></a>Reshape Function</h3><p>下面定义头实体和关系的$d$ 维Embedding分别为$\boldsymbol{e}_{s}=\left(a_{1}, \ldots, a_{d}\right), \boldsymbol{e}_{r}=\left(b_{1}, \ldots, b_{d}\right)$, 参数矩阵为$w$ 的卷积核大小为$k$, 并假设输入的矩阵$N$ 大小为$m \times n=2d$, 那么当卷积核对矩阵进行扫描时覆盖的区域$M_k$ 就应该有$M_k \subseteq N$, 且$M_k = N_{i:i+k, j:j+k}$. 我们将$\phi$ 记为Reshape Function, $\phi\left(\boldsymbol{e}_{s}, \boldsymbol{e}_{r}\right)$ 就代表了头实体和关系的Embedding后Reshape.</p><p>作者汇总了三种Reshape方法, 都非常好理解:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/interacte2.jpg" style="zoom: 50%;" /><ul><li>Stack: 就是简单的<strong>堆叠</strong>, 这种方法被ConvE中使用, 记为$\phi_{s t k}$.</li><li>Alternate: 将头实体和关系的Embedding<strong>按行交错</strong>排列, 记为$\phi_{a l t}^{\tau}$.</li><li><strong>Chequer(本文使用)</strong>: 像棋盘一样的对二者的Embedding按元素交错排列, 即相邻的元素一定与自己的类型不同, 记为$\phi_{c h k}$.</li></ul><h3 id="Interaction"><a href="#Interaction" class="headerlink" title="Interaction"></a>Interaction</h3><p>作者给出了交互精确的定义. 假设$M_{k} \subseteq \phi\left(\boldsymbol{e}_{s}, \boldsymbol{e}_{r}\right)$, $x,y$ 是$M_k$ 中的两个元素, 那么就能根据三元组$\left(x, y, M_{k}\right)$ 给出交互的定义. </p><p>如果$x,y$ 分别来自于$\boldsymbol{e}_{s},\boldsymbol{e}_{r}$, 则称该交互是<strong>异质</strong>(heterogeneous)的, 记为$\mathcal{N}_{h e t}(\phi, k)$. 反之, 如果来自于相同的Embedding, 则称该交互是<strong>同质</strong>(homogeneous)的, 记为$\mathcal{N}_{\text {homo}}(\phi, k)$.</p><p>对于Reshape Function $\phi$, 卷积核大小为$k$, 其中的元素交互次数定义为$\mathcal{N}(\phi, k)$, 如果有Padding, 可以将$\phi$ 替换为加上Padding的Reshape Function $\Omega(\phi)$.</p><p>下面用一个例子来说明交互的定义. 在一次$3\times 3$ 的卷积中, 子矩阵为$M_3$, 有5个元素来自$\boldsymbol{e}_{s}$, 4个元素来自$\boldsymbol{e}_{r}$. 那么异质交互的次数为:<br>$$<br>\mathcal{N}_{h e t}=2(5 \times 4)=40<br>$$<br>其中, $5\times 4$是来自$\boldsymbol{e}_{s}$ 和来自$\boldsymbol{e}_{r}$ 元素的组合次数, 前面的2代表每次组合相对两个元素分别有一次交互.</p><p>同理, 同质交互次数为:<br>$$<br>\mathcal{N}_{\text {homo}}=2\left[\left(\begin{array}{l}<br>5 \\<br>2<br>\end{array}\right)+\left(\begin{array}{l}<br>4 \\<br>2<br>\end{array}\right)\right]=32<br>$$<br>同质交互和异质交互的和是一个常数:<br>$$<br>\mathcal{N}_{h e t}(\phi, k)+\mathcal{N}_{h o m o}(\phi, k)=2\left(\begin{array}{c}<br>k^{2} \\<br>2<br>\end{array}\right)<br>$$</p><h2 id="InteractE"><a href="#InteractE" class="headerlink" title="InteractE"></a>InteractE</h2><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/interacte1.jpg" style="zoom: 50%;" /><p>在ConvE的基础上, InteractE基于三种方式来<strong>最大化</strong>头实体与关系Embedding间的交互次数:</p><ul><li>Feature Permutation</li><li>Checked Reshaping</li><li>Circular Convolution</li></ul><h3 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h3><h4 id="Feature-Permutation"><a href="#Feature-Permutation" class="headerlink" title="Feature Permutation"></a>Feature Permutation</h4><p>InteractE使用<strong>随机排列</strong>的方式对头实体Embedding $e_s$ 和关系Embedding $e_r$ 打乱. 如果随机打乱$t$ 次, 那么结果集为$\mathcal{P}_{t}=\left[\left(e_{s}^{1}, e_{r}^{1}\right) ; \ldots ;\left(e_{s}^{t}, e_{r}^{t}\right)\right]$, 在Reshape后只是因为随机排列导致不同排列中对应位置的元素不同, 但仍然保证相邻两元素的<strong>类型不同</strong>, 记为$\phi\left(e_{s}^{i}, e_{r}^{i}\right)$.</p><p>通过多次随机排列, 能保证对Embedding上的每个Dim都是公平的.</p><h4 id="Checkered-Reshaping"><a href="#Checkered-Reshaping" class="headerlink" title="Checkered Reshaping"></a>Checkered Reshaping</h4><p>$\phi_{chk}(\cdot)$ 能<strong>最大化异质交互</strong>次数, 我们对所有随机排列后的Embedding进行Reshape, 即:<br>$$<br>\phi\left(\mathcal{P}_{t}\right)=\left[\phi\left(e_{s}^{1}, e_{r}^{1}\right) ; \ldots ; \phi\left(e_{s}^{t}, e_{r}^{t}\right)\right]<br>$$</p><h4 id="Circular-Convolution"><a href="#Circular-Convolution" class="headerlink" title="Circular Convolution"></a>Circular Convolution</h4><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/interacte3.jpg" style="zoom: 50%;" /><p>其实从图上来看, 循环卷积不难理解, 只是当充分的利用了Padding的区域, 不再将其填充为0, 而是按照循环卷积(右图)的方式对其进行循环卷积的Padding, 利用其他的位置对原来填充0的位置进行有效的填补. 为了更好地描述该过程, 写出两种卷积Padding的矩阵, 标准卷积Padding(左), 循环卷积Padding(右):<br>$$<br>\begin{array}{cc}<br>\begin{bmatrix}<br>0 &amp; 0 &amp; 0 \\<br>0 &amp; x_{11} &amp; x_{12} \\<br>0 &amp; x_{21} &amp; x_{22}<br>\end{bmatrix} &amp;<br>\begin{bmatrix}<br>x_{44} &amp; x_{41} &amp; x_{42} \\<br>x_{14} &amp; x_{11} &amp; x_{12} \\<br>x_{24} &amp; x_{21} &amp; x_{22}<br>\end{bmatrix}<br>\\<br>\text{Standard Conv} &amp; \text{Circular Conv}<br>\end{array}<br>$$<br>若写出循环卷积其数学形式, 如下:<br>$$<br>[\boldsymbol{I} \star \boldsymbol{w}]_{p, q}=\sum_{i=-\lfloor k / 2\rfloor}^{\lfloor k / 2\rfloor} \sum_{j=-\lfloor k / 2\rfloor}^{\lfloor k / 2\rfloor} \boldsymbol{I}_{[p-i]_{m},[q-j]_{n}} \boldsymbol{w}_{i, j}<br>$$</p><p>在做循环卷积的时候, 使用的是Depth - wise Convolution. 也就是将随机排列Reshape后的Tensor集合$\phi\left(\mathcal{P}_{t}\right)$ 在Channel维度上Stack起来, 然后对每个Channel<strong>单独</strong>进行卷积.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/depthwiseconv.jpg" style="zoom:50%;" /><p>作者在这里还强调, 直接引入Depth维度上的卷积核<strong>参数共享</strong>能取得更好的效果.</p><p>引入循环卷积, 进一步的增加了二者的交互性.</p><h4 id="Score-Function"><a href="#Score-Function" class="headerlink" title="Score Function"></a>Score Function</h4><p>打分函数如下:</p><p>$$<br>\psi(s, r, o)=g\left(\operatorname{vec}\left(f\left(\phi\left(\mathcal{P}_{k}\right) \circledast \boldsymbol{w}\right)\right) \boldsymbol{W}\right) \boldsymbol{e}_{o}<br>$$</p><p>其中$\circledast$ 代表循环卷积, $\operatorname{vec}(\cdot)$ 代表Concat(其实就是Flatten), $e_{o}$ 代表尾实体的Embedding, $\boldsymbol{W}$ 代表变换矩阵. $f$ 和$g$ 分别代表$\text{ReLU}$ 和$\sigma$ 函数. 在训练时使用标准的<strong>二分类交叉熵</strong>作为损失函数, 并使用<strong>标签平滑</strong>. </p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Performance-Comparison"><a href="#Performance-Comparison" class="headerlink" title="Performance Comparison"></a>Performance Comparison</h3><p>这部分的实验意在比较InteractE与现有方法的性能比较, 下面是InteractE在三个数据集上的Link Prediction效果:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/interacte4.jpg" style="zoom: 67%;" /><blockquote><p>有一件非常有意思的事情, InteractE与AcrE(Serial)表现出几乎相同的性能, 而且在WN18RR上的表现也不好. 但AcrE的参数量却远少于InteractE.</p><p>我个人猜测是AcrE中使用的空洞卷积在一定程度上能与InteractE中的棋盘Reshape和循环卷积等效.</p></blockquote><h3 id="Effect-of-Feature-Reshaping-and-Circular-Convolution"><a href="#Effect-of-Feature-Reshaping-and-Circular-Convolution" class="headerlink" title="Effect of Feature Reshaping and Circular Convolution"></a>Effect of Feature Reshaping and Circular Convolution</h3><p>作者列举了三种Reshape的方式, 并做了它们分别与标准卷积核循环卷积的性能对比实验:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/interacte5.jpg" style="zoom: 67%;" /><p>从实验结果能看出, 除了使用$\tau=1$ 的交替排列方式, 在其他所有对照组种, 循环卷积的性能均高于标准卷积. 并且, $\tau$ 被定义为是$e_s, e_r$ 间重复的交替次数, $\tau$ 越小, 异质交互次数就越多, 随着其减小, 效果明显提升. </p><h3 id="Effect-of-Feature-Permutations"><a href="#Effect-of-Feature-Permutations" class="headerlink" title="Effect of Feature Permutations"></a>Effect of Feature Permutations</h3><p>作者选用了多种Feature Permutation, 将它们在三种不同的数据集上对比:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/interacte6.jpg" style="zoom: 50%;" /><p>YAGO3-10的实体量要远大于FB15k-237和WN18RR. 当超过某个<strong>阈值</strong>时, 排列数量将不再影响性能, 甚至变得多余.</p><h3 id="Evaluation-on-different-Relation-Types"><a href="#Evaluation-on-different-Relation-Types" class="headerlink" title="Evaluation on different Relation Types"></a>Evaluation on different Relation Types</h3><p>作者对比了RotatE, ConvE, InteractE在不同类别上的头尾预测效果:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/interacte7.jpg" style="zoom: 50%;" /><p>RotatE更善于处理1-1的简单一些的关系, InteractE更善于捕捉1-N, N-N的复杂关系, 佐证了交互带来的作用.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>InteractE通过尽可能<strong>最大化</strong>头实体和关系Embedding的<strong>交互</strong>来获得更好的嵌入表示. 其他Embedding方法不同的是, InteractE从<strong>Permutation</strong>和<strong>Reshape</strong>两个独特的角度处理了Embedding的问题. </p><p>但不知道作者有没有考虑过其他的Encoding Model能进一步增强Embedding之间的交互, 或者是尝试使用更复杂的Reshape方式, 在卷积时扩大交互范围(相应的开销也会提高).</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> 循环卷积 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CoLAKE: Contextualized Language and Knowledge Embedding</title>
      <link href="/posts/28100.html"/>
      <url>/posts/28100.html</url>
      
        <content type="html"><![CDATA[<h1 id="CoLAKE-Contextualized-Language-and-Knowledge-Embedding"><a href="#CoLAKE-Contextualized-Language-and-Knowledge-Embedding" class="headerlink" title="CoLAKE: Contextualized Language and Knowledge Embedding"></a>CoLAKE: Contextualized Language and Knowledge Embedding</h1><blockquote><p>本文前置知识:</p><ul><li>BERT</li><li>Self - Attention</li></ul><p><strong>2020.11.11</strong>: 想通了CoLAKE在训练时最关键的部分.</p><p><strong>2020.11.22</strong>: 在读完KEPLER后, 重温一遍CoLAKE, 更新实验部分.</p></blockquote><p>本文是论文<a href="http://arxiv.org/abs/2010.00309" target="_blank" rel="noopener">CoLAKE: Contextualized Language and Knowledge Embedding</a>的阅读笔记和个人理解. 这篇论文的很多工作与<strong>KEPLER</strong>相似, 建议先阅读<a href="https://adaning.github.io/posts/52897.html">我对KEPLER的讲解</a>, 再来看CoLAKE.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>这两年关于KGE的PTM很火爆, 论文开头便指出了现在将知识注入的PTM的劣势, 这些现存的嵌入一般都是空洞的, 静态的, 不灵活的. 这些PTM都有一些共同的<strong>短板</strong>:</p><ul><li>实体嵌入是<strong>单独</strong>训练的, 然后再使用到PTM中, 导致<strong>知识嵌入</strong>和<strong>语言嵌入</strong>不是同时嵌入的, 即没有真正做到<strong>联合嵌入</strong>.</li><li>在做实体嵌入时, 很少能全部的捕捉到丰富的<strong>上下文</strong>信息, 导致模型的性能被预训练的实体嵌入所<strong>限制</strong>.</li><li>预训练的实体嵌入是<strong>静态</strong>的, 当知识图谱发生轻微变化时(例如添加一个新的实体), 需要<strong>重新训练</strong>.</li></ul><p>基于上述缺点, 作者提出了CoLAKE, 这是一种能够根据<strong>上下文</strong>, 实现<strong>语言</strong>和<strong>知识</strong>的<strong>联合嵌入</strong>的<strong>Masked Language Model</strong>.</p><h2 id="CoLAKE"><a href="#CoLAKE" class="headerlink" title="CoLAKE"></a>CoLAKE</h2><p>CoLAKE(<strong>Co</strong>ntextualized <strong>L</strong>anguage <strong>a</strong>nd <strong>K</strong>nowledge <strong>E</strong>mbedding), CoLAKE能根据知识上下文和语言上下文来<strong>动态</strong>的表示实体. 对于每个实体, 将该实体与知识相<strong>关联</strong>的部分作为子图, 视为该实体的上下文. CoLAKE能动态访问不同的常识, 根据<strong>背景知识</strong>来更好的帮助模型理解实体在上下文中的含义, 而并非只关注实体本身.</p><blockquote><p>题外话:</p><p>其实我第一次听CoLAKE这个名字是在前两天举办的YSSNLP上, 邱锡鹏老师在预训练模型的演讲里提的. 我当时感觉哈利波特那个图(下面第一张图就是)好像在哪见过, 后来确实在邱老师9月份演讲的PDF里找到了, 只不过没标模型的名字而已. </p><p>当时还有听众提出了一个问题: 外部知识应该如何引入影响来语义的呢?</p><p>邱老师虽然没给出具体的方案, 但指出了大致的一条思路: Token的表示可能会根据外部知识的<strong>影响</strong>或<strong>变化</strong>, 从而改变它的表示, 这样能获得更<strong>精确</strong>的语义表示.</p><p>读完这篇10月份发的论文才明白, 其实邱老师说的是CoLAKE.</p></blockquote><h3 id="Word-Knowledge-Graph"><a href="#Word-Knowledge-Graph" class="headerlink" title="Word - Knowledge Graph"></a>Word - Knowledge Graph</h3><p>WK Graph(Word - Knowledge Graph)是为了处理<strong>异构</strong>的<strong>语言</strong>和<strong>知识图谱</strong>而引入的, 这种结构能将它们<strong>统一</strong>在同一个数据结构下. 知识在知识图谱中的存储方式是<strong>三元组</strong>, 而语言的存储方式一般是<strong>无结构化的文本数据</strong>. Transformer的<strong>自注意力</strong>机制可以看做是基于单词的<strong>全连接图</strong>, 所以图是表示知识和语言更<strong>通用</strong>的结构.</p><h4 id="WK-Graph-Example"><a href="#WK-Graph-Example" class="headerlink" title="WK Graph Example"></a>WK Graph Example</h4><p>下面来举一个例子体会WK Graph的作用, 在本节先关注这张图片的左侧:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/colake1.jpg" style="zoom: 67%;" /><p>像ERNIE, KnowBERT之类的模型的实体嵌入和语言嵌入是<strong>半上下文联合</strong>的, 而CoLAKE是<strong>全上下文联合</strong>嵌入. 那么模型在CoLAKE中如何理解下面这两个句子呢?</p><blockquote><ol><li>Harry Potter points his wnad at Lord Voldemort.</li><li>“You have Lily’s hazel eyes”, he told Harry Potter.</li></ol></blockquote><p>CoLAKE能在KG中搜索<code>Harry Potter</code>相关的知识, 得到<code>(Harry Potter, enemy of, Lord Voldemort)</code>和<code>(Harry Potter, mother, Lily Poter)</code>这两个三元组在图中的表示. 然后用前者来帮助理解句子1, 后者帮助理解句子2. 这样就使得知识能够在不同的上下文中更灵活的运用.</p><p>右侧说明了Word - Knowledge Graph的结构. <strong>内圈</strong>是原文中多个单词<strong>全连接</strong>形成的<strong>Word Graph</strong>, 而<strong>外圈</strong>是经过知识扩展过的<strong>Knowledge Subgraph</strong>. 二者以相同的实体为桥接处, 将Word Graph和Knowledge Subraph<strong>拼接</strong>就形成了Word - Knowledge Graph.</p><h4 id="Graph-Construction"><a href="#Graph-Construction" class="headerlink" title="Graph Construction"></a>Graph Construction</h4><p>本节主要说明WK Graph是如何生成的.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/colake2.jpg" style="zoom:67%;" /><p>生成图的方式其实没有想象中的复杂. 我们先通过实体连接器将KG中能被找到的实体作为<strong>锚点</strong>(Anchor Nodes), 然后将基于KG延伸出的Subgraph在这个锚点的基础上进行<strong>扩展</strong>. 锚点在图中用红色的虚线外轮廓标识. 延伸的过程有利于将Knowledge Subgraph和Word Graph的实体嵌入进同一个空间, 这也就是一直在强调的<strong>联合嵌入</strong>.</p><p>在Word Graph中有三种类型的节点, 分别是Word Nodes, Entity Nodes, Relation Nodes, 也就分别对应着图中黄色, 蓝色, 绿色的节点.</p><p>注意观察图中的节点编号, 在锚点的Knowledge Subgraph中, 锚点连接的实体和关系节点的编号是<strong>依赖于锚点</strong>的, 这种方式也称为<strong>Soft - Position Index</strong>, 在之后还会提到.</p><p>CoLAKE只使用了与锚点相邻的<strong>15</strong>个随机关系和实体作为Subgraph, 然后并入WK Graph中.</p><h3 id="CoLAKE-Architecture"><a href="#CoLAKE-Architecture" class="headerlink" title="CoLAKE Architecture"></a>CoLAKE Architecture</h3><p>CoLAKE的整个结构基于BERT. 所以使用的Basic Blcok是Transformer Encoder. 如果对BERT不了解的建议参考我以前写的<code>&lt;ELMo, GPT, BERT&gt;</code>, 对Transformer不理解的请参照<code>&lt;Transformer精讲&gt;</code>, 看完后理解起来会好一些.</p><p>话不多说, 直接看结构图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/colake3.jpg" style="zoom:67%;" /><p>请观察CoLAKE与BERT的<strong>不同点</strong>. 并且这幅图画的非常严谨, 在右侧的Knowledge Graph并不是全连接的, 左侧的Word Graph是全连接的, Token Embedding的颜色与之前Word Graph颜色也是相对应的.</p><h4 id="Embedding-Layer"><a href="#Embedding-Layer" class="headerlink" title="Embedding Layer"></a>Embedding Layer</h4><p>CoLAKE对BERT的Embedding做了改动. 在BERT中采用的Encoding是Token Encoding, <strong>Segment Encoding</strong>, Position Encoding. 而CoLAKE中使用的是Token Encoding, <strong>Type Encoding</strong>, Position Encoding. </p><h5 id="Token-Embedding"><a href="#Token-Embedding" class="headerlink" title="Token Embedding"></a>Token Embedding</h5><p>对于Token的Embedding不必多说, CoLAKE与BERT都还是使用<strong>查表</strong>的方式, 但CoLAKE是单词, 实体, 关系<strong>分别查找</strong>:</p><ul><li>对于单词的Embedding, CoLAKE使用了<strong>BPE</strong>, 能减小词表, 这已经是一种常见的Subword技巧. </li><li>对于实体和关系, 我们分别建两张表, 直接学习实体和关系的<strong>独立</strong>表示.</li></ul><h5 id="Type-Encoding"><a href="#Type-Encoding" class="headerlink" title="Type Encoding"></a>Type Encoding</h5><p>CoLAKE不涉及到Segment的问题, 所以将Segment Encoding替换成了Type Encoding. 在Word Graph中有三种类型的节点, 分别是Word Nodes, Entity Nodes, Relation Nodes. 所以要对这三种类型<strong>分别</strong>加以编码.</p><h5 id="Position-Encoding"><a href="#Position-Encoding" class="headerlink" title="Position Encoding"></a>Position Encoding</h5><p>与BERT相比, CoLAKE多了Knowledge Subgraph, 所以使用了<strong>Soft - Position Index</strong>的方式对Knowledge Subgraph中的节点进行位置编码.</p><blockquote><p>Soft - Position Index: 允许<strong>重复</strong>的Position出现, 将以锚点作为头实体的三元组记为$&lt;h, r, t&gt;$, 若锚点头实体在句子中的位置为$x$, 则关系$r$ 位置记为$x+1$, 尾实体位置记为$x+2$. 这样能够保证三元组的<strong>位置连续</strong>.</p></blockquote><h4 id="Masked-Transformer-Encoder"><a href="#Masked-Transformer-Encoder" class="headerlink" title="Masked Transformer Encoder"></a>Masked Transformer Encoder</h4><p>对于图中的节点矩阵$\mathbf{X}$ , 附加<strong>Mask</strong>的注意力机制如下:<br>$$<br>\begin{aligned}<br>\mathbf{Q}, \mathbf{K}, \mathbf{V} &amp;=\mathbf{X} \mathbf{W}^{Q}, \mathbf{X} \mathbf{W}^{K}, \mathbf{X} \mathbf{W}^{V} \\<br>\mathbf{A} &amp;=\frac{\mathbf{Q} \mathbf{K}^{\top}}{\sqrt{d_{k}}} \\<br>\operatorname{Attn}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) &amp;=\operatorname{Softmax}(\mathbf{A}+\mathbf{M}) \mathbf{V}<br>\end{aligned}<br>$$<br>Mask矩阵$\mathbf{M}$ 可以通过以下方式求得:<br>$$<br>\mathbf{M}_{i j}=\left\{\begin{array}{ll}<br>\quad 0 &amp; \text { if } x_{i} \text { and } x_{j} \text { are connected } \\<br>-\inf &amp; \text { if } x_{i} \text { and } x_{j} \text { are disconnected}<br>\end{array}\right.<br>$$</p><blockquote><p>认真看一下, 这个Mask的作用可不是Transformer Decoder中的Mask, 它是取决于<strong>节点矩阵</strong>的, 只是为了能让节点只能看到<strong>邻近一跳</strong>的信息.</p></blockquote><h3 id="Pre-Training-Objective"><a href="#Pre-Training-Objective" class="headerlink" title="Pre - Training Objective"></a>Pre - Training Objective</h3><p>CoLAKE也是一个Masked Language Model, 所以它也是通过对句子中的Token随机Mask, 然后从词表中基于上下文将Mask的Token预测出来. 将BERT的Mask模式迁移到图中, 只是将Mask Token变为Mask Node, 我们仍然是对15%的Node进行随机选中, 但选中后的操作<strong>略有不同</strong>, 下面将BERT和CoLAKE做个对比:</p><table><thead><tr><th>概率</th><th>BERT</th><th>CoLAKE</th></tr></thead><tbody><tr><td>80%</td><td>将被选中的Token替换为<code>[Mask]</code></td><td>将被选中的Node替换为<code>[Mask]</code></td></tr><tr><td>10%</td><td>将被选中的Token替换为<strong>任意词</strong></td><td>将被选中的Node替换为<strong>与原节点同类的节点</strong></td></tr><tr><td>10%</td><td>不做任何替换</td><td>不做任何替换</td></tr></tbody></table><p>在WK Graph中有三大类节点, 被Mask后所对应的意义是不同的:</p><ul><li><p>Masking Word Nodes: 与BERT的情况是一致的, 但CoLAKE在预测Word时除了依赖上下文还能依赖<strong>知识</strong>做出预测, 因为锚点与Word是全连接的关系, 而锚点受到知识的影响.</p></li><li><p>Masking Entity Nodes: 如果被Mask的实体是<strong>锚点</strong>, 那么则依靠它的上下文进行预测. 如果不是锚点, 那么CoLAKE的目标就是KGE.</p></li><li><p>Masking Relation Nodes: 如果Mask的是两锚点之间的关系,  那么目标就与<strong>关系抽取</strong>一致. 否则, 目标就是预测两实体之间的关系, 这与传统的KGE方法相似.</p></li></ul><p>然而, 在预训练时预测被Mask的锚点可能比较简单, 模型很容易就能用外部知识而不是依赖上下文完成这个人物, 因此在预训练时会<strong>丢掉50%</strong>的锚点邻居.</p><blockquote><p>在Mask做训练时, 从Knowledge Embedding的角度来看待, CoLAKE的方式有点像<strong>多任务训练</strong>. 因为Mask了不同属性的节点会导致CoLAKE所利用的信息不同, 执行任务的初始条件也不同.</p><p>我开始对Mask掉锚点后仍然能够利用知识库中的内容表示疑惑, 我开始认为在不知道锚点的情况下, 知识库中所存在的知识应该是不能成功进行链接的. 后来我从另一个角度出发, 作为人类, <strong>我们常已知了句子中的上下文和锚点相关的知识(除去锚点本身), 我们完全可以根据其他与锚点相关的属性和实体来猜出锚点是什么</strong>, 这样就完全说得通了.</p></blockquote><h3 id="Model-Training"><a href="#Model-Training" class="headerlink" title="Model Training"></a>Model Training</h3><p>CoLAKE通过对三种不同类型的节点进行训练, 采用交叉熵作为损失函数. 但是因为知识图谱中的实体数量实在是太庞大了, 所以实现CoLAKE有两个问题:</p><ul><li><p>实体数量较大, 训练输入时几乎不可能在GPU上维护一个entity embedding矩阵, 作者给出的解决方案是把entity embedding放在CPU上, 把模型的其他部分放在多张GPU上, GPU上的训练进程从CPU中读出entity embedding同时向CPU写入对应的梯度, CPU上的进程根据收集的梯度对entity embedding进行异步地更新.</p></li><li><p>实体数量过于庞大导致Softmax十分耗时, 所以简单的采用<strong>负采样</strong>解决这个问题.</p></li></ul><p>该部分来自<a href="https://zhuanlan.zhihu.com/p/263012775" target="_blank" rel="noopener">CoLAKE: 同时预训文本和知识</a>. 问题1隐射出当前KG和NLP发展可能还是受算力的制约, 相较与CV, NLP需要更多的计算来表达信息.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="DataSet-and-Implementation-Details"><a href="#DataSet-and-Implementation-Details" class="headerlink" title="DataSet and Implementation Details"></a>DataSet and Implementation Details</h3><p>CoLAKE主要用了两个数据集:</p><ul><li>English Wikipedia.</li><li>Wikidata5M(出自<strong>KEPLER</strong>).</li></ul><p>CoLAKE使用英文的维基百科作为预训练数据, 使用了Hugging Face的Transformer来实现, 和RoBERTa一样使用BPE, 直接使用了<strong>RoBERTa</strong>的权重初始化, 在1e-4的学习率下只训练了一轮.</p><h3 id="Knowledge-Driven-Tasks"><a href="#Knowledge-Driven-Tasks" class="headerlink" title="Knowledge Driven Tasks"></a>Knowledge Driven Tasks</h3><p>在<strong>知识驱动</strong>型任务上, 作者主要将CoLAKE与同样的注入知识的PLM进行<strong>横向对比</strong>, 其次与不注入知识的RoBERTa和BERT<strong>纵向对比</strong>, CoLAKE表现相当不错:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/colake5.jpg" style="zoom: 25%;" /><p>左侧Open Entity对应的任务为实体分类, 右侧FewRel对应的任务为关系抽取.</p><h3 id="Knowledge-Probing"><a href="#Knowledge-Probing" class="headerlink" title="Knowledge Probing"></a>Knowledge Probing</h3><p>LAMA(<strong>LA</strong>nguage <strong>M</strong>odel <strong>A</strong>nalysis) probe任务的目的是定性的测量模型到底存储了多少<strong>常识</strong>. 作者希望通过LAMA来观察CoLAKE对知识的掌握程度, 为公平起见, 作者还对所有模型只使用词汇之间有交集的部分.通过对Mask掉的知识进行预测, 取得它们的P@1, 实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/colake6.jpg" style="zoom: 50%;" /><p>BERT的表现倒是很亮眼. 作者认为K - Adapter比CoLAKE在两个数据集上效果好的原因是基于RoBERTa的LARGE版本, 并且使用了LAMA的一部分数据进行训练. CoLAKE总体而言的表现比RoBERTa要好非常多.</p><blockquote><p>BERT比使用更多数据的RoBERTa效果好太多了, 这与提出LAMA的论文<a href="https://arxiv.org/abs/1909.01066" target="_blank" rel="noopener">Language Models as Knowledge Bases?</a>结论一致.</p></blockquote><h3 id="Language-Understanding-Tasks"><a href="#Language-Understanding-Tasks" class="headerlink" title="Language Understanding Tasks"></a>Language Understanding Tasks</h3><p>目前的许多研究表明, 注入知识后, PLM的NLU能力可能会<strong>退化</strong>. 作者在GLUE常用数据集上对RoBERTa, KEPLER, CoLAKE做了实验: </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/colake7.jpg" style="zoom: 50%;" /><p>从实验结果来看, CoLAKE并没有发生很严重NLU能力退化, 与KEPLER相比, 真的只是比RoBERTa差一点点, KEPLER就退化的比较严重.</p><blockquote><p>从这个实验来看, NLU能力是和<strong>语言知识</strong>相关的, 如果给模型灌输一些<strong>世界知识</strong>, 原来存储语言知识的部分可能会被<strong>干扰</strong>, 可能是引入了许多的噪声, 我们或许还没有很好的掌握如何运用这些知识.</p><p>我认为将KEPLER和CoLAKE放在一起对比, 一定是CoLAKE的WK Graph起到了某种作用, 能够保留模型对语言模型的理解能力. 沿着这个思路, <strong>图</strong>或<strong>网</strong>的结构是非常重要的. 如果对CoLAKE的WK Graph进一步改进, 或许能够提升模型的NLU能力.</p></blockquote><h3 id="Word-Knowledge-Graph-Completion"><a href="#Word-Knowledge-Graph-Completion" class="headerlink" title="Word - Knowledge Graph Completion"></a>Word - Knowledge Graph Completion</h3><p>因为CoLAKE加入了 Word - Knowledge Graph的结构, 它本质上已经变为了一个预训练好的GNN. 作者希望利用这个特性, 来测试CoLAKE对结构和语义特征的建模能力. 只要在FewRel上做关系抽取, 就能使其对关系进行补全.</p><p>与KEPLER相同, 这里作者给出了两种设置:</p><ul><li><strong>Transductive setting</strong>: 对于每个样本, 两个实体$h, t$, 和它们的关系$r$, 可能分别在训练, 验证, 还是测试中出现过. 但它们的整体表示三元组$(h, r, t)$ 却没有在训练数据中出现.</li><li><strong>Inductive setting</strong>: 对于每个样本, 至少有一个实体在训练阶段是模型没见过的. 这更考验模型的推断能力. 即将两个实体中至少一个在训练阶段没见过的实体进行Mask, 然后利用其邻居节点来预测该节点的表示.</li></ul><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/colake4.jpg" style="zoom: 50%;" /><p>实验结果如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/colake8.jpg" style="zoom: 25%;" /><p>CoLAKE无论在Transductive Setting还是Inductive Setting上都比Baseline强大非常多.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>CoLAKE有如下特点(或说主要贡献):</p><ul><li>它是一个<strong>Masked Language Model</strong>, 并能将上下文知识表示和上下文语言表示<strong>联合嵌入</strong>.</li><li>由于<strong>Word - Knowledge Graph</strong>的存在, 它能够轻松地将<strong>异构</strong>的知识信息和语言信息<strong>融合</strong>.</li><li>因为它本质上是一个预训练的<strong>图神经网络</strong>, 所以具有结构感知能力, 并且易于<strong>扩展</strong>.</li></ul><p>CoLAKE真的是和我理想中将<strong>外部知识注入PTM</strong>的方式非常相似了.</p><p>CoLAKE提供了一个统一的数据结构. 这样非常有利于将来其他形式的数据注入到其中, 当然注入的方式可能是一个值得研究的问题.</p><p>顺带一提, 从这些Knowledge Enriched PTM来看, BERT非常受大家的青睐, 说明BERT与XLNet相比更为<strong>简洁</strong>, 容易被大家所理解和接受.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BERT </tag>
            
            <tag> KGE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AcrE: Atrous Convolution and Residual Embedding</title>
      <link href="/posts/59193.html"/>
      <url>/posts/59193.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识: </p><ul><li>膨胀卷积(空洞卷积)</li><li>残差连接</li></ul></blockquote><h1 id="Knowledge-Graph-Embedding-with-Atrous-Convolution-and-Residual-Learning"><a href="#Knowledge-Graph-Embedding-with-Atrous-Convolution-and-Residual-Learning" class="headerlink" title="Knowledge Graph Embedding with Atrous Convolution and Residual Learning"></a>Knowledge Graph Embedding with Atrous Convolution and Residual Learning</h1><p>本文是论文<a href="https://arxiv.org/abs/2010.12121" target="_blank" rel="noopener">Knowledge Graph Embedding with Atrous Convolution and Residual Learning</a>的阅读笔记和个人理解. 由Pytorch实现的源代码已经放到上<a href="https://github.com/neukg/AcrE" target="_blank" rel="noopener">Github</a>, 这是NEU KG组的论文! 刚发我就看完了. 该论文已经被<strong>COLING</strong>收录. 该论文是一篇KGE方向的论文, 用极简结构实现了非常好的性能, 并在多个常用数据集上SOTA.</p><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>在当今各类KGE模型大红大紫的情况下, 模型的<strong>复杂度</strong>和<strong>表达能力</strong>得不到很好的<strong>权衡</strong>. 最近流行的例如基于深度神经网络的嵌入模型, 基于图神经网络的嵌入模型, 都有非常高的<strong>耗时</strong>和模型<strong>复杂度</strong>, 导致不能在一些实时的场景下灵活运用. AcrE的目标就是实现<strong>简单</strong>, <strong>高效</strong>的知识嵌入, 同时兼具了参数量少, 计算量低的特征.</p><h2 id="ConvE"><a href="#ConvE" class="headerlink" title="ConvE"></a>ConvE</h2><p>本节作为背景知识为AcrE铺垫, 取自<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/17366/15884" target="_blank" rel="noopener">Convolutional 2D Knowledge Graph Embeddings</a>. 主要介绍KGE中的ConvE方法. ConvE只不过是将卷积使用在了KGE上, 有卷积基础的应该能够猜到使用方法, 所以不会细说.</p><p>得益于CNN的<strong>权重共享</strong>, 参数量非常少, 因此很高效, 并且很简单, 注意这个特点.</p><h3 id="1D-Convolution-VS-2D-Convolution"><a href="#1D-Convolution-VS-2D-Convolution" class="headerlink" title="1D Convolution VS 2D Convolution"></a>1D Convolution VS 2D Convolution</h3><p>作者指出, 相较于1维卷积, 2维卷积有更强的<strong>表达能力</strong>(其实从直觉来说也是这样).</p><p>在做1维卷积时, 卷积核最多只能与左侧或右侧离得比较近的元素<strong>交互</strong>:<br>$$<br>\left(\begin{array}{lll}<br>\left.\left[\begin{array}{lll}<br>a &amp; a &amp; a<br>\end{array}\right] ;\left[\begin{array}{lll}<br>b &amp; b &amp; b<br>\end{array}\right]\right)=\left[\begin{array}{llllll}<br>a &amp; a &amp; a &amp; b &amp; b &amp; b<br>\end{array}\right]<br>\end{array}\right.<br>$$<br>但2维卷积不一样, 除了能够与邻近的左右元素交互, 还能与上下元素进行交互:<br>$$<br>\left(\left[\begin{array}{lll}<br>a &amp; a &amp; a \\<br>a &amp; a &amp; a<br>\end{array}\right] ;\left[\begin{array}{lll}<br>b &amp; b &amp; b \\<br>b &amp; b &amp; b<br>\end{array}\right]\right)=\left[\begin{array}{lll}<br>a &amp; a &amp; a \\<br>a &amp; a &amp; a \\<br>b &amp; b &amp; b \\<br>b &amp; b &amp; b<br>\end{array}\right]<br>$$<br>如果两种元素代表的意义不同, 那么交换它们的拼接方式还能进一步的<strong>提升</strong>交互次数:<br>$$<br>\left[\begin{array}{lll}<br>a &amp; a &amp; a \\<br>b &amp; b &amp; b \\<br>a &amp; a &amp; a \\<br>b &amp; b &amp; b<br>\end{array}\right]<br>$$<br>由于交换了Concat方式, a和b交错, 能够实现更多次交互.</p><h3 id="ConvE-Architecture"><a href="#ConvE-Architecture" class="headerlink" title="ConvE Architecture"></a>ConvE Architecture</h3><p>直接看图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/conve1.jpg" style="zoom:67%;" /><p>整体流程也是非常简单, 将头实体和关系先Embedding, 然后再Reshape到一个合适的尺寸, 然后就用卷积来提取特征, 用全连接将其投影到与Embedding大小相同的隐空间中, 最后将隐空间的映射和尾实体的Embedding做相似度比较.</p><p>按照描述, 打分函数为:<br>$$<br>\psi_{r}\left(\mathbf{e}_{s}, \mathbf{e}_{o}\right)=f\left(\operatorname{vec}\left(f\left(\left[\overline{\mathbf{e}_{s}} ; \overline{\mathbf{r}_{r}}\right] \ast \omega\right)\right) \mathbf{W}\right) \mathbf{e}_{o}<br>$$<br>其中$\mathbf{e}_{s}, \mathbf{e}_{o}$ 分别代表头实体和尾实体的Embedding, $\overline{\mathbf{e}_{s}}, \overline{\mathbf{r}_{r}}$ 分别代表Reshape后的头实体和关系向量. $\omega$代表卷积核, $\mathbf{W}$ 代表投影矩阵. 这种方式通过内积来比较所获向量与尾实体的<strong>相似度</strong>, 越相似得分越高.</p><p>然后将该得分经过$\sigma$ 函数, 得到每个实体的概率:<br>$$<br>p=\sigma(\psi_{r}\left(\mathbf{e}_{s}, \mathbf{e}_{o}\right))<br>$$<br>使用二分类交叉熵进行优化:<br>$$<br>\mathcal{L}(p, t)=-\frac{1}{N} \sum_{i}\left(t_{i} \cdot \log \left(p_{i}\right)+\left(1-t_{i}\right) \cdot \log \left(1-p_{i}\right)\right)<br>$$<br>$t$ 是尾实体的独热编码向量. 除此外还加入了Dropout, BatchNorm, 标签平滑等防止过拟合的手段.</p><h2 id="AcrE"><a href="#AcrE" class="headerlink" title="AcrE"></a>AcrE</h2><p>AcrE(<strong>A</strong>trous <strong>C</strong>onvolution and <strong>R</strong>esidual <strong>E</strong>mbedding), 在ConvE的基础上主要做了两点改动, 也就是我们开头所需的前置知识, 空洞卷积和残差连接.</p><h3 id="Atrous-Convolution"><a href="#Atrous-Convolution" class="headerlink" title="Atrous Convolution"></a>Atrous Convolution</h3><p>Atrous Convolution也称<strong>空洞卷积</strong>或<strong>膨胀卷积</strong>. 由于空洞卷积只是作为一种CNN的变体形式的卷积, 因此它的机制在此不做过多讨论.</p><p> 空洞卷积相较于普通卷积, 有了一个新的参数”<strong>膨胀率</strong>“. 它指的是在卷积下, 每个卷积核元素之间的距离 - 1. 空洞卷积能在<strong>不引入额外参数</strong>的情况下获得更大的<strong>感受野</strong>, 如下图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/dilatedconv.jpg" style="zoom: 33%;" /><p>上图取自<a href="https://arxiv.org/abs/1511.07122" target="_blank" rel="noopener">Multi-Scale Context Aggregation by Dilated Convolutions</a>.</p><table><thead><tr><th align="center"></th><th align="center">最左侧</th><th align="center">正中间</th><th align="center">最右侧</th></tr></thead><tbody><tr><td align="center">卷积核</td><td align="center">(3, 3)</td><td align="center">(3, 3)</td><td align="center">(3, 3)</td></tr><tr><td align="center">感受野</td><td align="center">$3 \times 3$</td><td align="center">$7 \times 7$</td><td align="center">$15 \times 15$</td></tr><tr><td align="center">膨胀率</td><td align="center">1</td><td align="center">2</td><td align="center">4</td></tr></tbody></table><blockquote><p>空洞卷积常在语义分割和图像重建上使用, 与之相对的, 使用空洞卷积也会带来一些<strong>弊端</strong>, 如有需求请自行查询相关内容.</p></blockquote><p>在AcrE中, 它用来解决CNN在连续重复的下采样和池化而导致<strong>特征图分辨率丢失</strong>问题. 个人认为, 由于空洞卷积扩大了感受野, 能进一步增加实体和关系Embedding之间的交互, 以此将头实体和关系更<strong>紧密</strong>的联系到一起.</p><h3 id="Residual-Connection"><a href="#Residual-Connection" class="headerlink" title="Residual Connection"></a>Residual Connection</h3><p>引入残差连接主要的原因有两个:</p><ul><li><strong>梯度爆炸</strong>和<strong>梯度消失</strong>.</li><li>多次卷积导致的<strong>原始信息丢失</strong>问题.</li></ul><p>在ConvE中, 作者没有使用残差连接. 关于残差连接, 不懂可以去我之前写的<code>&lt;卷积神经网络发展史&gt;</code>中了解.</p><h3 id="AcrE-Architecture"><a href="#AcrE-Architecture" class="headerlink" title="AcrE Architecture"></a>AcrE Architecture</h3><p>AcrE有两种结构, 分别是<strong>串行</strong>(Serial)结构和<strong>并行</strong>(Parallel)结构. 无论哪种都使用了空洞卷积和残差连接. 但无论哪种结构都必须要有<strong>标准卷积</strong>的作用, 膨胀卷积虽然会提供更大的感受野, 但也有可能会因膨胀丧失局部信息.</p><h4 id="2D-Embedding-Representation"><a href="#2D-Embedding-Representation" class="headerlink" title="2D Embedding Representation"></a>2D Embedding Representation</h4><p>在ConvE中已经提到, 1D卷积没有2D卷积的表达能力强, 而且2D卷积能更多的增强头实体和关系间的交互, 所以都使用的是2D卷积. AcrE中, 首先要说一下KG中的三元组在2D中的表示方法, 可以视作是<strong>预处理</strong>. 对于三元组$&lt;h, r, t&gt;$, $\mathbf{h}, \mathbf{r}, \mathbf{t}$分别代表头实体, 关系和尾实体. </p><p>若$\tau$ 代表<strong>Reshape</strong>操作, $\mathbf{e}$ 代表实体的Embedding, $\mathbf{r}$ 代表关系的Embedding, $[;]$ 代表Concat, 则2D中的嵌入表示方法是$\tau([\mathbf{e};\mathbf{r}])$. </p><h4 id="Serial-AcrE-Model"><a href="#Serial-AcrE-Model" class="headerlink" title="Serial AcrE Model"></a>Serial AcrE Model</h4><p>在串行AcrE中, Embedding由一系列串行的卷积动作和最后的Flatten.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/acre1.jpg" alt=""></p><h5 id="Standard-Convolution-based-Learning"><a href="#Standard-Convolution-based-Learning" class="headerlink" title="Standard Convolution based Learning"></a>Standard Convolution based Learning</h5><p>在串行AcrE中, 将Reshape后的Embedding先经过标准卷积, 得到结果$\mathbf{C}_{0}$:<br>$$<br>\mathbf{C}_{0}^{i}=\omega_{0}^{i} \star \tau([\mathbf{e} ; \mathbf{r}])+\mathbf{b}_{0}^{i}<br>$$<br>其中$\star$ 代表卷及操作, $\omega_{0}^{i}$ 代表第i个卷积核, $b_0^i$ 代表第i个偏置. 假设有$F$ 个卷积核, 则$\mathbf{C}_{0}=\left[\mathbf{C}_{0}^{1}: \mathbf{C}_{0}^{2}: \mathbf{C}_{0}^{3}: \ldots: \mathbf{C}_{0}^{F}\right]$, 之后该卷积结果会经过一系列的空洞卷积. </p><blockquote><p>关于池化, 这里必须要提一下. </p><p>在CV任务中是卷积和池化交替使用, 因为其信息量通常是<strong>冗余</strong>的, 池化能够减小特征图的尺寸, 相当于求平均或最大值的效果, 池化有利于后面卷积抽取更关键的特征. </p><p>但目前在NLP相关的任务中, 信息非常复杂, 使用池化直接就导致了信息的<strong>损失</strong>. 并且, 因为我们是想将头实体和尾实体还有关系Embedding进一个同维度的空间下的, 如果使用池化会导致<strong>维度变化</strong>. </p><p>最后指出, 在实验中加入池化并没有很大程度的影响性能.</p></blockquote><h5 id="Atrous-Convolution-based-Learning"><a href="#Atrous-Convolution-based-Learning" class="headerlink" title="Atrous Convolution based Learning"></a>Atrous Convolution based Learning</h5><p>“空洞卷积大致是什么”这个问题在前面已经解决了, 其实就是在卷积核各元素之间插入一些小洞. 对于给定的输入向量$\mathbf{x}$, 长度为$K$的卷积核向量$\mathbf{w}$, 在空洞卷积下的输出$\mathbf{y}$ 由如下方式得来:<br>$$<br>y_{i}=\sum_{k=1}^{K} x_{i+l \times k} \times w_{k}<br>$$<br>其中$l$ 是膨胀率, 标准卷积的膨胀率为1.</p><p>在串行AcrE中, 卷积是一个接着一个<strong>串行</strong>的:<br>$$<br>\mathbf{C}_{\mathbf{t}}=\omega_{\mathbf{t}} \star \mathbf{C}_{t-1}+\mathbf{b}_{\mathbf{t}}<br>$$<br>$\mathbf{C}_{t-1}$ 代表上个卷积的输出结果, $\omega_{\mathbf{t}}$ 和$\mathbf{b}_{\mathbf{t}}$ 分别是卷积核和偏置向量.</p><h5 id="Feature-Vector-Generation"><a href="#Feature-Vector-Generation" class="headerlink" title="Feature Vector Generation"></a>Feature Vector Generation</h5><p>在串行的AcrE中, 不同种卷积一个接一个的执行, 每个卷积都能从之前抽取不同的实体和关系交互. 但越多的卷积使用, 就会导致越多的原始信息丢失, 这就导致了模型在学习时<strong>忘记</strong>了抽取出的特征到底有没有用. 同时, 为了缓解梯度爆炸和梯度消失, 在这使用残差连接来<strong>弥补</strong>原来丢失的信息. 在残差连接后, 紧接着使用<strong>ReLU</strong>做激活函数, 并做<strong>Flatten</strong>:<br>$$<br>\mathbf{o}=\text {Flatten}\left(\operatorname{ReLU}\left(\mathbf{C}_{T}+\tau([\mathbf{e} ; \mathbf{r}])\right)\right)<br>$$<br>其中$\mathbf{C}_{T}$ 是最后一个空洞卷积的输出, $T$ 是空洞卷积的次数.</p><h4 id="Parallel-AcrE-Model"><a href="#Parallel-AcrE-Model" class="headerlink" title="Parallel AcrE Model"></a>Parallel AcrE Model</h4><p>并行AcrE分别执行卷积后聚合.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/acre2.jpg" alt=""></p><p>注意, 在并行结构图中所示的<strong>卷积类型</strong>是不同的! 使用不同膨胀率的卷积核.</p><h5 id="Results-Integration"><a href="#Results-Integration" class="headerlink" title="Results Integration"></a>Results Integration</h5><p>与串行形态的AcrE不一样, 并行形态下, 2D Embedding分别用不同形式的卷积计算, 最后以<strong>某种形式</strong>聚合到一起.<br>$$<br>\mathbf{C}=\mathbf{C}_{\mathbf{0}} \oplus \mathbf{C}_{\mathbf{1}} \oplus \ldots \oplus \mathbf{C}_{\mathbf{T}}<br>$$<br>其中$\mathbf{C}_{\mathbf{0}}$ 是标准卷积, $\mathbf{C}_{\mathbf{i}}$ 是第i个空洞卷积. $\oplus$ 意味着某种聚合操作, 可以是基于Concat的操作, 也可以是基于加的操作等等. 在<strong>实验</strong>中会比较这两种方式的性能差异.</p><h5 id="Feature-Vector-Generation-1"><a href="#Feature-Vector-Generation-1" class="headerlink" title="Feature Vector Generation"></a>Feature Vector Generation</h5><p>与串行AcrE相仿, 将每个卷积结果聚合, 再加上残差的信息, 然后用ReLU, 再经过一次变换, 最后Flatten.<br>$$<br>\mathbf{c}=\text {Flatten}\left(\mathbf{W}_{\mathbf{1}} \operatorname{ReLU}(\mathbf{C}+\tau([\mathbf{e} ; \mathbf{r}]))\right)<br>$$<br>其中$\mathbf{W_1}$ 是变化矩阵, 这是比串行结构多出来的地方.</p><blockquote><p>我个人认为并行结构下的AcrE与Inception(详见<a href="https://adaning.github.io/posts/38085.html">卷积神经网络发展史</a>)中的<strong>多尺度</strong>是相同的道理, 通过不同的<strong>膨胀率</strong>实现了对实体和关系向量多个角度的抽取, 最后Concat到一起, 每个不同感受野的膨胀卷积都能提供不同的信息.</p></blockquote><h3 id="Score-Function-and-Loss-Function"><a href="#Score-Function-and-Loss-Function" class="headerlink" title="Score Function and Loss Function"></a>Score Function and Loss Function</h3><h4 id="Score-Function"><a href="#Score-Function" class="headerlink" title="Score Function"></a>Score Function</h4><p>打分函数其实与ConvE相似, 将输出向量先经过变换矩阵加上偏置, 再用点积比较与尾实体的<strong>相似度</strong>:<br>$$<br>\psi(h, r, t)=(\mathbf{o} \mathbf{W}+\mathbf{b}) \mathbf{t}^{\top}<br>$$<br>其中$\mathbf{W}, \mathbf{b}$ 分别是变化矩阵和偏置向量. 接着用$\sigma$ 函数获得所有候选实体的概率:<br>$$<br>p(t \mid h, r)=\operatorname{sigmoid}(\psi(h, r, t))<br>$$</p><h4 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h4><p>这点和ConvE一样, 使用了交叉熵作为损失函数:<br>$$<br>\mathcal{L}=-\frac{1}{N} \sum_{i=1}^{N}\left[t_{i} \log p\left(t_{i} \mid h, r\right)+\left(1-t_{i}\right) \log \left(1-p\left(t_{i} \mid h, r\right)\right)\right]<br>$$<br>其中$\mathbf{t}$ 是尾实体的独热编码. </p><h3 id="Other-Details"><a href="#Other-Details" class="headerlink" title="Other Details"></a>Other Details</h3><p>和ConvE一样, BatchNorm和卷积的向性比较好, 所以也使用了BatchNorm. 包括标签平滑啊之类的trick也都从ConvE沿用了下来. </p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>在实验的代码中, 只使用了三个卷积核. 有一个标准卷积核和两个不同膨胀率的膨胀卷积核.</p><h3 id="Settings"><a href="#Settings" class="headerlink" title="Settings"></a>Settings</h3><h4 id="DataSets"><a href="#DataSets" class="headerlink" title="DataSets"></a>DataSets</h4><p>我们将其他KGE方法在如下六个常用数据集中对比:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/acre3.jpg" style="zoom:50%;" /><h4 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h4><p>实验部分均采用<strong>Link Prediction</strong>的任务将其他KGE方法与AcrE进行对比. Link Prediction是基于已知的某端实体和关系对未知的另一端实体进行预测的任务. 即对于三元组$&lt;h,r,t&gt;$, 已知$&lt;h,r&gt;$ 预测$t$, 或已知$&lt;t,r&gt;$ 预测$h$. 并使用Hit@k, MRR作为指标进行对比.</p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><h4 id="Benchmark-Datasets-Experiment"><a href="#Benchmark-Datasets-Experiment" class="headerlink" title="Benchmark Datasets Experiment"></a>Benchmark Datasets Experiment</h4><p>在DB100k中的实验结果, 上面的内容全部取自SEEK的论文<a href="https://arxiv.org/pdf/2005.00856.pdf" target="_blank" rel="noopener">SEEK: Segmented Embedding of Knowledge Graphs</a>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/acre4.jpg" style="zoom:50%;" /><p>还有在其他五个数据集上的表现:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/acre5.jpg" style="zoom:67%;" /><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/acre6.jpg" style="zoom:67%;" /><p>只是在WN18RR上没有取得太好的效果, 在其他数据集上均SOTA或在平均性能上超过其他KGE方法.</p><p>实验的细节分了<strong>区分头尾</strong>的预测和<strong>按类别</strong>预测两种.</p><p>下面是<strong>区分头尾</strong>的预测:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/acre7.jpg" style="zoom:67%;" /><p>直接SOTA.</p><p>下面是<strong>按类别</strong>预测:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/acre8.jpg" style="zoom:67%;" /><h4 id="Ablation-Experiment"><a href="#Ablation-Experiment" class="headerlink" title="Ablation Experiment"></a>Ablation Experiment</h4><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/acre9.jpg" style="zoom:67%;" /><p>消融实验主要进行了如下工作:</p><ol><li>将<strong>串行</strong>和<strong>并行</strong>的AcrE对比, 发现一般情况下串行的AcrE都比并行AcrE性能要<strong>差</strong>.</li><li>将<strong>带有</strong>残差连接的AcrE和<strong>去掉</strong>残差连接的AcrE对比, 发现去掉后性能有<strong>明显下降</strong>, 说明了残差连接对AcrE非常重要.</li><li>将并行用Concat方式聚合的AcrE和并行用Add方式聚合的AcrE对比, 发现<strong>Concat</strong>效果要好一些.</li></ol><h4 id="Parameters-Comparison"><a href="#Parameters-Comparison" class="headerlink" title="Parameters Comparison"></a>Parameters Comparison</h4><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/acre10.jpg" style="zoom:67%;" /><p>ConvE的参数虽然比AcrE要少, 但在前面的对比实验中知道效果并没有AcrE好. AcrE的参数量要<strong>远少于</strong>除了ConvE的所有模型. 并且并行结构比串行结构的模型要稍大一些, 因为最后<strong>多一次变换</strong>.</p><p>另外, 调参的部分碍于篇幅问题, 没有在论文中贴出. 能够选择的参数范围非常小, 还是比较好调的, 参数在源码中都可以找到.</p><p>关于计算效率的优势, 因为涉及到其他训练的参数影响, 没法提供一个非常公平的环境去验证. 但作为ConvE的变体, 运行时间的复杂度应该与ConvE相仿.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>该论文简单易懂, 只看嵌入方式的<strong>图示</strong>和<strong>表格</strong>中给出的实验结果就能够把握文章重点. 尤其是最后的<strong>消融实验</strong>体现出了<strong>残差连接</strong>起到的作用. AcrE本身结构就简单到了<strong>令人发指</strong>的地步, 非常不可思议它甚至能达到一些高复杂度模型的结果… </p><p>在实验部分将那些没有在指定数据集上给出实验结果的论文的Embedding方法全都跑了一遍, 并做了<strong>大量的对比实验</strong>证明AcrE的有效性. 并且这是<strong>第一次</strong>将<strong>不同形式的卷积</strong>用到了<strong>KGE</strong>上的研究工作.</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KGE </tag>
            
            <tag> 空洞卷积 </tag>
            
            <tag> 残差连接 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer-XL与XLNet</title>
      <link href="/posts/35276.html"/>
      <url>/posts/35276.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文前置知识:</p><ul><li>Transformer(Masked Self - Attention和FFN)</li><li>BERT(与XLNet做对比)</li><li>Seq2Seq(AutoRegressive &amp; AutoEncoding)</li></ul><p><strong>2020.10.26</strong>: 更新了Transformer - XL的隐态更新维度分析.</p></blockquote><h1 id="Transformer-XL与XLNet"><a href="#Transformer-XL与XLNet" class="headerlink" title="Transformer - XL与XLNet"></a>Transformer - XL与XLNet</h1><p>本文是Transformer - XL和XLNet论文的阅读笔记和个人理解(实际上还包括Vanilla Transformer).</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>这两篇论文都没有从Transformer开始, 而是从<a href="https://arxiv.org/abs/1808.04444" target="_blank" rel="noopener">Character-Level Language Modeling with Deeper Self-Attention</a>开始, 该模型利用了Self - Attention在更深层次的语言模型上进行字符级建模, 因内容比单词级更长, 所以提出了直接将内容按照固定大小划分为<strong>Segment</strong>处理的方法. 该在文中被称为Vanilla Transformer, 被多次提及. 分段的方式伴随着多种缺点, Vanilla Transformer对Segment的划分没有进行任何优化. 而Transformer - XL对该模型存在的问题进行了优化, 而XLNet将Transformer - XL和BERT的优势结合到了一起.</p><h2 id="Vanilla-Transformer"><a href="#Vanilla-Transformer" class="headerlink" title="Vanilla Transformer"></a>Vanilla Transformer</h2><p> 中使用的Transformer被称为Vanilla Transformer(普通Transformer). 我们只需要知道它将一个Masked 多头Self - Attention和一个由两层全连接构成的FFN称为一个Transformer Layer, 然后堆叠多层Transformer Layer. 训练时使用<strong>语言模型</strong>的方式, 采用<strong>字符级</strong>数据. 它能够堆叠的非常深, 达到了64层, 其实它也是通过加深模型的方式来增长前文依赖能力的.</p><p>如果没有Transformer基础建议看我写过的<code>&lt;Transformer精讲&gt;</code>. 关于Vanilla Transformer, 其他具体内容如果想了解可以去看原论文.</p><p>在Transformer中通常需要设定一个固定的长度, 如果输入序列长小于固定长则进行<strong>填充</strong>. 因为使用字符级数据, 所以经常会出现大于序列长的情况. 它采用分段处理, 引入辅助函数训练. </p><p>与Transformer一样, 训练阶段, 模型能够一次处理<strong>整段数据</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerxl1.jpg" style="zoom: 50%;" /><p>评估阶段, 模型需要按照自回归每次前进一个Token:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerxl2.jpg" style="zoom: 67%;" /><h2 id="Transformer-XL"><a href="#Transformer-XL" class="headerlink" title="Transformer - XL"></a>Transformer - XL</h2><p>Transformer - XL出自论文<a href="https://arxiv.org/abs/1901.02860" target="_blank" rel="noopener">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a>. 其中XL指的是extra - long. 作者主要通过Segment - Level Recurrence的方式来缓解Transformer中的内容长度限定问题, 其衍生问题由Relative Positional Encoding解决. </p><p>在Vanilla Transformer中的分段导致了两个问题: </p><ol><li><strong>依赖</strong>的最大长度被固定长这个参数严格的限制了. </li><li>在划分时很有可能会<strong>截断上下文的含义</strong>, 导致不同的Segment之间考虑不到上下文联系, 这被称为上下文碎片化(Context framentation). </li></ol><p>不同段之间的梯度传播会被分段而<strong>截断</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerxl5.jpg" style="zoom: 50%;" /><p>这两个缺陷是Transformer - XL改进的动机. </p><h3 id="Segment-Level-Recurrence-with-State-Reuse"><a href="#Segment-Level-Recurrence-with-State-Reuse" class="headerlink" title="Segment - Level Recurrence with State Reuse"></a>Segment - Level Recurrence with State Reuse</h3><p>最首要的想法就是建立前后Segment之间的<strong>联系</strong>, 想办法将前面Segment的信息传到后面.</p><p>作者直接以Segment为单位进行操作, 每次都将<strong>同层</strong>上个Segment的隐态<strong>暂存</strong>下来, 而并非是对隐态重新计算. 但<strong>不保存梯度</strong>, 因为我们只是用同层的上个Segment隐态做为一种记忆信息对当前Segment产生影响, 并不需要进行反向传播. 储存的隐态数量尽可能多的.</p><p>$$<br>\begin{array}{l}<br>\widetilde{\mathbf{h}}_{\tau+1}^{n-1}=\left[\mathrm{SG}\left(\mathbf{h}_{\tau}^{n-1}\right) \circ \mathbf{h}_{\tau+1}^{n-1}\right] \\<br>\mathbf{q}_{\tau+1}^{n}, \mathbf{k}_{\tau+1}^{n}, \mathbf{v}_{\tau+1}^{n}=\mathbf{h}_{\tau+1}^{n-1} \mathbf{W}_{q}^{\top}, \widetilde{\mathbf{h}}_{\tau+1}^{n-1} \mathbf{W}_{k}^{\top}, \widetilde{\mathbf{h}}_{\tau+1}^{n-1} \mathbf{W}_{v}^{\top} \\<br>\mathbf{h}_{\tau+1}^{n}=\text { Transformer-Layer }\left(\mathbf{q}_{\tau+1}^{n}, \mathbf{k}_{\tau+1}^{n}, \mathbf{v}_{\tau+1}^{n}\right)<br>\end{array}<br>$$</p><p>其中$n$ 代表层数, $\tau$ 代表Segment号. $\mathrm{SG}(\cdot)$ 代表不求梯度, 不算作反向传播的一部分. 注意上面的式子, $\mathbf{k}_{\tau+1}^{n}$ 和 $\mathbf{v}_{\tau+1}^{n}$ 都是由当前Segment隐态和上个Segment隐态Concat后的$\widetilde{\mathbf{h}}_{\tau+1}^{n-1}$ 生成的, 而$\mathbf{q}_{\tau+1}^{n}$ <strong>仅由当前Segment隐态$\mathbf{h}_{\tau+1}^{n-1}$影响</strong>. 每个Segment都做相同的动作, 因此称该方法为Segment - Level Recurrence.</p><p>在训练时仍然每次前进一段:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerxl3.jpg" style="zoom: 50%;" /><p>与Vanilla Transformer不同的是, 因为能够直接<strong>复用</strong>整个Segment的信息, 能够极大加快Evaluation的速度, 所以每次前进Segment个Token:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerxl6.jpg" style="zoom: 50%;" /><blockquote><p>如果对前进Segment个Token有些模糊, 还是要理解作者是以整个Segment为单位进行操作的, 即使是进行自回归运算, 也是直接生成Segment个输出, 并输入进下个Segment.<br>$$<br>\mathbf{h}_{\tau}=f\left(\mathbf{h}_{\tau-1}, \mathbf{E}_{\mathbf{s}_{\tau}}+\mathbf{U}_{1: L}\right)<br>$$<br>如果在对前一个Segment的隐态和当前时刻隐态做Concat这个操作有疑惑, 我们来分析一下在这个计算过程中维度的变化. 请记住一个结论: <strong>Transformer隐态输出大小取决于Query的序列长度</strong>.</p><p>假设前一时刻隐态$\mathbf{h}_{\tau}^{n-1}$ 和当前时刻隐态$\mathbf{h}_{\tau+1}^{n-1}$ 的Size为$L\times d$, $W_q^T$, $W_v^T$, $W_k^T$ 的Size为$d_k \times d$. 在做完Concat后, 拼接后隐态$\widetilde{\mathbf{h}}_{\tau+1}^{n-1}$ Size为$2L \times d$, 看一下Attention分数的计算:<br>$$<br>\displaylines{<br>\mathbf{q}_{\tau+1}^{n}, \mathbf{k}_{\tau+1}^{n}, \mathbf{v}_{\tau+1}^{n}=\mathbf{h}_{\tau+1}^{n-1} \mathbf{W}_{q}^{\top}, \widetilde{\mathbf{h}}_{\tau+1}^{n-1} \mathbf{W}_{k}^{\top}, \widetilde{\mathbf{h}}_{\tau+1}^{n-1} \mathbf{W}_{v}^{\top} \\<br>\text{Attention Score}_{\tau+1}^n = \frac{\mathbf{q}_{\tau+1}^{n}{\mathbf{k}_{\tau+1}^{n}}^T}{\sqrt{d_k}}\mathbf{v}_{\tau+1}^{n}<br>}<br>$$<br>重复一遍之前的话, $\mathbf{k}_{\tau+1}^{n}$ 和 $\mathbf{v}_{\tau+1}^{n}$ 都是由当前Segment隐态和上个Segment隐态Concat后的$\widetilde{\mathbf{h}}_{\tau+1}^{n-1}$ 生成的, 而$\mathbf{q}_{\tau+1}^{n}$ <strong>仅由当前Segment隐态$\mathbf{h}_{\tau+1}^{n-1}$影响</strong>.</p><p>在求得分时, $\mathbf{q}_{\tau+1}^{n}$ 的维度是$L \times d_k$, $\mathbf{k}_{\tau+1}^{n}$ 和的$\mathbf{v}_{\tau+1}^{n}$维度都是$2L \times d$, 求完$\mathbf{q}_{\tau+1}^{n} \cdot {\mathbf{k}_{\tau+1}^{n}}^T$ 后维度是$L\times 2L$, 然后与$\mathbf{v}_{\tau+1}^{n}$ 再相乘, 维度变成$L\times d_k$. 乘完后的计算维度是与Transformer<strong>保持一致</strong>的.</p></blockquote><p>并且还能更好的联系前文信息, 每次能够依赖的最大长度也从$\text{Segment Length}$ 提升到$\text{Layer} \times \text{Segment Length}$的倍数:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerxl4.jpg" style="zoom: 67%;" /><h3 id="Relative-Positional-Encoding"><a href="#Relative-Positional-Encoding" class="headerlink" title="Relative Positional Encoding"></a>Relative Positional Encoding</h3><p>虽然已经利用Segment - Level Recurrence来解决固定长问题, 但在计算<strong>位置编码</strong>时候还没有彻底将<strong>段递归</strong>的思想融合进去. </p><h4 id="Absolute-Positional-Encoding"><a href="#Absolute-Positional-Encoding" class="headerlink" title="Absolute Positional Encoding"></a>Absolute Positional Encoding</h4><p>在Transformer中, 使用的是绝对位置编码: </p><p>$$<br>\begin{aligned}<br>\mathbf{A}_{i, j}^{\mathrm{abs}}&amp;= \left[ \mathbf{W}_{q}\left( \mathbf{E}_{x_{i}}+\mathbf{U}_i \right)\right]^{\top} \mathbf{W}_{k}\left(\mathbf{E}_{x_j}+\mathbf{U}_j\right)\\<br>&amp;=\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{E}_{x_{j}}}_{(a)}+\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{U}_{j}}_{(b)}<br>+\underbrace{\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{E}_{x_{j}}}_{(c)}+\underbrace{\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{U}_{j}}_{(d)}<br>\end{aligned}<br>$$</p><p>这个式子是由Transformer中的Attention拆来的, 其中$\mathbf{W}$ 代表权重矩阵, $\mathbf{E}$ 代表某个Token的Embedding, $\mathbf{U}$ 代表位置编码.</p><p>这样的位置编码在同一个Segment中是生效的, 但是对于不同的Segment就无法区分两个Segment中的同一对应位置, 即<strong>绝对位置不同</strong>, 但<strong>位置编码相同</strong>. 因此作者提出使用相对位置编码来解决该问题. </p><h4 id="Relative-Positional-Encoding-1"><a href="#Relative-Positional-Encoding-1" class="headerlink" title="Relative Positional Encoding"></a>Relative Positional Encoding</h4><p>作者先是说明了位置编码的作用: </p><blockquote><p>Conceptually, the positional encoding gives the model a <strong>temporal clue</strong> or “<strong>bias</strong>“ about how information should be gathered.</p></blockquote><p>概念上来说位置编码告诉模型一些如何收集信息的时序线索或者”偏置”. </p><blockquote><p>when a query vector $q_{τ,i}$ attends on the key vectors $k_{τ,≤i}$, it does not need to know the absolute position of each key vector to identify the temporal order of the segment. Instead, it suffices to know the relative distance between each key vector $k_{τ,j}$ and itself $q_{τ,i}$, i.e. $i−j$.</p></blockquote><p>但是在进行查询的时候, 每个key的绝对位置实际上通常是<strong>无关紧要</strong>的, 但与query的相对位置却非常关键. 也就是说我只要知道Query和Key的距离就足够了, 根本不关心它们到底处于原文中的哪个具体位置. 所以可以引入相对位置编码来告诉模型Token之间的相对位置关系, 从而代替更为复杂的绝对位置编码. </p><p>$$<br>\mathbf{A}_{i, j}^{\mathrm{rel}} =\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k, E} \mathbf{E}_{x_{j}}}_{(a)}+\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k, R} \mathbf{R}_{i-j}}_{(b)}<br>+\underbrace{u^{\top} \mathbf{W}_{k, E} \mathbf{E}_{x_{j}}}_{(c)}+\underbrace{v^{\top} \mathbf{W}_{k, R} \mathbf{R}_{i-j}}_{(d)}<br>$$</p><p>主要关注两个<strong>替换点</strong>:</p><ul><li>所有与Key对应的绝对位置$j$ 相关的项$\mathbf{U}_j$ 应该被替换为相对位置项$\mathbf{R}_{i-j}$.</li><li>由于Query总出于同一位置, 所有与Query对应的绝对位置$i$ 相关的项$\mathbf{U}_i^{\top}\mathbf{W}_q^{\top}$ 也应该被替换掉, 在这里替换成两个可训练参数$u$ 和$v$ .</li></ul><p>其中$\mathbf{R}_{i-j}$的编码方式与Transformer中的<strong>正余弦</strong>位置编码方式相同. </p><p>对于相对位置的四项, 作者做了汇总, 并给它们下了直观含义:</p><blockquote><p>term (a) represents contentbased addressing, term (b) captures a contentdependent positional bias, term (c) governs a global content bias, and (d) encodes a global positional bias.</p></blockquote><table><thead><tr><th align="center">项数</th><th align="center">绝对位置表示</th><th align="center">相对位置表示</th><th>相对位置含义</th></tr></thead><tbody><tr><td align="center">a</td><td align="center">$\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{E}_{x_{j}}$</td><td align="center">$\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k, E} \mathbf{E}_{x_{j}}$</td><td>基于内容的寻址</td></tr><tr><td align="center">b</td><td align="center">$\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{U}_{j}$</td><td align="center">$\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k, R} \mathbf{R}_{i-j}$</td><td>依赖内容的位置偏置</td></tr><tr><td align="center">c</td><td align="center">$\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{E}_{x_{j}}$</td><td align="center">$u^{\top} \mathbf{W}_{k, E} \mathbf{E}_{x_{j}}$</td><td>全局内容偏置</td></tr><tr><td align="center">d</td><td align="center">$\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{U}_{j}$</td><td align="center">$v^{\top} \mathbf{W}_{k, R} \mathbf{R}_{i-j}$</td><td>全局位置偏置</td></tr></tbody></table><blockquote><p>我开始很难理解(c)和(d)的含义到底是如何观察出来的. 如果从映射和查询的角度去理解的话, 可以看作是权重矩阵左侧的内容做了一次Embedding, 投影进潜在空间, 然后与权重矩阵右侧的内容计算得分. </p><p>个人理解还是有些模糊, 可解释性不够好.</p></blockquote><p>在加上相对位置编码后, 整个过程就完善了, 形成一个闭环:<br>$$<br>\begin{aligned}<br>\widetilde{\mathbf{h}}_{\tau}^{n-1}=&amp;\left[\mathrm{SG}\left(\mathbf{m}_{\tau}^{n-1}\right) \circ \mathbf{h}_{\tau}^{n-1}\right] \\<br>\mathbf{q}_{\tau}^{n}, \mathbf{k}_{\tau}^{n}, \mathbf{v}_{\tau}^{n}=&amp; \mathbf{h}_{\tau}^{n-1} \mathbf{W}_{q}^{n \top}, \widetilde{\mathbf{h}}_{\tau}^{n-1} \mathbf{W}_{k, E}^{n}, \widetilde{\mathbf{h}}_{\tau}^{n-1} \mathbf{W}_{v}^{n \top} \\<br>\mathbf{A}_{\tau, i, j}^{n}=&amp; \mathbf{q}_{\tau, i}^{\top} \mathbf{k}_{\tau, j}^{n}+\mathbf{q}_{\tau, i}^{n} \mathbf{W}_{k, R}^{n} \mathbf{R}_{i-j}<br>+u^{\top} \mathbf{k}_{\tau, j}+v^{\top} \mathbf{W}_{k, R}^{n} \mathbf{R}_{i-j} \\<br>\mathbf{a}_{\tau}^{n}=&amp; \text { Masked-Softmax }\left(\mathbf{A}_{\tau}^{n}\right) \mathbf{v}_{\tau}^{n} \\<br>\mathbf{o}_{\tau}^{n}=&amp; \text { LayerNorm }\left(\operatorname{Linear}\left(\mathbf{a}_{\tau}^{n}\right)+\mathbf{h}_{\tau}^{n-1}\right) \\<br>\mathbf{h}_{\tau}^{n}=&amp; \text { Positionwise-Feed-Forward }\left(\mathbf{o}_{\tau}^{n}\right)<br>\end{aligned}<br>$$<br>其中$\mathbf{m}_{\tau}^{n-1}$ 指的是第$\tau$ 个Segment时GPU缓存中的所有第$n-1$ 层的隐态.</p><blockquote><p>推荐阅读:</p><ul><li><a href="http://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html" target="_blank" rel="noopener">Transformer-XL: Unleashing the Potential of Attention Models</a> 是谷歌官方发布的Blog, 里面有动图.</li></ul></blockquote><h2 id="XLNet"><a href="#XLNet" class="headerlink" title="XLNet"></a>XLNet</h2><p>XLNet出自论文<a href="http://arxiv.org/abs/1906.08237" target="_blank" rel="noopener">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a>.</p><p>虽然BERT在各类任务上的出色表现, 但BERT是一个<strong>自编码</strong>(Auto Encoding)模型, 相较与<strong>自回归</strong>(Auto Regressive)模型, 自编码模型在面对<strong>生成式</strong>的任务是天生具有劣势的(我们后面再提). XLNet尝试在自回归的基础上将BERT身上的优点吸取过来.</p><p>但是怎么才能在自回归模型中引入双向捕捉上下文的能力呢? 在先前的任务中, AR模型一般只能通过Forward和Backward两次单向编码来尝试捕捉上下文, 但并没有双向捕捉上下文效果来的理想, 下游任务又非常需要这种双向建模的能力. 自回归和自编码的模型<strong>各有优劣</strong>, 很多时候特点既是一种<strong>优势</strong>, 也是一种<strong>限制</strong>. AR模型受限于自身结构, 不能从结构上改变. 作者巧妙的想到了利用<strong>打乱Token之间的顺序</strong>来获取上下文. </p><h3 id="Why-not-BERT"><a href="#Why-not-BERT" class="headerlink" title="Why not BERT?"></a>Why not BERT?</h3><h4 id="Disadvantage-of-BERT"><a href="#Disadvantage-of-BERT" class="headerlink" title="Disadvantage of BERT"></a>Disadvantage of BERT</h4><p>XLNet的作者指出了BERT的两个<strong>致命缺点</strong>:</p><ol><li><p>被BERT使用的人工制造符号譬如<code>[MASK]</code>完全贯穿于BERT的<strong>预训练</strong>过程, 但在用<strong>实际数据</strong>做Fine - tune时根本没有, 导致了预训练和微调之间的<strong>差异</strong>. 我记得在BERT中, 作者提到了缓解这个问题的方案, 但不能从根本上解决这个问题:</p><blockquote><p>在Fine tune的时候不可能对单词进行Mask, 这样就会导致预训练和微调的不匹配, 为缓解这种问题, 并非总是将选中的单词Mask, 而是在每个句子中, 有15%的词会被选中, 在选中单词后有三种可能性:</p><ol><li><strong>80%</strong>的概率将被选中的单词替换为<code>[MASK]</code>.</li><li><strong>10%</strong>的概率将被选中的单词替换为<strong>随机词</strong>.</li><li><strong>10%</strong>的概率对被选中的单词<strong>不进行替换</strong>.</li></ol></blockquote></li><li><p>BERT假设被Mask掉的Token与句子中剩下的内容是独立的, 但在实际任务中普遍有远程依赖.</p><blockquote><p>比如”New York is a city.” 假设Mask了”New”和”York”两个词语, 那么在已知”is a city”的情况下, 就不太可能预测准确.</p></blockquote></li></ol><h4 id="Comparsion-of-AutoEncoding-and-AutoRegressive"><a href="#Comparsion-of-AutoEncoding-and-AutoRegressive" class="headerlink" title="Comparsion of AutoEncoding and AutoRegressive"></a>Comparsion of AutoEncoding and AutoRegressive</h4><p>假设你真的不知道AE和AR是什么意思, 我们用大白话给AE何AR重新下个定义吧:</p><ul><li><p>自回归模型(AR): 将上一时刻的输出作为输入, 对当前时刻输出进行预测.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/xlnet2.jpg" style="zoom: 67%;" /></li><li><p>自编码模型(AE): 把完整的内容破坏掉一部分作为输入, 来预测被破坏掉的那部分.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/xlnet3.jpg" style="zoom: 67%;" /></li></ul><blockquote><p>上述图片出自NLPCC 2020中科大讯飞和HIT - SCIR做的报告&lt;Revisiting Pre-Trained Models for Natural Language Processing&gt;.</p></blockquote><p>对给定的文本输入序列$\mathbf{x}=\left[x_{1}, \cdots, x_{T}\right]$, $e(x)$ 代表Token $x$ 的Embedding.</p><p>自回归模型的任务目标是最大似然该函数:<br>$$<br>\max _{\theta}\quad \log p_{\theta}(\mathbf{x})=\sum_{t=1}^{T} \log p_{\theta}\left(x_{t} \mid \mathbf{x}_{&lt;t}\right)=\sum_{t=1}^{T} \log \frac{\exp \left(h_{\theta}\left(\mathbf{x}_{1: t-1}\right)^{\top} e\left(x_{t}\right)\right)}{\sum_{x^{\prime}} \exp \left(h_{\theta}\left(\mathbf{x}_{1: t-1}\right)^{\top} e\left(x^{\prime}\right)\right)}<br>$$<br>但是对于AE模型来说(主要以BERT作为对比对象), 由于会选中15%的Token做Mask, 假设被污染的Token用$\hat{\mathbf{x}}$ 表示, 自编码模型的任务目标是从<strong>带噪声</strong>的$\hat{\mathbf{x}}$ 中重建$\overline{\mathbf{x}}$:<br>$$<br>\max _{\theta}\quad \log p_{\theta}(\overline{\mathbf{x}} \mid \hat{\mathbf{x}}) \approx \sum_{t=1}^{T} m_{t} \log p_{\theta}\left(x_{t} \mid \hat{\mathbf{x}}\right)=\sum_{t=1}^{T} m_{t} \log \frac{\exp \left(H_{\theta}(\hat{\mathbf{x}})_{t}^{\top} e\left(x_{t}\right)\right)}{\sum_{x^{\prime}} \exp \left(H_{\theta}(\hat{\mathbf{x}})_{t}^{\top} e\left(x^{\prime}\right)\right)}<br>$$</p><p>其中当Token $x_t$ 被Mask时$m_t=1$, 其他时候为0. $H_\theta$ 是Transformer将长度为$T$ 的文本序列$\mathbf{x} $ 映射成隐态向量$H_{\theta}(\mathbf{x})=\left[H_{\theta}(\mathbf{x})_{1}, H_{\theta}(\mathbf{x})_{2}, \cdots, H_{\theta}(\mathbf{x})_{T}\right]$ .</p><p>注意这两个目标函数, 首先条件不同, 其次是隐态表示不同. AR模型中因为只能看到前文的信息, 而AE模型能够看到全部的上下文信息. 将二者比较后, 作者得出三个结论:</p><ul><li><p><strong>独立假设</strong>: 这里强调了在自编码任务目标中的$\approx$, BERT假设在给定$\hat{\mathbf{x}}$ 的情况下被Mask的词是相互独立的. 而AR模型不需要独立性假设.</p></li><li><p><strong>输入噪声</strong>: 在预训练时会出现特殊的人造Token, Fine - tune时没有, 导致不匹配. 而AR模型不对输入引入噪声, 所以不会遇到该问题.</p></li><li><p><strong>上下文依赖</strong>: AR模型只能捕捉到位置位于前面的信息$h_\theta(\mathbf{x_{1:t-1}})$, BERT能更好的捕捉双向上下文信息$H_\theta(\mathbf{x})_t$.</p><blockquote><p>对于双向捕捉可以参考<a href="https://adaning.github.io/posts/3996.html">ELMo, GPT, BERT</a>.</p></blockquote></li></ul><p>作者观察到的这三点非常重要, 作为模型的改进方向.</p><p>此外, AR模型的特性能带来一个AE模型不能带来的优势, 这也是BERT为捕捉双向上下文付出的代价. 我们考虑一个场景:</p><ul><li>BERT要对<code>[New, York, is, a, city]</code> 中的<code>[New, York]</code> 进行预测, 作为AE模型对<code>[New, York]</code> 做了Mask.</li><li>XLNet要对<code>[is, a, city, New, York]</code> 中的<code>[New, York]</code> 进行预测, 作为AR模型, 需要先预测出<code>[New]</code>, 再利用<code>[New]</code> 的信息预测<code>[York]</code>.</li></ul><p>那么这两种方式产生的信息量是绝对不一样的:<br>$$<br>\begin{aligned}<br>\mathcal{J}_{\mathrm{BERT}} &amp;=\log p(\text { New } \mid \text { is a city })+\log p(\text { York } \mid \text { is a city }) \\<br>\mathcal{J}_{\mathrm{XLNet}} &amp;=\log p(\mathrm{New} \mid \text { is a city })+\log p(\text { York } \mid \mathrm{New}, \text { is a city })<br>\end{aligned}<br>$$<br>很明显, XLNet因为是AR模型, 永远都能学到更多的依赖对. 至于为什么可以在二者输入不一样的情况下进行比较, 在读完下一小节后你就会恍然大悟.</p><blockquote><p>在附录中还有更多的比对, 更详细的内容请自行阅读原论文.</p></blockquote><h3 id="Permutation-Language-Model-Based-on-AutoRegressive-Model"><a href="#Permutation-Language-Model-Based-on-AutoRegressive-Model" class="headerlink" title="Permutation Language Model Based on AutoRegressive Model"></a>Permutation Language Model Based on AutoRegressive Model</h3><p>由于AR语言模型比较特殊, 不好实现像BERT那样双向捕捉上下文的效果. 作者提出用<strong>置换</strong>的方法, 既能保留AR模型的优势, 又允许模型捕捉上下文. </p><p>对于给定长度为$T$ 的序列$\mathbf{x}$, 总共有$T!$ 种不同的排列方式. 如果模型参数能通过多次随机的句子排列<strong>共享</strong>, 那么AR模型就能将所有位置上的信息结合到一起.</p><p>假设$\mathcal{Z}_{T}$ 代表长度为$T$ 的序列下标, $z_t$ 代表第$t$ 个元素,  $\mathbf{z}_{&lt;t}$ 代表置换后的前$t-1$ 个元素.</p><p>沿着之前AR模型的目标, 现在给置换语言模型(PLM)的基本优化目标重新下个定义吧:<br>$$<br>\max _{\theta} \mathbb{E}_{\mathbf{z} \sim \mathcal{Z}_{T}}\left[\sum_{t=1}^{T} \log p_{\theta}\left(x_{z_{t}} \mid \mathbf{x}_{\mathbf{z}&lt;t}\right)\right]<br>$$<br>我们的目标只是单纯的从原来有序的输入序列变成了<strong>位置随机重排</strong>后的序列, 这样在做自回归时就能够看到原本不属于该位置的元素, 这样就能双向捕捉上下文.</p><blockquote><p>作者在这里强调, 仅生成一个置换后的新下标序列, 而输入序列的原文顺序是不变的. 沿用与原始输入相对应的位置编码, 并依靠<strong>Attention</strong>来实现对下标的置换. 因为在<strong>FineTuning</strong>期间文本的顺序是正常而<strong>不发生置换</strong>的.</p></blockquote><p>作者给出了一张对置换部分的阐述图, 假设当前要对$x_3$ 进行预测, 在置换语言模型中只能通过Attention来关注置换后序列在$3$ 前的输入: </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/xlnet1.jpg" style="zoom: 67%;" /><p>例如左上角, 分解顺序为$3 \rightarrow 2 \rightarrow 4 \rightarrow 1$ 时, 只能对之前的隐态$\text{mem}$ 做Attention. 右上角, 分解顺序为$2 \rightarrow 4 \rightarrow 3 \rightarrow 1$ 时, 只能对置换顺序在$3$ 前的$2$ 和 $4$ 做Attention. 下面两幅图同理.</p><blockquote><p>是不是有些巧妙? 对, 置换的方法确实比较巧妙, 但是请思考一个问题, 假设置换后的顺序为$2 \rightarrow 4 \rightarrow 3 \rightarrow 1$, 马上就要预测$x_3$ 了, 由于打乱了顺序, 模型怎才能知道它要预测的是$x_3$ 还是 $x_1$ 呢?</p></blockquote><h3 id="Two-Stream-Self-Attention-for-Target-Aware-Representations"><a href="#Two-Stream-Self-Attention-for-Target-Aware-Representations" class="headerlink" title="Two - Stream Self - Attention for Target - Aware Representations"></a>Two - Stream Self - Attention for Target - Aware Representations</h3><h4 id="Target-Aware"><a href="#Target-Aware" class="headerlink" title="Target - Aware"></a>Target - Aware</h4><p>由于采用了置换的方式, 所以按照Transformer的方法处理目标可能会遇到一些问题.</p><p>如果我们沿用之前Transformer的方式, $h_{\theta}\left(\mathbf{x}_{\mathbf{z}&lt;t}\right)$ 代表$\mathbf{X}_{\mathbf{Z}&lt;t}$ 的隐态, 来计算下一个单词的分布:<br>$$<br>p_{\theta}\left(X_{z_{t}}=\right.\left.x \mid \mathbf{x}_{\mathbf{z}&lt;t}\right)=\frac{\exp \left(e(x)^{\top} h_{\theta}\left(\mathbf{x}_{\mathbf{z}&lt;t}\right)\right)}{\sum_{x^{\prime}} \exp \left(e\left(x^{\prime}\right)^{\top} h_{\theta}\left(\mathbf{x}_{\mathbf{z}&lt;t}\right)\right)}<br>$$<br>由于我们之前<strong>打乱</strong>了原有的句子顺序, 模型完全不知道该预测哪个Token了. 对于未知的所有Token, 它们有可能不在原来的位置上, 也有可能在原来的位置上. 换句话说, 如果使用Transformer的方法, 下一个要预测的Token分布与它原来的<strong>位置</strong>就没有关联了. </p><p>因此必须将下一个要预测的Token通过某种方式<strong>显式</strong>的告诉模型, “哦它才是打乱顺序后我该预测的下一个Token”, 这也就是论文中说的<strong>Target - Aware</strong>:<br>$$<br>p_{\theta}\left(X_{z_{t}}=x \mid \mathbf{x}_{z&lt;t}\right)=\frac{\exp \left(e(x)^{\top} g_{\theta}\left(\mathbf{x}_{\mathbf{z}&lt;t}, z_{t}\right)\right)}{\sum_{x^{\prime}} \exp \left(e\left(x^{\prime}\right)^{\top} g_{\theta}\left(\mathbf{x}_{\mathbf{z}&lt;t}, z_{t}\right)\right)}<br>$$<br>$g_{\theta}\left(\mathbf{x}_{\mathbf{z}&lt;t}, z_{t}\right)$ 代表加入目标位置信息的表示方法.</p><h4 id="Two-Stream-Self-Attention"><a href="#Two-Stream-Self-Attention" class="headerlink" title="Two - Stream Self - Attention"></a>Two - Stream Self - Attention</h4><p>因为置换把要预测的Token的<strong>位置信息</strong>直接搞掉了, 所以必须通过其他方式提供位置信息. 我们必须符合以下两个点的限制:</p><ol><li>预测$x_{z_t}$ 时, $g_{\theta}\left(\mathbf{x}_{\mathbf{z}&lt;t}, z_t\right) $ 不能$z_t$ 处的内容信息$x_{z_t}$, 只能使用$z_t$ 处的位置信息.</li><li>在预测一个位置上$j&gt;t$ 的Token$x_{z_j}$ 时, $g_{\theta}\left(\mathbf{x}_{\mathbf{z}&lt;t}, z_t\right) $ 应该将$x_{z_t}$ 编码来提供全部的上下文信息.</li></ol><p>基于这两点规则, XLNet采用<strong>双流注意力</strong>机制, 其中一种注意力提供<strong>上下文</strong>信息, 另一种提供<strong>位置</strong>信息.</p><ul><li>$h_\theta\left(\mathbf{x}_{\mathbf{z}\leq t}\right)$, 记为$h_{z_t}$, 表示包括$x_{z_t}$ 在内的编码的上下文信息, 与标准的Transformer相同, 被称作为<strong>内容流</strong>(Content Stream).</li><li>$g_\theta\left(\mathbf{x}_{\mathbf{z}_&lt;t}, z_t\right)$, 记为$g_{z_t}$, 表示仅包括$\mathbf{x}_{\mathbf{z} &lt; t}$ 在内的上下文信息和$z_t$ 的位置信息, 并没有$x_{z_t}$ 处的位置信息, 被称之为<strong>请求流</strong>(Query Stream).</li></ul><h5 id="Content-Stream-and-Query-Stream"><a href="#Content-Stream-and-Query-Stream" class="headerlink" title="Content Stream and Query Stream"></a>Content Stream and Query Stream</h5><p>下面就来以论文中的图为例子, 看一下具体的双流注意力在场景中的结构.</p><p>第一层的Query Stream被定义为一个可训练的向量, 记作$g_i^{(0)}=w$, 设置Content Stream是对应的Embedding, 记作$h_i^{(0)}=e(x_i)$. Self - Attention层记为$m=1,\dots,M$. </p><p>XLNet的每一层与Transformer - XL相同, 包括FFN, Layer Norm, 残差连接, 多头注意力… 这些细节都暂时省略掉, 到后面我们会串起来看.</p><p>先来看看最熟悉的Self - Attention, 这与Transformer中是保持一致的, 不要忘记它也被称为内容流:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/xlnet4.jpg" style="zoom: 50%;" /><p>Content Stream能使用<strong>包括其本身在内的所有上下文信息</strong>. 为了存储上下文信息, 所以Q依赖于上层对应位置的Content Stream, K和V均依赖于所有输入的隐态:<br>$$<br>h_{z_{t}}^{(m)} \leftarrow \text { Attention }\left(\mathrm{Q}=h_{z_{t}}^{(m-1)}, \mathrm{KV}=\mathbf{h}_{\mathrm{z} \leq t}^{(m-1)} ; \theta\right), \quad\left(\text {content stream: use both } z_{t} \text { and } x_{z_{t}}\right)<br>$$<br>下面来看看XLNet中独有的请求流$g_\theta$:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/xlnet5.jpg" style="zoom: 50%;" /><p>因为只用于<strong>发起请求</strong>, 请求流的Q只能由上一层的Query Stream生成, 而K和V不能使用当前位置的内容信息, 所以只能由<strong>其他位置</strong>的上下文信息决定:<br>$$<br>g_{z_{t}}^{(m)} \leftarrow \text { Attention }\left(\mathrm{Q}=g_{z_{t}}^{(m-1)}, \mathrm{KV}=\mathbf{h}_{\mathrm{z}&lt;t}^{(m-1)} ; \theta\right), \quad \text { (query stream: use } z_{t} \text { but cannot see } \left.x_{z_{t}}\right)<br>$$<br>对比一下Query Stream和Content Stream, 二者的不同点主要在<strong>Q的依赖来源</strong>和能否使用<strong>上层当前位置的内容信息</strong> $h_{z=t}^{(m-1)}$:<br>$$<br>\begin{aligned}<br>&amp;g_{z_{t}}^{(m)} \leftarrow \text { Attention }\left(\mathrm{Q}=g_{z_{t}}^{(m-1)}, \mathrm{KV}=\mathbf{h}_{\mathrm{z}&lt;t}^{(m-1)} ; \theta\right), \quad \text { (query stream: use } z_{t} \text { but cannot see } \left.x_{z_{t}}\right)\\<br>&amp;h_{z_{t}}^{(m)} \leftarrow \text { Attention }\left(\mathrm{Q}=h_{z_{t}}^{(m-1)}, \mathrm{KV}=\mathbf{h}_{\mathrm{z}&lt;t}^{(m-1)} ; \theta\right), \quad\left(\text {content stream: use both } z_{t} \text { and } x_{z_{t}}\right)<br>\end{aligned}<br>$$</p><h5 id="Permutation-Attention-Mask"><a href="#Permutation-Attention-Mask" class="headerlink" title="Permutation Attention Mask"></a>Permutation Attention Mask</h5><p>其实我们也不用做真正意义上的置换, 只需要将Attention中的<strong>Mask机制</strong>结合起来使用就好, 如下图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/xlnet6.jpg" style="zoom: 50%;" /><p>图中的白点代表被Mask, 红点代表可以被Attend.</p><p>置换后的顺序为$3\rightarrow2\rightarrow4\rightarrow1$ 时, 请求流是不能看到当前正在预测的Token的内容信息的, 所以请求流矩阵<strong>主对角线</strong>一定都是被Mask的. 因为第一个要预测的就是位置$3$ 的内容, 所以矩阵第三行全都被Mask. 在预测出位置$3$ 后的内容后, 在请求流矩阵第二行的第三列就变得已知了. 在预测位置$4$ 和$1$ 时同理.</p><p>当搞懂了请求流矩阵后, 再看内容流矩阵, 完全符合我们之前对这两种流的定义: <strong>内容流矩阵不过是请求流矩阵主对角线没被Mask而已</strong>, 因为请求流的要求是当前要预测的内容不可知, 内容流却只是存储上下文, 没有这种限制.</p><h5 id="Particial-Prediction"><a href="#Particial-Prediction" class="headerlink" title="Particial Prediction"></a>Particial Prediction</h5><p>在实验过程中, 发现如果对所有置换后的序列输入进行预测, 会导致<strong>收敛缓慢</strong>, 难以优化. 作者只去预测置换后序列的<strong>最后几个</strong>Token, 这样置换后序列$\mathbf{z}$ 就被分成了两个部分, 分别是分割点$c$ 前的$\mathbf{z}_{\leq c}$, 和<strong>需要预测</strong>的分割点后的序列$\mathbf{z}_{&gt;c}$. 根据这个变化, XLNet的优化目标变为:<br>$$<br>\max _{\theta} \mathbb{E}_{\mathbf{z} \sim \mathcal{Z}_{T}}\left[\log p_{\theta}\left(\mathbf{x}_{\mathbf{z}&gt;c} \mid \mathbf{x}_{\mathbf{z} \leq c}\right)\right]=\mathbb{E}_{\mathbf{z} \sim \mathcal{Z}_{T}}\left[\sum_{t=c+1}^{|\mathbf{z}|} \log p_{\theta}\left(x_{z_{t}} \mid \mathbf{x}_{\mathbf{z}&lt;t}\right)\right]<br>$$</p><p>关于分割点, 作者假设了超参数$K$, 设$|\mathbf{z}| /(|\mathbf{z}|-c) \approx K$. 有$\frac{1}{K}$ 个Token需要被预测, 不需要预测的Token所对应的Query Stream是不需要被计算的, 这样能够节省计算资源.</p><h5 id="Integrate-Attention-into-Transformer-Backbone"><a href="#Integrate-Attention-into-Transformer-Backbone" class="headerlink" title="Integrate Attention into Transformer Backbone"></a>Integrate Attention into Transformer Backbone</h5><p>我们现在可以把除了双流Attention以外的结构加进来了, 仍然也都是Transformer中的组件, 只是位置编码采用的是<strong>相对位置编码</strong>(这里的相对位置编码和Transformer - XL还不一样, 后面会提及).</p><p>对于时间步$t=1,\dots,T$, 将含有相对位置编码的Attention记为$\text{RelAttn}$, 将Position - wise的Feed Forward记为$\text{PosFF}$, 整个双流Attention<strong>更新</strong>过程如下:<br>$$<br>\begin{array}{l}<br>\hat{h}_{z_{t}}^{(m)}=\text { LayerNorm }\left(h_{z_{t}}^{(m-1)}+\operatorname{RelAttn}\left(h_{z_{t}}^{(m-1)},\left[\tilde{\mathbf{h}}^{(m-1)}, \mathbf{h}^{(m-1)}_{\mathbf{z}_{\leq t}}\right]\right)\right) \\<br>h_{z_{t}}^{(m)}=\text { LayerNorm }\left(\hat{h}_{z_{t}}^{(m)}+\operatorname{PosFF}\left(\hat{h}_{z_{t}}^{(m)}\right)\right) \\<br>\hat{g}_{z_{t}}^{(m)}=\text { LayerNorm }\left(g_{z_{t}}^{(m-1)}+\operatorname{RelAttn}\left(g_{z_{t}}^{(m-1)},\left[\tilde{\mathbf{h}}^{(m-1)}, \mathbf{h}_{\mathbf{z}&lt;t}^{(m-1)}\right]\right)\right) \\<br>g_{z_{t}}^{(m)}=\text { LayerNorm }\left(\hat{g}_{z_{t}}^{(m)}+\operatorname{PosFF}\left(\hat{g}_{z_{t}}^{(m)}\right)\right)<br>\end{array}<br>$$</p><h3 id="Incorporating-Ideas-From-Transformer-XL"><a href="#Incorporating-Ideas-From-Transformer-XL" class="headerlink" title="Incorporating Ideas From Transformer - XL"></a>Incorporating Ideas From Transformer - XL</h3><p>因为Transformer - XL是最近颇有成效的AR模型, 所以作者尝试将它的两种最关键的技术<strong>相对编码</strong>和<strong>段级递归</strong>引入, 与XLNet结合. 作者引入了两种相对编码, Transformer - XL一模一样的<strong>相对位置编码</strong>和结合段问题添加的<strong>相对段编码</strong>.</p><h4 id="Segement-Level-Recurrence"><a href="#Segement-Level-Recurrence" class="headerlink" title="Segement Level Recurrence"></a>Segement Level Recurrence</h4><p>其实和Transformer - XL中的段级递归一样. 假设现在有两段长序列$\mathbf{s}$, $\tilde{\mathbf{x}}=\mathbf{s}_{1: T}$ 与 $\mathbf{x}=\mathbf{s}_{T+1: 2 T}$, 其置换后的序列分别是$\tilde{\mathbf{z}}=\text{Permutation}([1\cdots T])$ 和 $\mathbf{z}=\text{Permutation}([T+1\cdots 2T])$. 那么计算第二段$\mathbf{x}$的内容流可以按照如下方式更新:<br>$$<br>h_{z_{t}}^{(m)} \leftarrow \text { Attention }\left(\mathrm{Q}=h_{z_{t}}^{(m-1)}, \mathrm{KV}=\left[\tilde{\mathbf{h}}^{(m-1)}, \mathbf{h}_{\mathbf{z}_{\leq t}}^{(m-1)}\right] ; \theta\right)<br>$$<br>将Memory中缓存的隐态$\tilde{\mathbf{h}}^{(m-1)}$ 和$\mathbf{x}$ 产生的隐态$\mathbf{h}_{\mathbf{z}_{\leq t}}^{(m-1)}$ Concat起来, 一起计算K和V.</p><h4 id="Multiple-Segments"><a href="#Multiple-Segments" class="headerlink" title="Multiple Segments"></a>Multiple Segments</h4><p>XLNet与BERT一样, 在预训练阶段也是随机选择了两段连续或不连续的上下文合并到一起, 作为一个序列的两个Segment输入进XLNet中, 但只对连续内容使用Segment Memory. 两段之间采用的符号也和BERT类似, <code>[CLS, A, SEP, B, SEP]</code>.</p><h4 id="Relative-Segment-Encoding"><a href="#Relative-Segment-Encoding" class="headerlink" title="Relative Segment Encoding"></a>Relative Segment Encoding</h4><p>注意, 这是<strong>相对段编码</strong>而并非相对位置编码. XLNet中的段编码方式与BERT中的绝对段编码方式不同, XLNet采用相对段编码.</p><p>对于给定的序列位置$i$ 和 $j$, 当这两个位置都位于同一个Segment时, 令$s_{ij}=s_+$, 否则$s_{ij}=s_-$. 其中$s_+$ 和$s_-$ 是每个注意力头<strong>分别</strong>学习的. 这种编码只关注<strong>两个Token是否位于同个Segment内</strong>, 而并<strong>不考虑来自于哪个特定的Segment</strong>, 这个角度来看与相对编码的思想是保持一致的.</p><p>在计算注意力时, $i$ 对$j$ 的注意力是这样计算的:<br>$$<br>a_{i j}=\left(\mathbf{q}_{i}+\mathbf{b}\right)^{\top} \mathbf{s}_{i j}<br>$$<br>$q_i$ 是标准注意力计算来的Query Vector, $\mathbf{b}$ 是一个可学习的参数, 最后将这个注意力$a_{ij}$和正常的注意力<strong>相加</strong>得到最终的注意力.</p><p>相对编码增强了模型的泛化能力, 并提供了完成多段输入任务的可能性.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Transformer - XL结合了<strong>段循环</strong>和<strong>注意力机制</strong>, 缓解了Transformer受限于输入长度的问题, 也节省了计算资源的开销, 减少了不必要的计算. 并在段循环的基础上做出相对位置编码的改进.</p><p>XLNet用置换的方法将AE模型引以为傲的双向捕捉上下文能力移植到AR模型上, 将Transformer - XL的特性也组装到自己的模型中.</p><p>从大家对XLNet的看法来说, 它是<strong>饱受争议</strong>的. 虽然根据所给出的实验数据来说, XLNet能够取得比较好的效果, 但是对比BERT使用的13G数据, XLNet使用了158G数据, 而BERT才仅仅使用了13G数据, 到底是模型本身的方法导致效果好, 还是数据堆上去的, 我个人认为很难说. 后人基于XLNet所作出的后续研究非常少. 相反, BERT的魔改模型都快一窝了. </p><p>我记得前段时间还有人说XLNet完爆BERT, 大家吵得不可开交. 具体见<a href="https://zhuanlan.zhihu.com/p/74812464" target="_blank" rel="noopener">XLNet团队：只要公平对比，BERT毫无还手之力</a>(可能也有点标题党的嫌疑).</p><p>另外吐槽一下, 因为XLNet整篇论文特别碎, 也很乱, 所以才一直拖着, 毕竟写这篇博客实在是太费劲了. 虽然明确的指出了BERT确实存在的问题, 但XLNet解决方案不是很优雅, 感觉像一堆零件<strong>拼凑</strong>到了一起.</p><p>虽然XLNet诟病繁多, 但XLNet为AR模型实现双向上下文捕捉能力提供了可能, 这种带领大家突破BERT框架的思维模式还是很重要的..</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ELMo, GPT, BERT</title>
      <link href="/posts/3996.html"/>
      <url>/posts/3996.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文的前置知识:</p><ul><li>RNN</li><li>Transformer</li><li>Language Model</li></ul></blockquote><h1 id="ELMo-GPT-BERT"><a href="#ELMo-GPT-BERT" class="headerlink" title="ELMo, GPT, BERT"></a>ELMo, GPT, BERT</h1><p>本文是对ELMo, GPT, BERT三个模型的结构介绍以及个人理解, 多图预警.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>由于NLP领域的<strong>数据标注</strong>难度非常大, 成本很高, 所以必须要采用<strong>无监督或半监督</strong>的方法来在数据集不够充足的情况下提高模型的<strong>通用表示能力</strong>. 从Word2Vec(2013)到Glove(2014), 再到ELMo(2017), GPT-1(2018), BERT(2018), GPT-2(2019), GPT-3(2020)… 前人已经在预训练领域做了不少的探索. 由于NLP领域在自然语言的表示上非常困难, 特别注重于<strong>感知</strong>和<strong>理解</strong>, 现有的算法对机器要求又比较高, 所以NLP的发展进度比CV是滞后一些的, 在CV上早就出现了预训练模型, 也就不难理解为什么会在预训练上摸索了.</p><p>之所以把ELMo, GPT, BERT这三个模型放在一起说, 是因为BERT作为NLP中的又一个<strong>里程碑</strong>(并不是因为BERT模型本身创新点有多少, 而是它代表本阶段的所有技术融合)将这三个模型进行了对比. 这三个模型像是NLP近四年在<strong>预训练领域</strong>的探索过程的缩影, 从ELMo出现以来, NLP的ImageNet时代就来临了. 除此外, 它们三个都与<strong>单词结合上下文表示</strong>有关. 但无论哪种预训练模型, 都离不开强大的数据集和高算力支撑.</p><p>本文的图片来自于:</p><ul><li><p><a href="https://jalammar.github.io/illustrated-bert/" target="_blank" rel="noopener">The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)</a></p></li><li><p><a href="https://jalammar.github.io/illustrated-gpt2/" target="_blank" rel="noopener">The Illustrated GPT-2 (Visualizing Transformer Language Models)</a></p></li><li><p><a href="https://jalammar.github.io/how-gpt3-works-visualizations-animations/" target="_blank" rel="noopener">How GPT3 Works - Visualizations and Animations</a></p></li><li><p>ELMo, GPT, BERT的原论文.</p></li><li><p>芝麻街那几个小人的图片来源出自网络.</p><p>其余图片的具体来源不再注明是来自于哪篇文章的, 请自行到对应模型的文章查找. 上述三篇文章均来自<code>jalammar</code>, 选择他的文章做图片来源的理由太简单了, 并不是因为内容准确(自然语言不具有像数学语言一样的精确性), 而是因为颜值高. </p></li></ul><p>当然Word2vec我认为没必要再细说了, 现在所有的词嵌入都是基于Word2vec的, Word2vec更像是一个”<strong>死</strong>“的稠密向量, 单词嵌入后就是唯一的表示, 不能根据其在句子中的位置和上下文关系而改变其含义, 这就为处理<strong>一词多义</strong>问题添加了难度.</p><h2 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h2><p>ELMo全称是<strong>Embeddings from Language Models</strong>, 出自<a href="https://arxiv.org/abs/1802.05365" target="_blank" rel="noopener">Deep contextualized word representations</a>. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/elmo.jpg" style="zoom: 25%;" /><p>上面这个小人就是芝麻街里的角色ELMo. ELMo是一种基于Embedding的高级词向量表示. </p><p>前面说过, Word2vec具有局限性, 不能够表示一个单词的多种意思. 作者认为一个好的词向量有两个优点:</p><ol><li>能表示复杂的单词特性, 例如语义和语法信息.</li><li>词向量能适应多种语境有不同的体现, 即结合语境的一词多义.</li></ol><p>ELMo也是基于这两个点出发, 尝试通过更深的方法来实现基于<strong>上下文</strong>来表示单词的模型, 能够学习到更复杂的语义特征, 所以也说它是更高级的词向量. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/elmo1.png" style="zoom: 33%;" /><p>在上图中, ELMo指出对于不同的语义, 具有不同的词向量.</p><p>ELMo在执行不同的Task时或不同的语境时, 同一个词得到的词嵌入也可能是不同的.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/elmo2.png" style="zoom:50%;" /><h3 id="Bidirectional-Language-Models"><a href="#Bidirectional-Language-Models" class="headerlink" title="Bidirectional Language Models"></a>Bidirectional Language Models</h3><p>虽然ELMo自称是双向语言模型, 但实则不然. 先抛出这个结论, 在本小节结束的时候再点出.</p><p>论文中指出, 高层LSTM能够捕捉结合语境的<strong>词意</strong>信息, 而底层LSTM能捕捉<strong>语法</strong>上的特征, 把它们结合起来, 非常有利于下游任务的执行, 这也恰恰就是作者认为的优秀的词向量的特性.</p><p>ELMo采用<strong>无监督</strong>的方法, 利用<strong>语言模型</strong>的特点, 对句子本身的单词进行移位预测, 依赖串行结构以当前时刻以前的单词预测本时刻单词输出的性质也叫作<strong>自回归</strong>(Transformer的<strong>Decoder</strong>也是自回归结构). 文本数量非常庞大, 无需任何标签就能从中学习.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/elmo3.png" style="zoom: 50%;" /><p>如图中, LSTM在已知前三个单词”let’s”, “stick”, “to”的情况下需要预测”improvisation”.</p><p>另外, 引入从左向右和从右向左的两个<strong>单向LSTM</strong>能更好的结合上下文, 获得更复杂的语义表示. 对于多层的LSTM, 每层输出的隐态被作为下一层的输入继续传递.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/elmo4.png" style="zoom:50%;" /><p>这两个单向而反向的LSTM的权重将在预训练完成后被保存, 在正式预测时派上用场.</p><p>对于上述过程, 我们用数学语言来描述. 对于序列中的$N$ 个Token, $\left(t_{1}, t_{2}, \ldots, t_{N}\right)$, 第$k$ 个Token前的序列被描述为$\left(t_{1}, \ldots, t_{k-1}\right)$, 那么根据语言模型, 序列$\left(t_{1}, t_{2}, \ldots, t_{N}\right)$ 的概率就应该表示为:<br>$$<br>p\left(t_{1}, t_{2}, \ldots, t_{N}\right)=\prod_{k=1}^{N} p\left(t_{k} \mid t_{1}, t_{2}, \ldots, t_{k-1}\right)<br>$$<br>如果把上面的方向叫做<strong>Forward</strong>, 相应的, 与之相反的方向就称为<strong>Backward</strong>, 描述如下:<br>$$<br>p\left(t_{1}, t_{2}, \ldots, t_{N}\right)=\prod_{k=1}^{N} p\left(t_{k} \mid t_{k+1}, t_{k+2}, \ldots, t_{N}\right)<br>$$<br>对与语言模型, 普遍采用<strong>极大似然</strong>来调整两个单向LSTM的参数, 只要最大化向前和向后的对数概率就好:<br>$$<br>\begin{array}{l}<br>\sum_{k=1}^{N}\left[\log p\left(t_{k} \mid t_{1}, \ldots, t_{k-1} ; \Theta_{x}, \overrightarrow{\Theta}_{L S T M}, \Theta_{s}\right)\right.<br>\left.+\log p\left(t_{k} \mid t_{k+1}, \ldots, t_{N} ; \Theta_{x}, \overleftarrow{\Theta}_{L S T M}, \Theta_{s}\right)\right]<br>\end{array}<br>$$<br>其中$\overrightarrow{\Theta}_{L S T M}$ 和 $\overleftarrow{\Theta}_{L S T M}$ 分别代表两个LSTM的参数, $\Theta_{x}$ 代表Token的表示, $\Theta_{s}$ 代表Softmax层在两个LSTM维护的参数. 在论文中提到, 在某些任务中加入L2正则$\lambda \lVert\mathbf{w}\rVert _{2}^{2}$ 效果可能会更好.</p><p>再强调一遍, 这两个LSTM是单向工作的:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/elmo7.png" style="zoom: 67%;" /><h3 id="ELMo-Architecture"><a href="#ELMo-Architecture" class="headerlink" title="ELMo Architecture"></a>ELMo Architecture</h3><p>在ELMo中, 并非直接采用Word2vec作为输入, 而是在Embedding后采用了字符级的CNN作为替代输入. 作者认为字符级CNN是一种更加<strong>不敏感</strong>的上下文表示, 也有可能Word2vec本身就限制了多语义的表达.</p><p>如果输入用$\mathbf{x}_{k}^{L M}$ 表示, 一共有$L$ 层LSTM, 在论文中采取$L = 2$. 而Forward的LSTM最终的隐态输出为$\overrightarrow{\mathbf{h}}_{k, j}^{L M}$, Backward输出为$\overleftarrow{\mathbf{h}}_{k, j}^{L M}$, 那么$k$ 应该能被$2L+1$ 个参数表示, 称这个参数集合为$R_k$, 并将两个LSTM的隐态合并后有:<br>$$<br>\begin{aligned}<br>\mathbf{h}_{k, j}^{L M}&amp;=\left[\overrightarrow{\mathbf{h}}_{k, j}^{L M} ; \overleftarrow{\mathbf{h}}_{k, j}^{L M}\right] \\<br>R_{k} &amp;=\left\{\mathbf{x}_{k}^{L M}, \overrightarrow{\mathbf{h}}_{k, j}^{L M}, \overleftarrow{\mathbf{h}}_{k, j}^{L M} \mid j=1, \ldots, L\right\} \\<br>&amp;=\left\{\mathbf{h}_{k, j}^{L M} \mid j=0, \ldots, L\right\}<br>\end{aligned}<br>$$<br>为了更好的解释ELMo对下游任务是如何运作的, 将$R_k$和所有参数都表示为一个向量:<br>$$<br>\mathbf{E} \mathbf{L M o}_{k}=E\left(R_{k} ; \mathbf{\Theta}_{e}\right)<br>$$<br>对下游任务, ELMo对不同的Task采用了不同的权重.<br>$$<br>\mathbf{E} \mathbf{L} \mathbf{M} \mathbf{o}_{k}^{t a s k}=E\left(R_{k} ; \Theta^{t a s k}\right)=\gamma^{t a s k} \sum_{j=0}^{L} s_{j}^{t a s k} \mathbf{h}_{k, j}^{L M}<br>$$<br>$\text { s }^{\text {task}}$ 是经过Softmax标准化后的权重, $\gamma^{\text {task}}$ 是一个缩放因子. 对于每层LSTM, 它的标准化权重都是分别计算的, 最终是将所有层LSTM的隐态分别加以Softmax, 再相加求和, 这样对于不同的Task, ELMo就能学习到词向量在面对不同任务时的不同线性组合.</p><p>如下图, 最终的ELMo向量是由两个LSTM拼接后的向量和每层LSTM在经过Softmax后得到的权重加权求和而成的.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/elmo5.png" style="zoom:50%;" /><p>下面是ELMo在面对不同任务时Softmax所学习到的权重情况的可视化, 确实证明了ELMo对不同的任务下, 关注的权重不同, 对同样的词语有不同的表达:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/elmo6.jpg" style="zoom: 67%;" /><blockquote><ul><li>SRL: Semantic role labeling.</li><li>Coref: Coreference resolution.</li><li>SNLI: Textual entailment.</li><li>SQuAD: Question Answer.</li><li>SST-5: Sentiment analysis.</li></ul></blockquote><h3 id="How-to-Use-ELMo"><a href="#How-to-Use-ELMo" class="headerlink" title="How to Use ELMo"></a>How to Use ELMo</h3><p>在使用ELMo的时候, 要冻结两个LSTM的参数. 最简单的用法就是把LSTM<strong>最后一层</strong>的隐态输出作为词向量使用, 也可以老老实实将每层的LSTM隐态输出都加权求和. 具体怎么使用, 还是要结合<strong>任务复杂程度</strong>来确定.</p><p>作者在论文中提到, 可以将$\mathbf{E} \mathbf{L} \mathbf{M} \mathbf{o}_{k}^{t a s k}$ 和 输入$\mathbf{x}_{k}$ 一起concat起来, 即$\left[\mathbf{x}_{k} ; \mathbf{E} \mathbf{L} \mathbf{M} \mathbf{o}_{k}^{t a s k}\right]$, 加强表示, 然后作为词向量送入RNN, 也可以将$\mathbf{x}_{k}$ 替换为$\mathbf{h}_{k}$, 和ELMo向量concat起来, 即$\left[\mathbf{h}_{k} ; \mathbf{E} \mathbf{L} \mathbf{M} \mathbf{o}_{k}^{t a s k}\right]$.</p><p>总而言之, <strong>怎么用都行</strong>.</p><h3 id="Fake-Bidirectional-Language-Model"><a href="#Fake-Bidirectional-Language-Model" class="headerlink" title="Fake Bidirectional Language Model"></a>Fake Bidirectional Language Model</h3><p>现在来说说小节开始时遇到的那个问题:</p><p>ELMo所谓的双向实际上是通过<strong>两个单向且反向的LSTM</strong>实现的, 并不是直接使用双向LSTM, ELMo并非是原论文中所称的”deep bidirectional language model (biLM)”, 这点在BERT论文中曾有提到过:</p><blockquote><ul><li><p>BERT uses masked language models to enable pre-trained <strong>deep bidirectional</strong> representations. This is also in <strong>contrast</strong> to Peters et al. (2018a), which uses a shallow concatenation of <strong>independently</strong> trained left-to-right and right-to-left LMs.</p></li><li><p>Similar to ELMo, their model is feature-based and <strong>not deeply bidirectional</strong>.</p></li></ul></blockquote><p>当你了解BERT后, 会发现作者虽然一直在强调双向语言模型, 但ELMo并不是真正意义上的双向语言模型.</p><p><strong>那么为什么不采用双向LSTM实现呢?</strong></p><p>两个单向的LSTM和一个双向LSTM之间的区别, 对于单层的LSTM, 双向和单向差别无非就是hidden state是否concat或者add到一起. 但对于多层来说, 涉及到序列前后<strong>信息泄露</strong>的问题, 深层双向LSTM会被泄露上下文词语的位置信息, 导致模型学到了不该学习的东西, 也就失去了预训练的效果.</p><h2 id="Transformer-Review"><a href="#Transformer-Review" class="headerlink" title="Transformer Review"></a>Transformer Review</h2><p>之所以把Transformer Review放在这里, 是因为剩下的两个模型和Transformer关系很大, 如果有基础的可以直接跳过.</p><h3 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h3><p>如果你还不了解Transformer, 在这个小节能帮你<strong>快速回顾</strong>关于Transformer的部分知识, 因此所有的描述都非常简陋, 更详细的内容请看我之前写的<code>&lt;Transformer精讲&gt;</code>. </p><p>Transformer采用Seq2Seq架构, 分为Encoder和Decoder两个部分:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview1.png" style="zoom: 33%;" /><p>Encoder包括一个前FFN层和一个自注意力层, 结构如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview2.png" style="zoom:33%;" /><p>Decoder包括FFN层, 一个对接Encoder的自注意力层, 和一个Mask自注意力层, 结构如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview3.png" style="zoom:33%;" /><h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self - Attention"></a>Self - Attention</h3><h4 id="General-Self-Attention"><a href="#General-Self-Attention" class="headerlink" title="General Self - Attention"></a>General Self - Attention</h4><p>这个注意力说的是不加Mask的自注意力, 即出现在Encoder的第一层和Decoder的第二层. 每个自注意力层通常有多个头, 类似于CV中的卷积核, 能够抽取不同角度的特征. 我们先不考虑多头, 反正它的操作也是一致的, 最后再合并即可.</p><p>但是经过自注意力的vector的大小是不发生变化的(下图可能有shape错误). 自注意力机制一共有三步.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview4.png" style="zoom:33%;" /><p>首先, 每个Embedding的Token分别经过三个矩阵, 创建query vector, key vector, value vector.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview5.png" style="zoom:33%;" /><p>其次, 根据Transformer中提出的缩放点积注意力, 分别计算上下文其他Token对当前的Score, 并经过Softmax, 得到注意力权重:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview6.png" style="zoom:33%;" /><p>最后, 将注意力权重与对应的value vector加权求和得到结果.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview7.png" style="zoom:33%;" /><h4 id="Masked-Self-Attention"><a href="#Masked-Self-Attention" class="headerlink" title="Masked Self - Attention"></a>Masked Self - Attention</h4><p>在Encoder中, 所有Embedding是并行输入的, 这并不会影响Decoder的输出. 但我们提过, Decoder具有自回归性:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview13.png" style="zoom: 33%;" /><p>在解码时, 解码是单向的, 如果我们将所有数据并行输入, 那么就会造成<strong>信息泄露</strong>. </p><p>Mask正是为保证Decoder的自回归性和计算并行而存在的, 因为在解码时, Decoder应该只能看到当前时刻应该解码的内容和之前解码过的内容, 如下图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview14.png" style="zoom:33%;" /><p>灰色的向量代表Decoder不应该看见那部分信息. 我们添加一个Mask, 使得并行输入的矩阵的对角线以上的部分都不能被Decoder所解码:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview9.png" style="zoom: 33%;" /><p>我们来复现这个过程, 在Softmax之前先计算出Score矩阵:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview10.png" style="zoom: 50%;" /><p>具体对Score加Mask的方法就是将主对角线上的所有元素都变为负无穷:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview11.png" style="zoom:50%;" /><p>这样在做Softmax时, 主对角线上的信息会自动被屏蔽为0:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview12.png" style="zoom:50%;" /><p>这也就等价于Decoder对当前时刻以后的内容是不可知的:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformerreview8.png" style="zoom:33%;" /><h2 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h2><p>GPT的想法非常简单, 因为在NLP中有相当多种类的任务, 尽管有大量的未标注数据, 但用于指定任务的标注过的数据却很少, 这不能很好地评判已训练过的模型性能. GPT尝试用一种通用, <strong>任务无关</strong>的模型结构解决所有NLP问题. 对于不同的Task, 只需要在无监督的预训练后进行监督的Fine tune就行了, 这与CV界的Transfer Learning相同:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vggpretrain.png" style="zoom:50%;" /><p>GPT的三个模型几乎都是<strong>相同架构</strong>, 只是有非常非常少量的改动. 但一代比一代更大, 也更烧钱. 所以我对GPT系列的特点就是: <strong>钞能力, 大就完事了.</strong> 其影响力和花费的成本是成正比的.</p><p>先抛出三代GPT的论文出处:</p><ul><li>GPT - 1: <a href="https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf" target="_blank" rel="noopener">Improving Language Understanding by Generative Pre-Training</a></li><li>GPT - 2: <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" target="_blank" rel="noopener">Language Models are Unsupervised Multitask Learners</a> (可能需要魔法才能看)</li><li>GPT - 3: <a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener">Language Models are Few-Shot Learners</a></li></ul><p>因为架构相同, 后两篇论文大多数内容都是对增加参数后成果的展示. 时间不充裕建议只看第一篇, 因为每代区别不大, 本节<strong>一代为主</strong>, <strong>二代为辅</strong>, 三代先挖个坑, 以后会补.</p><blockquote><p>jalammar并没有做GPT - 1的图, 并且每代间又没有明显的结构区别, 所以就直接用二代的图了.</p></blockquote><p>GPT - 1其实并没有广泛的引起人们的关注, 反倒是GPT - 2和3让它火了一把. 最出名的就是GPT - 2生成的那篇关于独角兽的文章:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/独角兽.jpg" style="zoom:50%;" /><p>对此, 我个人认为除了生成的结果十分惊艳外, 确实有些炒作的成分. 惊艳是因为第一次见到机器能够生成如此有趣而附带一些逻辑的内容. 炒作主要原因是媒体夸大事实宣传, 博人眼球, 次要原因不难理解OpenAI也是需要科研资金的嘛. GPT目前<strong>相对于</strong>其他的NLP模型来说, 强是肯定的, 只是说代价太大了. 第二代和第三代强调了GPT有<strong>Few shot Learning</strong>和<strong>Zero shot Learning</strong>的潜力, 在巨大数据集的情况下, 模型甚至都没有收敛… 三代甚至不需要Fine tune…</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gpt13.jpg" style="zoom: 67%;" /><blockquote><p>Few - shot: 给出一个自然语言任务的一些范例, 但这些范例不能更新权重(算是一个限制, 给出更多信息).</p><p>One - shot: 仅给出一个范例, 让模型给出结果.</p><p>Zero - shot: 不给任何范例, 直接让模型给出结果.</p><p>在论文中, GPT - 3的Zero - shot和One - shot在某些任务上确实超过SOTA, 但相较人类还有很大差距. Few - shot比前二者有很大提升. 说明在没给出足够多信息的条件下, GPT - 3对<strong>问题理解能力</strong>还是比较差的.</p></blockquote><h3 id="Why-Transformer-Decoder"><a href="#Why-Transformer-Decoder" class="headerlink" title="Why Transformer Decoder?"></a>Why Transformer Decoder?</h3><p>言归正传, 来说GPT的具体结构. 受Transformer影响, GPT(Generative Pre-Training)采用<strong>Transformer</strong>作为基本的Block结构. 作者指出LSTM将预测能力限制在<strong>短距离</strong>内, 所以才采用使用Attention的Transformer作为<strong>长距离</strong>的信息抽取器. 当然, 这里只使用<strong>Decoder</strong>, 就需要去掉Decoder对Encoder的自注意力层, <strong>Mask</strong>后的自注意力保护了Decoder的<strong>自回归性</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gpt3.jpg" style="zoom: 33%;" /><p>在GPT一代中, 采用了12个Decoder堆叠.</p><h3 id="Unsupervised-pre-training"><a href="#Unsupervised-pre-training" class="headerlink" title="Unsupervised pre-training"></a>Unsupervised pre-training</h3><p>与标准语言模型一样, 对于无监督语料库中的Token $\mathcal{U}=\left\{u_{1}, \ldots, u_{n}\right\}$, 都采用<strong>极大似然</strong>优化:<br>$$<br>L_{1}(\mathcal{U})=\sum_{i} \log P\left(u_{i} \mid u_{i-k}, \ldots, u_{i-1} ; \Theta\right)<br>$$<br>其中$k$ 为上下文窗口, $\Theta$ 是神经网络参数, 它们会用<strong>随机梯度下降</strong>得到训练.</p><p>对于GPT来说, 选用Transformer的<strong>Decoder</strong>做最基本的Block. 将输入一层一层嵌套, 经过$n$ 个Transformer Block, 其数学表达为:<br>$$<br>\begin{aligned}<br>h_{0} &amp;=U W_{e}+W_{p} \\<br>h_{l} &amp;=\text { transformer_block }\left(h_{l-1}\right) \forall i \in[1, n] \\<br>P(u) &amp;=\operatorname{softmax}\left(h_{n} W_{e}^{T}\right)<br>\end{aligned}<br>$$<br>其中, $U=\left(u_{-k}, \ldots, u_{-1}\right)$ 是上下文向量, $n$ 为Decoder的层数.</p><p>$W_e$ 是Token的Embedding矩阵(一代嵌入维度只有768一种):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gpt4.jpg" style="zoom:50%;" /><p>$W_p$ 是Positional Embedding矩阵. 与Transformer不同, GPT的位置编码并非是通过三角函数计算来的, 而是通过训练学习到的.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gpt5.png" style="zoom: 50%;" /><p>$h_0$ 是嵌入后的向量和学习到的位置编码向量之和:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gpt6.png" style="zoom:50%;" /><p>然后就经过Transformer Block的自回归得到Decoder部分的输出$h_l$ :</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gpt7.png" style="zoom:50%;" /><p>对Decoder部分的输出做个总结吧(因为重复太多次了, 省略了再跑一遍的过程, 想看完整的去<a href="https://jalammar.github.io/illustrated-gpt2/" target="_blank" rel="noopener">这里</a>):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gpt11.png" style="zoom: 50%;" /><p>在得到最后输出前, 最后还需要再与<strong>Embedding相乘</strong>一次, 再通过Softmax得到结果$P(u)$: </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gpt9.png" style="zoom:50%;" /><p>“logits”就是在Softmax前的值, 后一般接一个Softmax输出标准化的概率. 在GPT二代中, 不再像一代直接选择概率最高的单词, 而是从<code>top_k</code> 中以某个<strong>概率</strong>选择单词, 这样来避免陷入永无止境的循环之中.</p><p>参数量实在是太大了, 每个部分占到的参数都非常非常多:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gpt12.png" style="zoom:50%;" /><h3 id="Supervised-Fine-tuning"><a href="#Supervised-Fine-tuning" class="headerlink" title="Supervised Fine-tuning"></a>Supervised Fine-tuning</h3><p>首先要加载预训练的权重, 然后要让预训练模型适应特定的任务, 对<strong>额外参数</strong>进行微调, 所以还要继续在当前基础上再加Linear层适应输出. 假设给出数据集$\mathcal{C}=(x^{1}, \ldots, x^{m}, y)$, 在通过一堆Transformer Block后得到的最终输出为$h_{l}^{m}$, 最后加一个Linear层参数为$W_y$, 有:<br>$$<br>P\left(y \mid x^{1}, \ldots, x^{m}\right)=\operatorname{softmax}\left(h_{l}^{m} W_{y}\right)<br>$$<br>有了标注的数据, 仍然采用<strong>极大似然</strong>进行优化:<br>$$<br>L_{2}(\mathcal{C})=\sum_{(x, y)} \log P\left(y \mid x^{1}, \ldots, x^{m}\right)<br>$$<br>无监督学习反作用于监督学习可以有一些提升. 所以在Fine tune这里用了前面无监督预训练的Loss做<strong>辅助训练</strong>. $\lambda$ 是超参, 论文中设置$\lambda=0.5$.<br>$$<br>L_{3}(\mathcal{C})=L_{2}(\mathcal{C})+\lambda \ast L_{1}(\mathcal{C})<br>$$<br>在微调中, 只有最后外接的$W_y$ 和句子之间的代表分隔符的Token的Embedding(看Overview and GPT in Specific Task)是需要额外进行学习的.</p><h3 id="More-Details"><a href="#More-Details" class="headerlink" title="More Details"></a>More Details</h3><p>一些超参的设置请参考原论文, 这里只说容易被忽略的点.</p><h4 id="BPE"><a href="#BPE" class="headerlink" title="BPE"></a>BPE</h4><p>BPE(byte pair encode), 也称为<strong>字节对编码</strong>, 其主要目的是<strong>数据压缩</strong>, 现在已经被作为重要的提升NLP模型性能的算法. 这种编码拆除了语言学特性, 但通过统计学方法更好的解决语言类问题. 做法请参考论文<a href="https://arxiv.org/abs/1508.07909" target="_blank" rel="noopener">Neural Machine Translation of Rare Words with Subword Units</a>, 我认为日后有必要把几个subword技巧做个对比.</p><h4 id="GeLU"><a href="#GeLU" class="headerlink" title="GeLU"></a>GeLU</h4><p>在GPT中使用的激活函数是<strong>GeLU</strong>(<strong>G</strong>aussian <strong>E</strong>rror <strong>L</strong>inerar <strong>U</strong>nits)不是ReLU! </p><p>GeLU原文中作者给出的近似公式:<br>$$<br>\text{GeLU}(x) = 0.5x(1 + \text{tanh}[\sqrt{\frac{2}{\pi}}(x+0.044715x^3)])<br>$$<br>具体内容请见论文<a href="https://arxiv.org/abs/1606.08415" target="_blank" rel="noopener">Gaussian Error Linear Units (GELUs)</a>.</p><h4 id="WarmUp"><a href="#WarmUp" class="headerlink" title="WarmUp"></a>WarmUp</h4><p>Fine tune阶段使用了线性衰减的WarmUp, 请参考我在<code>&lt;Transformer精讲&gt;</code>中的内容.</p><h3 id="Overview-and-GPT-in-Specific-Task"><a href="#Overview-and-GPT-in-Specific-Task" class="headerlink" title="Overview and GPT in Specific Task"></a>Overview and GPT in Specific Task</h3><h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gpt1.jpg" style="zoom: 50%;" /><p>GPT的结构非常简单, 实质就是12个Transformer Decoder的堆叠, 最后再根据任务需要<strong>接入结构</strong>以完成最终任务.</p><blockquote><p>这里能看出GPT的一个缺点, 虽然Self - Attention能够很好地利用全局的文章信息, 但是由于Decoder自回归性的限制, GPT是一个<strong>单向语言模型</strong>, 在Summary部分会继续与其他模型进行对比.</p></blockquote><h4 id="Input-Transformations"><a href="#Input-Transformations" class="headerlink" title="Input Transformations"></a>Input Transformations</h4><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gpt2.jpg" style="zoom: 33%;" /><p>执行不同的任务时, 需要对输入进行相应的调整. 在微调的过程中, 对于每种任务的开始和结束都需要加入<strong>标记</strong>$\langle s\rangle,\langle e\rangle$, 下文不再强调, 两段不同的内容之间需要加入分隔符$ $ $ .</p><h5 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h5><p>做分类任务非常简单, 在原输入两侧加上开始结束标记即可, 可以直接使用原来的模型微调.</p><h5 id="Textual-entailment"><a href="#Textual-entailment" class="headerlink" title="Textual entailment"></a>Textual entailment</h5><p>文本蕴含任务, 将premise和hypothesis拼接起来, 在二者之间加入分隔符.</p><h5 id="Similarity"><a href="#Similarity" class="headerlink" title="Similarity"></a>Similarity</h5><p>句子相似度任务, 在句子A和句子B之间加上分隔符, 再交换句子A和句子B的位置, 分别传入Transformer, 将二者结果add起来, 传入Linear.</p><h5 id="Question-Answering-and-Commonsense-Reasoning"><a href="#Question-Answering-and-Commonsense-Reasoning" class="headerlink" title="Question Answering and Commonsense Reasoning"></a>Question Answering and Commonsense Reasoning</h5><p>QA和常识推理任务, 将内容文本和回答分别组合, 并在之间加上分隔符, 传入Transformer, 分别经过Linear, 经过Softmax, 最后得到概率分布.</p><p>GPT还能做其他的事情, 例如音乐生成之类的, 感兴趣自己了解下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gpt10.png" style="zoom: 50%;" /><h3 id="Difference-Between-GPT-1-and-GPT-2"><a href="#Difference-Between-GPT-1-and-GPT-2" class="headerlink" title="Difference Between GPT - 1 and GPT - 2"></a>Difference Between GPT - 1 and GPT - 2</h3><p>GPT一代到二代仅发生了几个不同:</p><ol><li><p>用了更大的数据集, 尤其是网页文本, 40G.</p></li><li><p>增加了海量参数, 并推出了几个不同的版本, 一个比一个大:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gpt8.png" style="zoom:50%;" /><p>除了使用的Decoder个数不同, 它们的Embedding维度也是不同的.</p></li><li><p>直接去掉了Fine - tune层, 直接输入任务和所需的内容就能得到输出.</p></li><li><p>将Layer Norm放到了每个子层的输入前, 并且在最后一个自注意力层后添加了Layer Norm. 通过放缩权重更换了残差层的初始方式.</p></li></ol><h3 id="Postscript"><a href="#Postscript" class="headerlink" title="Postscript"></a>Postscript</h3><p>其实GPT一直都存在伦理上的问题. 有些人认为语言模型训练来自于语料, 语料的偏见会导致模型带有偏见. 而且GPT当初被OpenAI认为过于危险, 可能在只经过fine tune后被恶意滥用, 没有将这个庞然大物的参数开放.</p><blockquote><p>但可以通过下面几个链接去体验GPT - 2(可能需要魔法):</p><ul><li><a href="http://textsynth.org/" target="_blank" rel="noopener">Text Synth</a></li><li><a href="https://talktotransformer.com/" target="_blank" rel="noopener">Talk to Transformer</a></li></ul></blockquote><p>最后说一些自己的感想, 不想看的可以跳过这段.</p><p>GPT经历了三代, 有那么一点<strong>哲学</strong>和<strong>讽刺</strong>的意味. 第三代的1750亿参数几乎已经逼近了参数量的极限, 但实际上GPT产生的文章并没有媒体文章渲染的那么恐怖, GPT生成的内容还是经常犯一些常识性的<strong>错误</strong>, 即使是靠堆参数, 模型也并没有真的做到”Natural Language Understanding”, “大”真的意味着它<strong>智能</strong>了吗? 我们接触的世界是一个<strong>多模态</strong>的世界, 而计算机不能真正触及我们所接触的任何物体, 只能通过我们提供的<strong>数据</strong>来做到”认知”. 尽管我对AI发展持乐观态度, 但现在人们所强调的技术方法绝对不能有效的构造一个<strong>智能体</strong>, 虽是一条过渡的必经之路, 但有些<strong>矫枉过正</strong>. 至少人类距离强人工智能还有非常遥远的一段距离(如果非要形容, 距离可能是光年为单位), <strong>任重而道远</strong>.</p><h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p>也受到<strong>Transformer</strong>的影响, BERT像GPT一样把Transformer加入了自然语言理解中, 也尝试训练一个预训练模型, 只经过微调就能适应NLP领域的各种任务. 现在的BERT已经遍地开花, 很多NLP任务都是BERT或者BERT的魔改在屠榜.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert.jpg" style="zoom:25%;" /><p>上面这个小人就是BERT, 与ELMo同样都来自芝麻街. 其实除了BERT, 大家还拼凑过Grover, ERNIE, Big Bird…这些全是芝麻街的小人, 只不过有些凑的比较强行就是了.</p><p>BERT出自论文<a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>, 我认为这篇论文的<strong>附录</strong>才是本体… 正文着重于讲BERT的训练方式, 与前人模型的区别, 以及取得的效果. 虽然正文也很重要, 但附录里才有BERT的具体实现方法, 以及与ELMO, GPT的对比. 所以在看这篇论文时, 一定记得看Appendix.</p><p>BERT(Bidirectional Encoder Representations from Transformers)延续了GPT的Pre - train + Fine tune的思路, BERT也是冲着<strong>通用语言模型</strong>的目标去的, 并适配了一套对任何任务<strong>不用变更输入模式</strong>的训练方法. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert7.png" style="zoom:50%;" /><p>在预训练和微调好了之后, 只需要接上一层FFN和Softmax就能做到分类:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert8.png" style="zoom: 50%;" /><p>另外, BERT可以<strong>抽取词向量</strong>, 近期已经被作为Word2vec的替代者了:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert18.png" style="zoom:50%;" /><p>在论文中, 作者对BERT提取的不同特征效果做了对比, 这也证实了BERT具有特征抽取能力:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert19.png" style="zoom:50%;" /><p>似乎将最后4层拼接起来的F1得分要高一些.</p><h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p>在BERT论文中指出了ELMo和GPT的不足: 它们不是<strong>双向语言模型</strong>, 所以BERT采用了<strong>多层双向的Transformer Encoder</strong>作为堆叠基础结构. </p><blockquote><p>所谓的双向Transformer实际上是Transformer Encoder, 所谓的”双向”是体现在MLM中(见Masked Language Model).</p><p>We note that in the literature the <strong>bidirectional Transformer</strong> is often referred to as a “<strong>Transformer encoder</strong>” while the left-context-only version is referred to as a “Transformer decoder” since it can be used for text generation.</p></blockquote><p>在论文中, BERT发表了两个不同的版本, $\mathbf{B E R T}_{\mathbf{B A S E}}$ 和 $\mathbf{B E R T}_{\mathbf{LARGE}}$.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert9.png" style="zoom: 33%;" /><p>根据论文中的参数给出对比二者的对比:</p><table><thead><tr><th align="center">模型</th><th align="center">堆叠层数$L$</th><th align="center">隐藏层大小$H$</th><th align="center">自注意力层头数$A$</th><th align="center">共计参数</th></tr></thead><tbody><tr><td align="center">BERT (Base)</td><td align="center">12</td><td align="center">786</td><td align="center">12</td><td align="center">110M</td></tr><tr><td align="center">BERT (Large)</td><td align="center">24</td><td align="center">1024</td><td align="center">16</td><td align="center">340M</td></tr><tr><td align="center">Transformer</td><td align="center">6</td><td align="center">512</td><td align="center">8</td><td align="center">/</td></tr></tbody></table><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert10.png" style="zoom: 33%;" /><p>此外, 在论文脚注中提到, FFN的大小为$4H$, 也采用了<strong>GeLU</strong>作为激活函数, 并使用了10000步的<strong>WarmUp</strong>.</p><h3 id="Input-Output-Representations"><a href="#Input-Output-Representations" class="headerlink" title="Input / Output Representations"></a>Input / Output Representations</h3><h4 id="CLS"><a href="#CLS" class="headerlink" title="[CLS]"></a>[CLS]</h4><p>在BERT中, 永远都将第一个位置输入<strong>分类提示符</strong><code>[CLS]</code>, 如果执行的是分类任务, 第一个位置最终会输出一个向量, 作为分类依据. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert11.png" style="zoom:50%;" /><p>BERT接受一些系列单词输入, 经过Transformer Encoder的堆叠, 得到输出.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert12.png" style="zoom:50%;" /><p>其实对于BERT来说, <code>[CLS]</code> 到底放在哪是无所谓的, 只是放在第一个习惯于我们理解. </p><p>在执行<strong>分类下游任务</strong>时(不是训练时), 其他位置无论有多少隐态输出, 我们都忽略, 只看<code>[CLS]</code> 对应位置上的输出, 也就是第一个位置:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert14.png" style="zoom:50%;" /><p>进行分类任务所搭建的网络也是在第一个位置上继续的, 结构也可以任意调整:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert15.png" style="zoom:50%;" /><h4 id="SEP"><a href="#SEP" class="headerlink" title="[SEP]"></a>[SEP]</h4><p>对于<strong>任何任务</strong>, BERT的输入永远都是将<strong>一对句子</strong>放在同一个序列中, 无论这对句子是真正连续的上下文还是随机拼接的. BERT句子和句子之间用分隔符<code>[SEP]</code> 隔开, 在结尾也要加上一个<code>[SEP]</code>.</p><h4 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h4><p>输入编码由三个部分组成:</p><ol><li>Token Embedding: 就是每个Token的Embedding.</li><li>Segment Embedding: 该Embedding起到了区分句子A和句子B的作用, 对A和B分别加以不同的编码.</li><li>Position Embedding: 与GPT一样, 位置信息也是学习来的, 而非公式计算.</li></ol><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert2.jpg" style="zoom: 67%;" /><h3 id="Pre-trained-Task"><a href="#Pre-trained-Task" class="headerlink" title="Pre - trained Task"></a>Pre - trained Task</h3><p>BERT采用了两种无监督任务, 使BERT能学到双向的上下文信息, 而且这种信息不是通过Forward和Backward获取的, 而是一次性获得的, 这就促使BERT成为一个双向语言模型. 在论文中, 指出预训练BERT的损失函数为下述两个任务的损失和.</p><h4 id="Masked-Language-Model"><a href="#Masked-Language-Model" class="headerlink" title="Masked Language Model"></a>Masked Language Model</h4><p>在双向深度的结构中, 会伴随着信息泄露, 在原文中称为”<strong>See itself</strong>“, 模型能够通过深层网络来自两个方向的信息得知此处的内容. 为了训练这种网络, BERT训练时采用<strong>随机Mask</strong>的方法, 使BERT必须通过上下文预测出这个位置的Token, 然后用<strong>极大似然</strong>来调整参数. 这其实就是在模仿我们做<strong>完形填空</strong>, 该方法也就是BERT能被称之为双向语言模型的根本原因.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert16.png" style="zoom:50%;" /><blockquote><p><strong>这种方法为什么有效?</strong> </p><p>因为BERT使用的是Transformer Encoder, 与Decoder不同, 是一种不带有自回归性的结构, 在Mask后, 无论怎么阅读句子, 对已经被Mask的位置内容都不可知, 只能强迫BERT根据上下文进行推测.</p></blockquote><p>因此, 该任务中, BERT直接根据上下文推测, 而非像ELMo一样采用两个单向的结构:<br>$$<br>P\left(w_{i} \mid w_{1}, \ldots, w_{i-1}, w_{i+1}, \ldots, w_{n}\right)<br>$$</p><blockquote><p>你看这式子, 是不是和<strong>CBOW</strong>非常像?</p></blockquote><p>但是在Fine tune的时候不可能对单词进行Mask, 这样就会导致预训练和微调的不匹配, 为缓解这种问题, 在每个句子中, 有15%的词会被选中, 在选中单词后有三种可能性:</p><ol><li><strong>80%</strong>的概率将被选中的单词替换为<code>[MASK]</code>.</li><li><strong>10%</strong>的概率将被选中的单词替换为<strong>随机词</strong>.</li><li><strong>10%</strong>的概率对被选中的单词<strong>不进行替换</strong>.</li></ol><p>这就给BERT一种非常迷惑的感觉, “<strong>即使没有Mask的单词仍然有可能是错的, 知道的也要预测, 不知道的还要预测</strong>“. 这就更加强迫BERT除了学到上下文关联外, 对每个词必须有理解能力.</p><p>另外, 随机替换在语料充足时并不会降低太多的模型性能, 因为它只有1.5%的几率发生.</p><p>当然, 因为每次只预测15%的Token, 模型的<strong>收敛速度</strong>会下降.</p><h4 id="Next-Sentence-Prediction"><a href="#Next-Sentence-Prediction" class="headerlink" title="Next Sentence Prediction"></a>Next Sentence Prediction</h4><p>另一种无监督任务比较好理解, 就是让BERT根据句子A和句子B, 在<code>[CLS]</code>处输出这两个句子是否是连贯的上下文. 无论是QA问题, 还是自然语言推理(NLI), 都是建立在理解相邻文本该关系基础之上的.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert17.png" style="zoom:50%;" /><p>在挑选两个句子时, 句子A与B是否相关各有50%的几率.</p><p>论文中给出示例如下:</p><pre><code>Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]Label = IsNextInput = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]Label = NotNext</code></pre><blockquote><p>##less是word piece产生的, 先挖个坑以后填.</p></blockquote><h3 id="Fine-Tune"><a href="#Fine-Tune" class="headerlink" title="Fine - Tune"></a>Fine - Tune</h3><p>从预训练到微调, BERT可以很轻松的发生转换:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert1.jpg" style="zoom:50%;" /><p>$C$ 为<code>[CLS]</code>对应位置的最终输出, $T_i$ 代表第$i$ 个Token对应位置的输出. $E_i$ 代表Embedding.</p><p>在不同任务上的微调方式可能是不同的, 但仍然不用对模型结构进行改动:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bert3.png"/><p>图中的四类任务分别为:</p><ul><li><p>(a): 句子匹配分类任务.</p></li><li><p>(b): 单个句子分类任务.</p></li><li><p>(c): QA类任务. SQuAD中问题的答案一定在原文中会出现, 所以输出的是在原文中起始和结束的位置. </p></li><li><p>(d): 命名体识别任务.</p></li></ul><p>(a)和(b)是Sequence级的任务, (c)和(d)是Token级的任务. BERT在这些任务上都有具体的处理方法, 详情参见论文.</p><h3 id="BERT-in-One-Word"><a href="#BERT-in-One-Word" class="headerlink" title="BERT in One Word"></a>BERT in One Word</h3><p>总的来说, BERT像一个近些年人们在NLP上探索成果的<strong>融合</strong>, 但结合了自监督学习, Token Mask + 双向LM训练, Pre - train + Finetune的思想, 将Transformer, 位置编码一起使用. 现在NLP已经进入到<strong>BERT时代</strong>, 几乎由BERT魔改得到的模型都能取得显著的成果.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>这部分, 我们把ELMo, GPT, BERT三个模型放在一起来看, 可能有些零碎.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/elmogptbert.jpg" style="zoom: 67%;" /><ul><li><p>在三个模型中, 只有BERT是真正能够捕捉所有层的上下文信息的. 这受益于Transformer中的自注意力机制, 将所有Token之间的距离直接缩短到1, 加权求和. ELMo和GPT都是单向捕捉信息的.</p></li><li><p>三个模型的Basic Block: LSTM, Transformer Decoder, Transformer Encoder. GPT和BERT都是用Transformer组件作为基础Block.</p></li><li><p>GPT在预训练时并没有引入<code>[CLS]</code>和<code>[SEP]</code>, BERT全程引入.</p></li><li><p>GPT和BERT是基于<strong>微调</strong>的方法, 模型结构不用发生变化, 而ELMo是基于<strong>特征</strong>的方法, 仅用于抽取特征.</p></li><li><p>对Input来讲, ELMo在Embedding后用<strong>字符级CNN</strong>, GPT采用<strong>BPE</strong>, BERT用了<strong>Word Piece</strong>. GPT有位置编码, BERT有位置编码和段编码. 这些不同也与模型的输入方式有关.</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 词向量 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch学习: 张量进阶操作</title>
      <link href="/posts/1216.html"/>
      <url>/posts/1216.html</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>2020.10.03</strong>: 因torch版本更新, 对gather描述进行了修正.</p><p><strong>2021.03.11</strong>: 更新了对gather的描述.</p></blockquote><h1 id="Pytorch学习-张量进阶操作"><a href="#Pytorch学习-张量进阶操作" class="headerlink" title="Pytorch学习: 张量进阶操作"></a>Pytorch学习: 张量进阶操作</h1><p>整理内容顺序来自龙龙老师的<code>&lt;深度学习与PyTorch入门实战教程&gt;</code>, 根据个人所需情况进行删减或扩充. 如果想要自己创建新的模块, 这些操作都是基本功, 需要掌握扎实.</p><h2 id="拼接与拆分"><a href="#拼接与拆分" class="headerlink" title="拼接与拆分"></a>拼接与拆分</h2><h3 id="cat"><a href="#cat" class="headerlink" title="cat"></a>cat</h3><p><code>torch.cat(*tensor, dim)</code>能在指定的维度上将tensor拼接:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'b.shape:'</span><span class="token punctuation">,</span> b<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a concat b:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>a<span class="token punctuation">,</span> b<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""a.shape: torch.Size([2, 3])b.shape: torch.Size([5, 3])a concat b: torch.Size([7, 3])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>当然, concat之前必须保证除了cat的维度的shape不同外, 其他维度的shape均相同.</p><h3 id="stack"><a href="#stack" class="headerlink" title="stack"></a>stack</h3><p>与<code>torch.cat()</code>不同, <code>torch.stack()</code>是将tensor堆叠在一个<strong>新的维度</strong>上, 即创建一个新的维度:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'b.shape:'</span><span class="token punctuation">,</span> b<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a concat b:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>a<span class="token punctuation">,</span> b<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a stack b:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>a<span class="token punctuation">,</span> b<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""a.shape: torch.Size([5, 3])b.shape: torch.Size([5, 3])a concat b: torch.Size([10, 3])a stack b: torch.Size([2, 5, 3])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>两个stack的tensor必须保持完全一致的维度.</p><h3 id="split"><a href="#split" class="headerlink" title="split"></a>split</h3><p><code>torch.split()</code>既可以按照指定dim的长度来拆分, 也可以按照类似步长的拆法来拆分.</p><p>当传入的参数是<code>list</code>时, 按照列表中指定的长度拆分:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span>c <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> b<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'c.shape:'</span><span class="token punctuation">,</span> c<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>s <span class="token operator">=</span> c<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token keyword">for</span> i<span class="token punctuation">,</span> n <span class="token keyword">in</span> zip<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'aa'</span><span class="token punctuation">,</span> <span class="token string">'bb'</span><span class="token punctuation">,</span> <span class="token string">'cc'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> s<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'{}.shape:{}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>i<span class="token punctuation">,</span> n<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""c.shape: torch.Size([3, 32, 8])aa.shape:torch.Size([1, 32, 8])bb.shape:torch.Size([1, 32, 8])cc.shape:torch.Size([1, 32, 8])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这样能将c分别拆分为长度为1, 1, 1的tensor.</p><p>当传入的参数是<code>int</code>时, 意在使指定dim上每个tensor应该具有多长, 也可以理解为<strong>步长</strong>:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span>c <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> b<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'c.shape:'</span><span class="token punctuation">,</span> c<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># s = c.split([1, 1, 1], dim=0)</span>s <span class="token operator">=</span> c<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token keyword">for</span> i<span class="token punctuation">,</span> n <span class="token keyword">in</span> zip<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'aa'</span><span class="token punctuation">,</span> <span class="token string">'bb'</span><span class="token punctuation">,</span> <span class="token string">'cc'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> s<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'{}.shape:{}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>i<span class="token punctuation">,</span> n<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""c.shape: torch.Size([3, 32, 8])aa.shape:torch.Size([2, 32, 8])bb.shape:torch.Size([1, 32, 8])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>虽然在dim=0上tensor长度为3, 但是还是能够拆分成2个tensor. 如果传入的整数是3, 则只会得到<code>aa.shape:torch.Size([3, 32, 8])</code>, 即每个tensor应该在dim=0上有3个元素.</p><h3 id="chunk"><a href="#chunk" class="headerlink" title="chunk"></a>chunk</h3><p>我认为<code>torch.chunk()</code>是为了和<code>torch.split()</code>同参数而<strong>防止歧义</strong>的区分函数.</p><p><code>torch.chunk()</code>中需要传入的就是需要分多少个tensor了, 还是上面那个例子, 体会区别:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span>c <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> b<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'c.shape:'</span><span class="token punctuation">,</span> c<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># s = c.split([1, 1, 1], dim=0)</span>s <span class="token operator">=</span> c<span class="token punctuation">.</span>chunk<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token keyword">for</span> i<span class="token punctuation">,</span> n <span class="token keyword">in</span> zip<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'aa'</span><span class="token punctuation">,</span> <span class="token string">'bb'</span><span class="token punctuation">,</span> <span class="token string">'cc'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> s<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'{}.shape:{}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>i<span class="token punctuation">,</span> n<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""c.shape: torch.Size([3, 32, 8])aa.shape:torch.Size([1, 32, 8])bb.shape:torch.Size([1, 32, 8])cc.shape:torch.Size([1, 32, 8])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="数学运算"><a href="#数学运算" class="headerlink" title="数学运算"></a>数学运算</h2><p>大多数操作与python中的基本类型操作区别不大, pytorch使用了<strong>运算符重载</strong>使得它们在tensor上保持相同的含义. pytorch也提供了函数为tensor做运算. 大部分函数仍然和numpy中的格式一致.</p><h3 id="加减乘除"><a href="#加减乘除" class="headerlink" title="加减乘除"></a>加减乘除</h3><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a + b和add:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>all<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>a <span class="token operator">+</span> b<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>add<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a - b和sub:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>all<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>a <span class="token operator">-</span> b<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>sub<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a * b和mul:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>all<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>a <span class="token operator">*</span> b<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a + b和div:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>all<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>a <span class="token operator">/</span> b<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>div<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""a + b和add: tensor(1, dtype=torch.uint8)a - b和sub: tensor(1, dtype=torch.uint8)a * b和mul: tensor(1, dtype=torch.uint8)a + b和div: tensor(1, dtype=torch.uint8)"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>当然上面的例子中说的乘法不是矩阵乘法, 而是点乘(element - wise).</p><h3 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h3><p>如果要实现矩阵乘有三种方式:</p><ul><li><code>torch.mm()</code><strong>仅限于2d - tensor</strong>.</li><li><code>torch.matmul()</code>推荐使用.</li><li><code>@</code>是重载的运算符.</li></ul><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'mm in 2d:\n'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'matmul:\n'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'@:\n'</span><span class="token punctuation">,</span> a @ b<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""mm in 2d: tensor([[0.8500, 0.8500],        [1.0964, 1.0964]])matmul: tensor([[0.8500, 0.8500],        [1.0964, 1.0964]])@: tensor([[0.8500, 0.8500],        [1.0964, 1.0964]])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>记住, 如果tensor的维度比2大, 则默认在<strong>最后两个维度</strong>上进行运算, 也可以理解为对多个矩阵<strong>并行</strong>做矩阵乘法.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'matmul(a, b).shape:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># matmul(a, b).shape: torch.Size([4, 3, 28, 28])</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="指数和对数"><a href="#指数和对数" class="headerlink" title="指数和对数"></a>指数和对数</h3><p>既可以沿用python中的<code>**</code>做幂计算, 也可以使用<code>Tensor.pow()</code>.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.pow(2):\n'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>pow<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a ** 2:\n'</span><span class="token punctuation">,</span> a <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a ** 2.sqrt():\n'</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>a <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a ** 2.rsqrt():\n'</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>a <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>rsqrt<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a ** 2 ** 0.5:\n'</span><span class="token punctuation">,</span> a <span class="token operator">**</span> <span class="token number">2</span> <span class="token operator">**</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""a.pow(2): tensor([[9., 9.],        [9., 9.]])a ** 2: tensor([[9., 9.],        [9., 9.]])a ** 2.sqrt(): tensor([[3., 3.],        [3., 3.]])a ** 2.rsqrt(): tensor([[0.3333, 0.3333],        [0.3333, 0.3333]])a ** 2 ** 0.5: tensor([[4.7288, 4.7288],        [4.7288, 4.7288]])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>Tensor.rsqrt()</code>代表的是求平方根后的倒数.</p><p>对数指数也是一样的用法:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'exp:'</span><span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'log:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""exp: tensor([[2.7183, 2.7183, 2.7183],        [2.7183, 2.7183, 2.7183]])log: tensor([[0., 0., 0.],        [0., 0., 0.]])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="近似值"><a href="#近似值" class="headerlink" title="近似值"></a>近似值</h3><p>近似值会在有小数点进行取舍时用到.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">3.14</span><span class="token punctuation">)</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'a.floor()'</span><span class="token punctuation">,</span> <span class="token string">'a.ceil()'</span><span class="token punctuation">,</span> <span class="token string">'a.trunc()'</span><span class="token punctuation">,</span> <span class="token string">'a.frac()'</span><span class="token punctuation">,</span> <span class="token string">'a.round()'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token string">':'</span><span class="token punctuation">,</span> eval<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""a.floor(): tensor(3.)a.ceil(): tensor(4.)a.trunc(): tensor(3.)a.frac(): tensor(0.1400)a.round(): tensor(3.)"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><code>floor</code>: 下取整.</li><li><code>ceil</code>: 上取整.</li><li><code>trunc</code>: 只要整数部分.</li><li><code>frac</code>: 只要小数部分.</li><li><code>round</code>: 四舍五入.</li></ul><h3 id="裁剪"><a href="#裁剪" class="headerlink" title="裁剪"></a>裁剪</h3><p>这个操作在<strong>梯度裁剪</strong>非常常用. 当发生梯度爆炸或消失时, 使用梯度裁剪能将梯度控制在可控范围内.</p><p>在pytorch中,<code>Tensor.clamp(min, max)</code>函数作用等价于numpy的<code>np.clip()</code>.</p><pre class="line-numbers language-python"><code class="language-python">grad <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">15</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'max:'</span><span class="token punctuation">,</span> grad<span class="token punctuation">.</span>max<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'min:'</span><span class="token punctuation">,</span> grad<span class="token punctuation">.</span>min<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'grad:\n'</span><span class="token punctuation">,</span> grad<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'clamp(10):\n'</span><span class="token punctuation">,</span> grad<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'clamp(0, 10):\n'</span><span class="token punctuation">,</span> grad<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""max: tensor(13.1130)min: tensor(1.6760)grad: tensor([[10.3501, 13.1130,  1.6760],        [ 3.1330,  7.5342,  4.7226]])clamp(10): tensor([[10.3501, 13.1130, 10.0000],        [10.0000, 10.0000, 10.0000]])clamp(0, 10): tensor([[10.0000, 10.0000,  1.6760],        [ 3.1330,  7.5342,  4.7226]])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="属性统计"><a href="#属性统计" class="headerlink" title="属性统计"></a>属性统计</h2><h3 id="norm"><a href="#norm" class="headerlink" title="norm"></a>norm</h3><p>这里的norm指的不是标准化的那个normalization, 而是指的范数. 具体对范数的定义, 在这里就不再给出了, 请自己查询.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>b <span class="token operator">=</span> a<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>c <span class="token operator">=</span> a<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.norm(1):'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'b.norm(1):'</span><span class="token punctuation">,</span> b<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'c.norm(1):'</span><span class="token punctuation">,</span> c<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.norm(2):'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'b.norm(2):'</span><span class="token punctuation">,</span> b<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'c.norm(2):'</span><span class="token punctuation">,</span> c<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""a.norm(1): tensor(8.)b.norm(1): tensor(8.)c.norm(1): tensor(8.)a.norm(2): tensor(2.8284)b.norm(2): tensor(2.8284)c.norm(2): tensor(2.8284)"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这样得到的结果都是标量, 也可以按照dim来求范数:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>b <span class="token operator">=</span> a<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>c <span class="token operator">=</span> a<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'dim 1:'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'b.norm(1):'</span><span class="token punctuation">,</span> b<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'c.norm(1):'</span><span class="token punctuation">,</span> c<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'b.norm(2):'</span><span class="token punctuation">,</span> b<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'c.norm(2):'</span><span class="token punctuation">,</span> c<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""dim 1:b.norm(1): tensor([4., 4.])c.norm(1): tensor([[2., 2.],        [2., 2.]])b.norm(2): tensor([2., 2.])c.norm(2): tensor([[1.4142, 1.4142],        [1.4142, 1.4142]])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="mean-sum-min-max-prod"><a href="#mean-sum-min-max-prod" class="headerlink" title="mean / sum / min / max / prod"></a>mean / sum / min / max / prod</h3><p>这一系列都是统计操作, 不是很难理解.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'a.min()'</span><span class="token punctuation">,</span> <span class="token string">'a.max()'</span><span class="token punctuation">,</span> <span class="token string">'a.mean()'</span><span class="token punctuation">,</span> <span class="token string">'a.prod()'</span><span class="token punctuation">,</span> <span class="token string">'a.sum()'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token string">':'</span><span class="token punctuation">,</span> eval<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""a.min(): tensor(0.)a.max(): tensor(7.)a.mean(): tensor(3.5000)a.prod(): tensor(0.)a.sum(): tensor(28.)"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="argmin-argmax"><a href="#argmin-argmax" class="headerlink" title="argmin / argmax"></a>argmin / argmax</h3><p>在求min和max时经常有一种操作, 找到一个tensor中最大或最小的元素并返回其<strong>索引</strong>, <code>Tensor.argmin()</code>和<code>Tensor.argmax()</code>就能实现这个功能. 不加参数默认为返回整个tensor中最大或最小的元素索引, 加dim后为沿着该维度切分tensor, 找到每个tensor最大或最小的元素索引.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.argmin():'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>argmin<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.argmin(dim=1):'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>argmin<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.argmax():'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.argmax(dim=1):'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""a.argmin(): tensor(2)a.argmin(dim=1): tensor([2, 2])a.argmax(): tensor(4)a.argmax(dim=1): tensor([0, 1])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="dim-keepdim"><a href="#dim-keepdim" class="headerlink" title="dim / keepdim"></a>dim / keepdim</h3><p><code>dim</code>和<code>keepdim</code>是作为<strong>参数</strong>放在前面所说的函数中的, 对于<code>dim</code>我们已经接触很多次了, 理解为沿着该维度进行某种操作. <code>keepdim</code>指的是在函数做完操作后还要不要维持原来的维度, 如:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.argmin(dim=1):'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.argmin(dim=1, keepdim=True):\n'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.argmin(dim=1).shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.argmin(dim=1, keepdim=True).shape:\n'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""a.argmin(dim=1): tensor([7, 2, 8, 7])a.argmin(dim=1, keepdim=True): tensor([[7],        [2],        [8],        [7]])a.argmin(dim=1).shape: torch.Size([4])a.argmin(dim=1, keepdim=True).shape: torch.Size([4, 1])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在保存维度后, 结果的shape是<code>torch.Size([4, 1])</code>, 否则会被自动消去, 即<code>torch.Size([4])</code>.</p><h3 id="topk-kthvalue"><a href="#topk-kthvalue" class="headerlink" title="topk / kthvalue"></a>topk / kthvalue</h3><p>Top - k也是很常用的操作, 函数能返回最大的前k个值的相关信息. 在pytorch中, <code>Tensor.topk(k)</code>能返回最值和它们的索引.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'top3 in dim 1:\n'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>topk<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""top3 in dim 1:torch.return_types.topk(values=tensor([[0.9354, 0.8272, 0.8214],        [0.9918, 0.8757, 0.8410],        [0.9744, 0.8817, 0.8365],        [0.9985, 0.8475, 0.8181]]),indices=tensor([[3, 6, 0],        [6, 0, 8],        [7, 0, 9],        [5, 4, 6]]))"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>因为输入的tensor大小为(4, 10), 并绑定dim=1, 所以返回了4条top3的value和index.</p><p>通过<code>largest</code>参数来控制选择最大值还是最小值, 当其为<code>True</code>时选择最大的k个值, <code>False</code>时选择最小的k个值.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'top-3 in dim 1:\n'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>topk<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> largest<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token string">""</span>"top<span class="token number">-3</span> <span class="token keyword">in</span> dim <span class="token number">1</span><span class="token punctuation">:</span>torch<span class="token punctuation">.</span>return_types<span class="token punctuation">.</span>topk<span class="token punctuation">(</span>values<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.0362</span><span class="token punctuation">,</span> <span class="token number">0.0593</span><span class="token punctuation">,</span> <span class="token number">0.1323</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0.1024</span><span class="token punctuation">,</span> <span class="token number">0.1237</span><span class="token punctuation">,</span> <span class="token number">0.1397</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0.0160</span><span class="token punctuation">,</span> <span class="token number">0.1430</span><span class="token punctuation">,</span> <span class="token number">0.2003</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0.2674</span><span class="token punctuation">,</span> <span class="token number">0.2686</span><span class="token punctuation">,</span> <span class="token number">0.4065</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>indices<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果你只是想要从小到大排列第k个值, 那么<code>Tensor.kthvalue</code>能满足你的需求.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a:'</span><span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.kthvalue(2):'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>kthvalue<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""a: tensor([[0.2830, 0.2628, 0.2188, 0.9593, 0.6418],        [0.8727, 0.2504, 0.8656, 0.3067, 0.6215],        [0.0977, 0.7201, 0.1081, 0.2605, 0.7691],        [0.4776, 0.9503, 0.9577, 0.4100, 0.6476]])a.kthvalue(2): torch.return_types.kthvalue(values=tensor([0.2628, 0.3067, 0.1081, 0.4776]),indices=tensor([1, 3, 2, 0]))"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="compare"><a href="#compare" class="headerlink" title="compare"></a>compare</h3><p>和python中的比较大小运算符一样, 有<code>&gt;</code>, <code>&gt;=</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>!=</code>, <code>==</code>. 它们都能比较tensor之间的大小关系, 它们也都有相应的缩写函数. 这里不详细说了, 非要用函数再从网上查就可以了.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a <span class="token operator">></span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""tensor([[ 0.1539, -0.9071, -1.6426],        [ 0.7671, -1.7312, -0.8053]])tensor([[0, 0, 0],        [1, 0, 0]], dtype=torch.uint8)"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>有一特殊函数<code>torch.eq(a, b)</code>和<code>torch.equal(a, b)</code>的返回值是不一样的, 前者返回Booltensor, 后者返回一个逻辑值, 只有全部相同时才为True, 否则为False.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>a<span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>equal<span class="token punctuation">(</span>a<span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""tensor([[1, 1, 1],        [1, 1, 1]], dtype=torch.uint8)True"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="高阶操作"><a href="#高阶操作" class="headerlink" title="高阶操作"></a>高阶操作</h2><h3 id="where"><a href="#where" class="headerlink" title="where"></a>where</h3><p>where的用法是<code>torch.where(condition, x, y)</code>, 返回对应位置上符合condition的tensor. 如果符合condition, 则对应元素为x, 否则为y.</p><pre class="line-numbers language-python"><code class="language-python">condition <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>condition<span class="token punctuation">)</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>condition <span class="token operator">></span> <span class="token number">0.5</span><span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""tensor([[0.0559, 0.1984],        [0.9894, 0.2738]])tensor([[1., 1.],        [0., 1.]])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="gather"><a href="#gather" class="headerlink" title="gather"></a>gather</h3><p><code>torch.gather(input, dim, index)</code>的作用是将输入tensor按照指定的dim和index<strong>重新组合</strong>, <strong>类似于查表</strong>, 返回一个新的tensor. 它能<strong>收集特定维度的指定位置的数值</strong>. 这种操作常用于将概率转化为具体的类. 理解起来比较抽象.</p><pre class="line-numbers language-python"><code class="language-python">prob <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>index <span class="token operator">=</span> prob<span class="token punctuation">.</span>topk<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'index:'</span><span class="token punctuation">,</span> index<span class="token punctuation">)</span>label <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">100</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'取到的classes:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>gather<span class="token punctuation">(</span>label<span class="token punctuation">.</span>expand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> index<span class="token operator">=</span>index<span class="token punctuation">.</span>long<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""index: tensor([[5, 3, 2],        [0, 6, 7],        [1, 4, 5],        [2, 9, 7]])取到的classes: tensor([[105, 103, 102],        [100, 106, 107],        [101, 104, 105],        [102, 109, 107]])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>index</code>传入的数据类型必须是<code>torch.LongTensor</code>.</p><p>再说的通俗一点, <code>index</code>的作用就像一个<strong>Mask</strong>一样,  存储的是<strong>指定dim上的位置索引</strong>, 再看一个更简单的例子:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>index_1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>index_2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>gather<span class="token punctuation">(</span>a<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> index<span class="token operator">=</span>index_1<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>gather<span class="token punctuation">(</span>a<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> index<span class="token operator">=</span>index_2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""tensor([[1., 2., 3.],        [4., 5., 6.]])tensor([[1., 5., 6.],        [1., 5., 3.]])tensor([[1., 2.],        [6., 4.]])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果还没明白, 参照官网给的3d - tensor解释结合起来:</p><pre class="line-numbers language-python"><code class="language-python">out<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> input<span class="token punctuation">[</span>index<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span>k<span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># if dim == 0</span>out<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> input<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>index<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">[</span>k<span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># if dim == 1</span>out<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> input<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span>index<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># if dim == 2</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>以官网的角度说一下上面那个例子. 假设gather后的向量名为<code>new_tensor</code>, 我们可以手动模拟这个过程和构造出gather相同的结果.</p><p><strong>dim=0</strong>时, 对于a来说, <strong>第0维</strong>位置index是通过<strong>查表</strong>获得的.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token triple-quoted-string string">"""out[i][j] = input[index[i][j]][j] if dim == 0a = torch.Tensor([[1,2,3],[4,5,6]])index_1 = torch.LongTensor([[0,1,1],[0,1,0]])"""</span>new_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>    <span class="token punctuation">[</span>a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token punctuation">[</span>a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>new_tensor<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""tensor([[1., 5., 6.],        [1., 5., 3.]])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>dim=1</strong>时, 对于a来说, <strong>第1维</strong>位置index是通过<strong>查表</strong>获得的.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token triple-quoted-string string">"""out[i][j] = input[i][index[i][j]] if dim == 1a = torch.Tensor([[1,2,3],[4,5,6]])index_1 = torch.LongTensor([[0,1,1],[0,1,0]])"""</span>new_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>    <span class="token punctuation">[</span>a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token punctuation">[</span>a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>new_tensor<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""tensor([[1., 2.],        [6., 4.]])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>index能遍历到沿dim上所有的元素, 最起码应该<strong>保证index和输入在除去指定的dim外其他dim上shape相同</strong>.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch学习: 张量基础操作</title>
      <link href="/posts/42255.html"/>
      <url>/posts/42255.html</url>
      
        <content type="html"><![CDATA[<h1 id="Pytorch学习-张量基础操作"><a href="#Pytorch学习-张量基础操作" class="headerlink" title="Pytorch学习: 张量基础操作"></a>Pytorch学习: 张量基础操作</h1><p>整理内容顺序来自龙龙老师的<code>&lt;深度学习与PyTorch入门实战教程&gt;</code>, 根据个人所需情况进行删减或扩充. 如果想要自己创建新的模块, 这些操作都是基本功, 需要掌握扎实.</p><h2 id="张量数据类型"><a href="#张量数据类型" class="headerlink" title="张量数据类型"></a>张量数据类型</h2><p>下表摘自<a href="https://pytorch.org/docs/master/tensors.html#tensor-doc" target="_blank" rel="noopener">Pytorch官方文档</a>, 介绍了现在pytorch中所有涉及到的数据类型.</p><table><thead><tr><th>Data type</th><th>dtype</th><th>CPU tensor</th><th>GPU tensor</th></tr></thead><tbody><tr><td>32-bit floating point</td><td><code>torch.float32</code> or <code>torch.float</code></td><td><code>torch.FloatTensor</code></td><td><code>torch.cuda.FloatTensor</code></td></tr><tr><td>64-bit floating point</td><td><code>torch.float64</code> or <code>torch.double</code></td><td><code>torch.DoubleTensor</code></td><td><code>torch.cuda.DoubleTensor</code></td></tr><tr><td>16-bit floating point <a href="https://pytorch.org/docs/master/tensors.html#id3" target="_blank" rel="noopener">1</a></td><td><code>torch.float16</code> or <code>torch.half</code></td><td><code>torch.HalfTensor</code></td><td><code>torch.cuda.HalfTensor</code></td></tr><tr><td>16-bit floating point <a href="https://pytorch.org/docs/master/tensors.html#id4" target="_blank" rel="noopener">2</a></td><td><code>torch.bfloat16</code></td><td><code>torch.BFloat16Tensor</code></td><td><code>torch.cuda.BFloat16Tensor</code></td></tr><tr><td>32-bit complex</td><td><code>torch.complex32</code></td><td></td><td></td></tr><tr><td>64-bit complex</td><td><code>torch.complex64</code></td><td></td><td></td></tr><tr><td>128-bit complex</td><td><code>torch.complex128</code> or <code>torch.cdouble</code></td><td></td><td></td></tr><tr><td>8-bit integer (unsigned)</td><td><code>torch.uint8</code></td><td><code>torch.ByteTensor</code></td><td><code>torch.cuda.ByteTensor</code></td></tr><tr><td>8-bit integer (signed)</td><td><code>torch.int8</code></td><td><code>torch.CharTensor</code></td><td><code>torch.cuda.CharTensor</code></td></tr><tr><td>16-bit integer (signed)</td><td><code>torch.int16</code> or <code>torch.short</code></td><td><code>torch.ShortTensor</code></td><td><code>torch.cuda.ShortTensor</code></td></tr><tr><td>32-bit integer (signed)</td><td><code>torch.int32</code> or <code>torch.int</code></td><td><code>torch.IntTensor</code></td><td><code>torch.cuda.IntTensor</code></td></tr><tr><td>64-bit integer (signed)</td><td><code>torch.int64</code> or <code>torch.long</code></td><td><code>torch.LongTensor</code></td><td><code>torch.cuda.LongTensor</code></td></tr><tr><td>Boolean</td><td><code>torch.bool</code></td><td><a href="https://pytorch.org/docs/master/tensors.html#torch.BoolTensor" target="_blank" rel="noopener"><code>torch.BoolTensor</code></a></td><td><code>torch.cuda.BoolTensor</code></td></tr></tbody></table><p>一般情况下, 常用的tensor类型只有<strong>float</strong>, <strong>int</strong>, <strong>bool</strong>. 至于使用多少位精度需要结合实际情况而定, 毕竟精度高了训练时间就长了. 其他的类型基本不需要去关心, 需要时再查查文档就好.</p><p>在CPU和在GPU上的tensor是完全不同的, 它们甚至不属于同一个类. 在CPU和GPU上训练的两个tensor除非迁移到同一个位置上, 否则不能发生交互.</p><p><strong>为什么没有<code>String</code> 类型的tensor?</strong></p><p>因为在深度学习中, 文本不会直接输入到框架中. 虽然在pytorch中没有string直接的数据类型, 但是可以根据需要把string做embedding或者one-hot转换成张量输入.</p><h2 id="创建张量"><a href="#创建张量" class="headerlink" title="创建张量"></a>创建张量</h2><p>大多数张量的基本操作也会穿插在这一节里面.</p><h3 id="自动数据类型"><a href="#自动数据类型" class="headerlink" title="自动数据类型"></a>自动数据类型</h3><p>创建张量, 使用<code>torch.tensor()</code>. 它可以创建一个标量(dim=0)或者一个张量. 如果传入的对象是一个数, 那么则创建一个标量, 如果传入的对象是一个<code>list</code>, 则创建一个张量. 这种方式是不指定数据类型的, tensor会自动分配数据类型.</p><p>使用<code>Tensor.shape</code>或<code>Tensor.size()</code>可以查看张量的大小.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token comment" spellcheck="true"># 创建一个标量</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">925</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.size():'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 创建一个张量</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">925</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'b.size()'</span><span class="token punctuation">,</span> b<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># output:</span><span class="token comment" spellcheck="true"># a.size():  torch.Size([])</span><span class="token comment" spellcheck="true"># b.size(): torch.Size([1])</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>标量和张量是不同的. 对于标量来说, 它本身就是一个单独的数, <strong>没有dim和size</strong>这一说. 使用<code>Tensor.dim()</code>和<code>len(Tensor.size())</code>是等价的, 对于标量来说, 结果应该是0.</p><p>同时, 对于标量来说, 使用<code>Tensor.item()</code>能获取标量的值.</p><p>可以通过<code>Tensor.type()</code>查看tensor的类型:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.type():'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>type<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># output:</span><span class="token comment" spellcheck="true"># a.type(): torch.LongTensor</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>使用<code>torch.tensor()</code>创建张量会被自动分配数据类型, 对于浮点和整型是不一样的:</p><pre class="line-numbers language-python"><code class="language-python">c <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">925</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'c.type():'</span><span class="token punctuation">,</span> c<span class="token punctuation">.</span>type<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># output:</span><span class="token comment" spellcheck="true"># c.type(): torch.FloatTensor</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>在GPU上的tensor和CPU上的tensor完全不是一个类型:</p><pre class="line-numbers language-python"><code class="language-python">c <span class="token operator">=</span> c<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'c.type():'</span><span class="token punctuation">,</span> c<span class="token punctuation">.</span>type<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># output:</span><span class="token comment" spellcheck="true"># c.type(): torch.cuda.FloatTensor</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p><code>Tensor.cuda()</code>可以返回一个tensor在cuda上的引用, 也就是将tensor移动到GPU上去.</p><h3 id="指定类型"><a href="#指定类型" class="headerlink" title="指定类型"></a>指定类型</h3><p>与<code>torch.tensor()</code>不同的是, 如果向指定数据类型的函数中传入一个数, 不再是创建一个指定类型的标量, 而是创建一个指定数据类型和指定dim和shape的tensor. 例如, <code>torch.FloatTensor(3)</code>是创建一个维度为1, shape为[3]的tensor. tensor中的数据<strong>全是随机</strong>的.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a:'</span><span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""outputa: tensor([9.2196e-41, 0.0000e+00, 7.0295e+28])a.shape: torch.Size([3])"""</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'b:'</span><span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'b.shape:'</span><span class="token punctuation">,</span> b<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""output:b: tensor([[0., 0., 0., 0.],        [0., 0., 0., 0.],        [0., 0., 0., 0.]])b.shape: torch.Size([3, 4])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>当然, 也可以传入<code>list</code>直接对tensor初始化:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a:'</span><span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.type():'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>type<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""output:a: tensor([1., 2., 3.])a.type(): torch.FloatTensor"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="通过nump创建"><a href="#通过nump创建" class="headerlink" title="通过nump创建"></a>通过nump创建</h3><p>也可以通过numpy创建tensor.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'numpy.a:'</span><span class="token punctuation">,</span> a<span class="token punctuation">)</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'torch.a:'</span><span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""numpy.a: [[1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.]]torch.a: tensor([[1., 1., 1., 1.],        [1., 1., 1., 1.],        [1., 1., 1., 1.]], dtype=torch.float64)"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>当然创建好了以后是一个<code>float64</code>的tensor, 从numpy导入的float其实是double类型.</p><h3 id="其他创建方法"><a href="#其他创建方法" class="headerlink" title="其他创建方法"></a>其他创建方法</h3><p>大多函数与numpy相同, 有numpy基础的建议直接跳过.</p><h4 id="基本创建方法"><a href="#基本创建方法" class="headerlink" title="基本创建方法"></a>基本创建方法</h4><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 均匀分布初始化</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'rand:'</span><span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 空张量(全是随机数)</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'empty:'</span><span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 随机整数</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'ranint:'</span><span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""output:rand: tensor([[0.5267, 0.8344, 0.6042],        [0.1859, 0.3207, 0.1803]])empty: tensor([[1.0561e-38, 1.0653e-38, 1.4013e-45],        [0.0000e+00, 1.4013e-45, 0.0000e+00]])ranint: tensor([[8, 7],        [1, 8]])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>torch.randint(low, high, size)</code>是遵循切片规则的, 即生成的随机整数包含左侧不包含右侧.</p><h4 id="xx-like"><a href="#xx-like" class="headerlink" title="xx_like"></a>xx_like</h4><p>和numpy一样, 也有<code>xx_like()</code>这个函数, 能按传入的tensor的shape创建tensor:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'b:'</span><span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># output:</span><span class="token comment" spellcheck="true"># b: tensor([[1., 1., 1.],</span><span class="token comment" spellcheck="true">#        [1., 1., 1.]])</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>xx</code>可以替换成前面所说的任意初始化的函数名称.</p><h4 id="arange-range"><a href="#arange-range" class="headerlink" title="arange / range"></a>arange / range</h4><p>numpy老朋友了. <code>torch.arange()</code>生成遵循切片规则[min, max)的tensor, 支持步长. <code>torch.range()</code>与前者功能相同, 因为完全可以代替, 可能会被移除, 不建议使用.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'[0, 6):'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'[0, 6), 步长为2:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># ouput:</span><span class="token comment" spellcheck="true"># [0, 6): tensor([0, 1, 2, 3, 4, 5])</span><span class="token comment" spellcheck="true"># [0, 6), 步长为2: tensor([0, 2, 4])</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="full"><a href="#full" class="headerlink" title="full"></a>full</h4><p>使用<code>torch.full()</code>创建一个指定shape的tensor并填满某个值.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'填满6:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'创建值为6的标量:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># output:</span><span class="token comment" spellcheck="true"># 填满6: tensor([[6., 6., 6.],</span><span class="token comment" spellcheck="true">#         [6., 6., 6.]])</span><span class="token comment" spellcheck="true"># 创建值为6的标量: tensor(6.)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="randn"><a href="#randn" class="headerlink" title="randn"></a>randn</h4><p>使用<code>torch.randn()</code>按照(0, 1)初始化, 使用<code>torch.normal()</code>按照指定均值和方差进行初始化.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 0, 1初始化</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'randn:'</span><span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 指定均值和标准差</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>normal<span class="token punctuation">(</span>mean<span class="token operator">=</span>torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> std<span class="token operator">=</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'normal:'</span><span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""randn: tensor([[-1.1174,  0.8060,  0.1918],        [-0.1511,  0.3734, -0.6192]])normal: tensor([-2.1135e+00,  4.9261e-01, -9.9956e-01,  3.7895e-01, -4.1920e-01,        -1.6493e-01,  3.6504e-01, -1.1884e-01, -1.1261e-03, -4.7203e-02])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="linspace-logspace"><a href="#linspace-logspace" class="headerlink" title="linspace / logspace"></a>linspace / logspace</h4><p><code>torch.linspace()</code>和<code>torch.arange()</code>非常相似, 只不过给出的是<strong>范围和所需的元素个数</strong>.</p><p><code>torch.logspace()</code>是<code>torch.linspace()</code>的对数版本.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'[0, 10), 2:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'[0, 10), 3:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'log[0, 1), 3:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>logspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""[0, 10), 2: tensor([ 0., 10.])[0, 10), 3: tensor([ 0.,  5., 10.])log[0, 1), 3: tensor([ 1.0000,  3.1623, 10.0000])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="ones-zeros-eye"><a href="#ones-zeros-eye" class="headerlink" title="ones / zeros / eye"></a>ones / zeros / eye</h4><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'0阵:\n'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'1阵:\n'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'单位阵:\n'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>eye<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""0阵: tensor([[0., 0., 0.],        [0., 0., 0.]])1阵: tensor([[1., 1., 1.],        [1., 1., 1.]])单位阵: tensor([[1., 0., 0.],        [0., 1., 0.]])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="randperm"><a href="#randperm" class="headerlink" title="randperm"></a>randperm</h4><p>这个函数说一下功能就懂了, 是用来做<strong>shuffle</strong>的.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a:'</span><span class="token punctuation">,</span> a<span class="token punctuation">)</span>index <span class="token operator">=</span> torch<span class="token punctuation">.</span>randperm<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'index:'</span><span class="token punctuation">,</span> index<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[index]:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""a: tensor([[0.7391, 0.1663, 0.1362, 0.9353],        [0.2951, 0.9289, 0.3369, 0.8836],        [0.2730, 0.8966, 0.7737, 0.5760]])index: tensor([0, 1])a[index]: tensor([[0.7391, 0.1663, 0.1362, 0.9353],        [0.2951, 0.9289, 0.3369, 0.8836]])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>顺带一提, <code>Tensor.numel()</code>能得到tensor中的所有参数个数.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'numel:'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># numel: 6</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="默认数据类型"><a href="#默认数据类型" class="headerlink" title="默认数据类型"></a>默认数据类型</h3><p>在使用<code>torch.tensor()</code>创建的数据类型默认是<code>torch.FloatTensor</code>, 可以使用<code>torch.set_default_tensor_type(tensor_data_type)</code>来设置默认创建的tensor类型.</p><h2 id="索引和切片"><a href="#索引和切片" class="headerlink" title="索引和切片"></a>索引和切片</h2><p>索引访问与python基本一致, python的list用的比较熟的建议跳过索引访问. 多了一些其他的新东西.</p><h3 id="索引访问"><a href="#索引访问" class="headerlink" title="索引访问"></a>索引访问</h3><p>就是python中list的访问方法, 完全一致.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[0].shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[0][0].shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[0][0][23][24].shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">23</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">24</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[0][0][23][24]:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">23</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">24</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""a.shape: torch.Size([4, 3, 28, 28])a[0].shape: torch.Size([3, 28, 28])a[0][0].shape: torch.Size([28, 28])a[0][0][23][24].shape: torch.Size([])a[0][0][23][24]: tensor(0.4280)"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>通过切片访问多个元素:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[:2].shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[:2, :1, :, :].shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[:2, 1:, :, :].shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[:2, -1:, :, :].shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""a.shape: torch.Size([4, 3, 28, 28])a[:2].shape: torch.Size([2, 3, 28, 28])a[:2, :1, :, :].shape: torch.Size([2, 1, 28, 28])a[:2, 1:, :, :].shape: torch.Size([2, 2, 28, 28])a[:2, -1:, :, :].shape: torch.Size([2, 1, 28, 28])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>通过步长:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[:, :, 0:28:2, 0:28:2].shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">28</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">28</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[:, :, ::2, ::2].shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># a[:, :, 0:28:2, 0:28:2].shape: torch.Size([4, 3, 14, 14])</span><span class="token comment" spellcheck="true"># a[:, :, ::2, ::2].shape: torch.Size([4, 3, 14, 14])</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="特殊用法"><a href="#特殊用法" class="headerlink" title="特殊用法"></a>特殊用法</h3><h4 id="index-select"><a href="#index-select" class="headerlink" title="index_select"></a>index_select</h4><p>多了一个<code>Tensor.index_select(dim, index)</code>函数, 像是对切片的封装, 不知道速度有没有提升, 在python中函数好像比切片要快一些, 记不太清了. 反正这个函数用起来是比较麻烦, <code>index</code>还必须是tensor类型的.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>index_select<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># torch.Size([2, 3, 28, 28])</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h4 id="auto-filled"><a href="#auto-filled" class="headerlink" title="auto filled"></a>auto filled</h4><p><code>...</code>代表了任意多的维度, 能根据其他维度<strong>自动填充</strong>, 当维度能够根据其他值推断出来的时候特别方便.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[...].shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[0, ...].shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[:, 1, ...].shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a[..., :2].shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""a.shape: torch.Size([4, 3, 28, 28])a[...].shape: torch.Size([4, 3, 28, 28])a[0, ...].shape: torch.Size([3, 28, 28])a[:, 1, ...].shape: torch.Size([4, 28, 28])a[..., :2].shape: torch.Size([4, 3, 28, 2])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="masked-select"><a href="#masked-select" class="headerlink" title="masked select"></a>masked select</h4><p>通过<code>torch.masked_select(x, mask)</code>能用Mask来筛选元素.</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'x:'</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span>mask <span class="token operator">=</span> x<span class="token punctuation">.</span>ge<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'mask:'</span><span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'masked_select:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>masked_select<span class="token punctuation">(</span>x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""x: tensor([[-0.7403, -1.3733,  0.8203],        [-0.0259,  0.4284,  0.7480]])mask: tensor([[0, 0, 1],        [0, 0, 1]], dtype=torch.uint8)masked_select: tensor([0.8203, 0.7480])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注意, 选择后的结果是<strong>Flatten</strong>的, 而非保持原来的shape.</p><h4 id="take"><a href="#take" class="headerlink" title="take"></a>take</h4><p>用的不是很多, 通过<code>torch.take(src, index_tensor)</code>能按照打平后的index进行访问.</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'x:'</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'第2和第4个元素:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>take<span class="token punctuation">(</span>x<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""x: tensor([[0, 1, 2],        [3, 4, 5]])第2和第4个元素: tensor([2, 4])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="维度变换"><a href="#维度变换" class="headerlink" title="维度变换"></a>维度变换</h2><p>很多函数也和numpy类似.</p><h3 id="view-reshape"><a href="#view-reshape" class="headerlink" title="view / reshape"></a>view / reshape</h3><p><code>Tensor.view(size)</code>能将tensor变形, 和reshape一样, 只要保证<strong>数据总数不变</strong>就能够进行shape变化. 它可以理解为<strong>将某个tensor中的元素按行依次取出</strong>, <strong>再根据指定的size按行依次填充进去</strong>.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>a <span class="token operator">=</span> a<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>a <span class="token operator">=</span> a<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">784</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""torch.Size([4, 1, 28, 28])torch.Size([4, 28, 28])torch.Size([4, 784])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>时刻注意每个dim所对应的含义, 否则在恢复时数据会失去原来的意义, 全部打乱掉. 在初学期, 最好要对数据的维度和意义进行<strong>追踪</strong>.</p></blockquote><p><code>Tensor.view(size)</code>和<code>Tensor.reshape(size)</code>的差别不是很大, 但仍有差别, 在后面说<code>transpose</code>的时候会提到.</p><h3 id="squeeze-unsqueeze"><a href="#squeeze-unsqueeze" class="headerlink" title="squeeze / unsqueeze"></a>squeeze / unsqueeze</h3><p>这一对函数主要是对维度进行提升或压缩.</p><p><code>Tensor.unsqueeze()</code>:</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>a <span class="token operator">=</span> x<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>a <span class="token operator">=</span> x<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>a <span class="token operator">=</span> x<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>a <span class="token operator">=</span> x<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>a <span class="token operator">=</span> x<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># wrong</span><span class="token comment" spellcheck="true"># a = x.unsqueeze(5)</span><span class="token triple-quoted-string string">"""torch.Size([4, 1, 28, 28])torch.Size([1, 4, 1, 28, 28])torch.Size([4, 1, 28, 28, 1])torch.Size([4, 1, 28, 28, 1])torch.Size([4, 1, 1, 28, 28])torch.Size([1, 4, 1, 28, 28])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>提升的维度也是遵循切片规则的, 区间为[-dim - 1, dim + 1).</p><p>与<code>Tensor.unsqueeze()</code>相反, <code>Tensor.squeeze</code>用于无用维度压缩.</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""torch.Size([1, 32, 1, 1])torch.Size([32])torch.Size([32, 1, 1])torch.Size([1, 32, 1])torch.Size([1, 32, 1, 1])torch.Size([32, 1, 1])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="expand-repeat"><a href="#expand-repeat" class="headerlink" title="expand / repeat"></a>expand / repeat</h3><p>这两个函数从最终效果来说完全等价, 但过程不同.</p><p><code>Tensor.expand(size)</code>实际上是在做<strong>BroadCasting</strong>, 被动复制数据, 只有在需要时候才复制. 而<code>Tensor.repeat(copy_times)</code>是直接复制. 建议使用前者减小内存压力.</p><blockquote><p>对broadcast不理解的可以查看<a href="https://www.runoob.com/numpy/numpy-broadcast.html" target="_blank" rel="noopener">NumPy 广播(Broadcast)</a>, 这是一个很重要的机制, 广播能减少内存消耗或减少我们的操作.</p></blockquote><p><code>Tensor.expand(size)</code>:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">14</span><span class="token punctuation">,</span> <span class="token number">14</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> b<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span>expand<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""torch.Size([4, 32, 14, 14]) torch.Size([1, 32, 1, 1])torch.Size([4, 32, 14, 14])torch.Size([1, 32, 1, 1])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>此处填入-1代表维度<strong>保持不变</strong>.</p><p><code>Tensor.repeat(copy_times)</code>传入的是在每个dim上<strong>复制的次数</strong>:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""torch.Size([1, 32, 1, 1])torch.Size([4, 1024, 1, 1])torch.Size([4, 32, 1, 1])torch.Size([4, 32, 32, 32])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>在学习过repeat后, 顺带复习一下view的含义, 注意下面操作为什么有些是不等价的:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>b <span class="token operator">=</span> a<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>c <span class="token operator">=</span> a<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>d <span class="token operator">=</span> a<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>c<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> c<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>d<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> d<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""tensor([0, 1, 2])torch.Size([6]), tensor([0, 1, 2, 0, 1, 2])torch.Size([6]), tensor([0, 0, 1, 1, 2, 2])torch.Size([6]), tensor([0, 1, 2, 0, 1, 2])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>虽然它们大小均相同, 同样都使用了repeat, 但结果却并不一致.</p></blockquote><h3 id="transpose-t-permute"><a href="#transpose-t-permute" class="headerlink" title="transpose / t / permute"></a>transpose / t / permute</h3><p>这三个使用频率非常高.</p><p><code>Tensor.t()</code>就是转置, 只能对<strong>2d-tensor</strong>使用, 否则会报错:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""torch.Size([2, 3])torch.Size([3, 2])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>Tensor.transpose()</code>能任意交换2个维度之间的数据, 只进行一次交换时候建议使用它:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>a<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'交换1, 3:'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""a.shape: torch.Size([4, 3, 32, 32])交换1, 3: torch.Size([4, 32, 32, 3])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>但是请注意, 在使用<code>transpose</code>和<code>permute</code>后, 只是改变了访问的方式(数组的访问步长), 而不会改变底层数组的存储方式, 这也就是所谓的”<strong>不连续</strong>“. </p><p><strong>而<code>Tensor.view()</code>要求tensor必须是连续的, 所以在<code>view</code>前必须使用<code>contiguous()</code>让tensor变连续, 新版中直接使用<code>reshape</code>函数更方便, 它等价于前面的操作</strong>.</p><blockquote><p>关于连续与否更详细的解释可以看<a href="https://zhuanlan.zhihu.com/p/64551412" target="_blank" rel="noopener">PyTorch中的contiguous</a>.</p></blockquote><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 错误做法</span>a1 <span class="token operator">=</span> a<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span> <span class="token operator">*</span> <span class="token number">32</span> <span class="token operator">*</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 正确做法</span>a2 <span class="token operator">=</span> a<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span> <span class="token operator">*</span> <span class="token number">32</span> <span class="token operator">*</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 用reshape</span>a3 <span class="token operator">=</span> a<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span> <span class="token operator">*</span> <span class="token number">32</span> <span class="token operator">*</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a.shape:'</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a1.shape:'</span><span class="token punctuation">,</span> a1<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a2.shape:'</span><span class="token punctuation">,</span> a2<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a3.shape:'</span><span class="token punctuation">,</span> a3<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a == a1?:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>all<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>a<span class="token punctuation">,</span> a1<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a == a2?:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>all<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>a<span class="token punctuation">,</span> a2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a == a3?:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>all<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>a<span class="token punctuation">,</span> a3<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""a.shape: torch.Size([4, 3, 32, 32])a1.shape: torch.Size([4, 3, 32, 32])a2.shape: torch.Size([4, 3, 32, 32])a3.shape: torch.Size([4, 3, 32, 32])a == a1?: tensor(0, dtype=torch.uint8)a == a2?: tensor(1, dtype=torch.uint8)a == a3?: tensor(1, dtype=torch.uint8)"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>一定要先view交换后的shape, 再transpose回来.</p><p><code>Tensor.permute()</code>更加灵活, 能随意交换所有dim之间的位置, 如果交换多个维度最好使用这个函数:</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""torch.Size([4, 3, 28, 32])torch.Size([4, 28, 32, 3])torch.Size([4, 28, 3, 32])torch.Size([4, 28, 3, 32])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>也同时要注意, 使用后也是不连续的.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>指针网络家族</title>
      <link href="/posts/36009.html"/>
      <url>/posts/36009.html</url>
      
        <content type="html"><![CDATA[<p>本文介绍了Pointer Network, CopyNet, Pointer-Generator Network以及Coverage机制在文本摘要与对话系统中的应用, 既可以作为知识点介绍, 也可以作为论文阅读笔记. 此外, 该部分内容为外部知识引入NLP任务中提供了思路.</p><blockquote><p>本文阅读所需的前置知识包括:</p><ul><li>Seq2Seq</li><li>Attention</li><li>Bi - directional RNN</li></ul></blockquote><h2 id="Pointer-Network"><a href="#Pointer-Network" class="headerlink" title="Pointer Network"></a>Pointer Network</h2><p>指针网络出自<a href="https://arxiv.org/abs/1506.03134" target="_blank" rel="noopener">Pointer Networks</a>. 在Seq2seq中, 经常有一种模型<strong>输出严重依赖输入</strong>的问题, 一旦问题规模发生变化, 那就必须重新训练网络. 作者利用Attention机制和Seq2Seq进行结合, 克服了这个问题.</p><p>“输出严重依赖输入”指的是输出往往是输入的子集. 比如在机器翻译中, 输出向量大小必须取决于字典长度, 并不能根据Encoder输入内容而发生变化. 在求凸包(Convex Hull)问题中, 输入和输出都是坐标序列, 规模是不固定的. <strong>可变大小序列的排序</strong>和<strong>各种组合优化</strong>都是这类问题. </p><blockquote><p>凸包是计算图形学中的概念, 通俗一点说凸包问题就是根据给定的二维平面点集找到能够包含点集中所有点的最外层点的连接线.</p></blockquote><p>而”指针”的命名来自于其Attention产生的权重直接决定了Decoder的输出对应着哪个Encoder的数据输入, 这样输出就从输入中进行选择, 能很好的解决该问题. 该结构非常简单, 与加权平均的注意力机制不同, Ptr - Net并非将Attention机制对信息进行筛选, 而是<strong>直接指出</strong>输出信息. 指针网络直接将Attention的权重最高者直接作为Decoder的输出, 如下图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ptrnet.jpg" style="zoom: 67%;" /><p>实际上这种结构不仅仅能用于解决凸包问题, 作者还在论文中给出利用Ptr - Net 解决三角形分割(Delaunay Triangulation), 旅行商问题(Travelling Salesman Problem). 作者在调整参数后, 取得了比LSTM好得多的结果:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ptrnet1.jpg" style="zoom:50%;" /><p>指针网络因为具有解决输出严重依赖输入问题的能力, 如果应用于NLP的摘要生成任务中, 可以从原文中复现重要的细节, 在某些程度上解决预训练词典大小不足的问题(也称为<strong>OOV问题</strong>, 即Out of Vocabulary). 虽然这种结构在处理特定问题上有了优势, 但仍然受<strong>结构局限</strong>, 无法完全应用到通用任务当中.</p><h2 id="CopyNet"><a href="#CopyNet" class="headerlink" title="CopyNet"></a>CopyNet</h2><p>CopyNet出自论文<a href="http://arxiv.org/abs/1603.06393" target="_blank" rel="noopener">Incorporating Copying Mechanism in Sequence-to-Sequence Learning</a>. 该论文作者将PtrNet使用于文本<strong>摘要提取</strong>和<strong>对话任务</strong>当中.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/copynet1.jpg" style="zoom:50%;" /><p>在原文中作者提到Copy机制与Seq2Seq结合的难点:</p><blockquote><p>From a cognitive perspective, the copying mechanism is related to rote memorization, requiring less understanding but ensuring high literal fidelity. From a modeling perspective, the copying operations are more rigid and symbolic, making it more difficult than soft attention mechanism to integrate into a fully differentiable neural model. </p></blockquote><ul><li>从<strong>认知角度</strong>来看, Copy与死记硬背有关, 不需要理解, 但复制可以确保很高的字面保真度. </li><li>从<strong>建模角度</strong>来看, Copy更加僵化和符号化, 使其比软注意力机制更难集成到完全可区分的神经模型中. </li></ul><p>因此, 作者提出了另一种复制机制, 能够端到端的只通过梯度下降训练Seq2Seq模型. </p><p>前面提到的指针网络只能重复原文内容, 而非从已有的字典中提取内容, 天生就受到了极大的限制. 如果想要破除这种劣势, 就必须让之前的对话生成结构与这种Pointer(或者说Copy)机制相结合.</p><blockquote><p>注: Copy和Pointer的作用都是一致的, 都是将某个时刻的输出调整为先前某个时刻的输入, Copy也是将先前输入作为输出, 指针也是同样效果.</p></blockquote><p>在摘要生成中, 通过复现原文内容的摘要生成称为”<strong>抽取式</strong>“摘要生成, 而从外部词典中取出的内容叫”<strong>生成式</strong>“摘要生成. 而作者用复制机制将这两种方式实现了软结合.</p><p>沿用Seq2Seq结构, 仍然分为Encoder和Decoder两个部分. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/copynet.jpg" style="zoom: 67%;" /><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>Encoder采用双向RNN提取句子信息, 并将每个时刻与输入$x_t$ 相对应的hidden state集合$\left\{\mathbf{h}_{1}, \ldots, \mathbf{h}_{T_{S}}\right\}$ 称为短期记忆$\mathrm{M}$ .</p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><h4 id="Predition"><a href="#Predition" class="headerlink" title="Predition"></a>Predition</h4><p>作者将预测分为<strong>生成模式</strong>和<strong>复制模式</strong>两种, 当前时刻的预测结果应该是两种模式混合的结果. g代表生成模式, c代表copy模式. 而每个模式的预测结果由Decoder在当前时刻的隐藏状态输出$s_t$, 上个时刻的预测结果$y_{t-1}$, Attention生成的当前时刻的上下文向量$c_t$, 以及短时记忆$\mathrm{M}$ 共同决定.<br>$$<br>\begin{array}{r}<br>p\left(y_{t} \mid \mathbf{s}_{t}, y_{t-1}, \mathbf{c}_{t}, \mathbf{M}\right)=p\left(y_{t}, g \mid \mathbf{s}_{t}, y_{t-1}, \mathbf{c}_{t}, \mathbf{M}\right)<br>+p\left(y_{t}, c \mid \mathbf{s}_{t}, y_{t-1}, \mathbf{c}_{t}, \mathbf{M}\right)<br>\end{array}<br>$$<br>作者将词语分为在词典中的词语集合$\mathcal{V}=\left\{v_{1}, \ldots, v_{N}\right\}$, 源序列的单词集合$X=\left\{x_{1}, \ldots, x_{T_{S}}\right\}$, OOV单词记为$\mathrm{UNK}$, 每个模式生成的结果计算方式如下:<br>$$<br>\begin{array}{l}<br>p\left(y_{t}, g \mid \cdot\right)=\left\{\begin{array}{cc}<br>\frac{1}{Z} e^{\psi_{g}\left(y_{t}\right)}, &amp; y_{t} \in \mathcal{V} \\<br>0, &amp; y_{t} \in \mathcal{X} \cap \bar{V} \\<br>\frac{1}{Z} e^{\psi_{g}(\mathrm{UNK})} &amp; y_{t} \notin \mathcal{V} \cup \mathcal{X}<br>\end{array}\right. \\<br>p\left(y_{t}, \mathrm{c} \mid \cdot\right)=\left\{\begin{array}{cc}<br>\frac{1}{Z} \sum_{j: x_{j}=y_{t}} e^{\psi_{c}\left(x_{j}\right)}, &amp; y_{t} \in \mathcal{X} \\<br>0 &amp; \text { otherwise }<br>\end{array}\right.<br>\end{array}<br>$$<br>其中$Z$ 为生成模式和copy模式共享的归一化项, 作者通过设计归一化项让两种模式通过Softmax来<strong>互相竞争</strong>(将其代入原式的分母中就是Softmax的形式):<br>$$<br>Z=\sum_{v \in \mathcal{V} \cup\{\mathrm{UNK}\}} e^{\psi_{g}(v)}+\sum_{x \in X} e^{\psi_{c}(x)}<br>$$<br>$\psi_{g}(\cdot)$ 和 $\psi_{c}(\cdot)$ 是两种打分函数, 会在后面提到如何计算.</p><p>作者还给出了图加以辅助说明计算概率时所对应的不同情况:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/copynet2.jpg" style="zoom: 33%;" /><p>生成模式下采用Attention, 打分公式为:<br>$$<br>\psi_{g}\left(y_{t}=v_{i}\right)=\mathbf{v}_{i}^{\top} \mathbf{W}_{o} \mathbf{s}_{t}, \quad v_{i} \in \mathcal{V} \cup \mathrm{UNK}<br>$$<br>$W_o$ 和$\mathbf{v}_{i}$ 相乘后能获得$v_i$ 的独热编码, 它与$s_t$ 乘后得到一个分数.</p><p>copy模式下, 打分公式为:<br>$$<br>\psi_{c}\left(y_{t}=x_{j}\right)=\sigma\left(\mathbf{h}_{j}^{\top} \mathbf{W}_{c}\right) \mathbf{s}_{t}, \quad x_{j} \in \mathcal{X}<br>$$<br>$W_c$ 是训练得到的, 与$\mathbf h_j$ 相乘经过$\sigma$ (原文使用的是tanh)添加非线性, 将$h_j$ 和 $s_t$ 投射到同一个语义空间.</p><p>综上, 当$y_t$ 没出现在源序列中时, $p\left(y_{t}, c \mid \cdot\right)=0$, 只启动生成模式, 当$y_t$ 只出现在源序列时, $p\left(y_{t}, g \mid \cdot\right)=0$, 只启动copy模式.</p><h4 id="State-Update-and-Reading-M"><a href="#State-Update-and-Reading-M" class="headerlink" title="State Update and Reading M"></a>State Update and Reading M</h4><p>作者在copy机制下对Decoder的状态更新做了改良, 普通Decoder的状态更新为:<br>$$<br>\begin{array}{l}<br>\mathbf{s}_{t}=f\left(y_{t-1}, \mathbf{s}_{t-1}, \mathbf{c}\right) \\<br>p\left(y_{t} \mid y_{&lt;t}, X\right)=g\left(y_{t-1}, \mathbf{s}_{t}, \mathbf{c}\right)<br>\end{array}<br>$$<br>作者将$y_{t-1}$ 用$y_{t-1}$ 的Embedding $\mathbf{e}(y_{t-1})$ 和$\zeta\left(y_{t-1}\right)$ 来表示, 并将其concat起来, 即:<br>$$<br>\left[\mathbf{e}\left(y_{t-1}\right) ; \zeta\left(y_{t-1}\right)\right]^{\top}<br>$$<br>因为使用的是双向RNN, 所以$\mathrm M$ 中既包含了上下文信息, 也包含了位置信息, 这对状态更新很重要, $\zeta\left(y_{t-1}\right)$ 是其中的核心内容, 计算方式如下:<br>$$<br>\begin{array}{l}<br>\zeta\left(y_{t-1}\right)=\sum_{\tau=1}^{T_{S}} \rho_{t \tau} \mathbf{h}_{\tau} \\<br>\rho_{t \tau}=\left\{\begin{array}{cc}<br>\frac{1}{K} p\left(x_{\tau}, \mathrm{c} \mid \mathbf{s}_{t-1}, \mathbf{M}\right), &amp; x_{\tau}=y_{t-1} \\<br>0 &amp; \text { otherwise }<br>\end{array}\right. \\<br>K = \sum_{\tau^{\prime}: x_{\tau^{\prime}}=y_{t-1}} p\left(x_{\tau^{\prime}}, c \mid \mathbf{s}_{t-1}, \mathbf{M}\right)<br>\end{array}<br>$$<br>$K$ 是归一化项, 作者将这个过程称为Selective Read, 即用$\zeta\left(y_{t-1}\right)$ 对$\mathbf h_\tau$ 进行加权求和. 从式子直观上来理解, 仅当Encoder接受的输入在上个Decoder输出时刻相同时, 这个$\rho$ 才有意义. 通过这种方式, 对于在前文已经出现的单词, Decoder就能拿到<strong>额外的上下文信息和位置信息</strong>. 一旦$\zeta$ 有值, 当前时刻的输出就会倾向于copy模式, 因为上个时刻的输出在原文中能够找到, 那么当前时刻的内容也有相当大的概率从原文中copy.</p><blockquote><p>这个$\zeta$ 似乎和$c_t$ 差不多, 不知道有没有信息上的<strong>冗余</strong>.</p></blockquote><p>综上, 将更新过程总结如下:<br>$$<br>\zeta\left(y_{t-1}\right) \stackrel{\text { update }}{\longrightarrow} \mathbf{s}_{t} \stackrel{\text { predict }}{\longrightarrow} y_{t} \stackrel{\text { sel. read }}{\longrightarrow} \zeta\left(y_{t}\right)<br>$$</p><blockquote><p>在原文中最后的实验对比结果中. CopyNet对于Copy任务做的效果都碾压基础模型, 但唯独对于结束符的生成准确率较差, 这也为<strong>大量生成重复内容</strong>埋下了隐患.</p></blockquote><h2 id="Pointer-Generator-Network"><a href="#Pointer-Generator-Network" class="headerlink" title="Pointer - Generator Network"></a>Pointer - Generator Network</h2><p>指针生成网络受到了PtrNet和CopyNet的影响, 仍然沿用原输入的复现能力与基于已有知识的文本生成能力做结合的思路, 并针对指针生成网络出现的问题做了改进. 与CopyNet相比更加简洁.</p><p>该结构出自<a href="https://arxiv.org/abs/1704.04368" target="_blank" rel="noopener">Get To The Point: Summarization with Pointer-Generator Networks</a>, 这是一篇非常不错的论文, 如果时间不充裕我建议阅读这篇, 作者逻辑清晰, 图片也简单易懂.</p><p>该模型应用于<strong>摘要生成</strong>任务中, 作者在论文开头便提出了现存模型在摘要生成中出现的问题:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ptrgennet1.jpg" style="zoom: 67%;" /><ol><li>对于图中的红色字, 存在内容抽取<strong>不准确</strong>的情况.</li><li>对于图中的绿色字, 存在多次与原文中生成<strong>重复且无意义内容</strong>的情况.</li></ol><p>指针生成网络沿用了CopyNet生成式和抽取式并存的思想, 分为抽取式摘要生成和基于单词表的生成式摘要生成, 并可以在生成模式和抽取模式之间更灵活的切换.</p><blockquote><p>Our pointer-generator network is a hybrid betweenour baseline and a pointer network, as it allows both copying words via pointing, and generating words from a fixed vocabulary.</p></blockquote><h3 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h3><p>在原论文中, 作者用标准的Seq2Seq + Attention作为Baseline:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ptrgennet2.jpg" style="zoom:67%;" /><p>这是一种非常直觉性的办法. Seq2Seq架构下用双向RNN提取隐藏状态$h_i$, Decoder解码得到$s_t$, 用Bahdanau Attention提取上下文信息$h^\ast_t$:<br>$$<br>\begin{array}{l}<br>e_{i}^{t}=v^{T} \tanh \left(W_{h} h_{i}+W_{s} s_{t}+b_{\text {attn }}\right) \\<br>a^{t}=\operatorname{softmax}\left(e^{t}\right) \\<br>h_{t}^{\ast}=\sum_{i} a_{i}^{t} h_{i}<br>\end{array}<br>$$<br>然后根据得出的上下文信息和Decoder当前时刻解码信息, 经过Softmax得到当前时刻的词语概率分布$P_{\text {vocab }}$:<br>$$<br>P_{\text {vocab }}=\operatorname{softmax}\left(V^{\prime}\left(V\left[s_{t}, h_{t}^{\ast}\right]+b\right)+b^{\prime}\right)<br>$$<br>为了后面体现出指针生成网络和Baseline的差异, 直接令$P_{\text{vocab}}$就是最终结果:<br>$$<br>P(w)=P_{\text {vocab }}(w)<br>$$<br>然后用对数似然做损失, 计算总共的Loss:<br>$$<br>\begin{aligned}<br>\operatorname{loss}_{t}&amp;=-\log P\left(w_{t}^{\ast}\right) \\<br>\operatorname{loss}&amp;=\frac{1}{T} \sum_{t=0}^{T} \operatorname{loss}_{t}<br>\end{aligned}<br>$$</p><h3 id="Pointer-Generator-Network-1"><a href="#Pointer-Generator-Network-1" class="headerlink" title="Pointer - Generator Network"></a>Pointer - Generator Network</h3><p>作者基于Baseline的缺点, 给出了指针生成网络:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/ptrgennet3.jpg" style="zoom:67%;" /><p>与Baseline的图相对比, 最大的变化就是多了一个$p_{\text{gen}}$, 导致有其他很多连带的内容不同.</p><p>和之前一样, 我们仍然计算Attention和绿色对应的词表概率分布, 但同时根据当前时刻Decoder的输入$x_t$ 能额外得到一个概率$p_{\text{gen}} \in [0, 1]$:<br>$$<br>p_{\mathrm{gen}}=\sigma\left(w_{h^{\ast}}^{T} h_{t}^{\ast}+w_{s}^{T} s_{t}+w_{x}^{T} x_{t}+b_{\mathrm{ptr}}\right)<br>$$<br>引入$p_{\text{gen}}$ 就是想在Baseline的基础上将生成式与抽取式利用<strong>开关</strong>做个<strong>软结合</strong>, 从而改善最终结果:<br>$$<br>P(w)=p_{\mathrm{gen}} P_{\mathrm{vocab}}(w)+\left(1-p_{\mathrm{gen}}\right) \sum_{i: w_{i}=w} a_{i}^{t}<br>$$<br>生成模式的概率是$p_{\text{gen}}$, 那么copy模式的概率就是$1 - p_{\text{gen}}$. copy模式中用到了$a^t$, 当单词$w$ 在之前的输入中出现过时候, 我们将其Attention累加起来, 所以在原文中出现次数越多的词使用copy模式的概率就越大, 被添入摘要的概率也就越大. 如果$w$ 直接OOV了, 则$P_{\mathrm{vocab}}(w)=0$, 如果$w$ 没在原文中出现, 那么令$\sum_{i: w_{i}=w} a_{i}^{t}=0$.</p><p>模型非常简单, 图画的也特别好, 以至于让人一目了然.</p><h2 id="Coverage-Mechanism"><a href="#Coverage-Mechanism" class="headerlink" title="Coverage Mechanism"></a>Coverage Mechanism</h2><p>覆盖(汇聚?)机制与指针生成网络出自同一篇论文. 在原文中作者提到:</p><blockquote><p>Repetition is a common problem for sequence-to-sequence models, and is especially pronounced when generating multi-sentence text. We adapt the coverage model of Tu et al. (2016) to solve the problem.</p></blockquote><p>对于Seq2Seq模型来说, <strong>重复</strong>是一个非常令人头疼的问题, 作者在指针生成网络的基础上提出了Converage Mechanism. 思想非常简单, 作者通过Coverage Mechanism来记录之前时刻Attention的和, 作为当前时刻对原文单词关注位置的依据, 令其为$c^t$:</p><p>$$<br>c^{t}=\sum_{t^{\prime}=0}^{t-1} a^{t^{\prime}}<br>$$</p><blockquote><p>注: 这里的$c^t$ 不是上下文关系, 而是coverage vector, 在本论文中采用$h_t^{\ast}$ 作为上下文关系.</p></blockquote><p>$c^t$ 代表了先前模型对这些单词关注的覆盖程度(不知道是不是”覆盖”二字的来源), 模型先前越有可能copy过, 那么覆盖向量就越大. </p><p>将Coverage Mechanism加入Attention中去, 将模型对单词的关注程度也作为Attention的依据:<br>$$<br>e_{i}^{t}=v^{T} \tanh \left(W_{h} h_{i}+W_{s} s_{t}+w_{c} c_{i}^{t}+b_{\mathrm{attn}}\right)<br>$$<br>单单影响Attention不能产生太大作用, 因为模型并不知道要往哪个方向进行优化, 还需要给模型一个引导, 将相关的内容作为Loss体现:<br>$$<br>\displaylines{<br>\operatorname{covloss}_{t}=\sum_{i} \min \left(a_{i}^{t}, c_{i}^{t}\right) \\<br>\operatorname{covloss}_{t} \leq \sum_{i} a_{i}^{t}=1<br>}<br>$$<br>$\text{covloss}$ 使得模型更容易选择不同的单词做copy, 而非大量重复copy. 若重复copy, $a^t$ 和 $c^t$ 都会很高, 模型会被惩罚的更严重.</p><p>令$\lambda$ 为超参数加权, 将$\text{covloss}$ 加入总体损失中:<br>$$<br>\operatorname{loss}_{t}=-\log P\left(w_{t}^{\ast}\right)+\lambda \sum_{i} \min \left(a_{i}^{t}, c_{i}^{t}\right)<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Attention </tag>
            
            <tag> 摘要生成 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer精讲</title>
      <link href="/posts/6744.html"/>
      <url>/posts/6744.html</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>2020.10.05</strong>: 更新训练技巧.</p><p><strong>2020.09.27</strong>: 更新Masked Multi - Head Attention理解.</p><p><strong>2021.06.08</strong>: 更新Teacher Forcing.</p></blockquote><h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>Transformer (<del>擎天柱/变形金刚</del>), 是一个基于<strong>Attention和SeqSeq</strong>的模型, 完全摆脱了CNN和RNN, 整个模型单单只由自注意力和前馈神经网络组成. 该模型出自<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a>, 作者探究了Attention机制真正发挥的作用. Transformer在<strong>机器翻译</strong>等领域取得了革命性的成果, 并且由它衍生了很多在NLP方面的模型, 比如NLP现在通用的模型<a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">Bert</a>家族, 以后也会详细研究一下.</p><p>本文的图片大多数来自原论文和<a href="http://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">The Illustrated Transformer</a>这篇博客, 该文很清楚的解释了Transformer中原文的每一个细节, 图片简洁明了, 强烈推荐阅读.</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>在原文中, 作者先在介绍Transformer的细节前抛出了Transformer的大致结构:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer.jpg" style="zoom: 80%;" /><p>如果抛去所有的细节不谈, 能够清晰的看出作者使用了Seq2Seq作为模型的基本结构, 左侧输入为<strong>Encoder</strong>部分, 右侧输出部分为<strong>Decoder</strong>, 在Decoder输出后, 用一个加以Softmax的神经层来分类. </p><p>在每个Encoder和Decoder之间, 还有Attention 相连接, </p><p>这种结构天生就非常适合机器翻译, 如果我们把分类结果与词进行转换, 从高阶的视角来看, 那么结果就是一个机器翻译的Pipeline:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer1.png" style="zoom:50%;" /><h2 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h2><p>不要忘记论文中给出的大致结构, 下面一步步剖析细节如何实现.</p><h3 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h3><p>对Encoder进行输入(Inputs)和对Decoder进行输入(Outputs)时用到了Embedding, Embedding这里不再多做解释了,  在DL中普遍用Embedding做词语向量化. 论文中使用到的$d_{model}=512$, 并且在Encoder和Decoder的Embedding<strong>共享</strong>相同参数.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer6.png" style="zoom:67%;" /><blockquote><p>值得一提的是, Transformer使用的词表并非是原始单词, 而是经过<strong>BPE(byte-pair encoding)</strong> 处理后的.</p></blockquote><h3 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h3><p>其实在大致模型结构中已经提到了, Transformer可以看做是一个许多Encoder组成的编码组件和一个许多Decoder组成的解码组件构成的.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer2.png" style="zoom:50%;" /><p>在论文中, 编码组件和解码组件的数量等同, 并且假设$N=6$, 即有6个编码器和6个解码器. 作者在后文中还尝试了取2, 4, 8, 但效果上来说没有6好.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer3.png" style="zoom: 33%;" /><p>值得注意的是, 这里的Encoder和Decoder都采用了堆叠的方式, 并且对于堆叠的结构, 每次只接受一个单词的输入, 所以并不是每个Encoder的隐藏状态都会提供给每个Decoder, 而是<strong>Encoder堆叠后的输出统一提供给每个Decoder</strong>.</p><blockquote><p>在RNN + Seq2Seq执行机器翻译任务时, 每个RNN - Encoder接受的是<strong>不同单词</strong>的输入, 在Transformer中Encoder以堆叠的形式存在, 对应的是<strong>同一单词</strong>, 而Encoder - Decoder之间的Attention是为了调整对不同单词的信息权重, 所以并非每个Encoder和每个Decoder之间都有Attention, 不要搞混.</p></blockquote><h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p>每个Encoder由一个自注意力层(Self - Attention)和一个前馈神经网络层(FFN)组成, 后面会提到它们是如何实现的.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer4.png" style="zoom:67%;" /><p>我并不认为这张图做的很好, 因为在右侧的箭头是具有<strong>歧义性</strong>的, 容易让人认为每层的Encoder都与Decoder有连接, 实际上只有最后一个Encoder和所有的Decoder有连接.</p><p>如果将Embedding并添加位置编码后的输入向量设为$x_i$, 经过Self - Attention层的输出设为$z_i$, Encoder目前的向量流如下所示: </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer7.png" style="zoom: 50%;" /><p>当然, 输出$r_i$ 会流入下个Encoder, 当做输入:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer8.png" style="zoom:50%;" /><h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><p>每个Decoder除了有和Encoder相同的自注意力和前馈神经网络层, 还多了一个对Encoder的Attention.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer5.png" style="zoom:67%;" /><p>同样, 本图也具有歧义性. </p><h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self - Attention"></a>Self - Attention</h3><p>自注意力是Transformer中提出的一种新结构, 也是核心组件. 这个注意力并非Decoder对所有Encoder的隐藏状态的不同注意力, 而是<strong>自身对自身</strong>的注意力, 在做语义编码时使得<strong>单词在编码时能够根据上下文找到自己的真正含义</strong>. 自注意力将注意力采用<strong>Q-K-V模式</strong>, 即Query, Key, Value. 用不同的矩阵与Embedding输入$x_i$ 做矩阵乘, 就能分别得到对应的$q_i$, $k_i$, $v_i$, 而矩阵是随机初始化得来的, 之后通过学习调整参数. 至于QKV是定义的, 请参考<code>&lt;Seq2Seq和Attention&gt;</code>. </p><blockquote><p>为什么叫Self - Attention呢? 假设我们在执行机器翻译任务, 这个Attention不再是作用于我们给出的一种语言的输入Source和目标语言的输出Target, 而是作用于Source和Source内部, 即<strong>源语言的语义编码与原始输入Source之间</strong>的Attention, 这样能够获得单词在句子中更好的表示.</p></blockquote><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer9.png" style="zoom:67%;" /><blockquote><p>这里注意一下维度, 在论文中的Embedding维度$d_{model}=512$, 给出的Key维度和Value维度均为64, 即$d_k=d_v=d_{model}/h=64$, 那么对应QKV的矩阵$W_Q$, $W_K$, $W_V$ 大小应该都是$(512, 64)$.</p></blockquote><p>这样就能根据输入得到一个查询向量$q_i$, 一组键值对$&lt;k_i, v_i&gt;$.</p><p>有了QKV, 接下来需要按照Attention的流程计算$q_i$ 和$k_i$ 的Score, 根据论文中提到的<strong>缩放点积注意力</strong>(Scaled Dot-Product Attention):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/scaledotattention.jpg" style="zoom: 67%;" /><p>先进行点积, 再进行缩放, 计算完$q_i$ 与句中所有单词的$k$ 的得分(这里采用点积得到)后, 再对Score除以$\sqrt{d_k}$, 完成缩放, 最后再通过Softmax得到Attention权重, 加权求和结果称为$z_i$.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer10.png" style="zoom: 67%;" /><p>注意, 我们上面的讨论全部都是针对一个单词的, 但是在实际的运算中, 由于Encoder是线性Stack起来的, 所以其实Encoder的训练是可以并行的, 即<strong>多个单词做完Embedding后作为一个矩阵并行计算</strong>, 假设输入矩阵$X$, 通过$W_Q$, $W_K$, $W_V$ 计算后可以得到$Q$, $K$, $V$:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer11.png" style="zoom:50%;" /><p>综上, 将自注意力总结为:<br>$$<br>\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V<br>$$<br><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer12.png" style="zoom:50%;" /></p><p>这个公式其实在<code>&lt;Seq2Seq和Attention&gt;</code>一文中提到过, 但这里作者对Score使用了归一化, 即除以$\sqrt{d_k}$, $\sqrt{d_k}$ 为Key的维度. 这属于训练的一个Trick, 作者对此的解释如下:</p><blockquote><p>We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by $\frac{1}{\sqrt{d_k}}$.</p></blockquote><p>当$d_k$ 非常大时, 求得的内积可能会非常大, 如果不进行缩放, 不同的内积大小可能差异会非常大, Softmax在指数运算可能将梯度推到特别小, 导致梯度消失.</p><h3 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi - head Attention"></a>Multi - head Attention</h3><p>Multi - head Attention的思路和CNN中的多个<strong>卷积核</strong>起到的作用明显是一致的. 所谓”多头”, 放在<strong>卷积神经网络</strong>里就是卷积层多个卷积核的特征提取过程, 在这里就是进行多次注意力的提取, 就像多个卷积核一样, 多次<strong>不同的初始化矩阵</strong>经过训练可能会有多种<strong>不同的特征,</strong> 更有利于<strong>不同角度</strong>的特征抽取和信息提取. </p><p>论文中用多个头求出多组的数据堆叠, 也就是图中的$h$ 维:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/multiheadattention.jpg" style="zoom: 50%;" /><p>这样就能得到多个不同的Attention结果:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer13.png" alt=""></p><p>论文中采用了8个头的注意力, 即$h=8$, 得到多个提取出来的特征:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer14.png" style="zoom:50%;" /><p>我们将所有Self - Attention提取的特征全部concat起来, 因为维度比较大, 所以要经过一个输出矩阵$W_O$, 对特征进行进一步提取, 直到大小和Encoder接收的输入相同:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer15.png" style="zoom:50%;" /><p>也就是说经过左侧的6个Encoder, 向量大小仍然不改变.</p><p>到现在总结一下流程:</p><ol><li>做Embedding和位置编码, 获得输入$X$.</li><li>通过多头获得多组对应的$Q$, $K$, $V$.</li><li>通过缩放点积注意力, 多个头分别加权求和求得$Z_i$.</li><li>将所有$Z_i$ 全部concat起来, 然后经过$W_O$ 的特征提取, 得到最终输出$Z$, 其大小与输入$X$ 是完全相同的.</li></ol><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer16.png" style="zoom:50%;" /><blockquote><p>图中的$R$ 代表除了最开始的Encoder, 其他Encoder都不需要Embedding, 用上一个Encoder的输出作为输入.</p></blockquote><p>对每个头对句子不同部分的注意力进行可视化, 能发现每个头的注意力都在不同的位置上, 作用确实类似于CNN的卷积核, 做到了不同的注意力表示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer17.png" style="zoom:67%;" /><h3 id="Encoder-Side"><a href="#Encoder-Side" class="headerlink" title="Encoder Side"></a>Encoder Side</h3><h4 id="Position-wise-Feed-Forward-neural-network"><a href="#Position-wise-Feed-Forward-neural-network" class="headerlink" title="Position-wise Feed Forward neural network"></a>Position-wise Feed Forward neural network</h4><p>前馈神经网络就是结构中提到的Feed Forward neural network. 当然不单单是一个全连接层, 这里还用到了<strong>ReLu</strong>作为激活函数, 并且加上了<strong>Layer Normalization</strong>. 只有一个隐藏层的神经网络也可以表示为2个<strong>一维</strong>的$1\times1$的卷积, 这二者是等价的:<br>$$<br>\operatorname{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}<br>$$</p><p>但这里值得说明的是, 在论文中有这样一段:</p><blockquote><p>In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position <strong>separately</strong> and <strong>identically</strong>. This consists of two linear transformations with a ReLU activation in between.</p></blockquote><p>对于并行计算的不同单词, 通过的FFN参数是<strong>共享</strong>的, 也可以看做不同单词先后通过同一个FFN, 如下图所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer20.png" style="zoom:50%;" /><p>这也就是为什么我前面要说FFN能够看做是<strong>一维卷积</strong>.</p><p>并且在经过每个Encoder后, 都<strong>不改变数据的大小</strong>.</p><h4 id="Layer-Norm"><a href="#Layer-Norm" class="headerlink" title="Layer Norm"></a>Layer Norm</h4><p>你应该接触过Batch Norm, Layer Norm也是一种类似于Batch Norm的归一化方式, 同样能起到加快收敛的作用, 在<strong>NLP任务</strong>中比较常用. Batch Norm中, 记录下多个Batch中每维Feature的均值和方差, 并进行放缩和平移, 即对<strong>不同样本的同一个通道特征</strong>进行归一化. 在Layer Norm中, 只是换了一个维度, 我们对<strong>同一个样本的不同特征</strong>进行归一化.<br>$$<br>\begin{aligned}<br>\mu_{j}&amp;=\frac{1}{m} \sum_{i=1}^{m} x_{i j} \\<br>\sigma_{j}^{2}&amp;=\frac{1}{m} \sum_{i=1}^{m}\left(x_{i j}-\mu_{j}\right)^{2}<br>\end{aligned}<br>$$<br>引入$\epsilon$ 做平滑, 使分母不为0:<br>$$<br>\text {LayerNorm}(x)=\frac{x_{i j}-\mu_{j}}{\sqrt{\sigma_{j}^{2}+\epsilon}}<br>$$<br>跟Batch Norm一样, 之后也进行平移和放缩:<br>$$<br>y = \text{LayerNorm}(x) \cdot \gamma + \beta<br>$$<br>下面这张图非常清晰:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/layernorm.png" style="zoom:50%;" /><blockquote><p>如果想了解它为什么和NLP领域比较契合, 详见下文:</p><ul><li><a href="https://zhuanlan.zhihu.com/p/54530247" target="_blank" rel="noopener">模型优化之Layer Normalization</a></li><li><a href="https://zhuanlan.zhihu.com/p/74516930" target="_blank" rel="noopener">NLP中 batch normalization与 layer normalization</a></li><li><a href="https://www.zhihu.com/question/395811291" target="_blank" rel="noopener">transformer 为什么使用 layer normalization, 而不是其他的归一化方法?</a></li><li>原论文<a href="https://arxiv.org/abs/1607.06450" target="_blank" rel="noopener">Layer Normalization</a></li></ul><p>大致原因是Batch Norm 对于Embedding后的数据进行归一化, 假设每个Batch是由多个Embedding组成的, 按照Batch方向对每个归一化, 就是对每个词的Embedding整体归一化. 这样做非常没有道理, 不符合NLP的规律, 它反而加强了不同词之间的相关性.</p><p>但如果按照Layer Norm, 按照Layer方向, 实际上是分别对每个Embedding后的词向量进行归一化, 这样每个词向量相对独立. </p><p>这主要还是CV和NLP的<strong>数据属性</strong>决定的. 在CV中, 不同样本之间的Channel信息是具有共性的(因为图像还是要用2D来表示), 这部分信息非常重要, 如果归一化会损失很多信息. 而NLP中, 数据是Embedding来的, 本来也没有包含位置信息, 反而不同词向量之间毫无相关性, 关注单词本身的归一化效果会更好.</p></blockquote><h4 id="Residual"><a href="#Residual" class="headerlink" title="Residual"></a>Residual</h4><p>残差连接从ResNet中提出也有一定年头了, 作为近些年使用频次比较高效果比较好的结构, 原理不再多赘述了, 在<code>&lt;卷积神经网络小结&gt;</code>和<code>&lt;卷积神经网络发展史&gt;</code>中都有对残差连接的详解. </p><p>在Encoder中残差连接伴随着Layer Norm, 每次经过一个子层都要做一次残差连接.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer21.png" style="zoom:50%;" /><p> 在Decoder中也是同样的, 每次经过子层也都要做残差连接.</p><h3 id="Decoder-Side"><a href="#Decoder-Side" class="headerlink" title="Decoder Side"></a>Decoder Side</h3><p>当了解了Encoder的结构后, 结合起Decoder来看一下信息流. 假设只有两个Encoder和两个Decoder的堆叠, 那么信息的流动方向是这样的:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer22.png" style="zoom:50%;" /><p>确实只有最后的Encoder将输出传递给了Decoder的Encoder - Decoder Attention. Encoder的输出也是和Decoder唯一的交互数据, 其最终输出就是经过多个堆叠的Encoder计算得来的与Encoder输入大小相同的向量, 在交互的Encoder - Decoder Attention中会体现出来.</p><h4 id="Encoder-Decoder-Attention"><a href="#Encoder-Decoder-Attention" class="headerlink" title="Encoder - Decoder Attention"></a>Encoder - Decoder Attention</h4><p>Decoder中的Encoder - Decoder Attention和Encoder中的多头Self - Attention运算机制<strong>一致</strong>, 唯一不同的就是<strong>输入数据的来源</strong>. 其中, $Q$ 对应的输入来源于Decoder的<strong>Masked Self - Attention</strong>(即每个Decoder的第一层输出), 而$K$ 和$V$ 对应的输入来源于整个<strong>Encoder</strong>的最终输出(图中Encoder上方的蓝色向量). 根据这三个输入再结合Encoder - Decoder Attention层中的$W_Q$, $W_K$, $W_V$ 分别得到$Q$, $K$, $V$. 然后再根据缩放点积得到Attention Value.</p><p>对于多个经过Encoder的单词形成含有多个单词的矩阵, Decoder是在这一环节实现翻译时对不同单词的Attention的.</p><blockquote><p>注: 下面两张动图比较大, 挂在github上了, 如果没挂梯子可能无法正常显示,</p></blockquote><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/transformerdecode1.gif" style="zoom:50%;" /><h4 id="Outputs"><a href="#Outputs" class="headerlink" title="Outputs"></a>Outputs</h4><p>因为采用了Seq2Seq的架构, Decoder每过一个时间步不光接受Encoder的输出, 还接受了上一个Timestep的Decoder输入, 即论文中提到的”<strong>shifted right</strong>“.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/transformerdecode2.gif" style="zoom:50%;" /><h4 id="Masked-Multi-Head-Attention"><a href="#Masked-Multi-Head-Attention" class="headerlink" title="Masked Multi - Head Attention"></a>Masked Multi - Head Attention</h4><p>Mask是Transformer中一个关键点. Masked Multi - Head Attention 只出现在Decoder中. 到了Decoder, 可就不再像Encoder那样直接把数据拿过来并行训练了, 如果也像Encoder那样把所有输入的词向量全一股脑堆进去, Decoder做Self - Attention可以无视解码的时间跨度, <strong>获知全部的信息</strong>, 因此需要用Mask将当前预测的单词和之后的单词全都<strong>遮盖</strong>, 否则就没法训练了. </p><p>若仍然沿用传统Seq2Seq+RNN的思路, Decoder是一个<strong>顺序操作</strong>的结构, 我们代入一个场景来看看. 假设我们要执行<strong>机器翻译</strong>任务, 要将<code>我 是 大宁</code>翻译为<code>I am DaNing</code>, 假设所有参数与论文中提到的参数一样, batch size视为1. 根据前面已知的知识, Encoder堆叠后的输入和Embedding的大小是相同的, 在这里有三个词语, Embedding且通过Encoder后的编码大小为$(3, 512)$. 下面对Decoder进行训练:</p><ol><li>将起始符<code>&lt;start&gt;</code> 作为初始Decoder输入, 经过Decoder处理和分类得到输出<code>I</code>.</li><li>将<code>&lt;start&gt; I</code>作为Decoder输入, 经过Decoder处理和分类得到输出<code>am</code>.</li><li>将<code>&lt;start&gt; I am</code>作为Decoder输入, 经过Decoder处理和分类得到输出<code>DaNing</code>.</li><li>将<code>&lt;start&gt; I am DaNing</code>作为Decoder输入, 经过Decoder处理和分类得到结束符<code>&lt;end&gt;</code>.</li></ol><p>这种预测的方式也称为<strong>自回归</strong>.</p><p>参考将RNN更改为Self - Attention的Encoder思路, 对于这种依赖于前一个时间步预测结果的结构Decoder, 如果想做到<strong>并行</strong>训练, 需要将上面的过程转化为一个这样的矩阵直接作为Decoder的输入:<br>$$<br>\begin{bmatrix}<br>\text{&lt;start&gt;}&amp;  &amp;   &amp;   \\<br>\text{&lt;start&gt;}&amp; \text{I} &amp;   &amp;   \\<br>\text{&lt;start&gt;}&amp; \text{I} &amp; \text{am} &amp;   \\<br>\text{&lt;start&gt;}&amp; \text{I} &amp; \text{am} &amp; \text{DaNing}<br>\end{bmatrix}<br>$$<br>因为在<strong>训练时已知任务标签</strong>, 所以可以产生类似的效果. 这种方法被称为Teacher Forcing, 仅在训练阶段使用, 而不能使用在推断过程, 我会在下一节训练技巧中讲讲.</p><blockquote><p>图片取自<a href="https://wmathor.com/index.php/archives/1438/" target="_blank" rel="noopener">Transformer 详解</a>.</p></blockquote><p>在论文的图中, Mask操作顺序被放在$Q$ 和$K$ 计算并缩放后, Softmax计算前. 如果继续计算下去, 不做Mask, 与$V$ 相乘后得到Attention, 所有时间步信息全部都被泄露给Decoder, 必须用Mask将当前预测的单词信息和之后的单词信息全部遮住.</p><p>遮住的方法非常简单, 首先不能使用0进行遮盖, 因为Softmax中用零填充会产生错误, $e^0=1$. 所以必须要用$-\infty$来填充那些不能被看见的部分. 我们直接生成一个下三角全为0, 上三角全部为<strong>负无穷</strong>的矩阵, 与原数据相加就能完成遮盖的效果:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer24.png" style="zoom: 50%;" /><p>做Softmax时, 所有的负无穷全变成了0, 不再干扰计算:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer25.png" style="zoom: 67%;" /><p>其实Mask在对句子的<strong>无效部分填充</strong>时, 也是用同样方法将所有句子补齐, 无效部分用负无穷填充的.</p><blockquote><p>强调: Decoder仍然依赖与先前输出结果作为输入, 所以在正式使用时不能实现并行预测, 但在训练的时结果是已知的, 可以实现并行训练.</p></blockquote><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>最后来谈谈位置编码. 因为Transformer采用了纯粹的Attention结构, 不像RNN一样能够通过时间步来反映句子中单词的前后关系, 即不能得知<strong>位置信息.</strong> 要知道, 在NLP任务中, <strong>语序</strong>是一个相当重要的属性, 所以必须要通过某种方式让Transformer得知单词的位置, 作者通过<strong>位置编码</strong>在每次进入Encoder和Decoder前将位置信息写入. 这样来看, 与其叫位置编码, 不如叫<strong>位置嵌入</strong>. 位置编码可以直接与Embedding的向量相加:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer18.png" style="zoom:50%;" /><p>作者的做法非常有意思, 对不同的单词位置, 不同的Embedding维度, 它的编码都是<strong>唯一</strong>的, 应用正弦和余弦函数也方便Transformer学到位置的特征. 如果将当前单词位置记为$pos$, 而词向量的某个维度记为$i$, 那么位置编码的方法为:<br>$$<br>\begin{aligned}<br>P E_{(p o s, 2 i)} &amp;=\sin \left(p o s / 10000^{2 i / d_{\text {model }}}\right) \\<br>P E_{(p o s, 2 i+1)} &amp;=\cos \left(p o s / 10000^{2 i / d_{\text {model }}}\right)<br>\end{aligned}<br>$$<br>计算出来的结果应该是这样的:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer19.png" style="zoom:50%;" /><p>如果上面那组式子看起来有些乱, 写成这样或许好些:<br>$$<br>\begin{aligned}<br>P E_{(p o s, 2 i)} &amp;=\sin \left(\frac{p o s}{10000^\frac{2 i}{d_{\text {model }}}}\right) \\<br>P E_{(p o s, 2 i+1)} &amp;=\cos \left(\frac{p o s}{10000^\frac{2 i}{d_{\text {model }}}}\right)<br>\end{aligned}<br>$$<br>如果按照论文中的设定$d_{model}=512$, 由于奇偶数的计算方式是不同的, 所以$i \in[0, 255]$.</p><p>在这个式子中, 编码周期不受单词位置$pos$ 影响, 仅仅与模型开始设计的$d_{model}$ 和Embedding的不同维度$i$ 相关. 对于不同的$i$, $PE$ 的周期是$[2\pi, 10000\cdot2\pi]$.</p><p>这样看, 同一位置上的词语, 对于不同的Embedding维度, 都得到不同的编码, 并且随着$i$ 的增大, 位置编码的值的变化就越来越慢. 这种编码对于不同维度的Embedding来说是<strong>唯一</strong>的, 因此模型能够学习到关于Embedding的位置信息. </p><p>下面这个热图非常直观, 分开来看. 对于相同的Position的词语, 它不同维度的Embedding往往具有不同周期而交错的$\sin$ 和$\cos$ 组合, 而对于Embedding的同一个维度和不同Position的单词, 在Position上呈现出周期性.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/位置编码1.png" style="zoom:50%;" /><p>同样, 用折线图表示不同位置和编码后值的关系:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/位置编码2.png" style="zoom: 67%;" /><p>起初, 我并不知道为什么这种方法Work. 在看过<a href="https://zhuanlan.zhihu.com/p/92017824" target="_blank" rel="noopener">浅谈 Transformer-based 模型中的位置表示</a>后, 感觉似乎有些道理. 作者意在利用正弦余弦的数学性质(周期性, 和角公式), 使得偏移了的一定position, 记为$k$ , 能够得到正弦余弦的<strong>不同线性组合</strong>(总感觉这种编码在通信的某个地方应该很常用, 只是DL第一次用而已). </p><p>三角函数性质:<br>$$<br>\left\{\begin{array}{l}<br>\sin (\alpha+\beta)=\sin \alpha \cos \beta+\cos \alpha \sin \beta \\<br>\cos (\alpha+\beta)=\cos \alpha \cos \beta-\sin \alpha \sin \beta<br>\end{array}\right.<br>$$<br>得到偏移后, 即$pos+k$ 的$PE$:<br>$$<br>\left\{\begin{array}{l}<br>PE(pos+k, 2i)=PE(pos, 2i) \times PE(k, 2i+1)+PE(pos, 2i+1) \times PE(k, 2i) \\<br>PE(pos+k, 2i+1)=PE(pos, 2i+1) \times PE(k, 2i+1)-PE(pos, 2i) \times PE(k, 2i)<br>\end{array}<br>\right.<br>$$</p><p>除了公式计算, 作者也实验了其他对于位置编码的方式, 比如通过训练得到, 但由于实际效果与计算得来相仿, 那么还不如通过公式计算直接得到位置编码.</p><h3 id="Final-Output"><a href="#Final-Output" class="headerlink" title="Final Output"></a>Final Output</h3><p>最终输出很简单, 根据Decoder的输出经过FC层和Softmax得到对应的单词.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer23.jpg" style="zoom:50%;" /><p>注意, Decoder的Embedding层和最后输出经过Softmax前的Linear层也是<strong>共享权重</strong>的.</p><h2 id="训练技巧"><a href="#训练技巧" class="headerlink" title="训练技巧"></a>训练技巧</h2><p>让Transformer跑的起来离不开下面说的这些训练技巧.</p><h3 id="WarmUp"><a href="#WarmUp" class="headerlink" title="WarmUp"></a>WarmUp</h3><p>在原文中, Optimizer使用Adam, 除了设置参数$\beta_1 = 0.9$, $\beta_2 = 0.98$, $\epsilon=10^{-9}$外, 还设置了Warmup:<br>$$<br>\text {lrate}=d_{\text {model }}^{-0.5} \cdot \min \left(\text {step}_{-} \text {num}^{-0.5}, \text {step_num} \cdot \text {warmup_steps}^{-1.5}\right)<br>$$<br>我按照step增长, 做出WarmUp对应的learning rate变化图:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer26.jpg" style="zoom: 25%;" /><p>在开始时学习率大, 然后逐渐变小. 对于越大的$d_{model}$ 来说, 初始的斜率越小. 论文中设置$\text{warmup_step}=4000$. 许多后续的模型也用到了WarmUp. </p><h3 id="Label-Smoothing"><a href="#Label-Smoothing" class="headerlink" title="Label Smoothing"></a>Label Smoothing</h3><p>在<code>&lt;卷积神经网络发展史&gt;</code>中提到过, 这里再重复一下. 标签平滑可以看做是一种添加到损失公式中的一种正规化组件, 因为独热编码后的标签只有0和1, 可能有些过于绝对, 标签平滑提供了一种手段来使其中的0也能分配到一些数值.  假设输入为$x$, 一共需要对$c$ 个类进行划分, 则标签平滑后的结果$y\prime$ 为:<br>$$<br>y\prime = (1 - \epsilon) \cdot x + \frac{\epsilon}{c}<br>$$<br>直接贴代码, Show you my code!</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># input_tensor is a tensor in pytorch</span><span class="token keyword">def</span> <span class="token function">label_smoothing</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> epsilon<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    classes <span class="token operator">=</span> input_tensor<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># compute the number of classes</span>    <span class="token keyword">return</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> epsilon<span class="token punctuation">)</span> <span class="token operator">*</span> input_tensor <span class="token operator">+</span> <span class="token punctuation">(</span>epsilon <span class="token operator">/</span> classes<span class="token punctuation">)</span> <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'label_smoothing(a):'</span><span class="token punctuation">,</span> label_smoothing<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'label_smoothing(b):'</span><span class="token punctuation">,</span> label_smoothing<span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""label_smoothing(a): tensor([0.0333, 0.0333, 0.9333])label_smoothing(b): tensor([0.0500, 0.9500])"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在论文中设置$\epsilon_{ls}=0.1$. 标签平滑可能会提高句子的困惑度, 因为添加了更多的不确定性, 但会提高准确率和BLEU.</p><h3 id="Residual-Dropout"><a href="#Residual-Dropout" class="headerlink" title="Residual Dropout"></a>Residual Dropout</h3><p>在进行残差连接时, 每个子层进行add和batch norm之前, 都添加了Dropout. 原论文中设置$P_{\text {drop}}=0.1$, 个人认为Dropout没有前两种技巧作用大.</p><h3 id="Teacher-Forcing"><a href="#Teacher-Forcing" class="headerlink" title="Teacher Forcing"></a>Teacher Forcing</h3><p>在模型的训练阶段, Decoder的所有正确输入是<strong>完全已知</strong>的, 如果自回归预测在某个时间步$t$ 解码出错, 则会导致$t$ 时刻后所有的预测结果都产生<strong>偏差</strong>.</p><p>Teacher Forcing通过将自回归模型解码过程中的所有输入<strong>强制修正</strong>为Ground Truth来避免了这个问题.</p><p>Teacher Forcing有诸多优点:</p><ul><li>解决了训练阶段自回归式模型的串行问题, 能够使得模型训练时<strong>并行</strong>.</li><li>避免了模型训练时预测<strong>一步错步步皆错</strong>的问题.</li><li>由于干涉了训练错误的情况, 加快了模型的<strong>收敛</strong>速度.</li></ul><p>但它也同时容易产生<strong>矫枉过正</strong>的问题:</p><ul><li><strong>Exposure Bias</strong>: 这是最为常见的问题. 在训练时因为受到干涉, 很容易产生<strong>训练推断不一致</strong>.</li><li><strong>Overcorrect</strong>: 有时候模型解码有自己的想法, 但因为Teacher Forcing的干涉, 导致生成的句子<strong>四不像</strong>.</li><li><strong>No diversity</strong>: Teacher Forcing对Ground Truth的约束是非常强的, 模型的<strong>多样性</strong>受到严重限制.</li></ul><blockquote><p>Teacher Forcing的衍生问题, 请阅读<a href="https://zhuanlan.zhihu.com/p/93030328" target="_blank" rel="noopener">关于Teacher Forcing 和Exposure Bias的碎碎念</a>.</p></blockquote><h2 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h2><blockquote><p>摘自<a href="https://www.nowcoder.com/discuss/258321" target="_blank" rel="noopener">牛客网: NLPer看过来, 一些关于Transformer的问题整理</a></p></blockquote><h3 id="Transformer相比于RNN-LSTM-有什么优势-为什么"><a href="#Transformer相比于RNN-LSTM-有什么优势-为什么" class="headerlink" title="Transformer相比于RNN/LSTM, 有什么优势? 为什么?"></a>Transformer相比于RNN/LSTM, 有什么优势? 为什么?</h3><ol><li>RNN系列的模型<strong>并行计算</strong>能力很差.</li><li>Transformer的<strong>特征抽取</strong>能力比RNN系列的模型要好(实验结论).</li></ol><h3 id="为什么说Transformer可以代替seq2seq"><a href="#为什么说Transformer可以代替seq2seq" class="headerlink" title="为什么说Transformer可以代替seq2seq?"></a>为什么说Transformer可以代替seq2seq?</h3><p>这里用代替这个词略显不妥当, seq2seq虽已老, 但始终还是有其用武之地, seq2seq最大的问题在于<strong>将Encoder端的所有信息压缩到一个固定长度的向量中</strong>, 并将其作为Decoder端首个隐藏状态的输入, 来预测Decoder端第一个单词(token)的隐藏状态. 在输入序列比较长的时候, 这样做显然会损失Encoder端的很多信息, 而且这样一股脑的把该固定向量送入Decoder端, Decoder端不能够关注到其想要关注的信息. 上述两点都是seq2seq模型的缺点, 后续论文对这两点有所改进, 如著名的<a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">Neural Machine Translation by Jointly Learning to Align and Translate</a>, 虽然确确实实对seq2seq模型有了实质性的改进, 但是由于主体模型仍然为RNN(LSTM)系列的模型, 因此模型的并行能力还是受限, 而transformer不但对seq2seq模型这两点缺点有了实质性的改进(多头交互式attention模块), 而且还引入了self-attention模块, 让源序列和目标序列首先”自关联”起来, 这样的话, 源序列和目标序列自身的embedding表示所蕴含的信息更加丰富, 而且后续的FFN层也增强了模型的表达能力(ACL 2018会议上有论文对Self-Attention和FFN等模块都有实验分析, 见论文: <a href="http://aclweb.org/anthology/P18-1167" target="_blank" rel="noopener">How Much Attention Do You Need?A Granular Analysis of Neural Machine Translation Architectures</a>), 并且Transformer并行计算的能力是远远超过seq2seq系列的模型, 因此我认为这是transformer优于seq2seq模型的地方. </p><h3 id="Transformer中句子的encoder表示是什么？如何加入词序信息的？"><a href="#Transformer中句子的encoder表示是什么？如何加入词序信息的？" class="headerlink" title="Transformer中句子的encoder表示是什么？如何加入词序信息的？"></a>Transformer中句子的encoder表示是什么？如何加入词序信息的？</h3><p>Transformer Encoder端得到的是整个输入序列的encoding表示, 其中最重要的是经过了self-attention模块, 让输入序列的表达更加丰富, 而加入词序信息是使用不同频率的正弦和余弦函数.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Transformer </tag>
            
            <tag> Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>卷积神经网络发展史</title>
      <link href="/posts/38085.html"/>
      <url>/posts/38085.html</url>
      
        <content type="html"><![CDATA[<h2 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h2><p>LeNet可以说是CNN的开山鼻祖之一了, 虽然它不是CNN的起点, 但是可以称为CNN兴起的标志. 它由图灵奖得主LeCun Yann在1998年的<a href="https://ieeexplore.ieee.org/document/726791?reload=true&arnumber=726791" target="_blank" rel="noopener">Gradient-based learning applied to document recognition</a>提出. </p><p>CNN最早的模型框架就是由LeNet给出的, 即卷积+池化+FC层的基本结构.</p><p>诸如局部感知, 权值共享, 池化等操作都是从这里开始的, 并深远的影响了以后的卷积神经网络发展.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lenet.jpg" style="zoom:67%;" /><p>对当时的深度神经网络(也就是多层感知机)来说冲击非常大. 相较于MLP来说, LeNet所采用的的<strong>权值共享</strong>和<strong>局部连接</strong>大幅度降低了模型参数数量, 并对于手写数字识别提升到更高的准确率. 并且LeNet还设计了<strong>MaxPool</strong>通过下采样提取特征. 当时采取的激活函数是Tanh, 因为它关于原点对称, 比Sigmoid<a href="https://liam.page/2018/04/17/zero-centered-active-function/" target="_blank" rel="noopener">收敛快</a>.</p><p>可惜当时LeNet受限于时代背景, 直到后来GPU运算的普及和2009年ImageNet这样大规模的数据集的出现, 才被人发现它的价值.</p><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>2012年另一个具有划时代意义的模型AlexNet横空出世. AlexNet在2012年ImageNet竞赛中以超过第二名10.9个百分点的绝对优势, 一举夺冠, 引起了相当大的轰动. 此前深度学习沉寂了很久, 但自AlexNet诞生后, 所有的ImageNet冠军都是用CNN来做的.</p><p>AlexNet的论文是<a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" target="_blank" rel="noopener">ImageNet Classification with Deep Convolutional Neural Networks</a>.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/alexnet.jpg" style="zoom: 67%;" /><p>AlexNet的特点:</p><ol><li>相较于LeNet, AlexNet的网络深度更<strong>深</strong>.</li><li>在每个卷积层后首次使用了<strong>ReLU</strong>作为激活函数.</li><li>添加了<strong>Local Response Normalization</strong>(LRN, 局部响应归一化) 层提高准确率, 但似乎2015年的VGG中却提到LRN没什么用.</li><li>设计和使用了<strong>Dropout</strong>, 来减轻模型过拟合程度.</li><li>使用了<strong>数据增强</strong>, 对训练数据施以裁剪, 旋转等.</li><li>基于当时的算力限制, AlexNet将图像分割为上下两块<strong>分别训练</strong>, 然后用FC层合并在一起.</li></ol><p>但是AlexNet的卷积核很大, 训练起来非常困难, 并且反向传播时最靠近输入端的卷积核更难利用梯度来更新自身的值.</p><h2 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h2><p>VGGNet是由牛津大学计算机视觉组和Google DeepMind公司的研究员一起研发, 在<a href="https://arxiv.org/abs/1409.1556v6" target="_blank" rel="noopener">Very Deep Convolutional Networks for Large-Scale Image Recognition</a>中提出的深度卷积神经网络, 取得了ILSVRC2014比赛分类项目的第二名, 而第一名是同年提出的GoogLeNet. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vgg16.png" style="zoom: 33%;" /><p>VGG使用了更小的卷积核, 首次提出用<strong>多个小卷积核代替一个大卷积核</strong>的观点. 比如用两个3x3卷积代替一个5x5卷积, 用三个3x3卷积代替一个7x7卷积, 这样能<strong>增加非线性映射</strong>, 保证了模型的感知域是相同的, 但却减小了模型参数量.</p><p>同时, VGG也指出了在AlexNet中使用的局部响应归一化可能没有增益, 因为在实验中并没有给VGG带来性能提升.</p><p>除此外, VGG构建了16-19层的稳定卷积神经网络, 证明了<strong>网络深度的增加可以带来性能提升</strong>.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vgg.jpg" style="zoom:67%;" /><h2 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h2><p>GoogLeNet也称为<strong>Inception</strong>, 其第一个版本在与VGG出世的同一年ImageNet中取得了冠军.</p><p>在VGGNet中发现, 在加宽和加深一个网络时, 模型效果会得到提升. 但是与之对应的就是参数数量上的增加, 和对梯度消失和弥散的不可控, 这就导致反向传播的效果差, 也意味着过拟合风险增加. 所以在加宽和加深模型的同时尽可能的<strong>减少参数</strong>, 肯定能够带来性能的提升.</p><p>其实上述所指的方法就是将全连接层替换为<strong>更加稀疏</strong>的网络结构. 因为大多数硬件是针对稠密数据计算的, 对于稀疏数据的计算没有优化, 所以稠密数据和稀疏数据的计算效果是相似的. 应该找到一种方法实现同时利用网络内的稀疏性, 又能利用密集矩阵的高性能计算. 根据文献人们知道, 将<strong>稀疏矩阵聚类为密集的子矩阵</strong>可以提高计算性能, 这就成为了GoogLeNet的核心思想: <strong>怎样用密集成分来近似最优的局部稀疏结构</strong>.</p><h3 id="Inception-v1"><a href="#Inception-v1" class="headerlink" title="Inception v1"></a>Inception v1</h3><p>稀疏连接有两种方法, 其一在空间上的局部连接, 也就是CNN的方法, 另一是在特征维度上将稀疏连接进行处理, 也就是在通道维度上处理. GoogLeNet v1通过设计密集结构来近似一个稀疏的CNN. 相关论文<a href="https://arxiv.org/abs/1409.4842" target="_blank" rel="noopener">Going deeper with convolutions</a>. 对于相同尺寸更多数量的卷积核, 提取出的特征更加稀疏, 而更少数量的卷积核提取的特征更加稠密. 同时使用<strong>不同尺寸</strong>的卷积核, 分别提取不同尺度的特征, 但保持总的卷积核数量不变, 这样提取出的特征密度更大, 并且不同尺度之间具有<strong>弱相关性</strong>, 这在现在的CNN中早已被广泛运用, 也称<strong>多尺度卷积</strong>. 最早的Inception模块如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetv1-1.jpg" style="zoom: 67%;" /><p>多尺度卷积时在图像周围使用<strong>补0</strong>的方式来维持不同尺度的卷积特征图尺寸相同, 即TF中CNN的SAME Padding方式.</p><p>上述的Inception块容易导致运算量很大, 因为最后不同尺度的卷积核和池化是concat在一起的, 就有可能导致并行堆叠产生的维度越来越大, 使计算更困难, 所以Inception采用了更多的<strong>1x1卷积</strong>来减少维度和参数. 这种1x1卷积除了减小参数量, 还增加了非线性拟合能力.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetv1-2.jpg" style="zoom: 67%;" /><p>Inception采用了大量的Inception块结构进行堆叠, 并另外增加了两个辅助的softmax分支, 意在<strong>避免梯度消失</strong>和为输出做<strong>辅助分类</strong>, 将loss加权后加到最终分类结果中, 文中认为辅助分类器只在训练末期提高准确率. 在最后实际测试时, 这两个辅助的分支会被去掉, 去掉后并不会导致模型性能有明显的降低. 作者认为辅助分类器起到的作用更像是<strong>正则化</strong>.</p><p>Inception v1已经尽可能的抛弃掉FC层, 只是为了方便大家对模型进行FineTune才在最后加入的FC.</p><p>Inception v1完整结构图如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetv1.png" style="zoom: 50%;" /><p>最终卷积层后采用<strong>全局池化</strong>, 而非FC层, 全局池化有助于减少参数量.</p><h3 id="Inception-v2-Inception-v3"><a href="#Inception-v2-Inception-v3" class="headerlink" title="Inception v2/Inception v3"></a>Inception v2/Inception v3</h3><p>凭借Inception v1的优秀表现, 2015年GoogLeNet团队又对其挖掘改进, 在<a href="https://arxiv.org/abs/1512.00567v3" target="_blank" rel="noopener">Rethinking the Inception Architecture for Computer Vision</a>提出了Inception v1的升级版本Inception v2/v3. </p><p>在论文开头, 作者提出了4个网络结构设计原则:</p><blockquote><ol><li>Avoid representational bottlenecks, especially early in the network.</li><li>Higher dimensional representations are easier to process locally within a network.</li><li>Spatial aggregation can be done over lower dimensional embeddings without much or any loss in representational power. </li><li>Balance the width and depth of the network.</li></ol></blockquote><ol><li>避免信息表征的瓶颈, 尤其是在网络早期. (<strong>不要过早压缩和降维原来的信息</strong>)</li><li>更高维的表征能更容易的在网络中局部处理. (增加每层的卷积核数量能获得更低耦合度的特征)</li><li>空间聚合在低维嵌入时能在不损失太多表征能力的情况下完成. (可以先用1x1对输入特征降维再执行大卷积核卷积, 如果对输入特征直接池化, 会损失很多信息)</li><li>网络的深度和宽度要平衡.</li></ol><p>Inception v2基于VGG对用多个小卷积核代替大卷积核的思想, 将Inception v1中设计的Inception块进行了优化, 将5x5的卷积核替换为2个3x3的卷积核.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetv2-1.jpg" style="zoom:50%;" /><p>有没有能压缩更多参数的方法呢? 确实有, 通过<strong>非对称卷积</strong>, 每个nxn的卷积核都可以继续由1个1xn卷积核和1个nx1卷积核代替(Figure 6), 这样比原来参数更少, 也能实现更加高效的降维. 在Inception v2中, 设定n=7. 不要忘记论文开头的第一个原则, 在<strong>网络早期使用这种操作效果</strong>并不好. 除去降维外, 基于第二个准则, 设计了Figure 7 这种用来对特征提升维度的Inception模块. 提升后的高维特征更有益于模型的理解和处理, 也更加稀疏. 这种结构只在最后粗糙的8x8输入使用.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetv2-2.jpg" style="zoom: 50%;" /><p>除此外, 还用并行结构对池化做了改进. 如果先池化后卷积可以减少, 直接进行池化, 可以减少运算量, 但会引入特征表示瓶颈. 为了遵循论文提出的第一个准则, 为了避免特征表示瓶颈, 先卷积后池化, 但这样运算量就会很大. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetv2-3.jpg" style="zoom: 33%;" /><p>作者找到了一种两全其美的方法, 即卷积和池化同时进行, 再将它们的结果concat起来, 这样即避免了特征表示瓶颈, 也没有引入很多的计算量.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetv2-4.jpg" style="zoom:50%;" /><p>在<a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">Batch Normalization: Accelerating Deep Network Training b y Reducing Internal Covariate Shift</a> 中已经提到过<strong>Batch Normalization</strong>, 能够对网络模块之间达到<strong>解耦</strong>的效果, 加快收敛速度. 顺带改进了Inception v2.</p><p>相较于Inception v2, Inception v3只做了少量改动, 在先前基础上还进行了如下更改:</p><ol><li><p>使用<strong>RMSProp</strong>做优化器.</p></li><li><p>使用<strong>标签平滑</strong>防止过拟合.</p><blockquote><p>标签平滑可以看做是一种添加到损失公式中的一种正规化组件, 因为独热编码后的标签只有0和1, 可能有些过于绝对, 标签平滑提供了一种手段来使其中的0也能分配到一些数值. 我下面给出代码, 便能告诉你它的实现方式.</p></blockquote><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>b <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># input_tensor is a tensor in tensorflow</span><span class="token keyword">def</span> <span class="token function">label_smoothing</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> epsilon<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    classes <span class="token operator">=</span> input_tensor<span class="token punctuation">.</span>get_shape<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>as_list<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># compute the number of classes</span>    <span class="token keyword">return</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> epsilon<span class="token punctuation">)</span> <span class="token operator">*</span> input_tensor <span class="token operator">+</span> <span class="token punctuation">(</span>epsilon <span class="token operator">/</span> classes<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>label_smoothing<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>label_smoothing<span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># label smoothing(a): [0.03333334, 0.03333334, 0.93333334]</span><span class="token comment" spellcheck="true"># label smoothing(b): [0.05 0.95]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>辅助分类器使用了<strong>Batch Norm,</strong> 加快模型收敛速度.</p></li><li><p>7x7卷积拆成了1x7和7x1卷积的<strong>叠加</strong>, 同时输入图像从224x224变为<strong>299x299</strong>.</p></li></ol><p>Inception v2/v3结构如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetv3.jpg" style="zoom: 67%;" /><h3 id="Inception-v4"><a href="#Inception-v4" class="headerlink" title="Inception v4"></a>Inception v4</h3><p>Inception v4比较保守, 对原来的版本进行了梳理, Inception v4在2016年的<a href="https://arxiv.org/abs/1602.07261" target="_blank" rel="noopener">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</a>中提出. 该论文中不但提出了Inception v4, 还借鉴了ResNet的思想, 研究了若干种Inception与残差连接之间的结合方式, 做了相当多的实验.</p><p>首先看Inception v4的大体结构(左)和主干部分(右):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetv4.jpg" style="zoom: 25%;" /><p>Inception模块都有了相应改动, 相比v3添加了平均池化和1x1卷积, 下图从左到右分别为Inception-A, Inception-B, Inception-C:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetv4-1.jpg" alt=""></p><p>还有两种降维结构, 下图分别为35x35到17x17的Reduction-A和17x17到8x8的Reduction-B:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetv4-2.jpg" style="zoom: 33%;" /><p>多次使用1x1卷积进一步降低了参数数量, 并且设计上更加<strong>简单一致</strong>, 抛去了Inception v3中不必要的部分, 比如辅助分类器等结构.</p><h3 id="Inception-ResNet"><a href="#Inception-ResNet" class="headerlink" title="Inception-ResNet"></a>Inception-ResNet</h3><p>Inception-ResNet结合了ResNet的思想, 与Inception v4出自同一篇论文. Inception-ResNet在Inception原有的基础上添加了残差链接, 作者研究了残差连接的作用, 指出残差连接并非能明显提升模型精度, 而是会加快训练的<strong>收敛速度</strong>. 在此基础上提出了两个版本的Inception-ResNet.</p><p>无论是Inception-ResNetv1还是Inception-ResNetv2都遵循同一个架构(左), 但Inception-ResNetv1采用了图示主干(右), 而Inception-ResNetv2却沿用了Inception v4的主干:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetres.jpg" style="zoom:67%;" /><h4 id="Inception-ResNetv1"><a href="#Inception-ResNetv1" class="headerlink" title="Inception-ResNetv1"></a>Inception-ResNetv1</h4><p>Inception-ResNetv1采用的Inception块如下, 下图从左到右分别为Inception-ResNet-A, Inception-ResNet-B, Inception-ResNet-C:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetresv1-1.jpg" style="zoom: 50%;" /><p>35x35到17x17的Reduction与Inception v4相同, 17x17到8x8是新设计的结构:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetresv1-2.jpg" style="zoom:50%;" /><p>Inception-ResNetv1与Inception v3计算复杂度相近.</p><h4 id="Inception-ResNetv2"><a href="#Inception-ResNetv2" class="headerlink" title="Inception-ResNetv2"></a>Inception-ResNetv2</h4><p>Inception-ResNetv2沿用了Inception v4的主干, Reduction结构与Inception-ResNetv1完全相同. </p><p>只有三种j调节过参数的Inception-Res块. 其实v1和v2的不同只在于<strong>参数的不同和主干Stem的不同</strong>.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetresv2-1.jpg" style="zoom:50%;" /><p>Inception-ResNetv2与Inception v4计算复杂度相近.</p><h4 id="残差连接的问题"><a href="#残差连接的问题" class="headerlink" title="残差连接的问题"></a>残差连接的问题</h4><p>在引入残差连接后, 卷积核数量如果太多, 会导致训练不稳定, 甚至会出现网络训练休眠, 在池化层之前输出就为0. 对此引入一个<strong>放缩因子</strong>, 通过乘以一个非常小的系数再相加, 这样使得网络重新激活. 一般scale取0.1 ~ 0.3之间的值, 引入后不会降低精度还能使得网络更加稳定.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/googlenetres-scaling.jpg" style="zoom: 50%;" /><p>当然图中的Inception可以替换为任意的子网络.</p><h3 id="Xception"><a href="#Xception" class="headerlink" title="Xception"></a>Xception</h3><p>2016年, Google利用深度可分离卷积对Inception v3进行了改良, 在<a href="https://arxiv.org/abs/1610.02357" target="_blank" rel="noopener">Xception: Deep Learning with Depthwise Separable Convolution</a> 中提出Xception模型(起名应该是模仿XGboost, 也是Extreme Inception的意思). </p><blockquote><p>关于深度可分离卷积可以单独开一篇文章了, 为了保证Inception系列的完整性, 先挖个坑.</p></blockquote><h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><blockquote><p>在《卷积神经网络小结》一文中, 在残差块部分有对残差网络的描述.</p></blockquote><p>随着网络深度的加深, 模型训练变得越来越复杂, 即使是使用Batch Norm或ReLU也无法解决梯度爆炸和梯度消失的现象, 仍然无法把网络做的足够深. 2015年何凯明在<a href="https://arxiv.org/abs/1512.03385v1" target="_blank" rel="noopener">Deep Residual Learning for Image Recognition</a> 中提出了带有跳跃连接的神经网络ResNet, 在2015年的ILSVRC中夺得冠军.</p><p>人们发现并不是网络越深性能越好, 例如下图中56层的模型无论是在训练集还是测试集的误差都比20层的模型要高得多, 到20层后的模型再向上堆叠效果反而会变差.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/resnet1.jpg" style="zoom: 33%;" /><p>在反向传播中, 神经网络层数越多, 信息损失的就越多, 十分有可能发生梯度爆炸和梯度消失. 对于一个准确率近乎饱和网络来说, 再在这个网络的基础上添加新的结构是会使性能变差的, 上述问题也称为<strong>退化问题</strong>. 除非添加的神经层实现的映射是<strong>恒等映射</strong> , 即f(x) = x, 这样就能使网络在加深的同时还没有增加误差, 解决了模型性能和模型深度之间的悖论. 也就是说如果某层已经达到了趋近于饱和的性能, 那么接下来的学习目标就变为学习恒等映射. 以保证后面的其他层不会因为冗余层而导致精度下降.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/resnet2.jpg" style="zoom:50%;" /><p>从图中能很明显的看出, 这种恒等映射思想和<strong>LSTM</strong>中的思想极其相似, 当时何凯明也是基于LSTM得到的启发. 二者都尝试通过<strong>借鉴邻近的短期信息</strong>来减缓深层模型带来的梯度消失或爆炸的问题, 使得邻近的特征得以更长的传播. 而加法与乘法不同, 加法的梯度变化取决于信息之间的<strong>独立叠加</strong>, 而非乘法一样体现出输入信息之间的<strong>相互控制</strong>.</p><p>“残差”二字来源于目标值H(x)与冗余层前的输入值x的差值, 即后面的学习目标应该使残差目标F(x) = H(x) - x 逼近与0. 这种跳跃连接也称为<strong>Skip connections</strong>, 这种结构有良好的数学性质, 在进行反向传播时, 恒等映射的导数为1, 不会造成导数丢失.</p><p>当然, 上述内容都不能违背一个基本假设: <strong>对于一个有能力学习到恒等映射的深层网络, 它的性能绝对不会它的近乎饱和的浅层网络性能差</strong>.</p><p>ResNet结构与VGG19的对比图如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/resnet.png" style="zoom: 33%;" /><p>在最右侧, 有两种Skip connection的实现:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/resnet3.jpg" style="zoom:50%;" /><p>左侧对应的是实线的实现方式, 右侧对应的是虚线的实现方式. 虚线的残差块中采用了1x1卷积来降低参数数量. 对于更深层的网络必须采用右侧的实现方式. </p><p>在论文中, 作者给出了不同层数的ResNet设计参数:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/resnet4.jpg" style="zoom: 50%;" /><p>ResNet确实也做到了使得网络足够深, 甚至能达到152层. 也解决了网络越深越难训练的问题. 但是冗余层过多会带来更多的<strong>计算问题</strong>. 并导致特征<strong>过于冗余</strong>.</p><h2 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h2><p>为了解决ResNet在特征上的冗余问题, 在2016年<a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="noopener">Densely Connected Convolutional Networks</a>中沿用了ResNet和Highway Network的思想, 提出了DenseNet. 在Skip connections的基础上实现了<strong>特征复用</strong>. 作者提出了Dense Block作为新的网络结构. 作者认为通过更多的连接能够保证层与层之间的信息能最大程度的保留.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/densenet.jpg" style="zoom: 50%;" /><p>Dense Block 相对于Residual Block来说更为<strong>激进和密集</strong>, 将同一个块中所有的层进行互联, 即每层都会接收前面所有层的输出作为额外输入, 与ResNet不同的地方在于, 连接并非相加, 而是直接<strong>concat</strong>, 这样就必须在块与块之间添加减小特征维数的结构.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/densenet1.jpg" alt=""></p><p>这种更密集的连接提供了更高的特征利用率, 在同等性能下减少了参数.</p><p>与ResNet相同, 在论文中作者也给出了不同深度的DenseNet设计参数:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/densenet2.jpg" style="zoom: 50%;" /><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文的内容也都是浅尝辄止, 能够快速掌握近些年CNN的发展方向, 想深入了解还是需要精读论文. 由于CV方向应用落地快, 近些年CNN的研究也是突飞猛进, 提出了很多新CNN的结构, 这些新的结构并没有被写入.</p><p>下图出自<a href="https://arxiv.org/abs/1810.00736" target="_blank" rel="noopener">Benchmark Analysis of Representative Deep Neural Network Architectures</a>. </p><p>在这张图中, 圆圈越大, 模型参数越多, 圆圈越靠上, Top1准确率越高, 圆圈越靠右, 计算量越大. 相对位置越靠左上角, 模型的综合素质越好.</p><p>通过这种可视化, 可以更方便的对比模型之间的差距, 也方便梳理CNN的发展历史. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/cnnsummary.jpg" style="zoom: 67%;" /><p>除文中提到的模型外, 在图中值得说一说的其他模型:</p><ul><li><strong>SqueeeNet</strong>: 2016年提出, 虽然准确率不高, 但大幅减少了模型参数, 减少了对算力和硬件的需求.</li><li><strong>MobileNet</strong>: 2017年提出, 在维持其准确率的同时, 保证了移动端的使用.</li><li><strong>NASNet</strong>: 2017年提出,  使AI能够在特殊任务上自行优化网络结构.</li><li><strong>SENet</strong>: 2017年提出, 通过网络反馈对不同的Feature Map分配权重.</li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>别再对类别变量独热编码</title>
      <link href="/posts/35661.html"/>
      <url>/posts/35661.html</url>
      
        <content type="html"><![CDATA[<h1 id="不要再对类别变量独热编码"><a href="#不要再对类别变量独热编码" class="headerlink" title="不要再对类别变量独热编码"></a>不要再对类别变量独热编码</h1><p>本文参考了<a href="https://towardsdatascience.com/stop-one-hot-encoding-your-categorical-variables-bbb0fba89809" target="_blank" rel="noopener">Stop One-Hot Encoding Your Categorical Variables</a>, 并对其内容在加以自身理解的情况下进行翻译.</p><h2 id="独热编码"><a href="#独热编码" class="headerlink" title="独热编码"></a>独热编码</h2><p>对不同的类别变量就需要用到独热编码, 独热编码是将<strong>类别变量映射为离散型特征</strong>的方法. 由于类别变量经常是以字符串类型存在的, 这是不能被机器所量化接受的, 必须通过某种方式映射成机器可以接受的数值才能够运算.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200904192711.png" style="zoom: 67%;" /><p>当类别比较多时, 独热编码产生的向量是非常巨大的<strong>稀疏向量</strong>, 这些增加进去的维度为问题扩大了搜索空间, 同时因为向量的稀疏性引入了很多不必要的计算, 提升了解决问题的难度. 因此在能够解决问题的情况下, 最好维持<strong>足够少的特征维度</strong>, 以保证任务在有限时间内解决的可行性. 尤其是对于<strong>神经网络</strong>, 独热编码有明显的副作用.</p><p>更麻烦的是由于独热编码是对类别变量加工产生稀疏向量的特性, 还会导致这些被编码的特征之间产生<a href="https://baike.baidu.com/item/%E5%A4%9A%E9%87%8D%E5%85%B1%E7%BA%BF%E6%80%A7/10201978" target="_blank" rel="noopener">多重共线性</a>, 即产生的稀疏特征向量线性相关, 在优化过程中就使得对这些稀疏向量之间互相依赖, 难以优化. 除非是算法对稀疏有优化, 否则对于较少的类别变量不会引入太大的问题, 一旦问题具备一定规模就会带来很大的危害.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200904193737.png" style="zoom:67%;" /><h2 id="目标编码"><a href="#目标编码" class="headerlink" title="目标编码"></a>目标编码</h2><p>目标编码是表示类别的一种有效方法, 只占用一个维度的特征. 它也被称为<strong>均值编码</strong>, 这种方法更好的表示类别变量和目标之间的关系, 在Kaggle中非常常见. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200904194435.png" style="zoom:67%;" /><p>对于上图来说, 简单点的做法它是将相同类别变量的<code>Score</code> 相加并求均值.</p><p>更普适的方法是利用样本的先验概率和后验概率结合权重函数形成一个凸组合来得到均值,  假设分类问题需要分出C个不同的类别, 那么对于要区分的每个类别, 都能有与之相对应的类别变量形成的均值特征. </p><p>由于区分的每个类别概率和为1, 在区分最后一个类时, 其概率一定与前面形成的特征线性相关, 所以可以舍弃它. 整个数据集就<strong>扩充了C-1列</strong>, 而不是C列.</p><p>更加详细的就不介绍了, 可以看<a href="https://zhuanlan.zhihu.com/p/117230627" target="_blank" rel="noopener">特征编码总结 Kaggle</a> 或者<a href="https://zhuanlan.zhihu.com/p/26308272" target="_blank" rel="noopener">平均数编码：针对高基数定性特征（类别特征）的数据预处理/特征工程</a>了解更多.</p><p>但这种方法也有缺点:</p><ol><li>模型更难学习均值编码和其他变量之间的关系, 它只与</li><li>对模型的标签极为敏感, 因为编码是与模型标签相关的, 如果数据集标签有细微变化, 都可能导致编码出现问题, 进而影响预测效果. 所以异常值对它影响非常大.</li><li>可能加剧过拟合. 因为均值编码是连续值, 在过拟合时可能将差别很小的两个编码样本认为是完全不同的两个类. </li></ol><p><code>category_encoders</code> 已经以<code>sklearn</code>的方式对这种编码进行了封装, 因为目标编码是一种监督学习编码, 所以必须同时传入<code>X</code>和<code>Y</code>.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> category_encoders <span class="token keyword">import</span> TargetEncoderencoder <span class="token operator">=</span> TargetEncoder<span class="token punctuation">(</span>cols<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'Name_of_col'</span><span class="token punctuation">,</span> <span class="token string">'Another_name'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>train_data <span class="token operator">=</span> encoder<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="留一编码"><a href="#留一编码" class="headerlink" title="留一编码"></a>留一编码</h2><p>留一编码通过<strong>排除当前行</strong>的值来计算均值来作为编码, 使得数值更加多样化, 一定程度上减轻了编码对标签的依赖.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200904211247.png" style="zoom:67%;" /><p>这个方法能够将类别变量映射到某个范围内, 而并非某个具体值. 例如图中的<code>State=California</code> 有0.5和0.4两种不同的取值, 所以泛化能力会更好.</p><p>同样使用<code>category_encoders</code> 实现.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> category_encoders <span class="token keyword">import</span> LeaveOneOutEncoderencoder <span class="token operator">=</span> LeaveOneOutEncoder<span class="token punctuation">(</span>cols<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'Name_of_col'</span><span class="token punctuation">,</span><span class="token string">'Another_name'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>train_data <span class="token operator">=</span> encoder<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>与留一法效果相似的一种方法是在编码中加入正态分布的噪声, 并将标准差作为一个可以调节的参数.</p><h2 id="贝叶斯目标编码"><a href="#贝叶斯目标编码" class="headerlink" title="贝叶斯目标编码"></a>贝叶斯目标编码</h2><p>贝叶斯目标编码的可解释性更差, 因为它是一种更偏向数学的方法. 它认为均值作为度量可能存在一定欺骗性, 因此应该加入其它的<strong>统计量</strong>进行运算, 例如<strong>方差和偏度</strong>. 然后通过贝叶斯模型将这些统计变量的分布属性纳入其中, 从而产生更贴近类别标签的分布编码.</p><h2 id="证据权重"><a href="#证据权重" class="headerlink" title="证据权重"></a>证据权重</h2><p>证据权重是对分类独立变量和因变量之间的关系的另一种更微妙的表现. Weight of Evidence(WoE) 是从信用评分领域演变来的, 曾被用于衡量违约用户和还款用户间的差异. 证据权重的数学定义是<strong>几率比的自然对数</strong>, 即不属于某个类的百分比和属于某个类的百分比之差的对数. 并且WoE是IV值的关键组件.</p><p>实现仍然是通过<code>category_encoders</code>:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> category_encoders <span class="token keyword">import</span> WOEEncoderencoder <span class="token operator">=</span> WOEEncoder<span class="token punctuation">(</span>cols<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'Name_of_col'</span><span class="token punctuation">,</span><span class="token string">'Another_name'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>train_data <span class="token operator">=</span> encoder<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 特征工程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KMP算法</title>
      <link href="/posts/35921.html"/>
      <url>/posts/35921.html</url>
      
        <content type="html"><![CDATA[<h1 id="KMP算法"><a href="#KMP算法" class="headerlink" title="KMP算法"></a>KMP算法</h1><h2 id="串"><a href="#串" class="headerlink" title="串"></a>串</h2><p>字符串是一种<strong>特殊的线性表</strong>, 其逻辑结构与线性表相同, 只是在数据类型上进行了约束, 要求元素全是字符类型. 串可以顺序存储, 链式存储, 或者堆存储. 堆结合了顺序和链式的优点, 实际在构造串也是采用的堆结构来存储, 能够方便动态扩展.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/DS/串1.jpg" style="zoom:50%;" /><p>方便理解可以使用顺序存储.</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">// 串的定义</span><span class="token comment" spellcheck="true">//typedef struct{</span><span class="token comment" spellcheck="true">//    char str[maxSize+1];</span><span class="token comment" spellcheck="true">//    int length; </span><span class="token comment" spellcheck="true">//}Str;</span><span class="token comment" spellcheck="true">// 或者</span><span class="token keyword">typedef</span> <span class="token keyword">struct</span><span class="token punctuation">{</span>    <span class="token keyword">char</span> <span class="token operator">*</span>ch<span class="token punctuation">;</span>    <span class="token keyword">int</span> length<span class="token punctuation">;</span><span class="token punctuation">}</span>Str<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>字符串以<code>&#39;\0&#39;</code>作为<strong>结束标记</strong>. 串的基本操作有赋值, 取串长, 串比较, 求子串, 串清空, 串连接等. 在实现起来没有多大难度, 就稍微注意一下结束标记的处理即可.</p><h2 id="字符串匹配"><a href="#字符串匹配" class="headerlink" title="字符串匹配"></a>字符串匹配</h2><p>对一个串中的某子串定位操作称为串的模式匹配, 而其中与主串进行对比的子串称为模式串. 在字符串中常用到字符串匹配. </p><h3 id="简单模式匹配算法"><a href="#简单模式匹配算法" class="headerlink" title="简单模式匹配算法"></a>简单模式匹配算法</h3><p>简单而朴素的匹配算法, 就是将主串与模式串的字符挨个进行比对, 如果相同则逐一比对主串和模式串的下一个元素, 如果不同, 则从主串的下一个元素重复逐个比对的过程. 全部相同则匹配成功, 否则匹配失败.</p><p>代码实现如下:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">// 简单模式匹配算法</span><span class="token comment" spellcheck="true">// 假设字符串储存在1 ~ length上 </span><span class="token keyword">int</span> <span class="token function">index</span><span class="token punctuation">(</span>Str str<span class="token punctuation">,</span> Str substr<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> j <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> k <span class="token operator">=</span> i<span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 其中i和j分别用来表示主串和子串的位置, k用来暂存主串被比对的位置 </span>    <span class="token keyword">while</span><span class="token punctuation">(</span>i <span class="token operator">&lt;=</span> str<span class="token punctuation">.</span>length <span class="token operator">&amp;&amp;</span> j <span class="token operator">&lt;=</span> substr<span class="token punctuation">.</span>length<span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>str<span class="token punctuation">.</span>ch<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> substr<span class="token punctuation">.</span>ch<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token operator">++</span>i<span class="token punctuation">;</span>            <span class="token operator">++</span>j<span class="token punctuation">;</span>        <span class="token punctuation">}</span>         <span class="token keyword">else</span><span class="token punctuation">{</span>            j <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>            i <span class="token operator">=</span> <span class="token operator">++</span>k<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 匹配失败 i从主串下一个位置开始 </span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// 到这里有两种可能性 一个是原串被遍历完了 还有就是子串被遍历完了 </span>    <span class="token comment" spellcheck="true">// 如果是子串遍历完了说明匹配成功 </span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>j <span class="token operator">></span> substr<span class="token punctuation">.</span>length<span class="token punctuation">)</span>        <span class="token keyword">return</span> k<span class="token punctuation">;</span>    <span class="token keyword">else</span>        <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 因为假设字符串从下标1开始, 0是没有字符的</span><span class="token punctuation">}</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个简单的算法就是单纯的暴力匹配, 没有任何的预处理, 如果字符串长为n, 模式串长为m, 那么<strong>最坏时间复杂度为O((n-m+1)*m)</strong>. 即每次主串与模式串匹配时总能一直搜索到模式串的最后一个字符, 并最后匹配没有成功.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/DS/串2.png" style="zoom: 33%;" /><h3 id="KMP算法-1"><a href="#KMP算法-1" class="headerlink" title="KMP算法"></a>KMP算法</h3><p>KMP算法是一种经典的字符串匹配算法, 相较于前面所说的简单字符串匹配算法, 在比较速度上有了相当大的提升. </p><p>来观察一个问题, 在下述串匹配过程中, 当在箭头所指的位置发生了字符不匹配:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/DS/kmp1.jpg" style="zoom: 50%;" /><p>如果是简单字符串匹配算法, 那么很简单, 说明以主串第一个元素<code>A</code> 为起始的字符串无法与模式串相匹配, 将回溯到主串第二个元素<code>B</code> 与模式串第一个元素<code>A</code> 进行比较, 然后继续逐一比较下去… </p><p>继续观察如何才能使得模式串指针直接移动到再次能够与箭头所指的主串字符<code>B</code> 与模式串相比较的位置:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/DS/kmp2.jpg" style="zoom: 50%;" /><p>由于主串第二个字符是<code>B</code>, 模式串第一个字符是<code>A</code>, 发生不匹配, 主串指针下移, 主串的第三四五个字符和模式串的第一二三个字符匹配, 又回到了对模式串某字符主串字符<code>B</code> 的比对. </p><p>除去简单字符串匹配, 有没有更取巧的办法, 利用<strong>模式串自身的特点</strong>, 或者说利用在主串和模式串发生不匹配时已经匹配字符的信息, 来压缩这个比较的过程呢? </p><p>继续逐一完成主串和模式串的比对, 又发现有一处不匹配.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/DS/kmp3.jpg" style="zoom: 50%;" /><p>如果是简单字符串匹配又要逐一后移, 直到达到下述状态, 才能再次进行主串中所指的<code>B</code> 和模式串比对.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/DS/kmp4.jpg" style="zoom: 50%;" /><p>经过这两个例子, 隐隐约约发现点问题. 总会出现一种状态, <strong>模式串中某个字符与主串字符发生不匹配, 但在这之前的所有字符都已经匹配了</strong>. 如果使用简单字符串匹配, 效率极其低下. 如果模式串的前半部分(从起始处向后取)和后半部分(从不匹配点向前取)有<strong>完全相同的子串</strong>, 那么很明显如果前半部分与主串完全匹配, 那么后半部分也一定与主串完全匹配, 那就不用重复进行比对了!</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/DS/kmp8.jpg" style="zoom: 50%;" /><p>为方便比较, 我将两次比对放在一起, 比如:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/DS/kmp5.jpg" style="zoom: 50%;" /><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/DS/kmp2.jpg" style="zoom: 50%;" /><p>这里从起始处向后取, 即黄色框内的<code>ABA</code> 和从不匹配点向前取的白色框内的<code>ABA</code> 就是完全相同的子串, 这对子串称为<strong>公共前后缀</strong>. 正是因为这对前后缀完全相同, 所以发生字符不匹配时, 才能直接使得模式串的指针停留在前缀的下一个位置上, 不再重复进行前缀的比对, 这也是简单字符串匹配和KMP算法的最大不同, 即<strong>模式串的比较指针不回溯</strong>. </p><blockquote><p>指针不回溯意味着对大规模的外存中的字符串匹配操作可以分段进行. 先读入内存一部分进行匹配, 完成后再写回外存, 确保在匹配时不需要将之前写回外存的部分再次读入, 减少了IO操作, 从而提高了效率.</p></blockquote><p>在取前后缀时, 可能会有多对, 那应该取哪一对呢? 以上述发生不匹配的图为例, 从左右分别取, 应该可以形成<code>A-A</code>, <code>ABA-ABA</code>, <code>ABABA-ABABA</code> 三对符合要求的公共前后缀. 其中<code>AB-BA</code> 和 <code>ABAB-BABA</code>不是完全相同的子串, 是倒过来的, 不是公共前后缀. 如果要尽可能的减少重复比对次数, 一定是公共前后缀越长越好, 越长说明已经比对过的字符越多. 同时需要注意, <code>ABABA-ABABA</code> 这对是长度完全和子串长度相等, 再次比对时就失去了意义, 虽然是公共前后缀, 但它应该<strong>不能被使用</strong>. <code>ABA-ABA</code> 就是<strong>最长公共前后缀</strong>, 取最长的一对公共前后缀作为指针不回溯的依据.</p><p>记住, 模式串的比较指针直接就指向了前缀的下一个位置. 再看一个例子, 加深理解:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/DS/kmp6.jpg" style="zoom:50%;" /><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/DS/kmp4.jpg" style="zoom: 50%;" /><p>在上述叙述的过程中, 发生主串和模式串的不匹配时, 模式串左侧与主串的对应位置一定是匹配的, 换句话说二者是一样的. 那么研究模式串就和研究主串是等价的了, 因此<strong>与主串无关</strong>, 仅保留模式串, 将指针不回溯的位置记录用数组下来, 当发生不匹配时, 指针就恢复到之前记录的位置即可. 这个数组称为<strong>next数组</strong>.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/DS/kmp7.jpg" style="zoom:50%;" /><p>为方便, 字符串下标从1开始. 对于模式串:</p><table><thead><tr><th align="center">发生不匹配的模式串下标</th><th align="center">最长公共前后缀长度</th><th align="center">主串当前与模式串比对的下标</th><th align="center">最长公共前后缀</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">0</td><td align="center">0(特殊)</td><td align="center">-</td></tr><tr><td align="center">2</td><td align="center">0</td><td align="center">1</td><td align="center">-</td></tr><tr><td align="center">3</td><td align="center">0</td><td align="center">1</td><td align="center">-</td></tr><tr><td align="center">4</td><td align="center">1</td><td align="center">2</td><td align="center"><code>A</code></td></tr><tr><td align="center">5</td><td align="center">2</td><td align="center">3</td><td align="center"><code>AB</code></td></tr><tr><td align="center">6</td><td align="center">3</td><td align="center">4</td><td align="center"><code>ABA</code></td></tr><tr><td align="center">7</td><td align="center">1</td><td align="center">2</td><td align="center"><code>A</code></td></tr><tr><td align="center">8</td><td align="center">1</td><td align="center">2</td><td align="center"><code>A</code></td></tr><tr><td align="center">9</td><td align="center">2</td><td align="center">3</td><td align="center"><code>AB</code></td></tr><tr><td align="center">10</td><td align="center">3</td><td align="center">4</td><td align="center"><code>ABA</code></td></tr><tr><td align="center">11</td><td align="center">4</td><td align="center">5</td><td align="center"><code>ABAB</code></td></tr><tr><td align="center">12</td><td align="center">5</td><td align="center">6</td><td align="center"><code>ABABA</code></td></tr></tbody></table><p>总结上述规律, 除去模式串第一个元素外, 主串当前元素与模式串元素比对的下标是最长前后缀长度+1. 如果下标为1发生不匹配, 主串的下一个元素与模式串下标元素为1开始比较. 最坏时间复杂度是O(n+m).</p><h4 id="简单匹配升级到KMP"><a href="#简单匹配升级到KMP" class="headerlink" title="简单匹配升级到KMP"></a>简单匹配升级到KMP</h4><p>先抛开next数组如何构造不谈, 先看看如何写KMP的代码. 如果KMP完成了简单字符串匹配算法的压缩, 那也应该能够由简单算法升级为KMP算法.</p><p>实现KMP代码如下:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token keyword">int</span> <span class="token function">KMP</span><span class="token punctuation">(</span>Str str<span class="token punctuation">,</span> Str substr<span class="token punctuation">,</span> <span class="token keyword">int</span> next<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> j <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 下标从1开始</span>    <span class="token keyword">while</span><span class="token punctuation">(</span>i <span class="token operator">&lt;</span> str<span class="token punctuation">.</span>length <span class="token operator">&amp;&amp;</span> j <span class="token operator">&lt;</span> substr<span class="token punctuation">.</span>length<span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">// 主串指针下移的情况</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>j <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">||</span> str<span class="token punctuation">.</span>ch<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> substr<span class="token punctuation">.</span>ch<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token operator">++</span>i<span class="token punctuation">;</span>            <span class="token operator">++</span>j<span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token comment" spellcheck="true">// 发生不匹配时模式串指针跳到next数组所指向的位置</span>        <span class="token keyword">else</span> j <span class="token operator">=</span> next<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// 遍历完模式串没发现不匹配 说明模式串与主串相匹配</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>j <span class="token operator">></span> substr<span class="token punctuation">.</span>length<span class="token punctuation">)</span> <span class="token keyword">return</span> i <span class="token operator">-</span> substr<span class="token punctuation">.</span>length<span class="token punctuation">;</span>    <span class="token keyword">else</span> <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="求next数组"><a href="#求next数组" class="headerlink" title="求next数组"></a>求next数组</h4><p>再来看看next数组是如何构建的. 让我们回到之前找最长公共前后缀的过程. 那时曾经说过, 公共前后缀是两段<strong>完全相同的子串</strong>, 那找最长公共前后缀的过程岂不是也是字符串匹配? 我们仍然要延续不重复做事情的思路, 利用已知信息去求next数组.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/DS/kmp9.jpg" style="zoom: 50%;" /><p>如果这样去想, 那么图中Pj所对应的next数组的值与Pt必然是有关联的.</p><p>当Pj和Pt的大小未知, 而前面模式串均相同时, 假设在Pj出发生不匹配, 模式串指针会跳转到最长公共前后缀+1处, 即next[j] = t. 有了这个初始条件, 可以根据Pj和Pt的大小关系推出next[j+1]. </p><p>假设P(j+1)处发生不匹配:</p><ul><li><p>如果Pj = Pt, 那么next[j+1] = next[j]+1 = t+1. </p></li><li><p>如果Pj != Pt, 就得在这两个串(本质是模式串自己) 中找到最长的公共前后缀, 也就<strong>回到了字符串匹配的问题中</strong>. 将P(j-t+1) ~ Pj 视为主串, P1 ~ Pt视为模式串, 继续做字符串匹配. 必须向前反复重定位指针, 找到一个位置使得Pj = Pt或满足t = 0, 即将t循环赋值为next[t], t = 0 时, 令next[j+1] = 1. </p><p>注意, 因为第二种情况与字符串匹配完全一致, 所以建立next数组的代码一定与KMP算法<strong>极其相似</strong>:</p></li></ul><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">// 求next数组的方法 substr为模式串 </span><span class="token comment" spellcheck="true">// j和t与上述图中相同</span><span class="token keyword">void</span> <span class="token function">getNext</span><span class="token punctuation">(</span>Str substr<span class="token punctuation">,</span> <span class="token keyword">int</span> next<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> t <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>    next<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>     <span class="token keyword">while</span><span class="token punctuation">(</span>j <span class="token operator">&lt;</span> substr<span class="token punctuation">.</span>length<span class="token punctuation">)</span><span class="token punctuation">{</span> <span class="token comment" spellcheck="true">// j&lt;= substr.length 会使next数组下标越界 </span>        <span class="token comment" spellcheck="true">// 模式串自身匹配</span>        <span class="token comment" spellcheck="true">// t可能被下面的else赋值为0, 在条件并入后会将t置为1</span>        <span class="token comment" spellcheck="true">// 并且第一次执行, 有next[2] = 1, 满足之前推导的结果</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>t <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">||</span> substr<span class="token punctuation">.</span>ch<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">==</span> substr<span class="token punctuation">.</span>ch<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token operator">++</span>j<span class="token punctuation">;</span>            <span class="token operator">++</span>t<span class="token punctuation">;</span>            next<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> t<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// next[j] = length+1 length实际上就是没++前的t</span>        <span class="token punctuation">}</span>        <span class="token keyword">else</span> t <span class="token operator">=</span> next<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 模式串指针重定位到next[t] </span>    <span class="token punctuation">}</span><span class="token punctuation">}</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="KMP算法改进"><a href="#KMP算法改进" class="headerlink" title="KMP算法改进"></a>KMP算法改进</h4><p>在求next数组时, 会一直用到向”前反复重定位指针”这个操作, 还可以有优化的余地.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/DS/kmp10.jpg" style="zoom:50%;" /><p>在这个例子中出现了<strong>连续且完全相同的字符</strong>, 在j = 5时发生不匹配, next[j] = 4, 将j重定位到next[j] 上. j又为4, next[j] 又为3… 反复重定位, 直到j为0, 才发现该位置的主串和模式串完全不匹配, 主串和模式串指针都应该后移一位. 在这个过程, 从1到4位置上的字符串是相等的, 应该直接给next[5] 赋值为0.</p><p>尝试在next数组的基础上, 构建一个重定向数组, 使得其能够根据之前比较的内容跳过多余的比较, 直接将next[j] 赋值为某个已知的next数组值, 这个数组叫<strong>nextval数组</strong>.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/DS/kmp11.jpg" style="zoom:50%;" /><p>在上面的图中, j位置的元素反复与Pd, Pc, Pb, Pa都进行了比较, 但明显前三者都是冗余比较, 不能给解决不匹配问题带来好处. 此处的nextval[j] 应该为a.</p><p>推广到一般情况, 路径上的元素都不是相邻的, 而现在j之前的nextval数组值都是已知的, 如何求j后的元素k的nextval[k] 呢?</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/DS/kmp12.jpg" style="zoom:50%;" /><p>如果k位置上的元素和j位置上的元素相等, 那么nextval[k] = nextval[next[k]], 如果不相等则令nextval[k] = next[k].</p><p>归纳为一般步骤:</p><ol><li>j = 1时, nextval[j] 赋值为 0, 作为特殊标记</li><li>j &gt; 1时:<ul><li>若Pj != P(next[j]), 则nextval[j] = next[j].</li><li>若Pj = P(next[j]), 则nextval[j] = nextval[next[j]].</li></ul></li></ol><p>求nextval数组的代码可以由求next数组的代码修改而来, 最好对比结合起来看, 实现如下:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token keyword">void</span> <span class="token function">getNextval</span><span class="token punctuation">(</span>Str substr<span class="token punctuation">,</span> <span class="token keyword">int</span> nextval<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> t <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>    nextval<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// j=1时nextval[j] = 0</span>    <span class="token keyword">while</span><span class="token punctuation">(</span>j <span class="token operator">&lt;</span> substr<span class="token punctuation">.</span>length<span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>t <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">||</span> substr<span class="token punctuation">.</span>ch<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">==</span> substr<span class="token punctuation">.</span>ch<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token operator">++</span>j<span class="token punctuation">;</span>            <span class="token operator">++</span>t<span class="token punctuation">;</span>            <span class="token comment" spellcheck="true">// 求解next数组时, 有next[j] = t; 那么t可以代替next[j] </span>            <span class="token keyword">if</span><span class="token punctuation">(</span>substr<span class="token punctuation">.</span>ch<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">!=</span> substr<span class="token punctuation">.</span>ch<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">)</span>                nextval<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> t<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// nextval[j] = next[j]</span>            <span class="token keyword">else</span>                nextval<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> nextval<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// nextval[j] = nextval[next[j]]</span>        <span class="token punctuation">}</span>        <span class="token keyword">else</span> t <span class="token operator">=</span> nextval<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
            <tag> KMP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Seq2Seq和Attention</title>
      <link href="/posts/40071.html"/>
      <url>/posts/40071.html</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>2020.09.25</strong>: 本质部分的猜想被证实.</p><p><strong>2020.09.21</strong>: 更新Attention的本质.</p><p><strong>2020.09.19</strong>: 在接触了更多NLP内容后, 发现Attention是一个有特殊表征意义的结构, 以后会加入更深的理解.</p></blockquote><h1 id="Seq2Seq和Attention"><a href="#Seq2Seq和Attention" class="headerlink" title="Seq2Seq和Attention"></a>Seq2Seq和Attention</h1><p>Seq2Seq和Attention被广泛的应用于RNN中, 当然现在不单单只是在NLP中使用, CV领域也有很多应用. </p><h2 id="RNN回顾"><a href="#RNN回顾" class="headerlink" title="RNN回顾"></a>RNN回顾</h2><p>在之前的循环神经网络小结中对RNN进行了介绍. 在此简单回顾一下.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rnn3.jpg" style="zoom: 50%;" /><p>RNN是针对时序序列数据而诞生的神经网络, 其输入是时序数据, 每个时刻$t$ 都会有一个相应的输出和隐藏状态. 对于$t \in T$, 有输入$[x_{1}, x_{2}, \ldots, x_{t}, \ldots, x_{T}]$ 和输出$[y_{1}, y_{2}, \ldots, y_{t}, \ldots, y_{T}]$ . 在经典RNN结构中给出的输入和输出是相同大小的.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rnn4.jpg" style="zoom:50%;" /><p>当前时刻隐藏状态$h_t$ 随着下个时刻$t+1$ 的输出$x_{t+1}$一起被共同作为下个时刻的输入. </p><h2 id="Sequence-to-Sequence-Encoder-Decoder"><a href="#Sequence-to-Sequence-Encoder-Decoder" class="headerlink" title="Sequence to Sequence(Encoder-Decoder)"></a>Sequence to Sequence(Encoder-Decoder)</h2><p>Seq2Seq作为一种不限制输入和输出的序列长度的结构, 被广泛应用于机器翻译, 文本摘要, 阅读理解, 语音识别等任务中. 在Seq2Seq结构中, <strong>编码器Encoder</strong>把所有的输入序列都编码成一个统一的语义向量, 保存在hidden state中, 然后再由<strong>解码器Decoder</strong>解码. 这种结构其实在介绍RNN结构时提到过. Seq2Seq和Encoder-Decoder描述的是同一种结构.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/encoder_decoder.jpg" style="zoom: 33%;" /><p>这种结构使得输入和输出与经典的RNN结构不同, 输入和输出的数据维度可以不同. 在解码器Decoder解码的过程中, 反复将上一个时刻$t-1$ 的输出作为当前时刻$t$ 的输入, 循环解码, 直到输出停止字符才停止. </p><p>下面以机器翻译为例, 来看一种Seq2Seq结构的一种经典实现方式. 将中文的”早上好”通过seq2seq转换成英文的”Good morning”.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/seq2seq1.jpg" style="zoom:33%;" /><ol><li>将”早上好”通过Encoder编码, 从$t=1$ 时刻到$t=3$ 时刻通过RNN反复完成语义向量的编码, 将$t=3$ 时刻最终的隐藏状态$h_3$ 作为语义向量.</li><li>$h_3$ 作为Decoder的初始隐藏状态$h_0$, 并在$t=1$ 时刻输入标识开始解码的特殊标识符<code>&lt;start&gt;</code>, 开始解码. 不断的将上一时刻输出作为当前时刻输入进行解码, 最终输出停止字符<code>&lt;stop&gt;</code>时, 预测解码停止.</li></ol><p>$t$ 时刻时, Decoder中的数据流流向如下所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/seq2seq2.jpg" style="zoom:33%;" /><ol><li>RNN先被输入大小为$[n_i, 1]$ 的向量$x_t$, 即红点.</li><li>结合传过来的语义向量, 加上这个时刻的输入, 经过RNN后输出大小为$[n_o, 1]$ 的向量$y_t$, 即蓝点.</li><li>在获取了输出向量后, 获取输出向量所对应的字符, 这就需要结合一层全连接层和激活函数(主要是Softmax), 将输出向量变为$y_t’$, 大小为$[n_c, 1]$, 即图中的黄点. $n_c$代表字典中的总字符数. 从$y_t’$ 中找到概率最大的字符index, 即图中橘红色点.</li><li>将分类后获取的字符index做一次Embedding(之前的NLP相关那篇文章中有提到过Word2vec), 输入给下个时刻, 如此循环.</li></ol><p>当然也有人将语义向量作为Decoder的输入, 而非隐藏状态提供给Decoder. Seq2Seq只是代表一种结构, 而非某种具体的实现方法.</p><p>但是Seq2Seq有许多的缺点:</p><ul><li>最大的局限性: <strong>编码和解码之间的唯一联系是固定长度的语义向量</strong>.</li><li>编码要把整个序列的信息压缩进一个<strong>固定长度</strong>的语义向量.</li><li>语义向量<strong>无法完全</strong>表达整个序列的信息.</li><li>先输入的内容携带的信息, 会被后输入的信息<strong>稀释</strong>掉, 或者被<strong>覆盖</strong>掉. 信息的量越大, 损失就越大.</li><li>输入序列<strong>越长</strong>, 这样的现象<strong>越严重</strong>, 这样使得在Decoder解码时一开始就没有获得足够的输入序列信息, 解码效果会打折扣. </li></ul><h2 id="Attention机制"><a href="#Attention机制" class="headerlink" title="Attention机制"></a>Attention机制</h2><blockquote><p>这里说的Attention都是<strong>软注意力</strong>机制.</p></blockquote><p>正是为了弥补基础的Encoder-Decoder的局限性, 提出了Attention. 因为语义向量表达信息的缺失性, 遗忘性, 以及向量长度的不可变性, Attention想要利用Encoder的隐藏状态$h_t$ 来解决语义向量存在的弊病. 当然, Attention不单单广泛的应用在NLP领域, 在CV领域也常有应用.</p><h3 id="Attention结构"><a href="#Attention结构" class="headerlink" title="Attention结构"></a>Attention结构</h3><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/seq2seq3.jpg" style="zoom:33%;" /><p>Attention的实质就是在Decoder的输入端, 将Encoder的隐藏状态<strong>加权信息</strong>, 也作为输入的一部分, 提供给Decoder. 换句话说, Encoder不再传递最后一个时间步的隐藏状态, 而是将所有时间步的隐藏状态加权提供给Decoder.</p><p>假设Encoder经过了3个时间步, 对应的隐藏状态分别是$h_1, h_2, h_3$, 分别的输入是”早”, “上”, “好”, 那么Decoder在$t=1$ 时刻通过三个不同的权重$w_{11}, w_{12}, w_{13}$ 能加权计算出一个向量$c_1$.<br>$$<br>c_{1}=h_{1} \cdot w_{11}+h_{2} \cdot w_{12}+h_{3} \cdot w_{13}<br>$$<br>将$c_1$ 和上一个状态拼接在一起形成一个新的向量, 一起输入到Decoder中, 计算结果:<br>$$<br>\bar{h}_{0} \leftarrow \operatorname{concat}\left(\bar{h}_{0}, c_{1}\right)=\operatorname{concat}\left(h_{3}, c_{1}\right)<br>$$<br>这样, 在Decoder的$t=1$ 时刻, 就应该根据之前Encoder的隐藏状态加权得到一个新的向量$c_2$, 和Decoder的上一个隐藏状态一起输入得到计算结果:<br>$$<br>\begin{array}{c}<br>c_{2}=h_{1} \cdot w_{21}+h_{2} \cdot w_{22}+h_{3} \cdot w_{23} \\<br>\bar{h}_{1} \leftarrow \operatorname{concat}\left(\bar{h}_{1}, c_{2}\right)<br>\end{array}<br>$$</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/attention1.jpg" style="zoom: 50%;" /><p>在Decoder的$t=2$ 和$t=3$ 时刻同理, 由于权重不同, 所以Decoder在解码时对隐藏状态关注的部分就不同, 权重越大注意力越强. 比如, 在翻译”好”时, 关注点应该在$h_3$ 上, 所以对应的$w_{13}$ 就应该比$w_{11}$ 和$w_{12}$ 大得多.</p><p>如果把隐藏状态和权重视为一层神经层, 那么就可以看做Encoder和Decoder之间引入了一层<strong>跨越时间步的神经层</strong>.</p><p>Attention又有 <a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau Attention</a> 和<a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">LuongAttention </a>等多种实现. 这里说一下LuongAttention.</p><h3 id="LuongAttention"><a href="#LuongAttention" class="headerlink" title="LuongAttention"></a>LuongAttention</h3><p>重新定义符号, 用$\bar{h}_{s}$ 代表Encoder的隐藏状态, $h_t$ 代表Decoder的隐藏状态, $\tilde{h}_{t}$代表Attention Layer输出的最终Decoder状态.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/luongattention.jpg" style="zoom: 50%;" /><p>首先要计算权重. 如果Decoder在$t$ 时刻的隐藏状态为$h_t$,  Encoder每一个隐藏状态$\bar{h}_{s}$ 的权重为$a_{t}$, 则权重是由规则$\mathrm {score}$ 经过$\mathrm {Softmax}$ 函数后得出的:<br>$$<br>a_{t}(s)=\frac{\exp \left(\operatorname{score}\left(h_{t}, \bar{h}_{s}\right)\right)}{\sum_{s^{\prime}} \exp \left(\operatorname{score}\left(h_{t}, \bar{h}_{s^{\prime}}\right)\right)}<br>$$<br>在luong Attention中, $\mathrm {score}$ 可以通过多种方式来计算:<br>$$<br>\operatorname{score}\left(h_{t}, \bar{h}_{s}\right)=\left\{\begin{array}{ll}<br>h_{t}^{T} \bar{h}_{s} &amp; \text { Dot } \\<br>h_{t}^{T} W_{a} \bar{h}_{s} &amp; \text { General } \\<br>v_{a}^{T} \tanh \left(W_{a} \cdot \operatorname{concat}\left(h_{t}, \bar{h}_{s}\right)\right) &amp; \text { Concat }<br>\end{array}\right.<br>$$<br>$\mathrm {Dot}$ 指的是向量内积, $\mathrm {General}$ 是再通过乘以权重矩阵$W_a$ 进行计算. 一般情况下来说$\mathrm {General}$ 要好于$\mathrm {Dot}$.</p><p>然后将计算得来的权重与Encoder的隐藏状态进行加权求和, 生成新的向量$c_t$.<br>$$<br>c_{t}=\sum_{s} a_{t}(s) \cdot \bar{h}_{s}<br>$$<br>接着将加权后的向量$c_t$ 与原始Decoder的隐藏状态$h_t$ 拼接在一起.<br>$$<br>\tilde{h}_{t}=\tanh \left(W_{c} \cdot \operatorname{concat}\left(c_{t}, h_{t}\right)\right)=\tanh \left(W_{c} \cdot\left[c_{t} ; h_{t}\right]\right)<br>$$<br>因为是拼接, 所以向量$c_t$ 和$h_t$ 拼接后的大小一定会发生变化. 如果想恢复成原来的形状则需要再乘一个恢复矩阵$W_c$ (也是用来重置大小的全连接层), 当然也可以不恢复, 只是会导致Decoder 的每个Cell大小逐渐变大.</p><p>最后, 对引入注意力的Decoder的 $\tilde{h}_{t}$ 经过一次线性运算后得到输出.<br>$$<br>y_{t}=W_{h o} \tilde{h}_{t}+b_{h o}<br>$$</p><p>也可以根据需要将新生成的状态$\tilde{h}_{t}$ 送入RNN继续学习. 上述过程提到的矩阵$W_a$, $W_c$, $W_{ho}$ 均通过学习得来.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/luongattention2.jpg" style="zoom: 33%;" /><h3 id="Attention的真正本质"><a href="#Attention的真正本质" class="headerlink" title="Attention的真正本质"></a>Attention的真正本质</h3><blockquote><p>该部分为<strong>Transformer</strong>的<strong>前置知识</strong>.</p><p>图和部分内容出自<a href="https://zhuanlan.zhihu.com/p/35571412" target="_blank" rel="noopener">浅谈Attention机制的理解</a>.</p></blockquote><p>虽然在Seq2Seq中, Attention可以看做是跨越时间步的神经层, 但只有抛弃掉开始接触的RNN和Seq2Seq, 才能看到Attention本身, 目前我们对Attention的理解是基于RNN和Seq2Seq的, 对这两种结构做了捆绑后并不能很好的看清它. 在抛弃了Seq2Seq后, 我们仍然说Attention的本质是对<strong>某些数据分配某些权重参数</strong>, 然后再对它们进行<strong>合并</strong>.</p><p>比较广泛的一个说法是, Attention的本质是一个<strong>查询</strong>(Query)到一系列<strong>键值对</strong>(&lt;Key - Value&gt;)的<strong>映射</strong>. 有些人将它比作<strong>软寻址</strong>的过程, 也是一样的道理, 当有Query = key的查询时, 根据内容的重要性, 从每个Value中<strong>都</strong>取出所需要的内容, 再通过某种方式合成起来形成输出.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/attention4.jpg" style="zoom: 67%;" /><p>假设Key和Value是不同语言中一一对应的单词, 如果根据Key能够产生一个与Value有关的概率分布, 不难发现, 这个过程和传统机器翻译中做的<strong>短语对齐</strong>起到的功能是类似的. </p><p>如果在目标中进行某数据的查找, 对于一个Query, 通过计算Query与所有Key之间的<strong>相似度或相关性</strong>, 再通过归一化得到与Value对应的概率分布(权重), 将其与对应的Value<strong>加权求和</strong>就得到了输出.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/attention5.jpg" style="zoom:67%;" /><p>如果<code>Key = Value</code> 则被称为普通模式, <code>Key != Value</code> 被称为键值对模式. 目前在大多数NLP研究中, <strong>Key和Value是相同的</strong>. </p><blockquote><p>这个结论我看很多人提到过, 我表示不理解, 想了很久目前也只有一个猜测, 我个人认为与<strong>应用领域</strong>有关. </p><p>重点在于, 之所以有Key-Value的映射结构, 很有可能在某些领域中Key的信息和对应的Value信息不是完全相同的, 这就意味着Key只用来生成权重系数, 而Value作为与Key不相同的语义信息可能有其他含义.</p></blockquote><p>可能上面写的比较散, 做个小总结, 工作机制有三步:</p><ol><li>计算Query与Key的相似度, 得到权值, 常用的相似度函数有点积，拼接，感知机等, 当然也可以不是相似度, 是其他的打分函数.</li><li>对权值进行各种Softmax归一化.</li><li>用归一化得来的权值与Value加权求和.</li></ol><p>因此, 有Attention:<br>$$<br>\begin{aligned}<br>&amp; \alpha _i = \operatorname {softmax}(\operatorname {similarity}(QK^T)) \\<br>&amp; attention((K, V), Q) = \sum_{i=1}^N \alpha_i v_i \quad(v_i \in V)<br>\end{aligned}<br>$$</p><blockquote><p>注: 该公式在Transformer中还会再次出现.</p></blockquote><p>如果把&lt;Key - Value&gt;这个映射称为<strong>字典</strong>, 那么Attention能够根据之前形成的字典轻松捕捉<strong>局部和长期依赖</strong>. 另外, Attention也可以看做是一种<strong>基于全连接的图模型</strong>, 只是连接权重是<strong>动态生成</strong>的.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/attention6.jpg" style="zoom:50%;" /><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>下面来总结一下, 图片出自<a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" target="_blank" rel="noopener">Visualizing A Neural Machine Translation Model</a>. </p><p>在Encoder如何获取经过Attention加权后的向量:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/attention2.jpg" style="zoom:50%;" /><p>从Decoder得到输出的计算过程:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/attention3.jpg" style="zoom:50%;" /><h3 id="优缺点分析"><a href="#优缺点分析" class="headerlink" title="优缺点分析"></a>优缺点分析</h3><p>Attention<strong>优点</strong>:</p><ul><li>在机器翻译时, 让生词不只是关注全局的语义向量, 增加了”<strong>注意力范围</strong>“. 表示接下来输出的词要重点关注输入序列种的哪些部分. 根据关注的区域来产生下一个输出. </li><li>不要求Encoder将所有信息全输入在一个固定长度的向量中. </li><li>将输入编码成一个向量的序列, 解码时, 每一步选择性的从序列中挑一个<strong>子集</strong>进行处理. </li><li>在每一个输出时, 能够充分利用输入携带的信息, 每个语义向量不一样, 注意力焦点不一样. </li></ul><p>Attention<strong>缺点</strong>:</p><ul><li>需要为每个输入输出组合分别计算Attention, 50个单词的输出输出序列需要计算2500个attention.</li><li>attention在决定专注于某个方面之前, 需要遍历一遍记忆再决定下一个输出是以什么.</li><li>纯粹的Attention机制并不能提供时序数据之间的<strong>位置信息</strong>, Transformer就是一个很好的例子.</li></ul><h3 id="可视化分析"><a href="#可视化分析" class="headerlink" title="可视化分析"></a>可视化分析</h3><p>如果将上述输入$x_t$ 对输出$y_t$ 的权重$a_{t}(s)$ 做一张热力图, 就能看出当预测某个单词时, 对句子其他部分的侧重程度, 也就是注意力:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/attention可视化3.png" style="zoom:50%;" /><p>能很明显的看出, 当翻译某个词时, 不考虑语态的情况下, 翻译和它邻近的几个词有强大的联系, 这也就是Attention的最直观体现.</p><p>下图揭露了句子中前后单词之间的联系, 颜色深浅表示联系的强弱, 并且对于不同的任务, Attention能够学习到不同的注意力结构. 图片出自<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Attention Is All You Need</a>(也是Transformer的论文).</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/attention可视化1.png" style="zoom: 67%;" /><p>在CV中, Attention同样有它的作用, 图片出自<a href="https://arxiv.org/pdf/1502.03044v1.pdf" target="_blank" rel="noopener">Show, Attend and Tell</a>.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/attention可视化2.png" style="zoom:67%;" /><h2 id="向RNN加入额外信息"><a href="#向RNN加入额外信息" class="headerlink" title="向RNN加入额外信息"></a>向RNN加入额外信息</h2><p>Attention机制其实就是将的Encoder的隐藏层状态加权后获得权重向量$c_t$, 额外加入到Decoder中, 从而使得网络有更完整的信息流.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/encoder_decoder2.jpg" style="zoom:67%;" /><p>如果有额外信息$z_t$ 想要添加到Decoder中, 那么主要有以下三种方式:</p><ul><li><p><strong>Add</strong>: 直接将额外信息$z_t$ 叠加在输出$y_t$ 上.<br>$$<br>y_{t} \leftarrow y_{t}+z_{t}<br>$$</p></li><li><p><strong>Concat</strong>: 将额外信息$z_t$ 拼接在隐藏层的隐藏装填$h_t$, 然后通过全连接恢复维度, luong attention中就是用的这种方法.<br>$$<br>h_{t} \leftarrow W_{c} \cdot \operatorname{concat}\left(h_{t}, z_{t}\right)<br>$$</p></li><li><p><strong>Mlp</strong>: 直接添加一个对额外信息$z$ 的神经层.<br>$$<br>\begin{array}{l}<br>h_{t}^{z h}=W_{z h} \cdot z_{t}+b_{z h} \\<br>h_{t} \leftarrow \tanh \left(h_{t}^{i h}+h_{t}^{h h}+h_{t}^{z t}\right) \\<br>\quad=\tanh \left(\left(W_{i h} \cdot x_{t}+b_{i h}\right)+\left(W_{h h} \cdot h_{t-1}+b_{h h}\right)+\left(W_{z h} \cdot z_{t}+b_{z h}\right)\right)<br>\end{array}<br>$$</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Attention </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++之模板</title>
      <link href="/posts/46956.html"/>
      <url>/posts/46956.html</url>
      
        <content type="html"><![CDATA[<h1 id="C-之模板"><a href="#C-之模板" class="headerlink" title="C++之模板"></a>C++之模板</h1><h2 id="函数模板"><a href="#函数模板" class="headerlink" title="函数模板"></a>函数模板</h2><p>在C++中, 模板被用于设计可重用的软件, 模板提供了将<strong>通用数据类型作为参数</strong>的能力.<br>比如有时, 在求一个最大值时, 不得不因为不同的数据类型而写许多除了数据类型外完全一致的代码: </p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token keyword">int</span> <span class="token function">maxValue</span><span class="token punctuation">(</span><span class="token keyword">int</span> val1<span class="token punctuation">,</span> <span class="token keyword">int</span> val2<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">return</span> val1<span class="token operator">></span>val2<span class="token operator">?</span>val1<span class="token operator">:</span>val2<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">double</span> <span class="token function">maxValue</span><span class="token punctuation">(</span><span class="token keyword">double</span> val1<span class="token punctuation">,</span> <span class="token keyword">double</span> val2<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">return</span> val1<span class="token operator">></span>val2<span class="token operator">?</span>val1<span class="token operator">:</span>val2<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">char</span> <span class="token function">maxValue</span><span class="token punctuation">(</span><span class="token keyword">char</span> val1<span class="token punctuation">,</span> <span class="token keyword">char</span> val2<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">return</span> val1<span class="token operator">></span>val2<span class="token operator">?</span>val1<span class="token operator">:</span>val2<span class="token punctuation">;</span><span class="token punctuation">}</span>string <span class="token function">maxValue</span><span class="token punctuation">(</span>string val1<span class="token punctuation">,</span> string val2<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">return</span> val1<span class="token operator">></span>val2<span class="token operator">?</span>val1<span class="token operator">:</span>val2<span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>当然在这个具体的例子中, 字符串的比较时二者地址的比较, 而非数组内容的比较.<br>它们几乎完全一样. 模板能使这个函数应用于所有的数据类型. </p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> T<span class="token operator">></span>T <span class="token function">maxValue</span><span class="token punctuation">(</span>T val1<span class="token punctuation">,</span> T val2<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">return</span> val1<span class="token operator">></span>val2<span class="token operator">?</span>val1<span class="token operator">:</span>val2<span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>模板的定义必须用关键字<code>template</code> 开始, 后面跟一个参数列表, 每个参数前面必须跟关键字<code>typename</code>或者<code>class</code>. 我个人更喜欢使用<code>typename</code>, 因为<code>class</code>具有一定的<strong>歧义性</strong>. 如果要有多个参数, 所有参数都可以放在尖括号内, 例如<code>template T&lt;typename p1, typename p2, typename p3&gt;</code>. </p><h2 id="模板类"><a href="#模板类" class="headerlink" title="模板类"></a>模板类</h2><p>模板类能实现具有通用类型的类. 例如实现一个通用类型的栈. 在<strong>类声明</strong>前也要加上模板前缀, 因为模板类的函数也是模板函数, 所以在<strong>函数头</strong>前也需要加前缀. 下面是一个简单的栈的实现: </p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token macro property"># <span class="token directive keyword">include</span><span class="token string">&lt;iostream></span></span><span class="token keyword">template</span> <span class="token operator">&lt;</span><span class="token keyword">typename</span> T<span class="token operator">></span><span class="token keyword">class</span> <span class="token class-name">Stack</span><span class="token punctuation">{</span><span class="token keyword">public</span><span class="token operator">:</span>    <span class="token function">Stack</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">bool</span> <span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">const</span><span class="token punctuation">;</span>    T <span class="token function">peek</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">const</span><span class="token punctuation">;</span>    <span class="token keyword">void</span> <span class="token function">push</span><span class="token punctuation">(</span>T val<span class="token punctuation">)</span><span class="token punctuation">;</span>    T <span class="token function">pop</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> <span class="token function">getSize</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">const</span><span class="token punctuation">;</span><span class="token keyword">private</span><span class="token operator">:</span>    T elements<span class="token punctuation">[</span><span class="token number">100</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> size<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 下面分离实现栈的函数</span><span class="token comment" spellcheck="true">// 注意要声明函数作用域, 并且要附带参数类型</span><span class="token comment" spellcheck="true">// 即Stack&lt;T>::</span><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> T<span class="token operator">></span>Stack<span class="token operator">&lt;</span>T<span class="token operator">></span><span class="token operator">::</span><span class="token function">Stack</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">this</span><span class="token operator">-</span><span class="token operator">></span>size <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> T<span class="token operator">></span><span class="token keyword">bool</span> Stack<span class="token operator">&lt;</span>T<span class="token operator">></span><span class="token operator">::</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">return</span> <span class="token keyword">this</span><span class="token operator">-</span><span class="token operator">></span>size <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> T<span class="token operator">></span>T Stack<span class="token operator">&lt;</span>T<span class="token operator">></span><span class="token operator">::</span><span class="token function">peek</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">const</span><span class="token punctuation">{</span>    <span class="token keyword">return</span> <span class="token keyword">this</span><span class="token operator">-</span><span class="token operator">></span>elements<span class="token punctuation">[</span>size <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> T<span class="token operator">></span><span class="token keyword">void</span> Stack<span class="token operator">&lt;</span>T<span class="token operator">></span><span class="token operator">::</span><span class="token function">push</span><span class="token punctuation">(</span>T val<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">this</span><span class="token operator">-</span><span class="token operator">></span>elements<span class="token punctuation">[</span>size<span class="token operator">++</span><span class="token punctuation">]</span> <span class="token operator">=</span> val<span class="token punctuation">;</span><span class="token punctuation">}</span> <span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> T<span class="token operator">></span>T Stack<span class="token operator">&lt;</span>T<span class="token operator">></span><span class="token operator">::</span><span class="token function">pop</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">return</span> <span class="token keyword">this</span><span class="token operator">-</span><span class="token operator">></span>elements<span class="token punctuation">[</span>—size<span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> T<span class="token operator">></span><span class="token keyword">int</span> Stack<span class="token operator">&lt;</span>T<span class="token operator">></span><span class="token operator">::</span><span class="token function">getSize</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">const</span><span class="token punctuation">{</span>    <span class="token keyword">return</span> <span class="token keyword">this</span><span class="token operator">-</span><span class="token operator">></span>size<span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>模板参数也可以像函数参数一样提供一个缺省数据类型<code>template &lt;typename T = int&gt;</code>. 也可以使用非类型参数, 例如: </p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token keyword">template</span> <span class="token operator">&lt;</span><span class="token keyword">typename</span> T<span class="token punctuation">,</span> <span class="token keyword">int</span> capacity<span class="token operator">></span><span class="token keyword">class</span> <span class="token class-name">Stack</span><span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">// ...</span><span class="token keyword">private</span><span class="token operator">:</span>    T elements<span class="token punctuation">[</span>capacity<span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> size<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果敲入<code>Stack&lt;string, 500&gt; stack;</code> 就声明了一个最大容量为500的字符串栈.<br><code>Vector</code>就是用模板实现的类, 它比数组更加灵活. 详情可以看STL那篇.<br>例如<code>vector&lt;vector&lt;int&gt; &gt;</code>声明了一个二维向量. 中间最好要加个空格, 否则有些编译器会报错. </p>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>STL常见用法</title>
      <link href="/posts/22779.html"/>
      <url>/posts/22779.html</url>
      
        <content type="html"><![CDATA[<h1 id="STL常见用法"><a href="#STL常见用法" class="headerlink" title="STL常见用法"></a>STL常见用法</h1><p>STL是一套非常好用的C++模板, 其中内置了很多已经封装好的数据结构和算法. 如果每次都要从头实现很麻烦, STL在刷算法题时候很好用. 本文是参照<a href="https://www.bilibili.com/video/BV1uJ411r74z" target="_blank" rel="noopener">steve-yu</a>视频做下的笔记.</p><h2 id="输入输出"><a href="#输入输出" class="headerlink" title="输入输出"></a>输入输出</h2><p>C++保留了原来C语言的输入和输出, 在基础上增加了cin和cout. </p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">// 导入cin cout头文件</span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;iostream></span></span><span class="token comment" spellcheck="true">// c程序中输入输出</span><span class="token keyword">int</span> a<span class="token punctuation">;</span><span class="token function">scanf</span><span class="token punctuation">(</span><span class="token string">"%d"</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>a<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%d"</span><span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// C++输入输出</span><span class="token keyword">int</span> a<span class="token punctuation">;</span>cin <span class="token operator">>></span> a<span class="token punctuation">;</span>cout <span class="token operator">&lt;&lt;</span> a<span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 连续输入输出变量</span><span class="token keyword">int</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> c<span class="token punctuation">;</span>cin <span class="token operator">>></span> a <span class="token operator">>></span> b <span class="token operator">>></span> c<span class="token punctuation">;</span>cout <span class="token operator">&lt;&lt;</span> a<span class="token operator">&lt;&lt;</span>b <span class="token operator">&lt;&lt;</span> c<span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 换行方便</span>cout <span class="token operator">&lt;&lt;</span> a<span class="token operator">&lt;&lt;</span> endl<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>cin和cout的效率是比scanf和printf低很多的, 在刷算法题时, 尽可能使用C语言的输入输出来避免超时.</p><h2 id="头文件"><a href="#头文件" class="headerlink" title="头文件"></a>头文件</h2><p>在使用任意的STL数据类型前, 都需要导入对应的头文件. 比如说使用<code>vector</code> 就必须导入<code>&lt;vector&gt;</code>. <code>&lt;algorithm&gt;</code> 中有许多内置好的算法, 例如<code>sort</code>, <code>reverse</code>等.</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;iostream></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;algorithm></span></span><span class="token keyword">using</span> <span class="token keyword">namespace</span> std<span class="token punctuation">;</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> a<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token punctuation">{</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token function">sort</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span>a<span class="token operator">+</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span><span class="token number">7</span><span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span>        cout<span class="token operator">&lt;&lt;</span>a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&lt;&lt;</span><span class="token string">" "</span><span class="token punctuation">;</span>    cout<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span>    <span class="token function">system</span><span class="token punctuation">(</span><span class="token string">"pause"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="String"><a href="#String" class="headerlink" title="String"></a>String</h2><p>字符串可以看做是char*的封装.</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">// c语言中的字符串定义和打印</span><span class="token keyword">char</span> ch<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token string">"asdkajbf"</span><span class="token punctuation">;</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>ch<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">!=</span><span class="token string">'\0'</span><span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%c"</span><span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">(</span>ch<span class="token operator">+</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// c++</span>string s <span class="token operator">=</span> <span class="token string">"zxcvbnm"</span><span class="token punctuation">;</span>cout<span class="token operator">&lt;&lt;</span> s <span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>获取一行字符串:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">// c</span><span class="token function">scanf</span><span class="token punctuation">(</span><span class="token string">"%s"</span><span class="token punctuation">,</span> ch<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// c++</span>string s<span class="token punctuation">;</span><span class="token function">getline</span><span class="token punctuation">(</span>cin<span class="token punctuation">,</span> s<span class="token punctuation">)</span><span class="token punctuation">;</span>cout <span class="token operator">&lt;&lt;</span> s<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>还可以使用<code>+=</code>  运算, 数字会被转换成asc码.</p><pre class="line-numbers language-cpp"><code class="language-cpp">string s<span class="token punctuation">;</span>s <span class="token operator">+</span><span class="token operator">=</span> <span class="token string">"hello"</span><span class="token punctuation">;</span>s <span class="token operator">+</span><span class="token operator">=</span> <span class="token string">" world"</span><span class="token punctuation">;</span>s <span class="token operator">+</span><span class="token operator">=</span> <span class="token string">'5'</span><span class="token punctuation">;</span>s <span class="token operator">+</span><span class="token operator">=</span> <span class="token number">10</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//10对应的asc码是换行</span><span class="token keyword">int</span> a <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//想把a加入字符串</span>s <span class="token operator">+</span><span class="token operator">=</span> <span class="token punctuation">(</span>a<span class="token operator">+</span><span class="token string">'0'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>cout <span class="token operator">&lt;&lt;</span> s<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>同样也可以进行排序:</p><pre class="line-numbers language-cpp"><code class="language-cpp">string s <span class="token operator">=</span> <span class="token string">"5418340"</span><span class="token punctuation">;</span><span class="token function">sort</span><span class="token punctuation">(</span>s<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> s<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>cout <span class="token operator">&lt;&lt;</span> s<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>使用<code>erase</code>和<code>substr</code>:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">// erase</span>string s <span class="token operator">=</span> <span class="token string">"5418340"</span><span class="token punctuation">;</span>s<span class="token punctuation">.</span><span class="token function">erase</span><span class="token punctuation">(</span>s<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//删除第一个</span>s<span class="token punctuation">.</span><span class="token function">erase</span><span class="token punctuation">(</span><span class="token operator">--</span>s<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//删除最后一个</span>cout <span class="token operator">&lt;&lt;</span> s<span class="token punctuation">;</span><span class="token comment" spellcheck="true">// substr</span>string s <span class="token operator">=</span> <span class="token string">"5418340"</span><span class="token punctuation">;</span>s <span class="token operator">=</span> s<span class="token punctuation">.</span><span class="token function">substr</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//取418,取索引为1，往后截断3个</span>s <span class="token operator">=</span> s<span class="token punctuation">.</span><span class="token function">substr</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//索引为1，截断到最后</span>cout <span class="token operator">&lt;&lt;</span> s<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Vector"><a href="#Vector" class="headerlink" title="Vector"></a>Vector</h2><p><code>vector</code>是封装好的向量, 类似数组. 使用前需导入<code>vector</code>头文件.</p><pre class="line-numbers language-cpp"><code class="language-cpp">vector<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> v<span class="token punctuation">;</span><span class="token comment" spellcheck="true">//定义一个空vector</span>vector<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> <span class="token function">v2</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//定义一个4个大小的vector，初始为0</span>vector<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> <span class="token function">v3</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//定义一个4个大小的vector，初始为6</span>vector<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> v<span class="token punctuation">{</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//定义一个vector，数字为1,2,3,4,5</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>获取元素:</p><pre class="line-numbers language-cpp"><code class="language-cpp">vector<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> v<span class="token punctuation">{</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">}</span><span class="token punctuation">;</span>cout <span class="token operator">&lt;&lt;</span> v<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//取索引为1</span>cout <span class="token operator">&lt;&lt;</span> v<span class="token punctuation">.</span><span class="token function">at</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//取索引为2的</span><span class="token comment" spellcheck="true">// 获取第一个</span>cout<span class="token operator">&lt;&lt;</span>v<span class="token punctuation">.</span><span class="token function">front</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>cout<span class="token operator">&lt;&lt;</span>v<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>cout<span class="token operator">&lt;&lt;</span><span class="token operator">*</span>v<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 获取最后一个</span>cout<span class="token operator">&lt;&lt;</span>v<span class="token punctuation">.</span><span class="token function">back</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>cout<span class="token operator">&lt;&lt;</span>v<span class="token punctuation">[</span>v<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//size是获取大小</span>cout<span class="token operator">&lt;&lt;</span><span class="token operator">*</span><span class="token operator">--</span>v<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>添加元素:</p><pre class="line-numbers language-cpp"><code class="language-cpp">vector<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> v<span class="token punctuation">;</span>v<span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">;</span>v<span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">;</span>v<span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">;</span>v<span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">;</span>v<span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>修改形状:</p><pre class="line-numbers language-cpp"><code class="language-cpp">v<span class="token punctuation">.</span><span class="token function">resize</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>删除元素:</p><pre class="line-numbers language-cpp"><code class="language-cpp">v<span class="token punctuation">.</span><span class="token function">erase</span><span class="token punctuation">(</span>v<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//删除第一个元素</span>v<span class="token punctuation">.</span><span class="token function">erase</span><span class="token punctuation">(</span><span class="token operator">--</span>v<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//删除最后一个元素</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>向量排序:</p><pre class="line-numbers language-cpp"><code class="language-cpp">vector<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> v<span class="token punctuation">{</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token function">sort</span><span class="token punctuation">(</span>v<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>v<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>less<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//从小到大</span><span class="token function">sort</span><span class="token punctuation">(</span>v<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>v<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>greater<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//从大到小排序</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="Stack"><a href="#Stack" class="headerlink" title="Stack"></a>Stack</h2><p>就是数据结构中的栈, 先进后出. 使用前需导入<code>stack</code>头文件.</p><pre class="line-numbers language-cpp"><code class="language-cpp">stack<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> s<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>栈的各种操作:</p><pre class="line-numbers language-cpp"><code class="language-cpp">s<span class="token punctuation">.</span><span class="token function">push</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 入栈</span>s<span class="token punctuation">.</span><span class="token function">push</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">;</span>cout <span class="token operator">&lt;&lt;</span> s<span class="token punctuation">.</span><span class="token function">top</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;&lt;</span> endl<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 获取顶端</span>s<span class="token punctuation">.</span><span class="token function">pop</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 出栈 无返回值</span>cout <span class="token operator">&lt;&lt;</span> s<span class="token punctuation">.</span><span class="token function">top</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;&lt;</span> endl<span class="token punctuation">;</span>cout <span class="token operator">&lt;&lt;</span> s<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;&lt;</span> endl<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 查看元素个数</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>用栈实现进制转换(decimal to binary):</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token keyword">int</span> <span class="token function">itob</span><span class="token punctuation">(</span><span class="token keyword">int</span> decimal<span class="token punctuation">)</span><span class="token punctuation">{</span>    stack<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> s<span class="token punctuation">;</span>    <span class="token keyword">int</span> res <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span>decimal <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">// 除二取余</span>        s<span class="token punctuation">.</span><span class="token function">push</span><span class="token punctuation">(</span>decimal<span class="token operator">%</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        decimal <span class="token operator">/</span><span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">while</span><span class="token punctuation">(</span><span class="token operator">!</span>s<span class="token punctuation">.</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token punctuation">{</span>        res <span class="token operator">=</span> res <span class="token operator">*</span> <span class="token number">10</span> <span class="token operator">+</span> s<span class="token punctuation">.</span><span class="token function">top</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        s<span class="token punctuation">.</span><span class="token function">pop</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">return</span> res<span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>用栈实现逆序单词打印:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;stack></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;sstream></span></span><span class="token keyword">void</span> <span class="token function">inverse</span><span class="token punctuation">(</span>string str<span class="token punctuation">)</span><span class="token punctuation">{</span>    stack<span class="token operator">&lt;</span>string<span class="token operator">></span> s<span class="token punctuation">;</span>    stringstream ss<span class="token punctuation">;</span>    ss <span class="token operator">&lt;&lt;</span> str<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 让字符串流入stringstream</span>    <span class="token keyword">while</span><span class="token punctuation">(</span>ss <span class="token operator">>></span> str<span class="token punctuation">)</span><span class="token punctuation">{</span>        s<span class="token punctuation">.</span><span class="token function">push</span><span class="token punctuation">(</span>str<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">while</span><span class="token punctuation">(</span><span class="token operator">!</span>s<span class="token punctuation">.</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        cout <span class="token operator">&lt;&lt;</span> s<span class="token punctuation">.</span><span class="token function">top</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        s<span class="token punctuation">.</span><span class="token function">pop</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>s<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">!=</span><span class="token number">0</span><span class="token punctuation">)</span> cout <span class="token operator">&lt;&lt;</span> <span class="token string">" "</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里使用了<code>stringstream</code>, 用来做数据转换, 十分安全和方便. 例如字符串转数字:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">// 方法1</span>string s <span class="token operator">=</span> <span class="token string">"1234"</span><span class="token punctuation">;</span><span class="token keyword">int</span> i<span class="token punctuation">;</span>stringstream ss<span class="token punctuation">;</span>ss <span class="token operator">&lt;&lt;</span> s<span class="token punctuation">;</span>ss <span class="token operator">>></span> i<span class="token punctuation">;</span>cout <span class="token operator">&lt;&lt;</span> i<span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 方法2</span>string s <span class="token operator">=</span> <span class="token string">"1234"</span><span class="token punctuation">;</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token function">stoi</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">;</span>cout <span class="token operator">&lt;&lt;</span> i<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>数字转字符串:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">// 方法1</span><span class="token keyword">int</span> a <span class="token operator">=</span> <span class="token number">1234</span><span class="token punctuation">;</span>string out<span class="token punctuation">;</span>stringstream ss<span class="token punctuation">;</span>ss <span class="token operator">&lt;&lt;</span> a<span class="token punctuation">;</span>ss <span class="token operator">>></span> out<span class="token punctuation">;</span>cout <span class="token operator">&lt;&lt;</span> out <span class="token operator">&lt;&lt;</span> endl<span class="token punctuation">;</span><span class="token comment" spellcheck="true">//方法2(c++ 11)</span><span class="token keyword">int</span> a <span class="token operator">=</span> <span class="token number">1234</span><span class="token punctuation">;</span>cout <span class="token operator">&lt;&lt;</span> <span class="token function">to_string</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span> <span class="token operator">&lt;&lt;</span> endl<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Queue"><a href="#Queue" class="headerlink" title="Queue"></a>Queue</h2><p>此为队列的封装实现. 使用前需要导入<code>&lt;queue&gt;</code>.</p><pre class="line-numbers language-cpp"><code class="language-cpp">queue<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> q<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>其操作为:</p><pre class="line-numbers language-cpp"><code class="language-cpp">q<span class="token punctuation">.</span><span class="token function">push</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//入队</span>q<span class="token punctuation">.</span><span class="token function">push</span><span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">;</span>cout<span class="token operator">&lt;&lt;</span>q<span class="token punctuation">.</span><span class="token function">front</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 取队首</span>q<span class="token punctuation">.</span><span class="token function">pop</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 出队</span>cout<span class="token operator">&lt;&lt;</span>q<span class="token punctuation">.</span><span class="token function">front</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span>cout<span class="token operator">&lt;&lt;</span>q<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 查看队列已有元素长度</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h2><p><code>map</code>是一个映射关系(键值对). 这里的<code>map</code>分为两种, 一种是有序的<code>map</code>, 一种是无序的<code>unordered_map</code>. 使用前都需要分别导入.</p><p>map:</p><pre class="line-numbers language-cpp"><code class="language-cpp">map<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token punctuation">,</span><span class="token keyword">int</span><span class="token operator">></span> m<span class="token punctuation">;</span><span class="token comment" spellcheck="true">//有序, 底层为树状结构</span>m<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">;</span>m<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">;</span>m<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">9</span><span class="token punctuation">;</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">auto</span> it<span class="token operator">=</span>m<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>it<span class="token operator">!=</span>m<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>it<span class="token operator">++</span><span class="token punctuation">)</span>    cout<span class="token operator">&lt;&lt;</span>it<span class="token operator">-</span><span class="token operator">></span>first<span class="token operator">&lt;&lt;</span><span class="token string">" "</span><span class="token operator">&lt;&lt;</span>it<span class="token operator">-</span><span class="token operator">></span>second<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">auto</span> tmp<span class="token operator">:</span>m<span class="token punctuation">)</span><span class="token punctuation">{</span>    cout<span class="token operator">&lt;&lt;</span>tmp<span class="token punctuation">.</span>first<span class="token operator">&lt;&lt;</span><span class="token string">" "</span><span class="token operator">&lt;&lt;</span>tmp<span class="token punctuation">.</span>second<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>unordered_map:</p><pre class="line-numbers language-cpp"><code class="language-cpp">unordered_map<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token punctuation">,</span><span class="token keyword">int</span><span class="token operator">></span> m<span class="token punctuation">;</span><span class="token comment" spellcheck="true">//无序, 底层为哈希结构</span>m<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">;</span>m<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">;</span>m<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">9</span><span class="token punctuation">;</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">auto</span> it<span class="token operator">=</span>m<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>it<span class="token operator">!=</span>m<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>it<span class="token operator">++</span><span class="token punctuation">)</span>    cout<span class="token operator">&lt;&lt;</span>it<span class="token operator">-</span><span class="token operator">></span>first<span class="token operator">&lt;&lt;</span><span class="token string">" "</span><span class="token operator">&lt;&lt;</span>it<span class="token operator">-</span><span class="token operator">></span>second<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">auto</span> tmp<span class="token operator">:</span>m<span class="token punctuation">)</span><span class="token punctuation">{</span>    cout<span class="token operator">&lt;&lt;</span>tmp<span class="token punctuation">.</span>first<span class="token operator">&lt;&lt;</span><span class="token string">" "</span><span class="token operator">&lt;&lt;</span>tmp<span class="token punctuation">.</span>second<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>pair的用法, 是一个映射结构.</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token keyword">bool</span> <span class="token function">cmp</span><span class="token punctuation">(</span>pair<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token punctuation">,</span><span class="token keyword">int</span><span class="token operator">></span> a<span class="token punctuation">,</span>pair<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token punctuation">,</span><span class="token keyword">int</span><span class="token operator">></span> b<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">return</span> a<span class="token punctuation">.</span>first<span class="token operator">></span>b<span class="token punctuation">.</span>first<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    unordered_map<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token punctuation">,</span><span class="token keyword">int</span><span class="token operator">></span> m<span class="token punctuation">;</span><span class="token comment" spellcheck="true">//无序, 底层为哈希结构</span>    m<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">;</span>    m<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">;</span>    m<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">9</span><span class="token punctuation">;</span>    vector<span class="token operator">&lt;</span>pair<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token punctuation">,</span><span class="token keyword">int</span><span class="token operator">>></span> <span class="token function">v</span><span class="token punctuation">(</span>m<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>m<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">sort</span><span class="token punctuation">(</span>v<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>v<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>cmp<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">auto</span> tmp<span class="token operator">:</span>v<span class="token punctuation">)</span><span class="token punctuation">{</span>        cout<span class="token operator">&lt;&lt;</span>tmp<span class="token punctuation">.</span>first<span class="token operator">&lt;&lt;</span>tmp<span class="token punctuation">.</span>second<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Set"><a href="#Set" class="headerlink" title="Set"></a>Set</h2><p>它是集合的封装实现, 同样存在一种有序结构<code>set</code>和一种无序结构<code>unordered_set</code>. 使用前需要分别导入.</p><pre class="line-numbers language-cpp"><code class="language-cpp">set<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> s<span class="token punctuation">;</span><span class="token comment" spellcheck="true">//树状结构 有序</span>unordered_set<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> s2<span class="token punctuation">;</span><span class="token comment" spellcheck="true">//哈希结构 无序</span>s<span class="token punctuation">.</span><span class="token function">insert</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">;</span>s<span class="token punctuation">.</span><span class="token function">insert</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>s<span class="token punctuation">.</span><span class="token function">insert</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>s<span class="token punctuation">.</span><span class="token function">insert</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>cout<span class="token operator">&lt;&lt;</span>s<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">auto</span> tmp<span class="token operator">:</span>s<span class="token punctuation">)</span>    cout<span class="token operator">&lt;&lt;</span>tmp<span class="token operator">&lt;&lt;</span><span class="token string">" "</span><span class="token punctuation">;</span>cout<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">auto</span> it<span class="token operator">=</span>s<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>it<span class="token operator">!=</span>s<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>it<span class="token operator">++</span><span class="token punctuation">)</span>    cout<span class="token operator">&lt;&lt;</span><span class="token operator">*</span>it<span class="token operator">&lt;&lt;</span><span class="token string">" "</span><span class="token punctuation">;</span>cout<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Deque"><a href="#Deque" class="headerlink" title="Deque"></a>Deque</h2><p>此为双端队列的封装实现, 使用前需要导入<code>&lt;deque&gt;</code>.</p><pre class="line-numbers language-cpp"><code class="language-cpp">deque<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> d<span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 4 9 1 2</span>d<span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 尾部添加</span>d<span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">;</span>d<span class="token punctuation">.</span><span class="token function">push_front</span><span class="token punctuation">(</span><span class="token number">9</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 首部添加</span>d<span class="token punctuation">.</span><span class="token function">push_front</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>d<span class="token punctuation">.</span><span class="token function">pop_back</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 尾部删除</span>d<span class="token punctuation">.</span><span class="token function">pop_front</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 头部删除</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">auto</span> tmp<span class="token operator">:</span>d<span class="token punctuation">)</span> cout<span class="token operator">&lt;&lt;</span>tmp<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">auto</span> it<span class="token operator">=</span>d<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>it<span class="token operator">!=</span>d<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>it<span class="token operator">++</span><span class="token punctuation">)</span> cout<span class="token operator">&lt;&lt;</span><span class="token operator">*</span>it<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span><span class="token function">sort</span><span class="token punctuation">(</span>d<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>d<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>greater<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 排序</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="List"><a href="#List" class="headerlink" title="List"></a>List</h2><p>此为双向链表的实现, 使用前需要导入<code>&lt;list&gt;</code>.</p><pre class="line-numbers language-cpp"><code class="language-cpp">list<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> li<span class="token punctuation">;</span>li<span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 末尾插入</span>li<span class="token punctuation">.</span><span class="token function">push_front</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 头部插入</span>li<span class="token punctuation">.</span><span class="token function">emplace_front</span><span class="token punctuation">(</span><span class="token number">9</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 也是头插</span>li<span class="token punctuation">.</span><span class="token function">emplace_back</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 也是尾插</span>li<span class="token punctuation">.</span><span class="token function">insert</span><span class="token punctuation">(</span><span class="token operator">++</span>li<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 第一个元素后插入</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">auto</span> tmp<span class="token operator">:</span>li<span class="token punctuation">)</span> cout<span class="token operator">&lt;&lt;</span>tmp<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">auto</span> it<span class="token operator">=</span>li<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>it<span class="token operator">!=</span>li<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>it<span class="token operator">++</span><span class="token punctuation">)</span> cout<span class="token operator">&lt;&lt;</span><span class="token operator">*</span>it<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span><span class="token comment" spellcheck="true">//排序不能使用头文件algorithm中的sort 使用list模板自定义的sort方法</span>li<span class="token punctuation">.</span><span class="token function">sort</span><span class="token punctuation">(</span>greater<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//从大到小排序</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><ul><li><a href="http://c.biancheng.net/stl/" target="_blank" rel="noopener">STL中文教程</a></li><li><a href="http://www.cplusplus.com/reference/stl/" target="_blank" rel="noopener">STL英文文档</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> STL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DL目标检测</title>
      <link href="/posts/59216.html"/>
      <url>/posts/59216.html</url>
      
        <content type="html"><![CDATA[<h1 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h1><p>目标检测是CV里一个重要方向, 对于一张图片, 我们应该能够给出图中含有的物体(单个或多个)的位置以及他们的大小和类别.</p><h2 id="目标定位"><a href="#目标定位" class="headerlink" title="目标定位"></a>目标定位</h2><p>假设我们已经能够利用CNN对一张图片是否含有某个物体而进行分类. 应该先搞清楚要的输出是什么.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/%E7%9B%AE%E6%A0%87%E5%AE%9A%E4%BD%8D.png" alt=""></p><p>在上图的例子中, $P_c$为图中是否有物体, $b_x, b_y, b_w, b_h$分别为物体的x, y, width, height即物体的未知参数, 有了它们就能画出物体的边界框bounding box, $C_1, C_2, C_3$是是否属于图中所示的三个类. 如果$P_c$为0, 那么剩下的数据也不用关心了.</p><h2 id="特征点检测"><a href="#特征点检测" class="headerlink" title="特征点检测"></a>特征点检测</h2><p>根据目标定位的原理, 我们甚至可以手动要求神经网络输出某些特殊的位置信息, 叫做特征点. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/特征点检测.png" style="zoom: 50%;" /><p>是否有人脸, 特征点对组成了神经网络的输出, 每个特征点对都可用$l_x, l_y$来表示. 然后可以根据得到的特征点位置来进行判断, 比如说人物的表情, 或其他人物面部分析等, 同理也可以左人体的体态识别. 当然数据集必须有人为标注的特征点位置.</p><h2 id="滑动窗口实现"><a href="#滑动窗口实现" class="headerlink" title="滑动窗口实现"></a>滑动窗口实现</h2><p>这里介绍了一种将神经网络替换成卷积神经网络从而降低参数个数, 但达到相同效果的方法, 即全卷积神经网络, 利用$1\times1$卷积代替稠密神经网络. 这里不再赘述.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/%E5%85%A8%E5%8D%B7%E7%A7%AF.png" alt=""></p><p>对一个$16\times 16$的图片进行卷积时,  如果我们的网络接受的输出是$14\times 14$的, 无需把原图片拆分成4个$14\times 14$的子集, 因为这样做会引入很高的卷积重复率和训练成本. 直接将$16\times 16$的图片作为一个整体, 所有原来的参数不变, 仍然进行卷积, 只是说最终的输出不再是$1\times 1$而是$2\times2$, 一次性计算就能完成上面四次计算的结果.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3.jpg" alt=""></p><h2 id="Bounding-Box"><a href="#Bounding-Box" class="headerlink" title="Bounding Box"></a>Bounding Box</h2><p>在上面已经了解了用卷积确定物体在图片中的位置, 以及物体的类别. 但是对于物体的具体位置, 不能用图片粗略的窗口划分代替物体的位置, 明显和物体的真实位置是不匹配的.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/Boundingbox.png" alt=""></p><p>假设将图片分为$3\times3$的区域, 每个区域会产生一个列向量, 内容和我们前面说过的预测值相同. 一张图就会产生$3\times3\times8$的结果. Yolo会取对象的中点, 将该对象分配给对象中点的格子.</p><h2 id="交并比"><a href="#交并比" class="headerlink" title="交并比"></a>交并比</h2><p>交并比是用来衡量边界框是否标注正确的函数. 函数非常简洁, 假设我们预测的边界框$A$和物体的真实边界框$B$, 交并比$IoU$为:<br>$$<br>IoU = \frac{A\cap B}{A\cup B}<br>$$<br>在CV里, 一般约定俗成的说, 如果$IoU\geq 0.5$视为检测正确.</p><h2 id="非极大值抑制"><a href="#非极大值抑制" class="headerlink" title="非极大值抑制"></a>非极大值抑制</h2><p>非极大值抑制可以保证每个物体只被检测一次. 如果采用比较细的划分, 可能物体会比每个划分出来的网格大, 那么每个网格都认为物体的中心在自己的格中, 这样物体就被计算了很多次.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/非极大值抑制.png" style="zoom:50%;" /><p>非极大值抑制会取概率最高的$P_c$格子(如果是预测多个类别, 那么$P_c$为多个类的概率的乘积), 并去掉周围$IoU$很高的一圈bounding box, 得到最终的结果. </p><ol><li>按照置信度对所有候选框进行排序</li><li>将置信度最高的候选框添加到输出候选框列表中, 并将其从输入候选框中删除</li><li>计算挑选出的候选框与当前所有输入候选框之间的IoU</li><li>删除IoU大于阈值的输入候选框</li><li>重复第2-4步, 直到输入候选框列表为空</li></ol><h2 id="Anchor-Boxes"><a href="#Anchor-Boxes" class="headerlink" title="Anchor Boxes"></a>Anchor Boxes</h2><p>Anchor解决了一个格子只能预测一个对象的问题, 使得其能检测多个对象. Anchor boxes是一种预先加载对象的形状的边界框, 将预测标签进行扩展, 每个anchor box对应一个bounding box的参数量.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/anchorbox.png" alt=""></p><p>每个对象除了被分配到一个格子中, 还被分配到一个和对象形状交并比最高的anchor box中.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CV </tag>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP相关知识</title>
      <link href="/posts/26379.html"/>
      <url>/posts/26379.html</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>2020.08.24</strong>: 更新word2vec的部分内容.</p></blockquote><h1 id="NLP相关知识"><a href="#NLP相关知识" class="headerlink" title="NLP相关知识"></a>NLP相关知识</h1><p>整个流程: 分词 Tokenize -&gt; 预处理 Preprocess -&gt; 特征工程 Feature engine -&gt;  ML.</p><h2 id="分词-Tokenize"><a href="#分词-Tokenize" class="headerlink" title="分词 Tokenize"></a>分词 Tokenize</h2><p>就是把每个句子按照词语分开, 包括标点. 只有分词后才方便后续对句子的过滤. 中文分词和英文分词是不一样的. 英文分词只需要直接分离标点和空格就行, 中文分词常会因为不同NLP库的处理模式不同而结果不唯一. 有时候分词没那么容易, 在社交语言中和常常会有拼写错误, 缩写, URL, emoji, 单位名称书写不统一… 常常用正则一块处理掉. 值得注意的是, 有时去除它们不一定能带来好的效果.</p><h2 id="文本预处理"><a href="#文本预处理" class="headerlink" title="文本预处理"></a>文本预处理</h2><p>库这用<strong>NLTK</strong>用的多. 对于词形归一和词干提取, 无论是哪种方法都不能正确地处理所有的单词, 常常会引入噪声, 需要结合实际效果而定.</p><h3 id="词形归一-Lemma"><a href="#词形归一-Lemma" class="headerlink" title="词形归一 Lemma"></a>词形归一 Lemma</h3><p>把所有词的变形, 全部都归为一个形式. 通过语言学家的wordnet进行归一化. 但是Lemma的过程中常常会因为不考虑单词词性而归一错误, 这时候就需要附加标注的词性(Pos Tag)进行映射.</p><h3 id="词干提取-Stemming"><a href="#词干提取-Stemming" class="headerlink" title="词干提取 Stemming"></a>词干提取 Stemming</h3><p>简单来说就是直接把不影响词性的词根直接砍掉. </p><h3 id="停止词-Stopwords"><a href="#停止词-Stopwords" class="headerlink" title="停止词 Stopwords"></a>停止词 Stopwords</h3><p>停止词也叫停用词, 在英语里面遇到的a, the, or 等使用频率很高的词基本都是停止词. 去掉停止词后仍然可以表达出句子的意思, 去掉停止词可以节省大量的空间. 但是停止词也不是什么任务都去掉的, 比如判断文章相似度之类或者给文章打分的任务就不应该去除停止词, 因为去除后会导致句子结构发生变化.</p><h2 id="特征工程-Feature-Engineering"><a href="#特征工程-Feature-Engineering" class="headerlink" title="特征工程 Feature Engineering"></a>特征工程 Feature Engineering</h2><h3 id="基本语义特征"><a href="#基本语义特征" class="headerlink" title="基本语义特征"></a>基本语义特征</h3><p>主要是一些句子上的差别. 将各种描述句子的特征加加减减.</p><p>问题1和问题2的符号差异, 问号差异, 问题1和问题2分别的句子长度, 长度差异, 长度差异率, 字符数量差异, 字符差异率, 情感分析差异, 起始词(疑问词)的差异, 共享词的交, 并, 数量差异, 数量差异率, fuzz_qratio, fuzz_WRatio, fuzz_partial_ratio, partal_token_sort_ratio, token_set_ratio…</p><p>距离特征有cosine_word2vec, cityblock_distance, canberra_distance, euclidean_distance, braycurits_distance, minkowski_distance, skew_q1, skew_q2, kur_q1, kur_q2, wmd.</p><h3 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h3><p>TFIDF(Term Frequency - Inverse Document Frequency) 词频 - 逆文本频率. TF即词的词频, IDF可以帮助我们理解这个词的重要程度.</p><p>TF(Term Frequency): 一个term在文档中出现的频繁程度. 但是词频并不能反映这个词语的重要性, 有时候它们出现很多次, 但却没有什么意义比如停用词, 没有意义的词语没法起到文本分类的作用.</p><p>对于词语$i$, 在文档$j$ 有:<br>$$<br>TF_{i, j} = \frac{n_{i, j}}{\sum_k n_{k, j}} = \frac{某个词在文章中出现的次数}{文章的总词数}\<br>$$<br>所以就需要将他们乘个缩放因子, 来平衡掉出现次数过多但无意义的影响.</p><p>IDF(Inverse Document Frequency) 逆文档频率, 它与一个词常见程度成反比, 这样越普遍的词语, 越没有实际意义. 对于语料库文章总数$\left| D\right|$, 包含词语$j$ 的文档数$\left| j:t_i \in d_j \right|$ 有:<br>$$<br>IDF_i= \log{\frac{\left | D \right |}{1+\left| j:t_i \in d_j \right|}} = \log{\frac{语料库的文档总数}{包含该词的文档数+1}}<br>$$<br>加1是为了规避词语不在语料库中分母为0的情况.</p><p>而TF-IDF就是TF和IDF的乘积. 能够看出来, 某个词语在某篇文章出现的次数越多越重要, 在所有文章中出现的次数越多越不重要.<br>$$<br>TFIDF = TF \times IDF<br>$$<br>但是TF-IDF也有缺陷, 它忽略了文本中词语的位置信息. 有些文章的段首明显句首的权重更高. 其次有些文章的关键词可能只出现了1-2次.</p><h3 id="词袋模型-Bag-of-words-model"><a href="#词袋模型-Bag-of-words-model" class="headerlink" title="词袋模型 Bag of words model"></a>词袋模型 Bag of words model</h3><p>词袋模型(Bag of words model) 将每段文本都由装着词的袋子表示, 对于比如对于以下文本:</p><ol><li>John likes to watch movies. Mary likes movies too.</li><li>John also likes to watch football games.</li></ol><p>能够生成一个含有10个不同词语的词表:</p><p>[John, likes, to, watch, movies, also, football, games, Mary, too]</p><p>然后结合单词出现的次数, 能够将句子表示为:</p><ol><li>[1, 2, 1, 1, 2, 1, 1, 0, 0, 0]</li><li>[1, 1, 1, 1, 0, 0, 0, 1, 1, 1]</li></ol><p>词袋模型能够将文本转化为向量, 但是却没有保留文本之间的语序, N元语法对这个问题进行了改善.</p><h3 id="N-Gram"><a href="#N-Gram" class="headerlink" title="N-Gram"></a>N-Gram</h3><p>N元语法(N-Gram)基于N-1马尔科夫假设, 即句子中的第$n$个单词被认为和前$m$个单词相关, 即:<br>$$<br>P(x_1, x_2,… , x_n) = P(x_1)P(x_2|x_1)\cdots P(x_n|x_{n-m},…,x_{n-1})<br>$$<br>如果一个词出现只依赖于它前面的一个词, 称为Bi-gram, 如果依赖于前面的两个词, 称为Tri-gram.</p><p>一般就采用$N=2$或$N=3$即Bi-gram和Tri-gram. 用极大似然估计来计算频率, 有:<br>$$<br>\begin{aligned}<br>&amp;p(w_n|w_{n-1})=\frac{C(w_{n-1}w_n)}{C(w_{n})}\\<br>&amp;p(w_n|w_{n-1}w_{n-2})=\frac{C(w_{n-2}w_{n-1}w_n)}{C(w_{n-2}w_{n-1})}\\<br>&amp;p(w_n|w_{n-1}\cdots w_2w_1)=\frac{C(w_1w_2\cdots w_n)}{C(w_1w_2\cdots w_{n-1})}<br>\end{aligned}<br>$$<br>举个二元语法的例子:</p><blockquote><ol><li>oh I am Sam oh</li><li>oh Sam I am oh</li><li>oh I do not like eggs and ham. oh</li></ol></blockquote><p><code>I</code> 出现了三次, <code>I am</code> 出现了两次, 所以求得$p(am|I) = \frac{C(I\ am)}{C(am)}=\frac{2}{3}$, 同样计算出:<br>$$<br>\begin{aligned}<br>P(I|oh) &amp;= \frac{2}{3}, P(Sam|am)=\frac{1}{2}, P(oh|Sam)=\frac{1}{2}\\<br>P(do|I)&amp;=\frac{1}{3}, P(not|do)=\frac{1}{1}, P(like|not)=\frac{1}{1}<br>\end{aligned}<br>$$</p><h3 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h3><p>Word to vector是一种浅层神经网络, <strong>构建一个保留了单词的上下文相似性的低维向量来表示语料库中的文本</strong>. 在训练完成后, Word2Vec可以映射每个词到一个向量(也就是词向量). 词向量一般是稠密的, 低维(不像One-hot维数很高). Word2Vec是<strong>词嵌入</strong>(Word Embedding)的一种. 在词嵌入空间当中, 词意相似的词语通常具有相同的方向, 比如各种水果可能在空间中的位置类似. 值得一提的是, Embedding这种技术在人脸识别当中也叫作编码, 经常将人脸图片在空间中编码为一个向量, 与其他的向量进行比对, 也是利用相似度算法判断是否为该人, 原理实际一致. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/word2vec.jpg" style="zoom:50%;" /><p>整体步骤如下:</p><ol><li>在Input Layer, 某个词语被转化为One-Hot向量(高维, 稀疏)</li><li>在Hidden Layer, 稀疏向量被做了一次<strong>线性变换</strong>, 即$Wx+b$. 或者视为隐藏层神经元不激活.</li><li>获得对应词语在稠密空间的映射.</li></ol><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/embedding1.png" style="zoom: 50%;" /><p>由于输入向量是一个独热稀疏向量, 那么实际上就可以直接获得Hidden Layer中的<strong>唯一神经元权重被激活</strong>, 也就对应了唯一的词向量, 完成这个映射过程, 也就获得了这个词所对应的embedding形式. 如下图左侧矩阵是某个词的独热向量, 右侧是词嵌入矩阵, 二者做矩阵乘法可以获得这个词所对应的嵌入式表示.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/embedding2.png" style="zoom:67%;" /><p>Xin Rong制作了展示<a href="https://ronxin.github.io/wevi/" target="_blank" rel="noopener">词嵌入是如何训练的</a>的网站. </p><p>现在基本上Word2Vec有两种变体用的是最多的, 一种是CBOW, 一种是Skip-gram. 两种算法在进行优化时所采用的函数不同, 这二者的结构是完全相反的. 详见<a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">原论文</a>.</p><h4 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h4><p>CBOW(Continuous Bag-of-Words Model), CBOW是<strong>通过上下文来预测中心词</strong>. 即在已知上文$w_{t-c}, w_{t-c+1}, \dots, w_{t-1}$和下文$w_{t+1}, w_{t+2}, \dots, w_{t+c}$1的情况下预测中心词$w_t$.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/cbow.jpg" style="zoom:50%;" /><p>在中间的隐藏层中, 我们对$w_{t-c}, \dots, w_{t-1}, w_{t+1}, \dots, w_{t+c}$的向量相加, 并除以$2c$. 最终输出层经过Softmax得到一个近似独热的向量.</p><p>根据极大似然, 对于单词集合$T$, 最终要最大化:<br>$$<br>L=\frac{1}{T}\sum_t{\log{P(w_t|w_{t-c}\cdots w_{t+c})}}<br>$$</p><h4 id="Skip-Gram"><a href="#Skip-Gram" class="headerlink" title="Skip-Gram"></a>Skip-Gram</h4><p>Skip-gram通过<strong>中心词预测上下文</strong>.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/skipgram.png" style="zoom: 50%;" /><p>最后也是通过Softmax得出来很多近似独热的向量, 也是最大化如下函数:<br>$$<br>J=\frac{1}{T}\sum_{t=1}^T{\sum_{-c \leq j \leq c, j \neq 0}{\log p(w_{t+j}|w_t)}}<br>$$</p><h4 id="相似度算法"><a href="#相似度算法" class="headerlink" title="相似度算法"></a>相似度算法</h4><h5 id="余弦相似度"><a href="#余弦相似度" class="headerlink" title="余弦相似度"></a>余弦相似度</h5><p>对于余弦$\cos$想必都是了解的, 其实余弦相似度就是通过<strong>两个向量之间的余弦值</strong>来衡量相似度. 两个向量越不相似, 夹角就越大, 余弦值也越大. 这个向量可以是来自于任何文本向量化后的向量(比如TF-IDF和Word2Vec). </p><p>由向量内积:<br>$$<br>a\cdot b = \vert \vert a \vert \vert \ \vert \vert b \vert \vert \cos{\theta}<br>$$<br>有:<br>$$<br>similarity = \cos{\theta} = \frac{A \cdot B}{\vert\vert A\vert\vert \ \vert\vert B\vert\vert} = \frac{\sum\limits_{i=1}^nA_i\times B_i}{\sqrt{\sum\limits_{i=1}^n(A_i)^2}\times \sqrt{\sum\limits_{i=1}^n(B_i)^2}}<br>$$<br>相似度的范围在$(-1, 1)$之间, 1表示它们完全相同, 0表示相互独立, -1表示完全相反. 如果是$TF-IDF$下的向量, 由于不能为负数, 两个向量角度不大于90度.</p><h5 id="Jaccard相似度"><a href="#Jaccard相似度" class="headerlink" title="Jaccard相似度"></a>Jaccard相似度</h5><p>Jaccard相似度是在没有将文本数据向量化之前的一种度量. 它处理的是<strong>集合</strong>, 在NLP问题上就是两个句子的词语集合. Jaccard相似度是由Jaccard系数引出来的.<br>$$<br>\displaylines{<br>J(A, B) = \frac{|A \cap B|}{|A \cup B|}\\<br>d_j(A, B) = 1 - J(A, B) = \frac{|A \cup B| - |A\cap B|}{|A \cup B|}<br>}<br>$$<br>与余弦相似度不同的是, <strong>面对重复的词, Jaccard相似度不会有影响</strong>, 因为它是集合上的运算, 自带去重的效果, 如果是余弦相似度则会受到影响.</p><h5 id="词移距离"><a href="#词移距离" class="headerlink" title="词移距离"></a>词移距离</h5><p>词移距离WMD(Word Move Distance)是基于Word2vec特性开发出来的, 当单词经过Word2Vec映射成一个词向量的时候, 语义相近的单词距离会比较近, 比如king和queen在词向量空间中的距离就比sky和apple的近. </p><p>我在项目中用到的是直接用所有单词的词向量加权求和, 然后再用欧氏距离进行比较, 就能衡量两个句子的相似度, 还有用TF-IDF作为权重, 加权求和. 但是词移距离后续的叙述很麻烦, 不再详细说了, 因为涉及到求解和词转移代价等.</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 词袋模型 </tag>
            
            <tag> Word2Vec </tag>
            
            <tag> 特征工程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之XGBoost</title>
      <link href="/posts/36969.html"/>
      <url>/posts/36969.html</url>
      
        <content type="html"><![CDATA[<h1 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h1><p><code>XGBoost</code>是<code>Extreme Gradient Boosting</code>的缩写, 作者是陈天奇大神. XGB因为其高准确率, 易于使用而在各类数据科学竞赛譬如Kaggle, 天池等十分流行. XGB与GBDT十分相似, 可以将XGB视为是GBDT的一个优化形式. 事实上, 如果不考虑工程实现和解决问题上的一些差异, XGB与GBDT比较大的不同就是目标函数的定义.</p><h2 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h2><h3 id="模型和参数"><a href="#模型和参数" class="headerlink" title="模型和参数"></a>模型和参数</h3><p>对于最为常见而简单的线性模型来说, 其预测值为:<br>$$<br>\hat{y}_i = \sum_j \theta_j x_{ij}<br>$$<br>对不同的任务和模型来说, $\theta$ 有着不同的含义. 这个$\theta$ 是模型中不确定的部分, 我们需要通过反复学习来调整它使得它趋近于最优.</p><h3 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h3><p>对于一般的任务来说, 在训练时就是要找到一个最优参数$\theta$ 使得预测的效果最好. 定义一个目标函数来衡量与训练数据的拟合程度.<br>$$<br>\text{obj}(\theta) = L(\theta) + \Omega(\theta)<br>$$<br>其中$L$ 是训练损失函数, $\Omega$ 是正则项. 一般来说损失函数有多种选择, 例如大多数梯度提升算法的MSE:<br>$$<br>L(\theta) = \sum_i (y_i-\hat{y}_i)^2<br>$$<br>或者Logistics损失:<br>$$<br>L(\theta) = \sum_i[ y_i\ln (1+e^{-\hat{y}_i}) + (1-y_i)\ln (1+e^{\hat{y}_i})]<br>$$<br>正则项的作用是通过约束模型的复杂度而避免过拟合, 一个模型过于复杂时, 它的泛化能力会被约束, 当预测新的没见过的数据时能力会大打折扣. 换言之, 一个好的模型会在方差和偏差之间达到平衡. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/xgb正则项.png" style="zoom: 67%;" /><p>L1正则:<br>$$<br>\Omega(w)=\sum_i^W|w_i|<br>$$<br>L2正则:<br>$$<br>\Omega(w)=\sum_i^Ww_i^2<br>$$</p><h2 id="回归树-CART"><a href="#回归树-CART" class="headerlink" title="回归树 CART"></a>回归树 CART</h2><p>CART(Classification And Regression Tree)是XGB的基学习器. 在XGB官网上给出了一个讲解CART回归树的例子, 利用CART来预测谁会玩电脑游戏.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/xgbcart.png" style="zoom:50%;" /><p>CART利用输入的特征来划分输入样本, 并在每个叶子节点上都有一个预测权重, 这与分类决策树有些不同. </p><p>树每次进行划分, 实际上是将搜索空间进行分割:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/xgbcart示例.png" style="zoom: 33%;" /><p>通常, 一棵树的预测能力不够强大, 所以需要将多棵树集成起来.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/xgbtwocart.png" style="zoom: 50%;" /><p>将这两棵树的得分都加起来, 发现小男孩的得分是最高的. 事实上这两棵树在做互补的工作. 假设我们有$K$ 棵树, 分类函数为$f$, $\mathcal {F}$ 是包含所有回归树的函数空间, 那么对于第$i$ 类最终的预测值$\hat{y_i}$为:<br>$$<br>\hat{y}_i = \sum_{k=1}^K f_k(x_i), f_k \in \mathcal{F}<br>$$<br>最终的目标函数就变成了:<br>$$<br>\text{obj}(\theta) = \sum_i^n l(y_i, \hat{y}_i) + \sum_{k=1}^K \Omega(f_k)<br>$$<br>我们必须要求出来的其实就是$f_k$, 一旦有了这个函数, 就能得知树的结构和叶子节点所对应的权值.</p><p>回归树的优势:</p><ul><li>应用广泛，比如 GBM 和随机森林等。几乎一半的数据挖掘比赛获胜者都是靠树的集成方法取胜的</li><li>你不需要将特征标准化，因为做不做输入特征缩放结果是一样的.</li><li>能够学习到更多特征之间的高阶关系.</li><li>具有可扩放行(scalable)，应用于工业界.</li></ul><h2 id="树的提升"><a href="#树的提升" class="headerlink" title="树的提升"></a>树的提升</h2><p>前面已经介绍完了基学习器, 接下来该说说如何训练它们了. 对于时间步$t$, 目标函数就变成了:<br>$$<br>\text{obj} = \sum_{i=1}^n l(y_i, \hat{y}_i^{(t)}) + \sum_{i=1}^t\Omega(f_i)<br>$$</p><h3 id="增量训练"><a href="#增量训练" class="headerlink" title="增量训练"></a>增量训练</h3><p>在XGB中, 作者指出学习树的结构比传统优化问题要困难得多, 并且一次性的学习到所有树结构很难, 所以在这里采用了<strong>一次向上加一棵树</strong>的策略:<br>$$<br>\begin{split}\hat{y}_i^{(0)} &amp;= 0\\<br>\hat{y}_i^{(1)} &amp;= f_1(x_i) = \hat{y}_i^{(0)} + f_1(x_i)\\<br>\hat{y}_i^{(2)} &amp;= f_1(x_i) + f_2(x_i)= \hat{y}_i^{(1)} + f_2(x_i)\\<br>&amp;\dots\\<br>\hat{y}_i^{(t)} &amp;= \sum_{k=1}^t f_k(x_i)= \hat{y}_i^{(t-1)} + f_t(x_i)\end{split}<br>$$<br>这实际上使得XGB每次都去拟合<strong>残差</strong>, 也就是预测值和真实值之间的差.</p><p>在每次训练都要优化目标函数:<br>$$<br>\begin{split}\text{obj}^{(t)} &amp; = \sum_{i=1}^n l(y_i, \hat{y}_i^{(t)}) + \sum_{i=1}^t\Omega(f_i) \\<br>          &amp; =\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(t-1)}+f_{t}\left(x_{i}\right)\right)+\Omega\left(f_{t}\right)+\text { constant }\end{split}<br>$$<br>假设MSE为损失函数, 代入目标函数式:<br>$$<br>\begin{split}\text{obj}^{(t)} &amp; = \sum_{i=1}^n \left(y_i - (\hat{y}_i^{(t-1)} + f_t(x_i))\right)^2 + \sum_{i=1}^t\Omega(f_i) \\<br>          &amp; = \sum_{i=1}^n [2(\hat{y}_i^{(t-1)} - y_i)f_t(x_i) + f_t(x_i)^2] + \Omega(f_t) + \mathrm{constant}\end{split}<br>$$<br>这里的$\hat{y}_i^{(t-1)} - y_i$就是上一轮的残差. 这里MSE体现出非常好的数学性质, 它既含有$f_t(x_i)$ 的一次项, 也含有二次项, 这对求出新树结构非常有帮助. 但是对于其他损失函数不一定能够获得这么好的数学性质, 因此一般都是通过泰勒展开来获得这个一次项和二次项. 不知你还记得不记得二阶泰勒展开:<br>$$<br>f(x+\Delta x) \approx f(x)+f^{\prime}(x) \Delta x+\frac{1}{2} f^{\prime \prime}(x) \Delta x^{2}<br>$$<br>因为我们每过一个时间步$t$ 拟合的都是残差, 所以我们可以把新加入的$f_t(x_i)$ 看做是$\Delta x$, $\hat{y}_{i}^{(t-1)}$ 看做是$x$, 就能将损失函数展开:<br>$$<br>l\left(y_{i}, \hat{y}_{i}^{(t-1)}+f_{t}\left(x_{i}\right)\right)=l(y_i, \hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i)<br>$$<br>其中的$g_i$ 和$h_i$ 分别是$f_t(x_i)$ 的一阶偏导和二阶偏导:<br>$$<br>\begin{split}g_i &amp;= \frac{\partial l(y_i, \hat{y}_i^{(t-1)})}{\partial{\hat{y}_i^{(t-1)}}}\\<br>h_i &amp;= \frac{\partial l(y_i, \hat{y}_i^{(t-1)})}{\partial^2{\hat{y}_i^{(t-1)}}} \end{split}<br>$$<br>注: 这步能够展的原因是$y_i$ 是一个常数, 其实$l\left(y_{i}, \hat{y}_{i}^{(t-1)}+f_{t}\left(x_{i}\right)\right)$ 只是一个与$\hat{y}_{i}^{(t-1)}$ 和$f_t(x_i)$ 相关的函数.</p><p>目标函数就变成了:<br>$$<br>\begin{split}<br>\text{obj}^{(t)} &amp; = \sum_{i=1}^n l(y_i, \hat{y}_i^{(t)}) + \sum_{i=1}^t\Omega(f_i) \\<br>          &amp; =\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(t-1)}+f_{t}\left(x_{i}\right)\right)+\Omega\left(f_{t}\right)+\text { constant } \\<br>          &amp; = \sum_{i=1}^n [l(y_i, \hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i)] + \Omega(f_t) + \mathrm{constant}<br> \end{split}<br>$$<br>对于每个新的时间步$t$, 上一个时间步$t-1$所对应的损失$l(y_i, \hat{y}_i^{(t-1)})$是一个常数, 去掉目标函数中所有与优化无关的常数部分:<br>$$<br>\sum_{i=1}^n [g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i)] + \Omega(f_t)<br>$$</p><h3 id="模型复杂度"><a href="#模型复杂度" class="headerlink" title="模型复杂度"></a>模型复杂度</h3><p>XGB在正则项这里对树模型的生长加以约束. 对于树模型, $T$ 为叶子节点的总数, 设$w$ 为叶子节点的权重序列, 是一个$T$ 维的向量, $q$ 为树的结构, 那么$q(x)$ 就能表示样本$x$ 落在叶子中的位置, $w_{q(x)}$ 就是叶子中样本$x$ 所对应的权重. $d$ 为输入实例的维数.<br>$$<br>f_t(x) = w_{q(x)}, w \in R^T, q:R^d\rightarrow \{1,2,\cdots,T\}<br>$$<br>例如:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/xgb复杂度.png" style="zoom: 33%;" /><p>在XGB中, 定义正则项如下:<br>$$<br>\Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2<br>$$<br>第一项为叶子节点的数目, 第二项为叶子节点权重的L2范数. 那么在上面那个例子中, 正则项$\Omega$ 为:<br>$$<br>\Omega = 3\gamma + \frac{1}{2}\lambda(4+0.01+1)<br>$$<br>假设用$I_j = \{i|q(x_i)=j\}$ 来表示叶子节点$j$ 的样本集, 其中包含叶子$j$ 的所有样本$x_i$. 将其从$i$ 表示转化为用$j$ 表示的形式:<br>$$<br>\begin{split}<br>\text{obj}^{(t)} &amp; \approx \sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right) \\<br>&amp;=\sum_{i=1}^{n}\left[g_{i} w_{q\left(x_{i}\right)}+\frac{1}{2} h_{i} w_{q\left(x_{i}\right)}^{2}\right]+\gamma T+\gamma \frac{1}{2} \sum_{j=1}^{T} w_{j}^{2} \\<br>&amp;=\sum_{j=1}^{T}\left[\left(\sum_{i \in I_{j}} g_{i}\right) w_{j}+\frac{1}{2}\left(\sum_{i \in I_{j}} h_{i}+\gamma\right) w_{j}^{2}\right]+\gamma T\end{split}<br>$$<br>并定义$G_j = \sum_{i\in I_j} g_i ,\quad H_j = \sum_{i\in I_j} h_i$, 则原目标函数又可以写为:<br>$$<br>\text{obj}^{(t)} = \sum^T_{j=1} [G_jw_j + \frac{1}{2} (H_j+\lambda) w_j^2] +\gamma T<br>$$<br>树的结构如果是已经固定的, $G_j$ 和 $H_j$ 就是可以计算的了. 此时目标函数就是一个关于$w_j$ 的一元二次方程, 直接利用最值公式得解:<br>$$<br>\begin{split}w_j^\ast &amp;= -\frac{G_j}{H_j+\lambda}\\<br>    \text{obj}^\ast &amp;= -\frac{1}{2} \sum_{j=1}^T \frac{G_j^2}{H_j+\lambda} + \gamma T\end{split}<br>$$<br>下面是一个计算$\text{obj}$ 的例子:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/xgbstruct_score.png" style="zoom: 67%;" /><p>分数越小, 结构就越好.</p><h3 id="学习树结构"><a href="#学习树结构" class="headerlink" title="学习树结构"></a>学习树结构</h3><p>优化工作已经基本做完了, 但是还没有提到如何确定一棵树的结构. XGB使用了和CART回归树一样的想法, 遍历所有特征的所有特征划分点, 但使用的目标函数不一样. 利用贪心算法, 不断地枚举不同树的结构, 然后利用打分函数来寻找出一个最优结构的树, 接着加入到模型中, 不断重复. 叶子分裂也是依赖于分裂前后的$\text{obj}$ 而决定的. 分裂后需要检测这次分裂是否会给损失函数带来增益:<br>$$<br>\begin{split}<br>Gain &amp;= \text{obj}_{L+R} - (\text{obj}_L + \text{obj}_R) \\<br>&amp; = [-\frac{1}{2} \frac{(G_L+G_R)^2}{H_L+H_R+\lambda}+\gamma] - [-\frac{1}{2} (\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda})+ 2\gamma ] \\<br>&amp;=\frac{1}{2} \left[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}\right] - \gamma<br>\end{split}<br>$$<br>分裂前后的信息$Gain$ 是如何理解的? $Gain$ 实际上是<strong>分裂后的信息减去了分裂包含的信息</strong>, 也就是目标函数$Obj$ 的相反数. 不分裂则视$L+R$ 为一个整体. 所以才有了$Gain$ 中的第三项.</p><p>如果分裂后$Gain&gt;0$ , 说明这次分裂能够使得目标函数下降. 如果$Gain&lt;0$, 说明分裂不能使得目标函数下降, 这次分裂失败了.</p><p>找到最优分枝点的有效方法其实就是<strong>在排序后的实例上从左到右线性地扫描</strong>, 这样就足以决定按此特征的最佳分割. 以年龄作为特征为例:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/xgb扫描.png" style="zoom: 67%;" /><p>只要求出每一边的$G$ 和$H$ 的和, 然后计算$Gain$, 就能找到分枝点.</p><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>XGB用Python就可以非常简单的使用.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> xgboost <span class="token keyword">as</span> xgb<span class="token comment" spellcheck="true"># read in data</span>dtrain <span class="token operator">=</span> xgb<span class="token punctuation">.</span>DMatrix<span class="token punctuation">(</span><span class="token string">'demo/data/agaricus.txt.train'</span><span class="token punctuation">)</span>dtest <span class="token operator">=</span> xgb<span class="token punctuation">.</span>DMatrix<span class="token punctuation">(</span><span class="token string">'demo/data/agaricus.txt.test'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># specify parameters via map</span>param <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'max_depth'</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'eta'</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'objective'</span><span class="token punctuation">:</span><span class="token string">'binary:logistic'</span> <span class="token punctuation">}</span>num_round <span class="token operator">=</span> <span class="token number">2</span>bst <span class="token operator">=</span> xgb<span class="token punctuation">.</span>train<span class="token punctuation">(</span>param<span class="token punctuation">,</span> dtrain<span class="token punctuation">,</span> num_round<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># make prediction</span>preds <span class="token operator">=</span> bst<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>dtest<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/xgb%E6%8E%A8%E5%AF%BC%E6%8C%87%E7%A4%BA%E5%9B%BE.jpeg" alt=""></p><ul><li><a href="https://xgboost.readthedocs.io/" target="_blank" rel="noopener">XGBoost官方文档</a> - 强推</li><li><a href="https://arxiv.org/pdf/1603.02754v1.pdf" target="_blank" rel="noopener">XGBoost论文</a></li><li><a href="https://www.yuque.com/agoclover/ml/axgv87" target="_blank" rel="noopener">这篇博客</a>也翻译的非常非常棒, 可以多读一下.</li><li>在知乎高赞中曾经有一篇文章写的不错, 原作者的博客已经无法使用了, 但在CSDN上有人曾<a href="https://blog.csdn.net/zhaojc1995/article/details/83094853" target="_blank" rel="noopener">转载</a>过.</li><li><a href="https://mp.weixin.qq.com/s?__biz=MzI1MzY0MzE4Mg==&mid=2247485159&idx=1&sn=d429aac8370ca5127e1e786995d4e8ec&chksm=e9d01626dea79f30043ab80652c4a859760c1ebc0d602e58e13490bf525ad7608a9610495b3d&scene=21" target="_blank" rel="noopener">20道XGB面试题</a></li><li><a href="[https://yuanxiaosc.github.io/2019/09/30/XGBoost%E7%9A%84%E5%B7%A5%E7%A8%8B%E5%B8%88%E6%89%8B%E5%86%8C/](https://yuanxiaosc.github.io/2019/09/30/XGBoost的工程师手册/)">XGBoost的工程师手册</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 决策树 </tag>
            
            <tag> XGB </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之随机森林</title>
      <link href="/posts/60644.html"/>
      <url>/posts/60644.html</url>
      
        <content type="html"><![CDATA[<h1 id="随机森林-Random-Forest"><a href="#随机森林-Random-Forest" class="headerlink" title="随机森林 Random Forest"></a>随机森林 Random Forest</h1><p>在集成学习中曾经提到过, <code>Bagging + 决策树 = 随机森林</code>. 这点很重要.</p><blockquote><p>Bagging(Bootstrap aggregating)并行训练多个同质弱学习器, 在取数据集时使用Boostrap(自助法). 自助法的意思是指在每次训练新的弱学习器时, 都从数据集中做<strong>放回抽样</strong>, 这样其他训练器在训练时也可能抽到相同的样本.</p></blockquote><p>在随机森林中, 弱学习器由若干个决策树组成. 这若干个决策树是<strong>相互独立</strong>的. 当有新的数据输入随机森林后, 就让森林中的每棵决策树都进行判断, 然后进行<strong>投票</strong>, 这个样本将会被预测为投票数最多的那类. 随机森林必须要充满多样性, 也就是使得基决策树在保证整体正确率的同时, 尽可能的不同. 如果基决策树都一样, 那么森林也就失去了多样性, 失去了意义.</p><h2 id="数据样本扰动"><a href="#数据样本扰动" class="headerlink" title="数据样本扰动"></a>数据样本扰动</h2><p>因为使用了Bagging, 假设原样本集中包含了$m$ 个样本, 那么采样时也从中放回抽样$m$ 次. 在取数据时做放回抽样, 所以每棵树的训练集都是不同的, 并且包含重复的样本. 这样从一定程度上使得每棵基决策树之间存在了差异, 降低了同质化. 这种通过产生不同数据子集训练不同个体学习器的做法称为<strong>数据样本扰动</strong>.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/随机森林.jpg" style="zoom:67%;" /><h2 id="输入属性扰动"><a href="#输入属性扰动" class="headerlink" title="输入属性扰动"></a>输入属性扰动</h2><p>为增强随机森林的随机性, 在选择划分属性时随机森林与决策树有些许不同. 传统的决策树是对当前节点的属性集合(假设有$d$ 个属性)中选取一个最优的属性, 而在随机森林中, 每个基决策树会先从该节点的属性集合中随机选取一个包含$k$ 个<strong>属性子集</strong>, 然后再从子集中选择一个最优属性用于划分. $k$ 引入了随机性的控制程度, 如果$k=d$, 则基决策树的构建等同于传统决策树, 如果$k=1$, 则随机选取一个属性用于划分. 一般情况下推荐$k=\log_2d$. 这种扰动称为<strong>输入属性扰动</strong>, 对于包含大量冗余属性的数据, 这种做法不仅能产生多样性大的个体, 还因为属性减少而节省了时间开销, 同时因为冗余属性多, 减少一部分属性后训练的学习器能力也不会差很多.</p><p>对于随机森林的分类效果来说, 森林中的树的相关性越大, 错误率就越大. 树分类的能力越强, 整体的错误率就越低.</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 集成学习 </tag>
            
            <tag> 随机森林 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之K-means</title>
      <link href="/posts/5309.html"/>
      <url>/posts/5309.html</url>
      
        <content type="html"><![CDATA[<h1 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h1><p>K-means是一种最为常用的<strong>硬聚类</strong>算法. 硬聚类指的是分出的样本必须只隶属于某一个类, 而不是给出隶属某几个类的概率. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kmeans.jpg" style="zoom: 33%;" /><h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><p>对于给定的$k$类, 聚类所得的簇划分$C_k$, 以及样本$\boldsymbol{x}$, K-means 的目标是最小化平方误差:<br>$$<br>E=\sum_{i=1}^{k} \sum_{\boldsymbol{x} \in C_{i}}\mid\mid\boldsymbol{x}-\boldsymbol{\mu}_{i}\mid\mid_{2}^{2}<br>$$<br>其中$\mu_i$是簇$C_i$的中心点(或者称为<strong>质心</strong>):<br>$$<br>\mu_i=\frac{1}{\mid C_i\mid}\sum_{\boldsymbol x \in C_i}\boldsymbol x<br>$$<br>直观来看, 误差在一定程度上刻画了簇内样本围绕簇均值向量的紧密程度, 误差越小簇内的样本相似度越高. 但是因为求解很难, 所以采用贪心算法通过迭代来近似求解. 具体步骤如下:</p><ol><li>从样本集中随机初始化, 随机选择$k$个样本集作为各类(簇)的中心点, $\{\mu_1,\mu_2,\dots, \mu_k\}$.</li><li>计算所有样本点与各个簇中心之间的距离, 然后把每个样本点划入离该点最近的簇中.</li><li>根据簇中已有的样本点, 重新计算簇中心$\mu_k$.</li><li>重复2 - 3, 直到所有的点无法再更新到其他分类或达到最大迭代次数, 算法结束.</li></ol><h2 id="缺陷"><a href="#缺陷" class="headerlink" title="缺陷"></a>缺陷</h2><p>如果按照上述过程进行求解, 对于一些特殊的情况, 它完全可能会陷入<strong>局部最优</strong>, 这取决于初始化中心点的初始位置. 有时候甚至能产生反直觉的聚类效果. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kmeans局部最优.png" style="zoom:50%;" /><p>在上图中, 因为随机初始化是一个局部最优情况, 所以下方的两个簇永远都只隶属于绿色, 上方一个簇被强行拆分成了两个簇. 为了尽可能消除这种情况, sklearn中的初始化非常粗暴, 直接随机初始化了若干组中心点, 看哪个效果比较好就采用哪组的结果. </p><p>另外, <strong>异常值</strong>会使得中心点有很大的变动, 所以K-means对异常值还是非常敏感的. 所以在处理前最好先将异常值去掉.</p><h2 id="改进-K-means"><a href="#改进-K-means" class="headerlink" title="改进 K-means++"></a>改进 K-means++</h2><p>K-means++ 改进了原来的初始化中心点选取方法, 初始聚类中心相互之间应该分得越开越好.</p><ol><li>在数据点随机选取一个样本点为第一个簇中心$C_1$.</li><li>计算剩余样本与所有簇中心的最短距离$D(x_i)=\min [dist(x_i, C_1), dist(x_i, C_2),\dots, dist(x_i, C_k)]$, 使得这个样本点被选为下一个簇中心的概率$P(x)=\frac{D(x)^{2}}{\sum_{x \in X} D(x)^{2}}$. 按照这个概率随机选取一个中心.</li><li>重复步骤2直到选出$k$个簇中心.</li></ol><p>样本点离已有的簇中心距离越远, 概率越大, 就越有可能被选为新的簇中心.</p><h2 id="适用性"><a href="#适用性" class="headerlink" title="适用性"></a>适用性</h2><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/kmeans划分情况.png" style="zoom:67%;" /><p>K-means只考虑簇中心(质心), 而不考虑簇的边缘和其他簇的关系, 所以不善于对狭长数据或质心位置与数据实际位置不着边的数据的聚类.</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 聚类 </tag>
            
            <tag> Kmeans </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>十大排序</title>
      <link href="/posts/62992.html"/>
      <url>/posts/62992.html</url>
      
        <content type="html"><![CDATA[<h1 id="十大排序"><a href="#十大排序" class="headerlink" title="十大排序"></a>十大排序</h1><p>十大排序包括插入排序, 选择排序, 冒泡排序, 归并排序, 希尔排序, 快速排序, 堆排序, 计数排序, 基数排序, 桶排序. </p><h2 id="基本知识"><a href="#基本知识" class="headerlink" title="基本知识"></a>基本知识</h2><p>基本概念:</p><ul><li><strong>稳定性</strong>: 在排序前有两个相同的关键字a和b, 若排序后仍能保证a和b的相对顺序不变(排序前a在b前, 则排序后还是a在b前), 则称为排序算法是稳定的, 否则是不稳定的.</li><li><strong>时间复杂度</strong>: 对排序数据的总的操作次数. 反映当n变化时, 操作次数呈现什么规律.</li><li><strong>空间复杂度</strong>: 是指算法在计算机内执行时所需存储空间的度量, 它也是数据规模n的函数.</li></ul><p>常见的算法排序可以分为两大类:</p><ul><li><strong>比较类排序</strong>: 通过比较来决定元素间的相对次序, 由于其时间复杂度不能突破$O(nlogn)$, 因此也称为非线性时间比较类排序.</li><li><strong>非比较类排序</strong>: 不通过比较来决定元素间的相对次序, 它可以突破基于比较排序的<strong>时间下界</strong>, 以线性时间运行, 因此也称为线性时间非比较类排序.</li></ul><p>比较排序最低的时间复杂度一定是$O(n)$, 你不比较怎么排序嘛, 每个元素比较一次, 最低时间复杂度一定是线性级的.</p><p>比较排序可以分为<strong>交换排序</strong>, <strong>插入排序</strong>, <strong>选择排序</strong>, <strong>归并排序</strong>四大类.</p><ul><li>交换排序: 就是根据序列中两个记录键值的比较结果来对换这两个记录在序列中的位置.<ul><li>冒泡排序</li><li>快速排序</li></ul></li><li>插入排序: 将一个记录插入到已经排好序的有序表中, 逐渐扩大有序表的规模至整个表.<ul><li>简单插入排序</li><li>希尔排序</li></ul></li><li>选择排序: 第一次从待排序的数据元素中选出最小(或最大) 的一个元素, 存放在序列的起始位置, 然后再从剩余的未排序元素中寻找到最小(大) 元素, 然后放到已排序的序列的某个位置(取决于结构).<ul><li>简单选择排序</li><li>堆排序</li></ul></li><li>归并排序: 用分治法, 每次将已有序的子序列合并, 最后得到完全有序的序列.</li></ul><h2 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a>插入排序</h2><p>插入排序我们每个人都会, 在打扑克的时候, 顺牌的过程其实就是多次插入排序的过程. 对于未排序的序列, 在已排序的序列中找到这个关键字的相应位置并插入.</p><ol><li>从第一个元素开始, 该元素可以认为已经被排序; </li><li>取出下一个元素, 在已经排序的元素序列中从后向前扫描; </li><li>如果该元素(已排序) 大于新元素, 将该元素移到下一位置; </li><li>重复步骤3, 直到找到已排序的元素小于或者等于新元素的位置; </li><li>将新元素插入到该位置后; </li><li>重复步骤2 ~ 5.</li></ol><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/DS/插入排序.gif" style="zoom:67%;" /><p>实现代码如下:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token keyword">void</span> <span class="token function">insertSort</span><span class="token punctuation">(</span><span class="token keyword">int</span> a<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">int</span> len<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> i<span class="token punctuation">,</span> j<span class="token punctuation">,</span> temp<span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 认为0号元素已经有序, 从无序序列挨个选择</span>    <span class="token keyword">for</span><span class="token punctuation">(</span>i<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">;</span> i<span class="token operator">&lt;</span>len<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span><span class="token punctuation">{</span>        temp <span class="token operator">=</span> a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>        j <span class="token operator">=</span> i <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// i-1为有序序列 </span>        <span class="token comment" spellcheck="true">// 当选中的元素比有序序列中元素小的时候, 有序序列元素依次后移 </span>        <span class="token keyword">while</span><span class="token punctuation">(</span>j <span class="token operator">>=</span> <span class="token number">0</span> <span class="token operator">&amp;&amp;</span> temp <span class="token operator">&lt;</span> a<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            a<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>            <span class="token operator">--</span>j<span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token comment" spellcheck="true">// 不要忘记插入选中的元素 </span>        a<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span>    <span class="token punctuation">}</span> <span class="token punctuation">}</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="选择排序"><a href="#选择排序" class="headerlink" title="选择排序"></a>选择排序</h2><p>选择排序是非常符合人类思维直觉的一种排序方式, 从序列中选出一个最小的或最大的元素, 加入到已经有序的排序的最左端或最右端. 最后使得整个序列都有序.</p><ol><li>初始状态：无序区为R[1..n], 有序区为空; </li><li>第i趟排序(i=1,2,3…n-1)开始时, 当前有序区和无序区分别为R[1..i-1]和R(i..n) . 该趟排序从当前无序区中-选出关键字最小的记录 R[k], 将它与无序区的第1个记录R交换, 使R[1..i]和R[i+1..n)分别变为记录个数增加1个的新有序区和记录个数减少1个的新无序区; </li><li>n-1趟结束, 数组有序化了.</li></ol><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/DS/选择排序.gif" style="zoom:67%;" /><p>实现代码如下:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token keyword">void</span> <span class="token function">selectSort</span><span class="token punctuation">(</span><span class="token keyword">int</span> a<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">int</span> len<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> i<span class="token punctuation">,</span> j<span class="token punctuation">;</span>    <span class="token keyword">int</span> temp<span class="token punctuation">;</span>    <span class="token keyword">int</span> min<span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 遍历每个元素 当然不包括最后一个元素 因为最后选择的一定是最大的 在最右侧</span>    <span class="token keyword">for</span><span class="token punctuation">(</span>i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> i<span class="token operator">&lt;</span>len<span class="token number">-1</span><span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span><span class="token punctuation">{</span>        min <span class="token operator">=</span> i<span class="token punctuation">;</span>         <span class="token comment" spellcheck="true">// 找到最小元素 </span>        <span class="token keyword">for</span><span class="token punctuation">(</span>j<span class="token operator">=</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">;</span> j<span class="token operator">&lt;</span>len<span class="token punctuation">;</span> <span class="token operator">++</span>j<span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token keyword">if</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">&lt;</span> a<span class="token punctuation">[</span>min<span class="token punctuation">]</span><span class="token punctuation">)</span> min <span class="token operator">=</span> j<span class="token punctuation">;</span>         <span class="token punctuation">}</span>        <span class="token comment" spellcheck="true">// 交换最小的元素和当前选中元素 </span>        temp <span class="token operator">=</span> a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>        a<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>min<span class="token punctuation">]</span><span class="token punctuation">;</span>        a<span class="token punctuation">[</span>min<span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h2><p>冒泡排序是实现起来最最最简单的算法, 三行实现. 它执行的过程就像鱼吐泡泡一样, 一点一点浮上来. 每趟排序都有一个元素停到它的最终位置.</p><ol><li>比较相邻的元素. 如果第一个比第二个大, 就交换它们两个; </li><li>对每一对相邻元素作同样的工作, 从开始第一对到结尾的最后一对, 这样在最后的元素应该会是最大的数; </li><li>针对所有的元素重复以上的步骤, 除了最后一个; </li><li>重复步骤1~3, 直到排序完成. </li></ol><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/DS/冒泡排序.gif" style="zoom:67%;" /><p>实现代码如下:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">// 三行版本 </span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> temp<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> len<span class="token operator">=</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token operator">/</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span> i<span class="token operator">&lt;</span>len<span class="token number">-1</span><span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> j<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> j<span class="token operator">&lt;</span>len<span class="token operator">-</span>i<span class="token number">-1</span><span class="token punctuation">;</span> <span class="token operator">++</span>j<span class="token punctuation">)</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token punctuation">;</span> <span class="token punctuation">(</span>temp<span class="token operator">=</span>a<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">></span>a<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>a<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token operator">=</span>a<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> a<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">=</span>temp<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 注: 不能放在函数里! 因为放在函数里传参时sizeof(a)会是一个指针的长度</span><span class="token comment" spellcheck="true">// 除非直接给len数组的长度</span><span class="token comment" spellcheck="true">// 函数版本</span><span class="token keyword">void</span> <span class="token function">bubbleSort</span><span class="token punctuation">(</span><span class="token keyword">int</span> a<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">int</span> len<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> i<span class="token punctuation">,</span> j<span class="token punctuation">;</span>    <span class="token keyword">int</span> temp<span class="token punctuation">;</span>     <span class="token keyword">int</span> flag<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 标识是否发生过交换 </span>    <span class="token comment" spellcheck="true">// 每次都有一个元素到最终位置, 只需要循环n-1次 </span>    <span class="token keyword">for</span><span class="token punctuation">(</span>i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> i<span class="token operator">&lt;</span>len<span class="token number">-1</span><span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span><span class="token punctuation">{</span>        flag <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">//  从头开始冒泡 </span>        <span class="token keyword">for</span><span class="token punctuation">(</span>j<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> j<span class="token operator">&lt;</span>len<span class="token number">-1</span><span class="token operator">-</span>i<span class="token punctuation">;</span> <span class="token operator">++</span>j<span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token keyword">if</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">></span> a<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>                temp <span class="token operator">=</span> a<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>                a<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>                a<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span>                flag <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>             <span class="token punctuation">}</span>        <span class="token punctuation">}</span>        <span class="token comment" spellcheck="true">// 如果没发生交换说明已全部有序 </span>        <span class="token keyword">if</span><span class="token punctuation">(</span>flag <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token keyword">return</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span> <span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h2><p>归并排序建立在归并操作上, 是<strong>分治法</strong>的典型应用. 将已有序的子序列合并, 得到完全有序的序列. </p><ol><li>把长度为n的输入序列分成两个长度为n/2的子序列; </li><li>对这两个子序列分别采用归并排序; </li><li>将两个排序好的子序列合并成一个最终的排序序列. </li></ol><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/DS/归并排序.gif" style="zoom:67%;" /><p>实现代码如下:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">// 归并操作 </span><span class="token comment" spellcheck="true">// 这里写出mid是因为主函数已经计算过了 </span><span class="token keyword">void</span> <span class="token function">merge</span><span class="token punctuation">(</span><span class="token keyword">int</span> a<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">int</span> low<span class="token punctuation">,</span> <span class="token keyword">int</span> mid<span class="token punctuation">,</span> <span class="token keyword">int</span> high<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> i<span class="token punctuation">,</span> j<span class="token punctuation">,</span> k<span class="token punctuation">;</span>    <span class="token keyword">int</span> n1 <span class="token operator">=</span> mid <span class="token operator">-</span> low <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span>     <span class="token keyword">int</span> n2 <span class="token operator">=</span> high <span class="token operator">-</span> mid<span class="token punctuation">;</span>    <span class="token keyword">int</span> L<span class="token punctuation">[</span>n1<span class="token punctuation">]</span><span class="token punctuation">,</span> R<span class="token punctuation">[</span>n2<span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 将原序列分装进两个数组 </span>    <span class="token keyword">for</span><span class="token punctuation">(</span>i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> i<span class="token operator">&lt;</span>n1<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span><span class="token punctuation">{</span>        L<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>low<span class="token operator">+</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>     <span class="token keyword">for</span><span class="token punctuation">(</span>j<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> j<span class="token operator">&lt;</span>n2<span class="token punctuation">;</span> <span class="token operator">++</span>j<span class="token punctuation">)</span><span class="token punctuation">{</span>        R<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>mid<span class="token operator">+</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    i <span class="token operator">=</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>    k <span class="token operator">=</span> low<span class="token punctuation">;</span>     <span class="token comment" spellcheck="true">// 开始比较归并</span>    <span class="token keyword">while</span><span class="token punctuation">(</span>i <span class="token operator">&lt;</span> n1 <span class="token operator">&amp;&amp;</span> j <span class="token operator">&lt;</span> n2<span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>L<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">&lt;</span> R<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span> a<span class="token punctuation">[</span>k<span class="token operator">++</span><span class="token punctuation">]</span> <span class="token operator">=</span> L<span class="token punctuation">[</span>i<span class="token operator">++</span><span class="token punctuation">]</span><span class="token punctuation">;</span>        <span class="token keyword">else</span> a<span class="token punctuation">[</span>k<span class="token operator">++</span><span class="token punctuation">]</span> <span class="token operator">=</span> R<span class="token punctuation">[</span>j<span class="token operator">++</span><span class="token punctuation">]</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// 如果还有序列没有装完</span>    <span class="token keyword">while</span><span class="token punctuation">(</span>i <span class="token operator">&lt;</span> n1<span class="token punctuation">)</span> a<span class="token punctuation">[</span>k<span class="token operator">++</span><span class="token punctuation">]</span> <span class="token operator">=</span> L<span class="token punctuation">[</span>i<span class="token operator">++</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span>j <span class="token operator">&lt;</span> n2<span class="token punctuation">)</span> a<span class="token punctuation">[</span>k<span class="token operator">++</span><span class="token punctuation">]</span> <span class="token operator">=</span> R<span class="token punctuation">[</span>j<span class="token operator">++</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">// 划分操作 把序列按递归划分为多个分组 </span><span class="token keyword">void</span> <span class="token function">mergeSort</span><span class="token punctuation">(</span><span class="token keyword">int</span> a<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">int</span> low<span class="token punctuation">,</span> <span class="token keyword">int</span> high<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>low <span class="token operator">&lt;</span> high<span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">int</span> mid <span class="token operator">=</span> <span class="token punctuation">(</span>low <span class="token operator">+</span> high<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">;</span>        <span class="token function">mergeSort</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> mid<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> high<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">mergeSort</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> low<span class="token punctuation">,</span> mid<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">merge</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> low<span class="token punctuation">,</span> mid<span class="token punctuation">,</span> high<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="希尔排序"><a href="#希尔排序" class="headerlink" title="希尔排序"></a>希尔排序</h2><p>虽然希尔排序基本上不怎么考虑实现, 但不代表它不重要. 希尔排序是第一个突破$O(n^2)$的算法, 是简单插入排序的改良版. 与插入排序的不同之处在于, 它会优先比较距离较远的元素. 希尔排序又叫<strong>缩小增量排序</strong>. 先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序.</p><ol><li>选择一个增量序列t1, t2, …, tk, 其中ti&gt;tj, tk=1; </li><li>按增量序列个数k, 对序列进行k 趟排序; </li><li>每趟排序, 根据对应的增量ti, 将待排序列分割成若干长度为m 的子序列, 分别对各子表进行直接插入排序. 仅增量因子为1 时, 整个序列作为一个表来处理, 表长度即为整个序列的长度. </li></ol><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/希尔排序.gif" style="zoom:67%;" /><p>实现代码如下:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token keyword">void</span> <span class="token function">shellSort</span><span class="token punctuation">(</span><span class="token keyword">int</span> arr<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">int</span> n<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> temp<span class="token punctuation">;</span>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> gap<span class="token operator">=</span>n<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">;</span> gap<span class="token operator">></span><span class="token number">0</span><span class="token punctuation">;</span> gap<span class="token operator">/</span><span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">{</span> <span class="token comment" spellcheck="true">// 每趟排序gap是原来的1/2倍 </span>        <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span>gap<span class="token punctuation">;</span> i<span class="token operator">&lt;</span>n<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span><span class="token punctuation">{</span> <span class="token comment" spellcheck="true">// 按照gap循环子序列 </span>            <span class="token comment" spellcheck="true">// 直接插入排序 每次都只比较j和j-gap这两个元素 与直插排序一样 </span>            temp <span class="token operator">=</span> arr<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>            <span class="token keyword">int</span> j<span class="token punctuation">;</span>             <span class="token keyword">for</span><span class="token punctuation">(</span>j<span class="token operator">=</span>i<span class="token punctuation">;</span> j <span class="token operator">>=</span> gap <span class="token operator">&amp;&amp;</span> arr<span class="token punctuation">[</span>j<span class="token operator">-</span>gap<span class="token punctuation">]</span> <span class="token operator">></span> temp<span class="token punctuation">;</span> j<span class="token operator">-</span><span class="token operator">=</span>gap<span class="token punctuation">)</span><span class="token punctuation">{</span>                  arr<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> arr<span class="token punctuation">[</span>j<span class="token operator">-</span>gap<span class="token punctuation">]</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>             <span class="token comment" spellcheck="true">// 如果不小于了 在j处将temp元素插入 </span>            arr<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span> <span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h2><p>名气第一名, 所有场景都通用. <strong>快排是必须会写的排序</strong>. 快排是冒泡排序的优化版, 它每次的交换不再像冒泡排序是交换相邻元素, 而是跳跃交换. 通过一趟排序将待排记录分隔成独立的两部分, 其中一部分记录的关键字均比另一部分的关键字小, 则可分别对这两部分记录继续进行排序, 以达到整个序列有序.</p><ol><li>从数列中挑出一个元素, 称为 “基准”(pivot) ; </li><li>重新排序数列, 所有元素比基准值小的摆放在基准前面, 所有元素比基准值大的摆在基准的后面(相同的数可以到任一边) . 在这个分区退出之后, 该基准就处于数列的中间位置. 这个称为分区(partition) 操作; </li><li>递归地(recursive) 把小于基准值元素的子数列和大于基准值元素的子数列排序. </li></ol><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/DS/快速排序.gif" style="zoom:67%;" /><p>实现代码如下:</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token keyword">void</span> <span class="token function">quickSortLR</span><span class="token punctuation">(</span><span class="token keyword">int</span> a<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">int</span> left<span class="token punctuation">,</span> <span class="token keyword">int</span> right<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> i <span class="token operator">=</span> left<span class="token punctuation">,</span> j <span class="token operator">=</span> right<span class="token punctuation">;</span>    <span class="token keyword">int</span> temp<span class="token punctuation">;</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>left <span class="token operator">&lt;</span> right<span class="token punctuation">)</span><span class="token punctuation">{</span>        temp <span class="token operator">=</span> a<span class="token punctuation">[</span>left<span class="token punctuation">]</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">// 无时不刻都要加上i &lt; j的条件</span>        <span class="token comment" spellcheck="true">// 指针相遇时应该立马停下 </span>        <span class="token keyword">while</span><span class="token punctuation">(</span>i <span class="token operator">&lt;</span> j<span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token comment" spellcheck="true">// 从右侧找一个比基准小的元素 </span>            <span class="token keyword">while</span><span class="token punctuation">(</span>i <span class="token operator">&lt;</span> j <span class="token operator">&amp;&amp;</span> a<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">>=</span> temp<span class="token punctuation">)</span><span class="token punctuation">{</span>                <span class="token operator">--</span>j<span class="token punctuation">;</span>            <span class="token punctuation">}</span>             <span class="token comment" spellcheck="true">// 直接扣到左指针上 </span>            <span class="token keyword">if</span><span class="token punctuation">(</span>i <span class="token operator">&lt;</span> j<span class="token punctuation">)</span><span class="token punctuation">{</span>                a<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>                <span class="token operator">++</span>i<span class="token punctuation">;</span>            <span class="token punctuation">}</span>            <span class="token comment" spellcheck="true">// 从左侧找一个比基准大的元素 </span>            <span class="token keyword">while</span><span class="token punctuation">(</span>i <span class="token operator">&lt;</span> j <span class="token operator">&amp;&amp;</span> a<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">&lt;=</span> temp<span class="token punctuation">)</span><span class="token punctuation">{</span>                <span class="token operator">++</span>i<span class="token punctuation">;</span>            <span class="token punctuation">}</span>            <span class="token comment" spellcheck="true">// 扣到右指针上 </span>            <span class="token keyword">if</span><span class="token punctuation">(</span>i <span class="token operator">&lt;</span> j<span class="token punctuation">)</span><span class="token punctuation">{</span>                a<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>                <span class="token operator">--</span>j<span class="token punctuation">;</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>        <span class="token comment" spellcheck="true">// 二者相遇时就是最终基准位置</span>        a<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span>        <span class="token function">quickSortLR</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> left<span class="token punctuation">,</span> i<span class="token number">-1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">quickSortLR</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> right<span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token punctuation">}</span><span class="token punctuation">}</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>上面代码实现是演示的左右交换版本, 实际上还有一种快慢指针的版本.</p><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">// 快慢指针版本 参考了豆豆的快排实现</span><span class="token comment" spellcheck="true">// bilibili:BV1Ab411s7To </span><span class="token comment" spellcheck="true">// 其实就是快慢指针交换比pivot小和比pivot大的过程 </span><span class="token comment" spellcheck="true">// 假定传进来的都是下标 </span><span class="token keyword">int</span> <span class="token function">partition</span><span class="token punctuation">(</span><span class="token keyword">int</span> a<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">int</span> left<span class="token punctuation">,</span> <span class="token keyword">int</span> right<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> i <span class="token operator">=</span> left <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 慢指针 并初始化为left左侧 </span>    <span class="token keyword">int</span> j<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 快指针 </span>    <span class="token keyword">int</span> temp<span class="token punctuation">;</span>    <span class="token keyword">int</span> pivot <span class="token operator">=</span> right<span class="token punctuation">;</span>  <span class="token comment" spellcheck="true">// 指定最右边的元素为基准 </span>    <span class="token comment" spellcheck="true">/*         若快指针所指元素比基准元素大        则慢指针什么都不做         若快指针所指向的元素比基准元素小        则让慢指针往前进一位 并交换快慢指针所指元素         因为只有快指针所指元素比基准元素小才让慢指针前进         所以在交换前慢指针指向的元素一定比基准元素大    */</span>     <span class="token keyword">for</span><span class="token punctuation">(</span>j<span class="token operator">=</span>left<span class="token punctuation">;</span> j<span class="token operator">&lt;</span>right<span class="token punctuation">;</span> <span class="token operator">++</span>j<span class="token punctuation">)</span><span class="token punctuation">{</span>         <span class="token keyword">if</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">&lt;</span> a<span class="token punctuation">[</span>pivot<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token operator">++</span>i<span class="token punctuation">;</span>            temp <span class="token operator">=</span> a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>            a<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>            a<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span>          <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// 慢指针i在内的所有左侧元素都比基准元素小</span>    <span class="token comment" spellcheck="true">// 所以交换i右侧的元素和基准元素的位置 </span>    temp <span class="token operator">=</span> a<span class="token punctuation">[</span>pivot<span class="token punctuation">]</span><span class="token punctuation">;</span>    a<span class="token punctuation">[</span>pivot<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    a<span class="token punctuation">[</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 返回的基准元素就是未交换时的a[pivot], 即基准元素 </span>    <span class="token keyword">return</span> i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">void</span> <span class="token function">quickSort</span><span class="token punctuation">(</span><span class="token keyword">int</span> a<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">int</span> left<span class="token punctuation">,</span> <span class="token keyword">int</span> right<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>left <span class="token operator">&lt;</span> right<span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">int</span> pivot <span class="token operator">=</span> <span class="token function">partition</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> left<span class="token punctuation">,</span> right<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">// 获取基准点 对基准的前后各使用快排 </span>        <span class="token function">quickSort</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> left<span class="token punctuation">,</span> pivot<span class="token number">-1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">quickSort</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> pivot<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> right<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h2><p>堆排序是利用堆这种数据结构所设计的一种排序算法. 因为堆是一个近似于完全二叉树的结构, 并且满足堆积的性质：即<strong>子结点的键值或索引总是小于(或者大于) 它的父节点</strong>. 在某些特殊的业务场景, 可能需要构建一个堆, 需要进行查找, 这时用堆排序就很合理.</p><ol><li>将初始待排序关键字序列(R1,R2….Rn)构建成大顶堆, 此堆为初始的无序区; </li><li>将堆顶元素R[1]与最后一个元素R[n]交换, 此时得到新的无序区(R1,R2,……Rn-1)和新的有序区(Rn),且满足R[1,2…n-1]&lt;=R[n]; </li><li>由于交换后新的堆顶R[1]可能违反堆的性质, 因此需要对当前无序区(R1,R2,……Rn-1)调整为新堆, 然后再次将R[1]与无序区最后一个元素交换, 得到新的无序区(R1,R2….Rn-2)和新的有序区(Rn-1,Rn). 不断重复此过程直到有序区的元素个数为n-1, 则整个排序过程完成. </li></ol><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/堆排序.gif" style="zoom:67%;" /><pre class="line-numbers language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">// 堆排序 选择排序 和冒泡排序非常像 </span><span class="token comment" spellcheck="true">// 建堆过程是一个交换的过程  </span><span class="token comment" spellcheck="true">// 直接采用顺序存储结构存储堆(完全二叉树)</span><span class="token keyword">int</span> heap<span class="token punctuation">[</span>maxSize<span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 用它来存堆(大根堆)</span><span class="token comment" spellcheck="true">// 堆调整 low和high选择调整的范围 可以理解为冒泡排序中的一次冒泡 </span><span class="token keyword">void</span> <span class="token function">Sift</span><span class="token punctuation">(</span><span class="token keyword">int</span> heap<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">int</span> low<span class="token punctuation">,</span> <span class="token keyword">int</span> high<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> i<span class="token operator">=</span>low<span class="token punctuation">,</span> j <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">*</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// heap[j]是i的左孩子 </span>    <span class="token keyword">int</span> temp <span class="token operator">=</span> heap<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span>j <span class="token operator">&lt;=</span> high<span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>j <span class="token operator">&lt;</span> high <span class="token operator">&amp;&amp;</span> heap<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">&lt;</span> heap<span class="token punctuation">[</span>j <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span> <span class="token comment" spellcheck="true">// 如果右孩子更大 把j指向右孩子 </span>            <span class="token operator">++</span>j<span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token comment" spellcheck="true">// 当temp比子孙节点中最大的都小, 交换ij, 并将ij顺着树下移一层        </span>        <span class="token keyword">if</span><span class="token punctuation">(</span>temp <span class="token operator">&lt;</span> heap<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            heap<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> heap<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>            i <span class="token operator">=</span> j<span class="token punctuation">;</span>            j <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">*</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span>         <span class="token punctuation">}</span>        <span class="token keyword">else</span> <span class="token keyword">break</span><span class="token punctuation">;</span>     <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// 当j>high时, 将叶子节点调整为temp </span>    heap<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">// 能够看出来整个排序过程和冒泡非常像 只是添加了对堆调整的操作 </span><span class="token keyword">void</span> <span class="token function">heapSort</span><span class="token punctuation">(</span><span class="token keyword">int</span> a<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">int</span> n<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> i<span class="token punctuation">;</span>    <span class="token keyword">int</span> temp<span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 调整为大根堆 </span>    <span class="token keyword">for</span><span class="token punctuation">(</span>i<span class="token operator">=</span>n<span class="token operator">/</span><span class="token number">2</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">;</span> i<span class="token operator">>=</span><span class="token number">0</span><span class="token punctuation">;</span> <span class="token operator">--</span>i<span class="token punctuation">)</span><span class="token punctuation">{</span> <span class="token comment" spellcheck="true">// n/2-1是最后一个非叶子节点 对所有非叶子的堆依次进行调整</span>        <span class="token function">Sift</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> i<span class="token punctuation">,</span> n<span class="token number">-1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token punctuation">}</span>     <span class="token comment" spellcheck="true">// 将根节点和最后一个叶子节点进行交换, 删去原来的根节点, 当i=1即只剩一个节点的时候排序完成 </span>    <span class="token keyword">for</span><span class="token punctuation">(</span>i<span class="token operator">=</span>n<span class="token number">-1</span><span class="token punctuation">;</span> i<span class="token operator">></span><span class="token number">0</span><span class="token punctuation">;</span> <span class="token operator">--</span>i<span class="token punctuation">)</span><span class="token punctuation">{</span> <span class="token comment" spellcheck="true">// 其实i指代的是当前堆有几个节点 </span>        temp <span class="token operator">=</span> a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>        a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> arr<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>        a<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span>         <span class="token function">Sift</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> i<span class="token number">-1</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 每次交换后都进行堆调整 但不要最后一个节点</span>                            <span class="token comment" spellcheck="true">// 因为最后一个节点在调整\之前符合大根堆的定义 </span>                            <span class="token comment" spellcheck="true">// 已经被调到了指定的位置 </span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="计数排序"><a href="#计数排序" class="headerlink" title="计数排序"></a>计数排序</h2><p>计数排序不是基于比较的排序算法, 它的复杂度能到线性级. 效率虽然高, 但使用场景十分受限. 它的原理规定了输入的数据必须是<strong>有确定范围的整数</strong>.</p><ol><li>找出待排序的数组中最大和最小的元素; </li><li>统计数组中每个值为i的元素出现的次数, 存入数组C的第i项; </li><li>对所有的计数累加(从C中的第一个元素开始, 每一项和前一项相加) ; </li><li>反向填充目标数组：将每个元素i放在新数组的第C(i)项, 每放一个元素就将C(i)减去1. </li></ol><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/DS/计数排序.gif" style="zoom:67%;" /><h2 id="基数排序"><a href="#基数排序" class="headerlink" title="基数排序"></a>基数排序</h2><p>“基”这个字, 指的就是十进制的”<strong>个十百千</strong>“. 基数排序先按照个位进行排序, 然后再按照十位进行排序… 先按照低位进行排序, 然后收集到一起, 再按照高位进行排序, 以此类推,  最后就能获得一个有序的序列.</p><ol><li><p>取得数组中的最大数, 并取得位数; </p></li><li><p>arr为原始数组, 从最低位开始取每个位组成radix数组; </p></li><li><p>对radix进行计数排序(利用计数排序适用于小范围数的特点) ; </p></li></ol><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/DS/基数排序.gif" style="zoom:67%;" /><h2 id="桶排序"><a href="#桶排序" class="headerlink" title="桶排序"></a>桶排序</h2><p>桶排序跟计数排序一样, 是一种稳定的线性时间排序算法, 这个在机器学习有相似的应用. 按照特征的取值范围对特征进行划分, 然后抽成独热向量. 这里桶排序工作的原理是将数列分到有限数量的桶里, 每个桶再个别排序. 当要被排序的数组内的数值是均匀分配的时候, 桶排序时间复杂度是线性级.</p><ol><li>设置一个定量的数组当作空桶; </li><li>遍历输入数据, 并且把数据一个一个放到对应的桶里去; </li><li>对每个不是空的桶进行排序; </li><li>从不是空的桶里把排好序的数据拼接起来. </li></ol><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/DS/桶排序.gif" style="zoom:67%;" /><h2 id="复杂度对比"><a href="#复杂度对比" class="headerlink" title="复杂度对比"></a>复杂度对比</h2><table><thead><tr><th><strong>排序算法</strong></th><th><strong>平均时间复杂度</strong></th><th><strong>最好情况</strong></th><th><strong>最坏情况</strong></th><th><strong>空间复杂度</strong></th><th><strong>稳定性</strong></th><th>排序方式</th></tr></thead><tbody><tr><td>冒泡排序</td><td>$O(n^2)$</td><td>$O(n)$</td><td>$O(n^2)$</td><td>$O(1)$</td><td>稳定</td><td>In-place</td></tr><tr><td>选择排序</td><td>$O(n^2)$</td><td>$O(n^2)$</td><td>$O(n^2)$</td><td>$O(1)$</td><td>不稳定</td><td>In-place</td></tr><tr><td>插入排序</td><td>$O(n^2)$</td><td>$O(n)$</td><td>$O(n^2)$</td><td>$O(1)$</td><td>稳定</td><td>In-place</td></tr><tr><td>希尔排序</td><td>$O(n\log n)$</td><td>$O(n^{1.3}$)</td><td>$O(n^2)$</td><td>$O(1)$</td><td>不稳定</td><td>In-place</td></tr><tr><td>归并排序</td><td>$O(n\log n)$</td><td>$O(n\log n)$</td><td>$O(n\log n)$</td><td>$O(n)$</td><td>稳定</td><td>Out-place</td></tr><tr><td>快速排序</td><td>$O(n\log n)$</td><td>$O(n\log n)$</td><td>$O(n^2)$</td><td>$O(\log n)$</td><td>不稳定</td><td>In-place</td></tr><tr><td>堆排序</td><td>$O(n\log n)$</td><td>$O(n\log n)$</td><td>$O(n\log n)$</td><td>$O(1)$</td><td>不稳定</td><td>In-place</td></tr><tr><td>桶排序</td><td>$O(n + k)$</td><td>$O(n + k)$</td><td>$O(n^2)$</td><td>$O(n + k)$</td><td>稳定</td><td>Out-place</td></tr><tr><td>计数排序</td><td>$O(n + k)$</td><td>$O(n + k)$</td><td>$O(n + k)$</td><td>$O(k)$</td><td>稳定</td><td>Out-place</td></tr><tr><td>基数排序</td><td>$O(n × m)$</td><td>$O(n × m)$</td><td>$O(n × m)$</td><td>$O(n + m)$</td><td>稳定</td><td>Out-place</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
            <tag> 排序 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之支持向量机</title>
      <link href="/posts/44188.html"/>
      <url>/posts/44188.html</url>
      
        <content type="html"><![CDATA[<h1 id="支持向量机SVM-Support-Vector-Machine"><a href="#支持向量机SVM-Support-Vector-Machine" class="headerlink" title="支持向量机SVM Support Vector Machine"></a>支持向量机SVM Support Vector Machine</h1><p>SVM是一个监督学习下运作的<strong>线性分类器</strong>. 但是由于核技巧的存在, 使得它本质上成为一个非线性分类器. 因为SVM涉及到很多关于凸优化的内容, 我自己本身不是很了解, 所以尽可能的避开这些内容, 以大致了解工作原理为基准.</p><p>SVM的目标就是寻找一个<strong>最优超平面</strong>, 使得数据能够被这个超平面分开. 超平面这个概念可以由三维和二维进行递推, 在二维空间中的超平面就是一维的, 即一条线, 在三维空间中的超平面是一个二维的平面. 那么以此类推, 在N维空间中的超平面是N-1 维平面.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/svm超平面.png" style="zoom: 50%;" /><p>把超平面定义为:<br>$$<br>f(x)=\operatorname{sign}(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b)<br>$$<br><code>sign</code>的意思是说, 在超平面上面取正号, 下方取负号. 那么很明显$w$ 是法向量, $b$ 是位移项. </p><p>SVM的优化理念是最大化超平面到各类样本之间的距离, 使得这个超平面能够成功分开各类, 并且离这些样本点尽可能的远. 为什么要最大化这个间距呢? 因为在真实数据中的误差可能会干扰实际的预测效果, 间距大意味着更强的<strong>鲁棒性</strong>. 即使加入了一些噪声, 也会因为含有间距而不导致或尽可能少的导致预测错误.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/svm最大化间距.png" style="zoom: 67%;" /><p>把支撑超平面的点(距离超平面最近的样本点)叫做<strong>支持向量</strong>, 也就是图中的超平面旁的实心红色方块和实心蓝色圆. 一般情况下, 最佳超平面一定是到两边支持向量距离相等的那个.</p><h2 id="硬间隔-Hard-Margin"><a href="#硬间隔-Hard-Margin" class="headerlink" title="硬间隔 Hard Margin"></a>硬间隔 Hard Margin</h2><p>硬间隔SVM被转化为一个<strong>寻找最大间隔超平面</strong>的数学问题, 所以Hard Margin SVM也被称为最大间隔分类器.</p><p>样本空间中的样本$x$到超平面的距离$r$ 可以表示为:<br>$$<br>r=\frac{\left|\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\right|}{|\boldsymbol{w}|}<br>$$</p><p>如果超平面能正确分类样本, 则标签$y_i = +1$,对应的范围设为超平面上方, 标签$y_i = -1$,对应的范围设为超平面下方, 即:<br>$$<br>\left\{\begin{array}{ll}<br>\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b &gt; 0, &amp; y_{i}=+1 \\<br>\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b &lt;0, &amp; y_{i}=-1<br>\end{array}\right.<br>$$</p><p>若令间隔为1, 则上述约束更改为:<br>$$<br>\left\{\begin{array}{ll}<br>\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b \geqslant+1, &amp; y_{i}=+1 \\<br>\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b \leqslant-1, &amp; y_{i}=-1<br>\end{array}\right.<br>$$<br>那么相应的两个不同类的支持向量到超平面的距离为:<br>$$<br>\gamma=\frac{2}{|\boldsymbol{w}|}<br>$$<br>如下图所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/svm支持向量.jpg" style="zoom: 33%;" /><p>若想找到最大间隔划分超平面, 其实就是为了找到相应的$w$ 和$b$, 使得间距$\gamma$ 最大.<br>$$<br>\begin{aligned}<br>\max _{\boldsymbol{w}, b} &amp; \frac{2}{|\boldsymbol{w}|} \\<br>\text { s.t. } &amp; y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1, \quad i=1,2, \ldots, m<br>\end{aligned}<br>$$<br>最小化$|\boldsymbol{w}|^{-1}$的目标等同于最大化$|\boldsymbol{w}|^2$, 重写为:<br>$$<br>\begin{aligned}<br>\min _{\boldsymbol{w}, b} &amp; \frac{1}{2}|\boldsymbol{w}|^2 \\<br>\text { s.t. } &amp; y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1, \quad i=1,2, \ldots, m<br>\end{aligned}<br>$$<br>这就是SVM的基本形式.</p><p>其实满足约束的是一个<strong>凸二次规划</strong>问题, 计算量很大. 利用<strong>拉格朗日乘子法</strong>解决其<strong>对偶问题</strong>, 向上述约束添加拉格朗日乘子:<br>$$<br>L(\boldsymbol{w}, b, \boldsymbol{\alpha})=\frac{1}{2}|\boldsymbol{w}|^{2}+\sum_{i=1}^{m} \alpha_{i}\left(1-y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)\right)<br>$$<br>那么对$\boldsymbol{w}$和$b$求偏导, 则得到:<br>$$<br>\boldsymbol{w} =\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i} \quad<br>0 =\sum_{i=1}^{m} \alpha_{i} y_{i}<br>$$<br>带回原式将$\boldsymbol{w}$和$b$消去.<br>$$<br>\begin{aligned}<br>&amp;\max _{\boldsymbol{\alpha}} \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j} \\<br>&amp;\text { s.t. } \sum_{i=1}^{m} \alpha_{i} y_{i}=0 \\<br>&amp; \alpha_{i} \geqslant 0, \quad i=1,2, \ldots, m<br>\end{aligned}<br>$$<br>如果能够解出来$\boldsymbol{\alpha}$ ,  就能解出超平面:<br>$$<br>\begin{aligned}<br>f(\boldsymbol{x}) &amp;=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b \\<br>&amp;=\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}+b<br>\end{aligned}<br>$$<br>解决非线性规划问题需要用到更一般的拉格朗日乘子法, 额外加上KKT(Karush-Kuhn-Tucker)条件:<br>$$<br>\left\{\begin{array}{l}<br>\alpha_{i} \geqslant 0 \\<br>y_{i} f\left(\boldsymbol{x}_{i}\right)-1 \geqslant 0 \\<br>\alpha_{i}\left(y_{i} f\left(\boldsymbol{x}_{i}\right)-1\right)=0<br>\end{array}\right.<br>$$<br>如果$\alpha_{i}&gt;0$, 必有$y_if(\boldsymbol{x_i})=1$, 即该样本点位于最大间隔边界上.</p><p>利用<strong>SMO(Sequential minimal optimization, 序列最小优化算法)</strong>, 求解其中的各个$\alpha_i$. SMO固定了$\alpha_i$以外的其他参数, 每次选取一对需要更新的$\alpha_i$和$\alpha_j$, 利用约束$\sum_{i=1}^{m} \alpha_{i} y_{i}=0$来得到更新后的$\alpha_i$和$\alpha_j$.</p><p>上述部分的数学推导有很多细节为了理解而省略了, 数学底子有些薄弱, 暂时无法细究. 日后会补上. 这部分也是面试时的核心, 尤其是对偶, 拉格朗日乘子法和KKT, SMO.</p><h2 id="核函数-Kernel-Function"><a href="#核函数-Kernel-Function" class="headerlink" title="核函数 Kernel Function"></a>核函数 Kernel Function</h2><p>这么搞一定会有些问题. SVM本来就是一个线性分类器, 假如样本分布真的是线性不可分的呢? 核函数和核技巧使得SVM具有一定的非线性分割能力. 通过核函数, 使得样本从低维映射到高维, 如果样本在高维空间中线性可分, 那么SVM就可以成功将样本分开.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/svm核函数.jpg" style="zoom:67%;" /><p>而核函数的定义实际上也隐式的定义了额外的特征空间, 使得这些特征在空间中线性可分. 所以和函数的选择和结果有着直接的关联, 它也是SVM里最大的变数. 文本数据一般采用线性核, 情况不明时可以先尝试高斯核.</p><h2 id="软间隔-Soft-Margin"><a href="#软间隔-Soft-Margin" class="headerlink" title="软间隔 Soft Margin"></a>软间隔 Soft Margin</h2><p>在现实任务中收集的数据很难确定核函数, 即便找到了某个核函数使得所有训练数据在特征空间中线性可分, 也很难确定这个结果是不是过拟合造成的. 缓解该问题的办法是允许SVM在样本划分时出现一些错误, 引入<strong>软间隔</strong>的概念.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/svm软间隔.jpg" style="zoom:50%;" /><p>与硬间隔相比, 软间隔的优化目标更加的松弛, 体现在对约束条件加入松弛变量.<br>$$<br>\begin{aligned}<br>&amp;y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)\geq 1-\xi_{i}, \quad \xi_{i} \geq 0 \\<br>&amp;\min \sum_{i=1}^m \xi_i<br>\end{aligned}<br>$$<br>那么相应的优化目标也变为:<br>$$<br>\begin{array}{cl}<br>\min\limits_{w, b, \xi_i} &amp; \frac{1}{2}|\boldsymbol{w}|^{2}+C \sum\limits_{i=1}^{n} \xi_{i} \\<br>\text {s.t.} &amp; y_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right) \geq 1-\xi_{i} \\<br>&amp; \xi_{i} \geqslant 0 \quad i=1,2, \cdots, n<br>\end{array}<br>$$<br>松弛变量$\xi_i$ 为可替换的损失函数, 一般使用hinge损失, 对数损失, 指数损失.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/svm损失.jpg" style="zoom:50%;" />]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Markdown公式整理</title>
      <link href="/posts/54490.html"/>
      <url>/posts/54490.html</url>
      
        <content type="html"><![CDATA[<h1 id="Markdown公式整理"><a href="#Markdown公式整理" class="headerlink" title="Markdown公式整理"></a>Markdown公式整理</h1><p>Markdown支持Latex的数学公式简直是太棒了. 但是目前的<code>Mathtype3</code>仍然没有迁移所有的Latex指令过来, 只是支持其中的一部分, 大多数不支持的指令其实都是比较冷门的, 作用不是很大. 感谢<a href="https://www.zybuluo.com/codeep/note/163962" target="_blank" rel="noopener">Cmd Markdown 公式指导手册</a>和<a href="https://www.cnblogs.com/1024th/p/11623258.html" target="_blank" rel="noopener">LaTeX公式手册</a>, 整理时参考了它们. 如果你不想手敲公式, 想直接进行用图片进行识别, 可以上<a href="https://www.latexlive.com/" target="_blank" rel="noopener">妈咪叔的网站</a>或者使用<code>Mathpix</code>直接对公式进行识别, 它甚至能够识别中文, 在写论文和报告时如果有公式依据, 妈咪叔的网站会非常好用.</p><h2 id="公式使用基础"><a href="#公式使用基础" class="headerlink" title="公式使用基础"></a>公式使用基础</h2><h3 id="插入公式"><a href="#插入公式" class="headerlink" title="插入公式"></a>插入公式</h3><p>$\LaTeX$ 的数学公式分为两种, 分别是行内公式和独立公式.</p><p>行内公式可以这样使用:</p><p><code>$ 公式内容 $</code></p><p>行内公式是嵌入在一行文字中的公式, 例如:</p><p><code>$ J_\alpha(x) = \sum_{m=0}^\infty \frac{(-1)^m}{m! \Gamma (m + \alpha + 1)} {\left({ \frac{x}{2} }\right)}^{2m + \alpha} \text {，行内公式示例} $</code></p><p>$ J_\alpha(x) = \sum_{m=0}^\infty \frac{(-1)^m}{m! \Gamma (m + \alpha + 1)} {\left({ \frac{x}{2} }\right)}^{2m + \alpha} \text {，行内公式示例} $</p><p>独立公式可以这样使用:</p><p><code>$$ 公式内容 $$</code></p><p>独立公式可以写多行连续的内容, 适合写一段公式. 例如:</p><p><code>$$ J_\alpha(x) = \sum_{m=0}^\infty \frac{(-1)^m}{m! \Gamma (m + \alpha + 1)} {\left({ \frac{x}{2} }\right)}^{2m + \alpha} \text {，独立公式示例} $$</code><br>$$<br>J_\alpha(x) = \sum_{m=0}^\infty \frac{(-1)^m}{m! \Gamma (m + \alpha + 1)} {\left({ \frac{x}{2} }\right)}^{2m + \alpha} \text {，独立公式示例}<br>$$</p><h3 id="上下标"><a href="#上下标" class="headerlink" title="上下标"></a>上下标</h3><p><code>^</code> 表示上标, <code>_</code> 表示下标. 如果上下标的内容多于一个字符, 需要用 <code>{}</code> 将这些内容括成一个整体. 上下标可以嵌套, 也可以同时使用. </p><p>例如:</p><p><code>$$ x^{y^z}=(1+{\rm e}^x)^{-2xy^w} $$</code></p><p>显示:<br>$$<br>x^{y^z}=(1+{\rm e}^x)^{-2xy^w}<br>$$<br>如果想在左右两边都加上上下标, 可以使用<code>\sideset</code>命令:</p><p><code>$$ \sideset{^1_2}{^3_4}\bigotimes $$</code></p><p>显示:<br>$$<br>\sideset{^1_2}{^3_4}\bigotimes<br>$$</p><p>或者:</p><p><code>$${}_1^2\!X_3^4$$</code><br>$$<br>{}_1^2X_3^4<br>$$</p><h3 id="括号和分隔符"><a href="#括号和分隔符" class="headerlink" title="括号和分隔符"></a>括号和分隔符</h3><p><code>()</code>, <code>[]</code> 和 <code>|</code> 表示符号本身, 使用 <code>\{\}</code> 来表示 <code>{}</code> . 当要显示大号的括号或分隔符时,要用 <code>\left</code> 和 <code>\right</code> .</p><p>例如:</p><p><code>$$ f(x,y,z) = 3y^2z \left( 3+\frac{7x+5}{1+y^2} \right) $$</code><br>$$<br>f(x,y,z) = 3y^2z \left( 3+\frac{7x+5}{1+y^2} \right)<br>$$<br> <code>\left.</code> 或 <code>\right.</code> 能够进行匹配而不显示本身, 例如:</p><p><code>$$ \left. \frac{ {\rm d}u}{ {\rm d}x} \right| _{x=0} $$</code><br>$$<br>\left. \frac{ {\rm d}u}{ {\rm d}x} \right| _{x=0}<br>$$<br><code>$$\left. \frac{a}{b} \right \}$$</code><br>$$<br>\left. \frac{a}{b} \right \}<br>$$<br>下面有些特殊的括号:</p><table><thead><tr><th align="center">显示</th><th align="left">输入</th><th align="center">显示</th><th align="left">输入</th></tr></thead><tbody><tr><td align="center">$\langle$</td><td align="left"><code>\langle</code></td><td align="center">$\rangle$</td><td align="left"><code>\rangle</code></td></tr><tr><td align="center">$\lceil$</td><td align="left"><code>\lceil</code></td><td align="center">$\rceil$</td><td align="left"><code>\rceil</code></td></tr><tr><td align="center">$\lfloor$</td><td align="left"><code>\lfloor</code></td><td align="center">$\rfloor$</td><td align="left"><code>\rfloor</code></td></tr><tr><td align="center">$\lbrace$</td><td align="left"><code>\lbrace</code></td><td align="center">$\rbrace$</td><td align="left"><code>\rbrace</code></td></tr><tr><td align="center">$\lVert$</td><td align="left"><code>\lVert</code></td><td align="center">$\rVert$</td><td align="left"><code>\rVert</code></td></tr></tbody></table><p>可以使用 <code>\big, \Big, \bigg, \Bigg</code> 控制括号的大小, 例如:</p><p><code>$$\Bigg ( \bigg [ \Big \{ \big \langle \left | \| \frac{a}{b} \| \right | \big \rangle \Big \} \bigg ] \Bigg )$$</code></p><p>显示:<br>$$<br>\Bigg ( \bigg [ \Big \{ \big \langle \left | | \frac{a}{b} | \right | \big \rangle \Big \} \bigg ] \Bigg )<br>$$</p><h3 id="分数"><a href="#分数" class="headerlink" title="分数"></a>分数</h3><p>通常使用 <code>\frac {分子} {分母}</code> 命令产生一个分数，分数可嵌套。<br>便捷情况可直接输入 <code>\frac ab</code> 来快速生成一个 。<br>如果分式很复杂，亦可使用 <code>分子 \over 分母</code> 命令，此时分数仅有一层。</p><p>例如:</p><p><code>$$\frac{a-1}{b-1} \quad and \quad {a+1\over b+1}$$</code></p><p>显示:<br>$$<br>\frac{a-1}{b-1} \quad and \quad {a+1\over b+1}<br>$$</p><p>也可以控制分数的大小, 如<code>$$\tfrac{2}{4} = 0.5$$</code>是小型分数:<br>$$<br>\tfrac{2}{4} = 0.5<br>$$<br>同理, <code>\cfrac</code>(连分数使用)和<code>\dfrac</code>都表示大型分数.</p><h3 id="开根号"><a href="#开根号" class="headerlink" title="开根号"></a>开根号</h3><p>使用 <code>\sqrt [根指数，省略时为2] {被开方数}</code> 命令输入开根号. 例如:</p><p><code>$$\surd, \sqrt{2}, \sqrt[n]{}, \sqrt[3]{\frac{x^3+y^3}{2}}$$</code><br>$$<br>\surd \quad \sqrt{2} \quad \sqrt[n]{} \quad \sqrt[3]{\frac{x^3+y^3}{2}}<br>$$</p><h3 id="向量"><a href="#向量" class="headerlink" title="向量"></a>向量</h3><p>使用 <code>\vec{向量}</code> 来自动产生一个向量. 也可以使用 <code>\overrightarrow</code> 等命令自定义字母上方的符号. 例如:</p><p><code>$$\vec{a} \cdot \vec{b}=0$$</code><br>$$<br>\vec{a} \cdot \vec{b}=0<br>$$<br><code>$$\overleftarrow{xy} \quad and \quad \overleftrightarrow{xy} \quad and \quad \overrightarrow{xy}$$</code><br>$$<br>\overleftarrow{xy} \quad and \quad \overleftrightarrow{xy} \quad and \quad \overrightarrow{xy}<br>$$</p><h3 id="导数和微分"><a href="#导数和微分" class="headerlink" title="导数和微分"></a>导数和微分</h3><p>常见的微分符号通过<code>\mathrm{d}{符号}</code>来输入一个常见的微分. 其中的<code>\mathrm</code>是为了美观才加上的, 也可以直接输入<code>d符号</code>. 使用<code>\partial{符号}</code>来获得一个偏导符号, <code>\nabla</code>生成一个梯度符号.</p><p><code>$$dt, \mathrm{d}t, \partial t, \nabla\psi$$</code><br>$$<br>dt, \mathrm{d}t, \partial t, \nabla\psi<br>$$<br><code>$$dy/dx, \mathrm{d}y/\mathrm{d}x, \frac{dy}{dx}, \frac{\mathrm{d}y}{\mathrm{d}x}, \frac{\partial^2}{\partial x_1\partial x_2}y$$</code><br>$$<br>dy/dx, \mathrm{d}y/\mathrm{d}x, \frac{dy}{dx}, \frac{\mathrm{d}y}{\mathrm{d}x}, \frac{\partial^2}{\partial x_1\partial x_2}y<br>$$<br><code>$$\prime, \backprime, f^\prime, f&#39;, f&#39;&#39;, f^{(3)}, \dot y, \ddot y$$</code><br>$$<br>\prime, \backprime, f^\prime, f’, f’’, f^{(3)}, \dot y, \ddot y<br>$$</p><h3 id="积分"><a href="#积分" class="headerlink" title="积分"></a>积分</h3><p>使用 <code>\int_积分下限^积分上限 {被积表达式}</code> 来输入一个积分. 例如:</p><p><code>$$\int_0^1 {x^2} \,{\rm d}x$$</code></p><p>显示:<br>$$<br>\int_0^1 {x^2} \,{\rm d}x<br>$$<br><code>\,</code> 和 <code>{\rm d}</code> 部分可省略, 加入能使式子更美观. <code>{\rm d}</code>可以用<code>\mathrm{d}</code>等价替换.</p><p><code>$$\iint_{D}^{W} \, \mathrm{d}x\,\mathrm{d}y$$</code><br>$$<br>\iint_{D}^{W} \, \mathrm{d}x\,\mathrm{d}y<br>$$<br><code>$$\iiint_{E}^{V} \, \mathrm{d}x\,\mathrm{d}y\,\mathrm{d}z$$</code><br>$$<br>\iiint_{E}^{V} \, \mathrm{d}x\,\mathrm{d}y\,\mathrm{d}z<br>$$<br><code>$$\oint_{C} x^3\, \mathrm{d}x + 4y^2\, \mathrm{d}y$$</code><br>$$<br>\oint_{C} x^3\, \mathrm{d}x + 4y^2\, \mathrm{d}y<br>$$</p><h3 id="极限"><a href="#极限" class="headerlink" title="极限"></a>极限</h3><p>使用 <code>\lim_{变量 \to 表达式} 表达式</code> 来输入一个极限. 如有其他需求, 可以更改 <code>\to</code> 符号至任意符号. 例如:</p><p><code>$$ \lim_{n \to +\infty} \frac{1}{n(n+1)} \quad and \quad \lim_{x\leftarrow{示例}} \frac{1}{n(n+1)} $$</code></p><p>显示:<br>$$<br>\lim_{n \to +\infty} \frac{1}{n(n+1)} \quad and \quad \lim_{x\leftarrow{示例}} \frac{1}{n(n+1)}<br>$$<br>如果在行内的话, 显示会将极限放到下标的位置:</p><p>行内公式示例:</p><p><code>$\lim_{x \to \infty} \frac{1}{n(n+1)}$</code></p><p>$\lim_{x \to \infty} \frac{1}{n(n+1)}$</p><p>可以通过<code>\limits</code>强制加到下方或上方.</p><p><code>$\lim\limits_{x \to \infty} \frac{1}{n(n+1)}$</code></p><p>$\lim\limits_{x \to \infty} \frac{1}{n(n+1)}$</p><p>或者通过<code>\displaystyle</code>强制转为独立公式模式:</p><p><code>$\displaystyle \lim_{x \to \infty} \frac{1}{n(n+1)}$</code></p><p>$\displaystyle \lim_{x \to \infty} \frac{1}{n(n+1)}$</p><h3 id="累加和累乘"><a href="#累加和累乘" class="headerlink" title="累加和累乘"></a>累加和累乘</h3><p>使用 <code>\sum_{下标表达式}^{上标表达式} {累加表达式}</code> 来输入一个累加. 与之类似, 使用 <code>\prod</code> <code>\bigcup</code> <code>\bigcap</code> 来分别输入累乘、并集和交集. 此类符号在行内显示时上下标表达式将会移至右上角和右下角. </p><p>独立公式示例:</p><p><code>$$\sum_{i=1}^n \frac{1}{i^2} \quad and \quad \prod_{i=1}^n \frac{1}{i^2} \quad and \quad \bigcup_{i=1}^{2} R$$</code></p><p>显示:<br>$$<br>\sum_{i=1}^n \frac{1}{i^2} \quad and \quad \prod_{i=1}^n \frac{1}{i^2} \quad and \quad \bigcup_{i=1}^{2} R<br>$$</p><blockquote><p>和极限一样, 行内可以通过<code>\limts</code>将内容强制加到上方或下方.</p></blockquote><h3 id="二项式"><a href="#二项式" class="headerlink" title="二项式"></a>二项式</h3><p>看例子即可.</p><p>二项式系数:</p><p><code>$$\dbinom{n}{r}=\binom{n}{n-r}=\mathrm{C}_n^r=\mathrm{C}_n^{n-r}$$</code><br>$$<br>\dbinom{n}{r}=\binom{n}{n-r}=\mathrm{C}_n^r=\mathrm{C}_n^{n-r}<br>$$<br>小二项式系数:</p><p><code>$$\tbinom{n}{r}=\tbinom{n}{n-r}=\mathrm{C}_n^r=\mathrm{C}_n^{n-r}$$</code><br>$$<br>\tbinom{n}{r}=\tbinom{n}{n-r}=\mathrm{C}_n^r=\mathrm{C}_n^{n-r}<br>$$<br>大型二项式系数:</p><p><code>$$\binom{n}{r}=\dbinom{n}{n-r}=\mathrm{C}_n^r=\mathrm{C}_n^{n-r}$$</code><br>$$<br>\binom{n}{r}=\dbinom{n}{n-r}=\mathrm{C}_n^r=\mathrm{C}_n^{n-r}<br>$$</p><h3 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h3><p>使用<code>\operatorname{函数名}</code>就能将其作为自定义的函数进行使用, 例如:</p><p><code>$$\operatorname{sh}k, \operatorname{ch}l, \operatorname{th}m, \operatorname{coth}n$$</code></p><p>显示:<br>$$<br>\operatorname{sh}k, \operatorname{ch}l, \operatorname{th}m, \operatorname{coth}n<br>$$</p><h3 id="模运算"><a href="#模运算" class="headerlink" title="模运算"></a>模运算</h3><p>直接看例子即可.</p><p><code>$$s_k \equiv 0 \pmod{m}$$</code><br>$$<br>s_k \equiv 0 \pmod{m}<br>$$<br><code>$$a \bmod b$$</code><br>$$<br>a \bmod b<br>$$</p><h3 id="绝对值和范数"><a href="#绝对值和范数" class="headerlink" title="绝对值和范数"></a>绝对值和范数</h3><p>用<code>\left\vert s \right\vert</code>即可给Z两侧加上绝对值.</p><p>$$<br>\left\vert Z \right\vert<br>$$<br>用<code>\lVert s \rVert</code>即可给Z两侧加上范数.<br>$$<br>\lVert Z \rVert<br>$$<br>符号函数<code>\sgn</code>在当前版本中暂不支持.</p><h3 id="连接"><a href="#连接" class="headerlink" title="连接"></a>连接</h3><table><thead><tr><th align="center">显示</th><th align="left">输入</th></tr></thead><tbody><tr><td align="center">$\fbox{a+b+c+d}$</td><td align="left"><code>\fbox{a+b+c+d}</code></td></tr><tr><td align="center">$\overleftarrow{a+b+c+d}$</td><td align="left"><code>\overleftarrow{a+b+c+d}</code></td></tr><tr><td align="center">$\overrightarrow{a+b+c+d}$</td><td align="left"><code>\overrightarrow{a+b+c+d}</code></td></tr><tr><td align="center">$\overleftrightarrow{a+b+c+d}$</td><td align="left"><code>\overleftrightarrow{a+b+c+d}</code></td></tr><tr><td align="center">$\underleftarrow{a+b+c+d}$</td><td align="left"><code>\underleftarrow{a+b+c+d}</code></td></tr><tr><td align="center">$\underrightarrow{a+b+c+d}$</td><td align="left"><code>\underrightarrow{a+b+c+d}</code></td></tr><tr><td align="center">$\underleftrightarrow{a+b+c+d}$</td><td align="left"><code>\underleftrightarrow{a+b+c+d}</code></td></tr><tr><td align="center">$\overline{a+b+c+d}$</td><td align="left"><code>\overline{a+b+c+d}</code></td></tr><tr><td align="center">$\underline{a+b+c+d}$</td><td align="left"><code>\underline{a+b+c+d}</code></td></tr><tr><td align="center">$\overbrace{a+b+c+d}^{Sample}$</td><td align="left"><code>\overbrace{a+b+c+d}^{Sample}</code></td></tr><tr><td align="center">$\underbrace{a+b+c+d}_{Sample}$</td><td align="left"><code>\underbrace{a+b+c+d}_{Sample}</code></td></tr><tr><td align="center">$\overbrace{a+\underbrace{b+c}_{1.0}+d}^{2.0}$</td><td align="left"><code>\overbrace{a+\underbrace{b+c}_{1.0}+d}^{2.0}</code></td></tr><tr><td align="center">$\underbrace{a\cdot a\cdots a}_{b\text{ times}}$</td><td align="left"><code>\underbrace{a\cdot a\cdots a}_{b\text{ times}}</code></td></tr><tr><td align="center">$\underrightarrow{1℃/min}$</td><td align="left"><code>\underrightarrow{1℃/min}</code></td></tr></tbody></table><h3 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h3><p>用<code>\text {文字}</code>来添加方程中的注释文本, 在注释中仍然可以使用<code>$ 公式 $</code>将内容公式化.</p><pre><code>f(n)= \begin{cases}n/2, &amp; \text {if $n$ is even} \\3n+1, &amp;\text{if $n$ is odd}\end{cases} </code></pre><p>显示:<br>$$<br>f(n)= \begin{cases}<br>n/2, &amp; \text {if $n$ is even} \\<br>3n+1, &amp;\text{if $n$ is odd}<br>\end{cases}<br>$$</p><h3 id="空格"><a href="#空格" class="headerlink" title="空格"></a>空格</h3><table><thead><tr><th>效果</th><th>写法</th><th align="center">间距</th><th>名称</th></tr></thead><tbody><tr><td>$\alpha\qquad\beta$</td><td><code>\alpha\qquad\beta</code></td><td align="center">${\displaystyle mm}$</td><td>2 个 quad 空格</td></tr><tr><td>$\alpha \quad \beta$</td><td><code>\alpha\quad\beta</code></td><td align="center">${\displaystyle m}$</td><td>quad 空格</td></tr><tr><td>$\alpha\ \beta$</td><td><code>\alpha\ \beta</code></td><td align="center">${\displaystyle {\frac{m}{3}}}$</td><td>大空格</td></tr><tr><td>$\alpha\;\beta$</td><td><code>\alpha\;\beta</code></td><td align="center">${\displaystyle {\frac {2m}{7}}}$</td><td>中等空格</td></tr><tr><td>$\alpha\,\beta$</td><td><code>\alpha\,\beta</code></td><td align="center">${\displaystyle {\frac {m}{6}}}$</td><td>小空格</td></tr><tr><td>$\alpha\beta$</td><td><code>\alpha\beta</code></td><td align="center">${\displaystyle 0}$</td><td>没有空格</td></tr><tr><td>$\alpha!\beta$</td><td><code>\alpha\!\beta</code></td><td align="center">${\displaystyle {-\frac {m}{6}}}$</td><td>紧贴</td></tr></tbody></table><h3 id="删除线"><a href="#删除线" class="headerlink" title="删除线"></a>删除线</h3><p>使用删除线功能必须声明 <code>$$</code> 符号. </p><p>在公式内使用 <code>\require{cancel}</code> 来允许 <strong>片段删除线</strong> 的显示.<br>声明片段删除线后, 使用 <code>\cancel{字符}</code>、<code>\bcancel{字符}</code>、<code>\xcancel{字符}</code> 和 <code>\cancelto{字符}</code> 来实现各种片段删除线效果. </p><pre><code>$$\require{cancel}\begin{array}{rl}\verb|y+\cancel{x}| &amp; y+\cancel{x}\\\verb|\cancel{y+x}| &amp; \cancel{y+x}\\\verb|y+\bcancel{x}| &amp; y+\bcancel{x}\\\verb|y+\xcancel{x}| &amp; y+\xcancel{x}\\\verb|y+\cancelto{0}{x}| &amp; y+\cancelto{0}{x}\\\verb+\frac{1\cancel9}{\cancel95} = \frac15+&amp; \frac{1\cancel9}{\cancel95} = \frac15 \\\end{array}$$</code></pre><p>可得:<br>$$<br>\require{cancel}<br>\begin{array}{rl}<br>\verb|y+\cancel{x}| &amp; y+\cancel{x}\\<br>\verb|\cancel{y+x}| &amp; \cancel{y+x}\\<br>\verb|y+\bcancel{x}| &amp; y+\bcancel{x}\\<br>\verb|y+\xcancel{x}| &amp; y+\xcancel{x}\\<br>\verb|y+\cancelto{0}{x}| &amp; y+\cancelto{0}{x}\\<br>\verb+\frac{1\cancel9}{\cancel95} = \frac15+&amp; \frac{1\cancel9}{\cancel95} = \frac15 \\<br>\end{array}<br>$$<br>使用 <code>\require{enclose}</code> 来允许 <strong>整段删除线</strong> 的显示.<br>声明整段删除线后, 使用 <code>\enclose{删除线效果}{字符}</code> 来实现各种整段删除线效果.<br>其中, 删除线效果有 <code>horizontalstrike</code>、<code>verticalstrike</code>、<code>updiagonalstrike</code> 和 <code>downdiagonalstrike</code>, 可叠加使用. </p><h2 id="使用参考"><a href="#使用参考" class="headerlink" title="使用参考"></a>使用参考</h2><h3 id="大括号和行标"><a href="#大括号和行标" class="headerlink" title="大括号和行标"></a>大括号和行标</h3><p>使用 <code>\left</code> 和 <code>\right</code> 来创建自动匹配高度的 (圆括号), [方括号] 和 {花括号} .  在每个公式末尾前使用 <code>\tag{行标}</code> 来实现行标. 例如:</p><pre><code>$$f\left(   \left[      \frac{       1+\left\{x,y\right\}     }{       \left(          \frac{x}{y}+\frac{y}{x}       \right)       \left(u+1\right)     }+a   \right]^{3/2}\right)\tag{行标}$$</code></pre><p>显示:<br>$$<br>f\left(<br>   \left[<br>     \frac{<br>       1+\left\{x,y\right\}<br>     }{<br>       \left(<br>          \frac{x}{y}+\frac{y}{x}<br>       \right)<br>       \left(u+1\right)<br>     }+a<br>   \right]^{3/2}<br>\right)<br>\tag{行标}<br>$$<br>如果你需要在不同的行显示对应括号, 可以在每一行对应处使用 <code>\left.</code> 或 <code>\right.</code> 来放一个”影子”括号: </p><pre><code>\begin{aligned}a=&amp;\left(1+2+3+  \cdots \right. \\&amp; \cdots+ \left. \infty-2+\infty-1+\infty\right)\end{aligned}</code></pre><p>显示:<br>$$<br>\begin{aligned}<br>a=&amp;\left(1+2+3+  \cdots \right. \\<br>&amp; \cdots+ \left. \infty-2+\infty-1+\infty\right)<br>\end{aligned}<br>$$<br>如果你需要将行内显示的分隔符也变大, 可以使用 <code>\middle</code> :</p><pre><code>\left\langle    q\middle\|  \frac{\frac{x}{y}}{\frac{u}{v}}\middle|    p \right\rangle</code></pre><p>显示:<br>$$<br>\left\langle<br>  q<br>\middle|<br>  \frac{\frac{x}{y}}{\frac{u}{v}}<br>\middle|<br>   p<br>\right\rangle<br>$$</p><h3 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h3><h4 id="无框矩阵"><a href="#无框矩阵" class="headerlink" title="无框矩阵"></a>无框矩阵</h4><p>在开头使用 <code>begin{matrix}</code>, 在结尾使用 <code>end{matrix}</code>, 在中间插入矩阵元素, 每个元素之间插入 <code>&amp;</code> , 并在每行结尾处使用 <code>\\</code> .<br>使用矩阵时必须声明 <code>$</code> 或 <code>$$</code> 符号. </p><p>例如:</p><pre><code>$$        \begin{matrix}        1 &amp; x &amp; x^2 \\        1 &amp; y &amp; y^2 \\        1 &amp; z &amp; z^2 \\        \end{matrix}$$</code></pre><p>显示:<br>$$<br>\begin{matrix}<br>        1 &amp; x &amp; x^2 \\<br>        1 &amp; y &amp; y^2 \\<br>        1 &amp; z &amp; z^2 \\<br> \end{matrix}<br>$$</p><h4 id="边框矩阵"><a href="#边框矩阵" class="headerlink" title="边框矩阵"></a>边框矩阵</h4><p>在开头将 <code>matrix</code> 替换为 <code>pmatrix</code> <code>bmatrix</code> <code>Bmatrix</code> <code>vmatrix</code> <code>Vmatrix</code>, 就能获得不同样式的带框矩阵.</p><p>例如:</p><pre><code>$ \begin{matrix} 1 &amp; 2 \\ 3 &amp; 4 \\ \end{matrix} $$ \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ \end{pmatrix} $$ \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ \end{bmatrix} $$ \begin{Bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ \end{Bmatrix} $$ \begin{vmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ \end{vmatrix} $$ \begin{Vmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ \end{Vmatrix} $</code></pre><p>显示:</p><table><thead><tr><th align="center">matrix</th><th align="center">pmatrix</th><th align="center">bmatrix</th><th align="center">Bmatrix</th><th align="center">vmatrix</th><th align="center">Vmatrix</th></tr></thead><tbody><tr><td align="center">$ \begin{matrix} 1 &amp; 2 \\\ 3 &amp; 4  \end{matrix} $</td><td align="center">$ \begin{pmatrix} 1 &amp; 2 \\\ 3 &amp; 4  \end{pmatrix} $</td><td align="center">$ \begin{bmatrix} 1 &amp; 2 \\\ 3 &amp; 4  \end{bmatrix} $</td><td align="center">$ \begin{Bmatrix} 1 &amp; 2 \\\ 3 &amp; 4  \end{Bmatrix} $</td><td align="center">$ \begin{vmatrix} 1 &amp; 2 \\\ 3 &amp; 4  \end{vmatrix} $</td><td align="center">$ \begin{Vmatrix} 1 &amp; 2 \\\ 3 &amp; 4  \end{Vmatrix} $</td></tr></tbody></table><h4 id="带省略号的矩阵"><a href="#带省略号的矩阵" class="headerlink" title="带省略号的矩阵"></a>带省略号的矩阵</h4><p>使用 <code>\cdots</code> $\cdots$ , <code>\ddots</code> $\ddots$ , <code>\vdots</code> $\vdots$ 来输入省略符号. </p><p>例如:</p><pre><code>$$        \begin{pmatrix}        1 &amp; a_1 &amp; a_1^2 &amp; \cdots &amp; a_1^n \\        1 &amp; a_2 &amp; a_2^2 &amp; \cdots &amp; a_2^n \\        \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\        1 &amp; a_m &amp; a_m^2 &amp; \cdots &amp; a_m^n \\        \end{pmatrix}$$</code></pre><p>得到:<br>$$<br>\begin{pmatrix}<br>        1 &amp; a_1 &amp; a_1^2 &amp; \cdots &amp; a_1^n \\<br>        1 &amp; a_2 &amp; a_2^2 &amp; \cdots &amp; a_2^n \\<br>        \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>        1 &amp; a_m &amp; a_m^2 &amp; \cdots &amp; a_m^n<br> \end{pmatrix}<br>$$</p><h4 id="带分割符号的矩阵"><a href="#带分割符号的矩阵" class="headerlink" title="带分割符号的矩阵"></a>带分割符号的矩阵</h4><p>利用数组或表格的排版进行分割.</p><pre><code>$$\left[    \begin{array}{cc|c}      1&amp;2&amp;3\\      4&amp;5&amp;6    \end{array}\right]$$</code></pre><p>其中 <code>cc|c</code> 代表在一个三列矩阵中的第二和第三列之间插入分割线. </p><p>显示:<br>$$<br>\left[<br>    \begin{array}{cc|c}<br>      1&amp;2&amp;3\\<br>      4&amp;5&amp;6<br>    \end{array}<br>\right]<br>$$</p><h4 id="行中矩阵"><a href="#行中矩阵" class="headerlink" title="行中矩阵"></a>行中矩阵</h4><p>例如:</p><p><code>$\bigl( \begin{smallmatrix} a &amp; b \\ c &amp; d \end{smallmatrix} \bigr)$</code></p><p>显示$\bigl( \begin{smallmatrix} a &amp; b \\\ c &amp; d \end{smallmatrix} \bigr)$.</p><h3 id="方程组"><a href="#方程组" class="headerlink" title="方程组"></a>方程组</h3><h4 id="条件表达式"><a href="#条件表达式" class="headerlink" title="条件表达式"></a>条件表达式</h4><p>使用 <code>begin{cases}</code> 来创造一组条件表达式, 在每一行条件中插入 <code>&amp;</code> 来指定需要对齐的内容, 并在每一行结尾处使用 <code>\\</code>, 以 <code>end{cases}</code> 结束. </p><pre><code>$$        f(n) =        \begin{cases}        n/2,  &amp; \text{if $n$ is even} \\        3n+1, &amp; \text{if $n$ is odd}        \end{cases}$$</code></pre><p>显示:<br>$$<br>f(n) =<br>        \begin{cases}<br>        n/2,  &amp; \text{if $n$ is even} \\<br>        3n+1, &amp; \text{if $n$ is odd}<br>        \end{cases}<br>$$<br>如果想要让条件表达式变为左对齐显示, 可以使用如下方式:</p><pre><code>$$        \left.        \begin{array}{l}        \text{if $n$ is even:}&amp;n/2\\        \text{if $n$ is odd:}&amp;3n+1        \end{array}        \right\}        =f(n)$$</code></pre><p>显示:<br>$$<br>\left.<br>        \begin{array}{l}<br>        \text{if $n$ is even:}&amp;n/2\\<br>        \text{if $n$ is odd:}&amp;3n+1<br>        \end{array}<br>        \right\}<br>        =f(n)<br>$$<br>在换行时可以用<code>\\[?ex]</code>使得其适配行高<code>?</code>. <em><em>一个 <code>[ex]</code> 指一个 “X-Height”, 即x字母高度. 可以根据情况指定多个 <code>[ex]</code>, 如 <code>[3ex]</code>、<code>[4ex]</code> 等. </em></em> 其实可以在任何地方使用 <code>\\[2ex]</code> 语句, 只要你觉得合适. </p><h4 id="输入方程组"><a href="#输入方程组" class="headerlink" title="输入方程组"></a>输入方程组</h4><p>方程组不光可以通过条件表达式的<code>\begin{cases}…\end{cases}</code>实现, 还可以使用 <code>\begin{array}…\end{array}</code> 和 <code>\left\{…\right.</code> 来创建一个方程组. 例如:</p><pre><code>\left\{ \begin{array}{c}a_1x+b_1y+c_1z=d_1 \\ a_2x+b_2y+c_2z=d_2 \\ a_3x+b_3y+c_3z=d_3\end{array}\right. </code></pre><p>$$<br>\left\{<br>\begin{array}{c}<br>a_1x+b_1y+c_1z=d_1 \\<br>a_2x+b_2y+c_2z=d_2 \\<br>a_3x+b_3y+c_3z=d_3<br>\end{array}<br>\right.<br>$$</p><h4 id="对齐方程组"><a href="#对齐方程组" class="headerlink" title="对齐方程组"></a>对齐方程组</h4><p>使用 <code>\begin{aligned}…\end{aligned}</code>获得一列整齐且居中的方程式序列. 通过<code>&amp;</code>来控制方程对齐的位置.</p><p>例如:</p><pre><code>\begin{aligned}f(x) &amp; = (m+n)^2 \\     &amp; = m^2+2mn+n^2 \\\end{aligned}</code></pre><p>显示:<br>$$<br>\begin{aligned}<br>f(x) &amp; = (m+n)^2 \\<br>     &amp; = m^2+2mn+n^2 \\<br>\end{aligned}<br>$$</p><h3 id="数组和表格"><a href="#数组和表格" class="headerlink" title="数组和表格"></a>数组和表格</h3><p>通常, 一个格式化后的表格比单纯的文字或排版后的文字更具有可读性. 数组和表格均以 <code>begin{array}</code> 开头, 并在其后定义列数及每一列的文本对齐属性, <code>c</code> <code>l</code> <code>r</code> 分别代表居中、左对齐及右对齐. 若需要插入垂直分割线, 在定义式中插入 <code>|</code> , 若要插入水平分割线, 在下一行输入前插入 <code>\hline</code> . 与矩阵相似, 每行元素间均须要插入 <code>&amp;</code> , 每行元素以 <code>\\</code> 结尾, 最后以 <code>end{array}</code> 结束数组. </p><p>例如:</p><pre><code>$$\begin{array}{c|lcr}n &amp; \text{左对齐} &amp; \text{居中对齐} &amp; \text{右对齐} \\\hline1 &amp; 0.24 &amp; 1 &amp; 125 \\2 &amp; -1 &amp; 189 &amp; -8 \\3 &amp; -20 &amp; 2000 &amp; 1+10i\end{array}$$</code></pre><p>显示:<br>$$<br>\begin{array}{c|lcr}<br>n &amp; \text{左对齐} &amp; \text{居中对齐} &amp; \text{右对齐} \\<br>\hline<br>1 &amp; 0.24 &amp; 1 &amp; 125 \\<br>2 &amp; -1 &amp; 189 &amp; -8 \\<br>3 &amp; -20 &amp; 2000 &amp; 1+10i<br>\end{array}<br>$$</p><h4 id="嵌套"><a href="#嵌套" class="headerlink" title="嵌套"></a>嵌套</h4><p>多个数组/表格可 <strong>互相嵌套</strong> 并组成一组数组/一组表格. 使用嵌套前必须声明 <code>$$</code> 符号.  例如:</p><pre><code>$$% outer vertical array of arrays 外层垂直表格\begin{array}{c}    % inner horizontal array of arrays 内层水平表格    \begin{array}{cc}        % inner array of minimum values 内层&quot;最小值&quot;数组        \begin{array}{c|cccc}        \text{min} &amp; 0 &amp; 1 &amp; 2 &amp; 3\\        \hline        0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\        1 &amp; 0 &amp; 1 &amp; 1 &amp; 1\\        2 &amp; 0 &amp; 1 &amp; 2 &amp; 2\\        3 &amp; 0 &amp; 1 &amp; 2 &amp; 3        \end{array}    &amp;        % inner array of maximum values 内层&quot;最大值&quot;数组        \begin{array}{c|cccc}        \text{max}&amp;0&amp;1&amp;2&amp;3\\        \hline        0 &amp; 0 &amp; 1 &amp; 2 &amp; 3\\        1 &amp; 1 &amp; 1 &amp; 2 &amp; 3\\        2 &amp; 2 &amp; 2 &amp; 2 &amp; 3\\        3 &amp; 3 &amp; 3 &amp; 3 &amp; 3        \end{array}    \end{array}    % 内层第一行表格组结束    \\    % inner array of delta values 内层第二行Delta值数组        \begin{array}{c|cccc}        \Delta&amp;0&amp;1&amp;2&amp;3\\        \hline        0 &amp; 0 &amp; 1 &amp; 2 &amp; 3\\        1 &amp; 1 &amp; 0 &amp; 1 &amp; 2\\        2 &amp; 2 &amp; 1 &amp; 0 &amp; 1\\        3 &amp; 3 &amp; 2 &amp; 1 &amp; 0        \end{array}        % 内层第二行表格组结束\end{array}$$</code></pre><p>显示:<br>$$<br>\begin{array}{c}<br>    \begin{array}{cc}<br>        \begin{array}{c|cccc}<br>        \text{min} &amp; 0 &amp; 1 &amp; 2 &amp; 3\\<br>        \hline<br>        0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\<br>        1 &amp; 0 &amp; 1 &amp; 1 &amp; 1\\<br>        2 &amp; 0 &amp; 1 &amp; 2 &amp; 2\\<br>        3 &amp; 0 &amp; 1 &amp; 2 &amp; 3<br>        \end{array}<br>    &amp;<br>        \begin{array}{c|cccc}<br>        \text{max}&amp;0&amp;1&amp;2&amp;3\\<br>        \hline<br>        0 &amp; 0 &amp; 1 &amp; 2 &amp; 3\\<br>        1 &amp; 1 &amp; 1 &amp; 2 &amp; 3\\<br>        2 &amp; 2 &amp; 2 &amp; 2 &amp; 3\\<br>        3 &amp; 3 &amp; 3 &amp; 3 &amp; 3<br>        \end{array}<br>    \end{array}<br>    \\<br>        \begin{array}{c|cccc}<br>        \Delta&amp;0&amp;1&amp;2&amp;3\\<br>        \hline<br>        0 &amp; 0 &amp; 1 &amp; 2 &amp; 3\\<br>        1 &amp; 1 &amp; 0 &amp; 1 &amp; 2\\<br>        2 &amp; 2 &amp; 1 &amp; 0 &amp; 1\\<br>        3 &amp; 3 &amp; 2 &amp; 1 &amp; 0<br>        \end{array}<br>        % 内层第二行表格组结束<br>\end{array}<br>$$</p><h3 id="连分数"><a href="#连分数" class="headerlink" title="连分数"></a>连分数</h3><p>就像输入分式时使用 <code>\frac</code> 一样, 使用 <code>\cfrac</code> 来创建一个连分数. 不要使用普通的 <code>\frac</code> 或 <code>\over</code> 来创建, 否则会看起来很丑. 例如:</p><pre><code>$$x = a_0 + \cfrac{1^2}{a_1          + \cfrac{2^2}{a_2          + \cfrac{3^2}{a_3 + \cfrac{4^4}{a_4 + \cdots}}}}$$</code></pre><p>显示:<br>$$<br>x = a_0 + \cfrac{1^2}{a_1<br>          + \cfrac{2^2}{a_2<br>          + \cfrac{3^2}{a_3 + \cfrac{4^4}{a_4 + \cdots}}}}<br>$$<br>反例, 使用<code>\frac</code>和<code>\over</code>:</p><pre><code>$$x = a_0 + \frac{1^2}{a_1          + \frac{2^2}{a_2          + \frac{3^2}{a_3 + \frac{4^4}{a_4 + \cdots}}}}$$</code></pre><p>显示:<br>$$<br>x = a_0 + \frac{1^2}{a_1<br>          + \frac{2^2}{a_2<br>          + \frac{3^2}{a_3 + \frac{4^4}{a_4 + \cdots}}}}<br>$$<br>当然, 可以使用 <code>\frac</code> 来表达连分数的 <strong>紧缩记法</strong> .  例如:</p><pre><code>x = a_0 + \frac{1^2}{a_1+}          \frac{2^2}{a_2+}          \frac{3^2}{a_3 +} \frac{4^4}{a_4 +} \cdots</code></pre><p>显示:<br>$$<br>x = a_0 + \frac{1^2}{a_1+}<br>          \frac{2^2}{a_2+}<br>          \frac{3^2}{a_3 +} \frac{4^4}{a_4 +} \cdots<br>$$<br>连分数通常都太大以至于不易排版, 建议使用 <code>[a0;a1,a2,a3,…]</code> 一样的紧缩记法. </p><h2 id="符号汇总"><a href="#符号汇总" class="headerlink" title="符号汇总"></a>符号汇总</h2><p>这里的符号正常使用绝对够了, 如果还有更奇葩的符号需要使用可以参考之前我引用的文章. 因为同一个符号可能有不同的使用领域, 所以不同的表可能有重叠的地方.</p><p><code>|</code> 符号在被当作分隔符时会产生错误的间隔, 因此在需要分隔时最好使用 <code>\mid</code> 来代替它. </p><h3 id="三角"><a href="#三角" class="headerlink" title="三角"></a>三角</h3><table><thead><tr><th align="center">显示</th><th align="left">输入</th><th align="center">显示</th><th align="left">输入</th><th align="center">显示</th><th align="left">输入</th></tr></thead><tbody><tr><td align="center">$30^\circ$</td><td align="left"><code>30^\circ</code></td><td align="center">$\bot$</td><td align="left"><code>\bot</code></td><td align="center">$\angle A$</td><td align="left"><code>\angle A</code></td></tr><tr><td align="center">$\sin$</td><td align="left"><code>\sin</code></td><td align="center">$\cos$</td><td align="left"><code>\cos</code></td><td align="center">$\tan$</td><td align="left"><code>\tan</code></td></tr><tr><td align="center">$\csc$</td><td align="left"><code>\csc</code></td><td align="center">$\sec$</td><td align="left"><code>\sec</code></td><td align="center">$\cot$</td><td align="left"><code>\cot</code></td></tr><tr><td align="center">$\sinh$</td><td align="left"><code>\sinh</code></td><td align="center">$\cosh$</td><td align="left"><code>\cosh</code></td><td align="center">$\tanh$</td><td align="left"><code>\tanh</code></td></tr><tr><td align="center">$\arcsin$</td><td align="left"><code>\arcsin</code></td><td align="center">$\arccos$</td><td align="left"><code>\arccos</code></td><td align="center">$\arctan$</td><td align="left"><code>\arctan</code></td></tr><tr><td align="center">$\textrm{arccsc}$</td><td align="left"><code>\textrm{arccsc}</code></td><td align="center">$\textrm{arcsec}$</td><td align="left"><code>\textrm{arcsec}</code></td><td align="center">$\textrm{arccot}$</td><td align="left"><code>\textrm{arccot}</code></td></tr><tr><td align="center">$\sin^{-1}$</td><td align="left"><code>\sin^{-1}</code></td><td align="center">$\cos^{-1}$</td><td align="left"><code>\cos^{-1}</code></td><td align="center">$\tan^{-1}$</td><td align="left"><code>\tan^{-1}</code></td></tr><tr><td align="center">$\sinh^{-1}$</td><td align="left"><code>\sinh^{-1}</code></td><td align="center">$\cosh^{-1}$</td><td align="left"><code>\cosh^{-1}</code></td><td align="center">$\tanh^{-1}$</td><td align="left"><code>\tanh^{-1}</code></td></tr><tr><td align="center">$\sphericalangle$</td><td align="left"><code>sphericalangle</code></td><td align="center">$\measuredangle$</td><td align="left"><code>\measuredangle</code></td><td align="center"></td><td align="left"></td></tr></tbody></table><h3 id="对数"><a href="#对数" class="headerlink" title="对数"></a>对数</h3><table><thead><tr><th align="center">显示</th><th align="center">输入</th><th align="center">显示</th><th align="center">输入</th><th align="center">显示</th><th align="center">输入</th></tr></thead><tbody><tr><td align="center">$\log$</td><td align="center"><code>\log​</code></td><td align="center">$\lg$</td><td align="center"><code>\lg​</code></td><td align="center">$\ln$</td><td align="center"><code>\ln</code></td></tr><tr><td align="center">$\exp$</td><td align="center"><code>\exp​</code></td><td align="center">$\log_{e}$</td><td align="center"><code>\log_{e}​</code></td><td align="center">$\log_{10}$</td><td align="center"><code>\log_{10}</code></td></tr></tbody></table><h3 id="微积分和导数"><a href="#微积分和导数" class="headerlink" title="微积分和导数"></a>微积分和导数</h3><table><thead><tr><th align="center">显示</th><th align="left">输入</th><th align="center">显示</th><th align="left">输入</th><th align="center">显示</th><th align="left">输入</th></tr></thead><tbody><tr><td align="center">$\int$</td><td align="left"><code>\int</code></td><td align="center">$\iint$</td><td align="left"><code>\iint</code></td><td align="center">$\iiint$</td><td align="left"><code>\iiint</code></td></tr><tr><td align="center">$\iiiint$</td><td align="left"><code>\iiiint</code></td><td align="center">$\oint$</td><td align="left"><code>\oint</code></td><td align="center">$\prime$</td><td align="left"><code>\prime</code></td></tr><tr><td align="center">$\lim$</td><td align="left"><code>\lim</code></td><td align="center">$\infty$</td><td align="left"><code>\infty</code></td><td align="center">$\nabla$</td><td align="left"><code>\nabla</code></td></tr></tbody></table><h3 id="其他公式"><a href="#其他公式" class="headerlink" title="其他公式"></a>其他公式</h3><table><thead><tr><th align="center">显示</th><th align="left">输入</th><th align="center">显示</th><th align="left">输入</th><th align="center">显示</th><th align="left">输入</th></tr></thead><tbody><tr><td align="center">$\inf$</td><td align="left"><code>\inf</code></td><td align="center">$\arg$</td><td align="left"><code>\arg</code></td><td align="center">$\det$</td><td align="left"><code>\det</code></td></tr><tr><td align="center">$\dim$</td><td align="left"><code>\dim</code></td><td align="center">$\gcd$</td><td align="left"><code>\gcd</code></td><td align="center">$\ker$</td><td align="left"><code>\ker</code></td></tr><tr><td align="center">$\Pr$</td><td align="left"><code>\Pr</code></td><td align="center">$\deg$</td><td align="left"><code>\deg</code></td><td align="center">$\sup$</td><td align="left"><code>\sup</code></td></tr><tr><td align="center">$\hom$</td><td align="left"><code>\hom</code></td><td align="center">$\max$</td><td align="left"><code>\max</code></td><td align="center">$\min$</td><td align="left"><code>\min</code></td></tr></tbody></table><h3 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h3><table><thead><tr><th align="center">符号</th><th align="left">公式</th><th align="center">符号</th><th align="left">公式</th><th align="center">符号</th><th align="left">公式</th></tr></thead><tbody><tr><td align="center">$+$</td><td align="left"><code>+</code></td><td align="center">$-$</td><td align="left"><code>-</code></td><td align="center">$\pm$</td><td align="left"><code>\pm</code></td></tr><tr><td align="center">$\mp$</td><td align="left"><code>\mp</code></td><td align="center">$\dotplus$</td><td align="left"><code>\dotplus</code></td><td align="center">$\times$</td><td align="left"><code>\times</code></td></tr><tr><td align="center">$\div$</td><td align="left"><code>\div</code></td><td align="center">$\divideontimes$</td><td align="left"><code>\divideontimes</code></td><td align="center">$/$</td><td align="left"><code>/</code></td></tr><tr><td align="center">$\backslash$</td><td align="left"><code>\backslash</code></td><td align="center">$\cdot$</td><td align="left"><code>\cdot</code></td><td align="center">$*$</td><td align="left"><code>*</code>或<code>\ast</code></td></tr><tr><td align="center">$\star$</td><td align="left"><code>\star</code></td><td align="center">$\circ$</td><td align="left"><code>\circ</code></td><td align="center">$\bullet$</td><td align="left"><code>\bullet</code></td></tr><tr><td align="center">$\boxplus$</td><td align="left"><code>\boxplus</code></td><td align="center">$\boxminus$</td><td align="left"><code>\boxminus</code></td><td align="center">$\boxtimes$</td><td align="left"><code>\boxtimes</code></td></tr><tr><td align="center">$\boxdot$</td><td align="left"><code>\boxdot</code></td><td align="center">$\oplus$</td><td align="left"><code>\oplus</code></td><td align="center">$\ominus$</td><td align="left"><code>\ominus</code></td></tr><tr><td align="center">$\otimes$</td><td align="left"><code>\otimes</code></td><td align="center">$\oslash$</td><td align="left"><code>\oslash</code></td><td align="center">$\odot$</td><td align="left"><code>\odot</code></td></tr><tr><td align="center">$\circleddash$</td><td align="left"><code>\circleddash</code></td><td align="center">$\circledcirc$</td><td align="left"><code>\circledcirc</code></td><td align="center">$\circledast$</td><td align="left"><code>\circledast</code></td></tr><tr><td align="center">$\bigoplus$</td><td align="left"><code>\bigoplus</code></td><td align="center">$\bigotimes$</td><td align="left"><code>\bigotimes</code></td><td align="center">$\bigodot$</td><td align="left"><code>\bigodot</code></td></tr></tbody></table><h3 id="集合相关"><a href="#集合相关" class="headerlink" title="集合相关"></a>集合相关</h3><table><thead><tr><th align="center">符号</th><th align="left">公式</th><th align="center">符号</th><th align="left">公式</th><th align="center">符号</th><th align="left">公式</th></tr></thead><tbody><tr><td align="center">$\{ \}$</td><td align="left"><code>\{ \}</code></td><td align="center">$\O$</td><td align="left"><code>\O</code>, <code>\empty</code>, <code>emptyset</code></td><td align="center">$\varnothing$</td><td align="left"><code>\varnothing</code></td></tr><tr><td align="center">$\in$</td><td align="left"><code>\in</code></td><td align="center">$\notin$</td><td align="left"><code>\notin</code>或<code>\not\in</code></td><td align="center">$\ni$</td><td align="left"><code>\ni</code></td></tr><tr><td align="center">$\not\ni$</td><td align="left"><code>\not\ni</code></td><td align="center">$\cap$</td><td align="left"><code>\cap</code></td><td align="center">$\Cap$</td><td align="left"><code>\Cap</code></td></tr><tr><td align="center">$\sqcap$</td><td align="left"><code>\sqcap</code></td><td align="center">$\bigcap$</td><td align="left"><code>\bigcap</code></td><td align="center">$\cup$</td><td align="left"><code>\cup</code></td></tr><tr><td align="center">$\Cup$</td><td align="left"><code>\Cup</code></td><td align="center">$\sqcup$</td><td align="left"><code>\sqcup</code></td><td align="center">$\bigcup$</td><td align="left"><code>\bigcup</code></td></tr><tr><td align="center">$\bigsqcup$</td><td align="left"><code>\bigsqcup</code></td><td align="center">$\uplus$</td><td align="left"><code>\uplus</code></td><td align="center">$\biguplus$</td><td align="left"><code>\biguplus</code></td></tr><tr><td align="center">$\bigvee$</td><td align="left"><code>\bigvee</code></td><td align="center">$\bigwedge$</td><td align="left"><code>\bigwedge</code></td><td align="center"></td><td align="left"></td></tr><tr><td align="center">$\setminus$</td><td align="left"><code>\setminus</code></td><td align="center">$\smallsetminus$</td><td align="left"><code>\smallsetminus</code></td><td align="center">$\times$</td><td align="left"><code>\times</code></td></tr><tr><td align="center">$\supset$</td><td align="left"><code>\subset</code></td><td align="center">$\Subset$</td><td align="left"><code>\Subset</code></td><td align="center">$\sqsubset$</td><td align="left"><code>\sqsubset</code></td></tr><tr><td align="center">$\circleddash$</td><td align="left"><code>\supset</code></td><td align="center">$\Supset$</td><td align="left"><code>\Supset</code></td><td align="center">$\sqsupset$</td><td align="left"><code>\sqsupset</code></td></tr><tr><td align="center">$\subseteq$</td><td align="left"><code>\subseteq</code></td><td align="center">$\nsubseteq$</td><td align="left"><code>\nsubseteq</code></td><td align="center">$\subsetneq$</td><td align="left"><code>\subsetneq</code></td></tr><tr><td align="center">$\varsubsetneq$</td><td align="left"><code>\varsubsetneq</code></td><td align="center">$\sqsubseteq$</td><td align="left"><code>\sqsubseteq</code></td><td align="center"></td><td align="left"></td></tr><tr><td align="center">$\supseteq$</td><td align="left"><code>\supseteq</code></td><td align="center">$\nsupseteq$</td><td align="left"><code>\nsupseteq</code></td><td align="center">$\supsetneq$</td><td align="left"><code>\supsetneq</code></td></tr><tr><td align="center">$\varsupsetneq$</td><td align="left"><code>\varsupsetneq</code></td><td align="center">$\sqsupseteq$</td><td align="left"><code>\sqsupseteq</code></td><td align="center"></td><td align="left"></td></tr><tr><td align="center">$\subseteqq$</td><td align="left"><code>\subseteqq</code></td><td align="center">$\nsubseteqq$</td><td align="left"><code>\nsubseteqq</code></td><td align="center">$\subsetneqq$</td><td align="left"><code>\subsetneqq</code></td></tr><tr><td align="center">$\varsubsetneqq$</td><td align="left"><code>\varsubsetneqq</code></td><td align="center"></td><td align="left"></td><td align="center"></td><td align="left"></td></tr><tr><td align="center">$\supseteqq$</td><td align="left"><code>\supseteqq</code></td><td align="center">$\nsupseteqq$</td><td align="left"><code>\nsupseteqq</code></td><td align="center">$\supsetneqq$</td><td align="left"><code>\supsetneqq</code></td></tr><tr><td align="center">$\varsupsetneqq$</td><td align="left"><code>\varsupsetneqq</code></td><td align="center"></td><td align="left"></td><td align="center"></td><td align="left"></td></tr></tbody></table><h3 id="关系符号"><a href="#关系符号" class="headerlink" title="关系符号"></a>关系符号</h3><table><thead><tr><th align="center">符号</th><th align="left">公式</th><th align="center">符号</th><th align="left">公式</th><th align="center">符号</th><th align="left">公式</th></tr></thead><tbody><tr><td align="center">$=$</td><td align="left"><code>=</code></td><td align="center">$\ne$</td><td align="left"><code>\ne</code>或<code>\neq</code></td><td align="center">$\equiv$</td><td align="left"><code>\equiv</code></td></tr><tr><td align="center">$\not\equiv$</td><td align="left"><code>\not\equiv</code></td><td align="center">$\doteq$</td><td align="left"><code>\doteq</code></td><td align="center">$\doteqdot$</td><td align="left"><code>\doteqdot</code></td></tr><tr><td align="center">$\sim$</td><td align="left"><code>\sim</code></td><td align="center">$\nsim$</td><td align="left"><code>\nsim</code></td><td align="center">$\backsim$</td><td align="left"><code>\backsim</code></td></tr><tr><td align="center">$\thicksim$</td><td align="left"><code>\thicksim</code></td><td align="center">$\simeq$</td><td align="left"><code>\simeq</code></td><td align="center">$\backsimeq$</td><td align="left"><code>\backsimeq</code></td></tr><tr><td align="center">$\eqsim$</td><td align="left"><code>\eqsim</code></td><td align="center">$\cong$</td><td align="left"><code>\cong</code></td><td align="center">$\ncong$</td><td align="left"><code>\ncong</code></td></tr><tr><td align="center">$\approx$</td><td align="left"><code>\approx</code></td><td align="center">$\thickapprox$</td><td align="left"><code>\thickapprox</code></td><td align="center">$\approxeq$</td><td align="left"><code>\approxeq</code></td></tr><tr><td align="center">$\asymp$</td><td align="left"><code>\asymp</code></td><td align="center">$\propto$</td><td align="left"><code>\propto</code></td><td align="center">$\varpropto$</td><td align="left"><code>\varpropto</code></td></tr><tr><td align="center">$&lt;$</td><td align="left"><code>&lt;</code></td><td align="center">$\nless$</td><td align="left"><code>\nless</code></td><td align="center">$\ll$</td><td align="left"><code>\ll</code></td></tr><tr><td align="center">$\not\ll$</td><td align="left"><code>\not\ll</code></td><td align="center">$\lll$</td><td align="left"><code>\lll</code></td><td align="center">$\not\lll$</td><td align="left"><code>\not\lll</code></td></tr><tr><td align="center">$\lessdot$</td><td align="left"><code>\lessdot</code></td><td align="center">$\le$</td><td align="left"><code>\le</code>或<code>\leq</code></td><td align="center">$\lneq$</td><td align="left"><code>\lneq</code></td></tr><tr><td align="center">$\leqq$</td><td align="left"><code>\leqq</code></td><td align="center">$\nleq$</td><td align="left"><code>\nleq</code></td><td align="center">$\nleqq$</td><td align="left"><code>\nleqq</code></td></tr><tr><td align="center">$\lneqq$</td><td align="left"><code>\lneqq</code></td><td align="center">$\lvertneqq$</td><td align="left"><code>\lvertneqq</code></td><td align="center">$\leqslant$</td><td align="left"><code>\leqslant</code></td></tr><tr><td align="center">$\nleqslant$</td><td align="left"><code>\nleqslant</code></td><td align="center">$\eqslantless$</td><td align="left"><code>\eqslantless</code></td><td align="center">$\lesssim$</td><td align="left"><code>\lesssim</code></td></tr><tr><td align="center">$\lnsim$</td><td align="left"><code>\lnsim</code></td><td align="center">$\lessapprox$</td><td align="left"><code>\lessapprox</code></td><td align="center">$\lnapprox$</td><td align="left"><code>\lnapprox</code></td></tr><tr><td align="center">$&gt;$</td><td align="left"><code>&gt;</code></td><td align="center">$\ngtr$</td><td align="left"><code>\ngtr</code></td><td align="center">$\gg$</td><td align="left"><code>\gg</code></td></tr><tr><td align="center">$\not\gg$</td><td align="left"><code>\not\gg</code></td><td align="center">$\ggg$</td><td align="left"><code>\ggg</code></td><td align="center">$\not\ggg$</td><td align="left"><code>\not\ggg</code></td></tr><tr><td align="center">$\gtrdot$</td><td align="left"><code>\gtrdot</code></td><td align="center">$\ge$</td><td align="left"><code>\ge</code>或<code>\geq</code></td><td align="center">$\gneq$</td><td align="left"><code>\gneq</code></td></tr><tr><td align="center">$\geqq$</td><td align="left"><code>\geqq</code></td><td align="center">$\ngeq$</td><td align="left"><code>\ngeq</code></td><td align="center">$\ngeqq$</td><td align="left"><code>\ngeqq</code></td></tr><tr><td align="center">$\gneqq$</td><td align="left"><code>\gneqq</code></td><td align="center">$\gvertneqq$</td><td align="left"><code>\gvertneqq</code></td><td align="center">$\geqslant$</td><td align="left"><code>\geqslant</code></td></tr><tr><td align="center">$\ngeqslant$</td><td align="left"><code>\ngeqslant</code></td><td align="center">$\eqslantgtr$</td><td align="left"><code>\eqslantgtr</code></td><td align="center">$\gtrsim$</td><td align="left"><code>\gtrsim</code></td></tr><tr><td align="center">$\gnsim$</td><td align="left"><code>\gnsim</code></td><td align="center">$\gtrapprox$</td><td align="left"><code>\gtrapprox</code></td><td align="center">$\gnapprox$</td><td align="left"><code>\gnapprox</code></td></tr><tr><td align="center">$\lessgtr$</td><td align="left"><code>\lessgtr</code></td><td align="center">$\lesseqgtr$</td><td align="left"><code>\lesseqgtr</code></td><td align="center">$\lesseqqgtr$</td><td align="left"><code>\lesseqqgtr</code></td></tr><tr><td align="center">$\gtrless$</td><td align="left"><code>\gtrless</code></td><td align="center">$\gtreqless$</td><td align="left"><code>\gtreqless</code></td><td align="center">$\gtreqqless$</td><td align="left"><code>\gtreqqless</code></td></tr></tbody></table><h3 id="逻辑符号"><a href="#逻辑符号" class="headerlink" title="逻辑符号"></a>逻辑符号</h3><table><thead><tr><th align="center">符号</th><th align="left">公式</th><th align="center">符号</th><th align="left">公式</th><th align="center">符号</th><th align="left">公式</th></tr></thead><tbody><tr><td align="center">$\forall$</td><td align="left"><code>\forall</code></td><td align="center">$\exists$</td><td align="left"><code>\exists</code></td><td align="center">$\nexists$</td><td align="left"><code>\nexists</code></td></tr><tr><td align="center">$\therefore$</td><td align="left"><code>\therefore</code></td><td align="center">$\because$</td><td align="left"><code>\because</code></td><td align="center">$\And$</td><td align="left"><code>\And</code></td></tr><tr><td align="center">$\or$</td><td align="left"><code>\or</code>, <code>\lor</code>, <code>\vee</code></td><td align="center">$\and$</td><td align="left"><code>\and</code>, <code>\land</code>, <code>\wedge</code></td><td align="center">$\overline{abc}$</td><td align="left"><code>\overline{abc}</code></td></tr><tr><td align="center">$\neg$</td><td align="left"><code>\lnot</code>, <code>\neg</code></td><td align="center">$\not\operatorname{R}$</td><td align="left"><code>\not\operatorname{R}</code></td><td align="center">$\bar{abc}$</td><td align="left"><code>\bar{abc}</code></td></tr></tbody></table><h3 id="各种箭头"><a href="#各种箭头" class="headerlink" title="各种箭头"></a>各种箭头</h3><table><thead><tr><th align="center">箭头</th><th align="left">公式</th><th align="center">箭头</th><th align="left">公式</th><th align="center">箭头</th><th align="left">公式</th></tr></thead><tbody><tr><td align="center">$\Rrightarrow$</td><td align="left"><code>\Rrightarrow</code></td><td align="center">$\Lleftarrow$</td><td align="left"><code>\Lleftarrow</code></td><td align="center">$\Rightarrow$</td><td align="left"><code>\Rightarrow</code></td></tr><tr><td align="center">$\nRightarrow$</td><td align="left"><code>\nRightarrow</code></td><td align="center">$\Longrightarrow$</td><td align="left"><code>\Longrightarrow</code></td><td align="center">$\implies$</td><td align="left"><code>\implies</code></td></tr><tr><td align="center">$\Leftarrow$</td><td align="left"><code>\Leftarrow</code></td><td align="center">$\nLeftarrow$</td><td align="left"><code>\nLeftarrow</code></td><td align="center">$\Longleftarrow$</td><td align="left"><code>\Longleftarrow</code></td></tr><tr><td align="center">$\Leftrightarrow$</td><td align="left"><code>\Leftrightarrow</code></td><td align="center">$\nLeftrightarrow$</td><td align="left"><code>\nLeftrightarrow</code></td><td align="center">$\Longleftrightarrow$</td><td align="left"><code>\Longleftrightarrow</code></td></tr><tr><td align="center">$\iff$</td><td align="left"><code>\iff</code></td><td align="center">$\Uparrow$</td><td align="left"><code>\Uparrow</code></td><td align="center">$\Downarrow$</td><td align="left"><code>\Downarrow</code></td></tr><tr><td align="center">$\Updownarrow$</td><td align="left"><code>\Updownarrow</code></td><td align="center">$\rightarrow$</td><td align="left"><code>\rightarrow</code>, <code>\to</code></td><td align="center">$\nrightarrow$</td><td align="left"><code>\nrightarrow</code></td></tr><tr><td align="center">$\longrightarrow$</td><td align="left"><code>\longrightarrow</code></td><td align="center">$\leftarrow$</td><td align="left"><code>\leftarrow</code>,<code>\gets</code></td><td align="center">$\nleftarrow$</td><td align="left"><code>\nleftarrow</code></td></tr><tr><td align="center">$\longleftarrow$</td><td align="left"><code>\longleftarrow</code></td><td align="center">$\leftrightarrow$</td><td align="left"><code>\leftrightarrow</code></td><td align="center">$\nleftrightarrow$</td><td align="left"><code>\nleftrightarrow</code></td></tr><tr><td align="center">$\longleftrightarrow$</td><td align="left"><code>\longleftrightarrow</code></td><td align="center">$\uparrow$</td><td align="left"><code>\uparrow</code></td><td align="center">$\downarrow$</td><td align="left"><code>\downarrow</code></td></tr><tr><td align="center">$\updownarrow$</td><td align="left"><code>\updownarrow</code></td><td align="center">$\nearrow$</td><td align="left"><code>\nearrow</code></td><td align="center">$\swarrow$</td><td align="left"><code>\swarrow</code></td></tr><tr><td align="center">$\nwarrow$</td><td align="left"><code>\nwarrow</code></td><td align="center">$\searrow$</td><td align="left"><code>\searrow</code></td><td align="center">$\mapsto$</td><td align="left"><code>\mapsto</code></td></tr><tr><td align="center">$\longmapsto$</td><td align="left"><code>\longmapsto</code></td><td align="center">$\rightharpoonup$</td><td align="left"><code>\rightharpoonup</code></td><td align="center">$\rightharpoondown$</td><td align="left"><code>\rightharpoondown</code></td></tr><tr><td align="center">$\leftharpoonup$</td><td align="left"><code>\leftharpoonup</code></td><td align="center">$\leftharpoondown$</td><td align="left"><code>\leftharpoondown</code></td><td align="center">$\upharpoonleft$</td><td align="left"><code>\upharpoonleft</code></td></tr><tr><td align="center">$\upharpoonright$</td><td align="left"><code>\upharpoonright</code></td><td align="center">$\downharpoonleft$</td><td align="left"><code>\downharpoonleft</code></td><td align="center">$\downharpoonright$</td><td align="left"><code>\downharpoonright</code></td></tr><tr><td align="center">$\rightleftharpoons$</td><td align="left"><code>\rightleftharpoons</code></td><td align="center">$\leftrightharpoons$</td><td align="left"><code>\leftrightharpoons</code></td><td align="center">$\curvearrowleft$</td><td align="left"><code>\curvearrowleft</code></td></tr><tr><td align="center">$\circlearrowleft$</td><td align="left"><code>\circlearrowleft</code></td><td align="center">$\Lsh$</td><td align="left"><code>\Lsh</code></td><td align="center">$\upuparrows$</td><td align="left"><code>\upuparrows</code></td></tr><tr><td align="center">$\rightrightarrows$</td><td align="left"><code>\rightrightarrows</code></td><td align="center">$\rightleftarrows$</td><td align="left"><code>\rightleftarrows</code></td><td align="center">$\rightarrowtail$</td><td align="left"><code>\rightarrowtail</code></td></tr><tr><td align="center">$\looparrowright$</td><td align="left"><code>\looparrowright</code></td><td align="center">$\curvearrowright$</td><td align="left"><code>\curvearrowright</code></td><td align="center">$\circlearrowright$</td><td align="left"><code>\circlearrowright</code></td></tr><tr><td align="center">$\Rsh$</td><td align="left"><code>\Rsh</code></td><td align="center">$\downdownarrows$</td><td align="left"><code>\downdownarrows</code></td><td align="center">$\leftleftarrows$</td><td align="left"><code>\leftleftarrows</code></td></tr><tr><td align="center">$\leftrightarrows$</td><td align="left"><code>\leftrightarrows</code></td><td align="center">$\leftarrowtail$</td><td align="left"><code>\leftarrowtail</code></td><td align="center">$\looparrowleft$</td><td align="left"><code>\looparrowleft</code></td></tr><tr><td align="center">$\hookrightarrow$</td><td align="left"><code>\hookrightarrow</code></td><td align="center">$\hookleftarrow$</td><td align="left"><code>\hookleftarrow</code></td><td align="center">$\multimap$</td><td align="left"><code>\multimap</code></td></tr><tr><td align="center">$\leftrightsquigarrow$</td><td align="left"><code>\leftrightsquigarrow</code></td><td align="center">$\rightsquigarrow$</td><td align="left"><code>\rightsquigarrow</code></td><td align="center">$\twoheadrightarrow$</td><td align="left"><code>\twoheadrightarrow</code></td></tr><tr><td align="center">$\twoheadleftarrow$</td><td align="left"><code>\twoheadleftarrow</code></td><td align="center"></td><td align="left"></td><td align="center"></td><td align="left"></td></tr></tbody></table><h3 id="带帽符号"><a href="#带帽符号" class="headerlink" title="带帽符号"></a>带帽符号</h3><table><thead><tr><th align="center">效果</th><th>公式</th><th align="center">效果</th><th>公式</th><th align="center">效果</th><th>公式</th></tr></thead><tbody><tr><td align="center">$\dot{a}$</td><td><code>\dot{a}</code></td><td align="center">$\ddot{a}$</td><td><code>\ddot{a}</code></td><td align="center">$\acute{a}$</td><td><code>\acute{a}</code></td></tr><tr><td align="center">$\grave{a}$</td><td><code>\grave{a}</code></td><td align="center">$\check{a}$</td><td><code>\check{a}</code></td><td align="center">$\tilde{a}$</td><td><code>\tilde{a}</code></td></tr><tr><td align="center">$\bar{a}$</td><td><code>\bar{a}</code></td><td align="center">$\hat{a}$</td><td><code>\hat{a}</code></td><td align="center">$\widehat{abc}$</td><td><code>\widehat{abc}</code></td></tr><tr><td align="center">$\vec{a}$</td><td><code>\vec{a}</code></td><td align="center">$\breve{a}$</td><td><code>\breve{a}</code></td><td align="center">$\widetilde{abc}$</td><td><code>\widetilde{abc}</code></td></tr></tbody></table><h3 id="希腊字母"><a href="#希腊字母" class="headerlink" title="希腊字母"></a>希腊字母</h3><h4 id="大写希腊字母"><a href="#大写希腊字母" class="headerlink" title="大写希腊字母"></a>大写希腊字母</h4><table><thead><tr><th align="center">字母</th><th align="left">公式</th><th align="center">字母</th><th align="left">公式</th><th align="center">字母</th><th align="left">公式</th></tr></thead><tbody><tr><td align="center">$\Alpha$</td><td align="left"><code>\Alpha</code></td><td align="center">$\Beta$</td><td align="left"><code>\Beta</code></td><td align="center">$\Gamma$</td><td align="left"><code>\Gamma</code></td></tr><tr><td align="center">$\Delta$</td><td align="left"><code>\Delta</code></td><td align="center">$\Epsilon$</td><td align="left"><code>\Epsilon</code></td><td align="center">$\Zeta$</td><td align="left"><code>\Zeta</code></td></tr><tr><td align="center">$\Eta$</td><td align="left"><code>\Eta</code></td><td align="center">$\Theta$</td><td align="left"><code>\Theta</code></td><td align="center">$\Iota$</td><td align="left"><code>\Iota</code></td></tr><tr><td align="center">$\Kappa$</td><td align="left"><code>\Kappa</code></td><td align="center">$\Lambda$</td><td align="left"><code>\Lambda</code></td><td align="center">$\Mu$</td><td align="left"><code>\Mu</code></td></tr><tr><td align="center">$\Nu$</td><td align="left"><code>\Nu</code></td><td align="center">$\Xi$</td><td align="left"><code>\Xi</code></td><td align="center">$\Omicron$</td><td align="left"><code>\Omicron</code></td></tr><tr><td align="center">$\Pi$</td><td align="left"><code>\Pi</code></td><td align="center">$\Rho$</td><td align="left"><code>\Rho</code></td><td align="center">$\Sigma$</td><td align="left"><code>\Sigma</code></td></tr><tr><td align="center">$\Tau$</td><td align="left"><code>\Tau</code></td><td align="center">$\Upsilon$</td><td align="left"><code>\Upsilon</code></td><td align="center">$\Phi$</td><td align="left"><code>\Phi</code></td></tr><tr><td align="center">$\Chi$</td><td align="left"><code>\Chi​</code></td><td align="center">$\Psi$</td><td align="left"><code>\Psi</code></td><td align="center">$\Omega$</td><td align="left"><code>\Omega</code></td></tr></tbody></table><p>PS: 如果公式出现标红, 只是因为新版本的MathType不支持在当前页面的显示.</p><h4 id="小写希腊字母"><a href="#小写希腊字母" class="headerlink" title="小写希腊字母"></a>小写希腊字母</h4><table><thead><tr><th align="center">字母</th><th align="left">公式</th><th align="center">字母</th><th align="left">公式</th><th align="center">字母</th><th align="left">公式</th></tr></thead><tbody><tr><td align="center">$\alpha$</td><td align="left"><code>\alpha</code></td><td align="center">$\beta$</td><td align="left"><code>\beta</code></td><td align="center">$\gamma$</td><td align="left"><code>\gamma</code></td></tr><tr><td align="center">$\delta$</td><td align="left"><code>\delta</code></td><td align="center">$\epsilon$</td><td align="left"><code>\epsilon</code></td><td align="center">$\zeta$</td><td align="left"><code>\zeta</code></td></tr><tr><td align="center">$\eta$</td><td align="left"><code>\eta</code></td><td align="center">$\theta$</td><td align="left"><code>\theta</code></td><td align="center">$\iota$</td><td align="left"><code>\iota</code></td></tr><tr><td align="center">$\kappa$</td><td align="left"><code>\kappa</code></td><td align="center">$\lambda$</td><td align="left"><code>\lambda</code></td><td align="center">$\mu$</td><td align="left"><code>\mu</code></td></tr><tr><td align="center">$\nu$</td><td align="left"><code>\nu</code></td><td align="center">$\omicron$</td><td align="left"><code>\omicron</code></td><td align="center">$\xi$</td><td align="left"><code>\xi</code></td></tr><tr><td align="center">$\pi$</td><td align="left"><code>\pi</code></td><td align="center">$\rho$</td><td align="left"><code>\rho</code></td><td align="center">$\sigma$</td><td align="left"><code>\sigma</code></td></tr><tr><td align="center">$\tau$</td><td align="left"><code>\tau</code></td><td align="center">$\upsilon$</td><td align="left"><code>\upsilon</code></td><td align="center">$\phi$</td><td align="left"><code>\phi</code></td></tr><tr><td align="center">$\chi$</td><td align="left"><code>\chi</code></td><td align="center">$\psi$</td><td align="left"><code>psi</code></td><td align="center">$\omega$</td><td align="left"><code>\omega</code></td></tr></tbody></table><h4 id="部分字母变量专用形式"><a href="#部分字母变量专用形式" class="headerlink" title="部分字母变量专用形式"></a>部分字母变量专用形式</h4><p>以<code>\var-</code>开头.</p><table><thead><tr><th align="center">字母</th><th>公式</th><th align="center">字母</th><th>公式</th></tr></thead><tbody><tr><td align="center">$\varepsilon$</td><td><code>\varepsilon</code></td><td align="center">$\varrho$</td><td><code>\varrho</code></td></tr><tr><td align="center">$\varphi$</td><td><code>\varphi</code></td><td align="center">$\varsigma$</td><td><code>\varsigma</code></td></tr><tr><td align="center">$\varkappa$</td><td><code>\varkappa</code></td><td align="center">$\vartheta$</td><td><code>\vartheta</code></td></tr><tr><td align="center">$\varpi$</td><td><code>\varpi</code></td><td align="center">$\digamma$</td><td><code>\digamm</code></td></tr></tbody></table><h4 id="类字母和常数"><a href="#类字母和常数" class="headerlink" title="类字母和常数"></a>类字母和常数</h4><table><thead><tr><th align="center">字母</th><th align="left">公式</th><th align="center">字母</th><th align="left">公式</th><th align="center">字母</th><th align="left">公式</th></tr></thead><tbody><tr><td align="center">$\infty$</td><td align="left"><code>\infty</code></td><td align="center">$\aleph$</td><td align="left"><code>\aleph</code></td><td align="center">$\complement$</td><td align="left"><code>\complement</code></td></tr><tr><td align="center">$\backepsilon$</td><td align="left"><code>\backepsilon</code></td><td align="center">$\eth$</td><td align="left"><code>\eth</code></td><td align="center">$\Finv$</td><td align="left"><code>\Finv</code></td></tr><tr><td align="center">$\hbar$</td><td align="left"><code>\hbar</code></td><td align="center">$\theta$</td><td align="left"><code>\Im</code></td><td align="center">$\imath$</td><td align="left"><code>\imath</code></td></tr><tr><td align="center">$\jmath$</td><td align="left"><code>\jmath</code></td><td align="center">$\Bbbk$</td><td align="left"><code>\Bbbk</code></td><td align="center">$\ell$</td><td align="left"><code>\ell</code></td></tr><tr><td align="center">$\mho$</td><td align="left"><code>\mho</code></td><td align="center">$\wp$</td><td align="left"><code>\wp</code></td><td align="center">$\Re$</td><td align="left"><code>\Re</code></td></tr><tr><td align="center">$\circledS$</td><td align="left"><code>\circledS</code></td><td align="center"></td><td align="left"></td><td align="center"></td><td align="left"></td></tr></tbody></table><h2 id="字体转换"><a href="#字体转换" class="headerlink" title="字体转换"></a>字体转换</h2><p>用<code>{\字体 {需转换的部分字符}}</code>来变更字体, 公式默认为意大利体.</p><table><thead><tr><th align="center">输入</th><th align="center">说明</th><th align="center">显示</th><th align="center">输入</th><th align="center">说明</th><th align="center">显示</th></tr></thead><tbody><tr><td align="center"><code>\rm</code></td><td align="center">罗马体</td><td align="center">${\rm Sample}$</td><td align="center"><code>\cal</code></td><td align="center">花体</td><td align="center">${\cal Sample}$</td></tr><tr><td align="center"><code>\it</code></td><td align="center">意大利体</td><td align="center">${\it Sample}$</td><td align="center"><code>\Bbb</code></td><td align="center">黑板粗体</td><td align="center">${\Bbb Sample}$</td></tr><tr><td align="center"><code>\bf</code></td><td align="center">粗体</td><td align="center">${\bf Sample}$</td><td align="center"><code>\mit</code></td><td align="center">数学斜体</td><td align="center">${\mit Sample}$</td></tr><tr><td align="center"><code>\sf</code></td><td align="center">等线体</td><td align="center">${\sf Sample}$</td><td align="center"><code>\scr</code></td><td align="center">手写体</td><td align="center">${\scr Sample}$</td></tr><tr><td align="center"><code>\tt</code></td><td align="center">打字机体</td><td align="center">${\tt Sample}$</td><td align="center"><code>\frak</code></td><td align="center">旧德式字体</td><td align="center">${\frak Sample}$</td></tr></tbody></table><p>如果想直接从斜体变为非斜体, 可以使用<code>\text{内容}</code>.<br>$$<br>\begin{array}{cc}<br>\mathrm{Bad} &amp; \mathrm{Better} \\<br>\hline \\<br>\int_0^1 x^2 dx &amp; \int_0^1 x^2 \,{\rm d}x<br>\end{array}<br>$$</p><h2 id="更改字体颜色"><a href="#更改字体颜色" class="headerlink" title="更改字体颜色"></a>更改字体颜色</h2><p><code>MathJax3</code>还不支持渲染字体的颜色, 大多数情况下这个颜色也是一个鸡肋功能. 如果想知道如何添加可以看文章最开头说的两篇博客, 里面有写到更改颜色的详细方法.</p>]]></content>
      
      
      <categories>
          
          <category> Markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Latex </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之奇异值分解</title>
      <link href="/posts/6556.html"/>
      <url>/posts/6556.html</url>
      
        <content type="html"><![CDATA[<h1 id="奇异值分解SVD-Singular-Value-Decomposition"><a href="#奇异值分解SVD-Singular-Value-Decomposition" class="headerlink" title="奇异值分解SVD Singular Value Decomposition"></a>奇异值分解SVD Singular Value Decomposition</h1><p>SVD(Singular Value Decomposition) 是一种基于原矩阵进行分解的<strong>特征分解</strong>手段. 主要是用小的多的数据来表示原始的数据集, 实质是对数据的过滤和去噪.</p><p>先说一下特征值分解, 对与$A_{n\times n}$ 能够以如下方式进行特征值的分解, $Q$ 为特征向量矩阵, 但这种方式仅限于方阵的分解. 当对任意大小的矩阵都生效时, 就有了SVD.<br>$$<br>A = Q\Sigma Q^T=<br>Q\left[<br>\begin{matrix}<br>    \lambda_1 &amp; \cdots &amp; \cdots &amp; \cdots\\<br>    \cdots &amp; \lambda_2 &amp; \cdots &amp; \cdots\\<br>    \cdots &amp; \cdots &amp; \ddots &amp; \cdots\\<br>    \cdots &amp; \cdots &amp; \cdots &amp; \lambda_m<br>\end{matrix}<br>\right]Q^T<br>$$<br>SVD同样也是对特征值的分解, 但是SVD不要求被分解的矩阵为方阵. 假设$A_{m\times n}$, $U_{m\times m}$, $V_{n\times n}$, $\Sigma_{m\times n}$. 其中$U$, $V$ 均为标准的正交矩阵. 它有如下性质:<br>$$<br>U^TU = I = V^TV<br>$$</p><p> $\Sigma$ 是对角阵, 且对角线上的每个元素都称为奇异值, 一般形式如下:<br>$$<br>\Sigma =<br>\left[<br>    \begin{matrix}<br>    \sigma_1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\<br>    0 &amp; \sigma_2 &amp; 0 &amp; 0 &amp; 0\\<br>    0 &amp; 0 &amp; \ddots &amp; 0 &amp; 0\\<br>    0 &amp; 0 &amp; 0 &amp; \ddots &amp; 0<br>    \end{matrix}<br>\right]_{m\times n}<br>$$</p><p>如果我们将$A$ 和 $A^T$ 做乘法, 就能得到满足 $U$ 大小的方阵, 就能够进行特征值分解. 所得到的的$m$ 个特征值对应的特征向量的张成就是矩阵$U$. 并将其中每个特征向量成为$A$ 的左奇异向量. 即:<br>$$<br>AA^Tu_i = \lambda_iu_i<br>$$<br>同理, 将$A^T$和$A$ 做乘法, 可以得到满足$V$ 大小的方阵. 称其为右奇异矩阵.<br>$$<br>A^TAv_i = \lambda_iv_i<br>$$<br>如何求出$\Sigma$ 呢? 由于除去它对角线上的奇异值其他位置都是0, 所以只需要求出每个奇异值.<br>$$<br>\displaylines{<br>A = U\Sigma V^T \Rightarrow AV = U\Sigma V^TV  \Rightarrow AV = U\Sigma \\<br>Av_i = \sigma_i u_i \\<br>\sigma = \frac{Av_i}{u_i}}<br>$$<br>这样求很麻烦, 不如直接用我们已知的条件:<br>$$<br>\begin{aligned}<br>A &amp;= U\Sigma V^T \\<br>A^T &amp;= V\Sigma U^T \\<br>A^TA &amp;= V\Sigma U^TU\Sigma V^T \\<br>&amp;= V \Sigma ^2 V^T<br>\end{aligned}<br>$$<br>能够看到最后求出来的等式直接能得到:<br>$$<br>\sigma_i = \sqrt{\lambda_i}<br>$$<br>对于奇异值, 在奇异值矩阵中从大到小排列, 很多时候前10%甚至1%的奇异值占到了全部奇异值之和的99%. 所以我们能用最大的$k$个奇异值和对应的左右奇异向量来近似描述矩阵.<br>$$<br>A_{m\times n} = U_{m \times m} \Sigma_{m\times n}V^T_{n\times n} \approx U_{m \times k} \Sigma_{k\times k}V^T_{k\times n}<br>$$</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/SVD.jpg" style="zoom: 25%;" />]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SVD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之集成学习</title>
      <link href="/posts/30251.html"/>
      <url>/posts/30251.html</url>
      
        <content type="html"><![CDATA[<h1 id="集成学习-Ensemble-Learning"><a href="#集成学习-Ensemble-Learning" class="headerlink" title="集成学习 Ensemble Learning"></a>集成学习 Ensemble Learning</h1><p>Boosting, Bagging, Stacking都是集成学习的方式, 都是考虑用多个弱学习器通过某种方式集合在一起, 形成一个泛化性能更强的强学习器.</p><h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p>Boosting是一种通过组合<strong>同质</strong>弱学习器来<strong>顺序学习</strong>产生强学习器的通用且有效的方法, 最早出现在AdaBoost, 它的启发来自于管委员会中每个成员只提供一些不成熟的判断, 但整个委员会却产生较为准确的决策. 弱学习器在一起做的决策共同做决策, 仍然可以得到一个不错的结果. Adaboost让每次迭代对每个样本都维护一个权重分布, 权重较大的误分类样本会比权重较小的误分类样本贡献更大的训练错误率. 为了获得更小的加权错误率, 弱分类器必须更多的聚焦于高权重的样本, 保证对它们准确的预测. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/boosting1.jpg" style="zoom:67%;" /><p>每次训练一个新的弱学习器, 误差为最小加权训练误差, <strong>每一轮提高分类错误的样本权重, 降低分类正确的样本权重</strong>. 并且每次训练的目标是找到一个函数来拟合上一轮的<strong>残差</strong>(实际观察值与估计值之差). 代表算法Adaboost(Adaptive Boosting).</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/boosting2.png" style="zoom: 67%;" /><h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p>Bagging(Bootstrap aggregating)并行训练多个同质弱学习器, 在取数据集时使用Boostrap(自助法). 自助法的意思是指在每次训练新的弱学习器时, 都从数据集中做放回抽样, 这样其他训练器在训练时也可能抽到相同的样本.</p><p>Boostrap就是一个在自身样本重采样的方法来估计真实分布的问题, 在不知道实际分布时, boostrap十分有用. 举个Boostrap的例子:</p><blockquote><p>假设一下, 第一次重新捕鱼100条, 发现里面有标记的鱼12条, 记下为12%, 放回去, 再捕鱼100条, 发现标记的为9条, 记下9%, 重复重复好多次之后, 假设取置信区间95%, 你会发现, 每次捕鱼平均在10条左右有标记, 所以, 我们可以大致推测出鱼塘有1000条左右. 其实是一个很简单的类似于一个比例问题. 这也是因为提出者Efron给统计学顶级期刊投稿的时候被拒绝的理由–”太简单”. 这也就解释了, 为什么在小样本的时候, bootstrap效果较好, 你这样想, 如果我想统计大海里有多少鱼, 你标记100000条也没用啊, 因为实际数量太过庞大, 你取的样本相比于太过渺小, 最实际的就是, 你下次再捕100000的时候, 发现一条都没有标记, 这就很尴尬了. </p></blockquote><p>回来说Bagging, 重复抽样和训练这个过程许多次, 就能获得很多同质的弱学习器. 最后在进行预测时采用投票机制, 所有弱分类器共同投票得出结果. 代表算法随机森林就是这样具有了很强的鲁棒性.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/bagging.png" alt=""></p><h2 id="Bagging和Boosting的区别"><a href="#Bagging和Boosting的区别" class="headerlink" title="Bagging和Boosting的区别"></a>Bagging和Boosting的区别</h2><p>人们常常把Bagging和Boosting混淆, 其实这两个种集成是完完全全不同的.</p><table><thead><tr><th></th><th>Bagging</th><th>Boosting</th></tr></thead><tbody><tr><td>样本选择</td><td>有放回抽样, 训练集每次都不同</td><td>训练集每次相同, 但在分类器中权重不同</td></tr><tr><td>样本权重</td><td>权重一致</td><td>不断调整, 错误率越大权重越高</td></tr><tr><td>预测权重</td><td>投票得出, 权重相等</td><td>错误率高的学习器比重小</td></tr><tr><td>并行计算</td><td>独立并行计算</td><td>只能串行, 每次新学习器需要上个学习器的结果</td></tr><tr><td>优化方向</td><td>方差, 即降低过拟合</td><td>偏差, 即降低欠拟合</td></tr></tbody></table><ol><li>Bagging + 决策树 = 随机森林</li><li>AdaBoost + 决策树 = 提升树</li><li>Gradient Boosting + 决策树 = GBDT</li></ol><h2 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h2><p>Stacking就比较特殊了, 前面二者都是基于样本的, 而Stacking是基于模型的组合. Stacking是在原有的模型预测结果上, 再重新训练一个模型, 就仿佛在原来的模型上进行”堆叠” 一个模型, 这样做能够从不同模型中提取出不同信息. 由于不同的特征, 可能不同类型的模型会在不同特征上有不同的预测效果, Stacking可以取其精华弃其糟粕, 舍弃它们不好的预测部分, 从而达到提升效果的目的.</p><p>对于Stacking来讲, 使用不同算法的模型可以增强效果, 在不同数据上训练得来的模型也可以增强效果.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/stacking.png" alt=""></p><p>根据上图, 比如已知数据集的大小为1000, 训练时应该遵循如下流程:</p><ol><li>取一个模型$\rm Model_1$, 对$\rm Model_1$做5折交叉验证, 得到5个用不同部分数据训练出的同质模型, 每次使用的训练集样本数量应该为800, 验证集样本数量为200.</li><li>提取每个同质模型中对验证集的预测结果(大小为200)作为堆叠的下一个模型$\rm Model_{next}$的数据集的一部分, 因为有5折交叉验证, 所以将五个$200\times 1$ 的数据拼起来得到的$1000\times 1$的数据集. 同时标签也拼接到这刚好和总共的数据集大小一致.</li><li>如果还有其他的模型作为第一层的模型, 那么将它们对自己验证集的结果拼接起来, 形成$1000\times n$的数据集作为下一层模型的训练数据.</li><li>反复执行.</li></ol><p>理解了训练过程, 测试过程就很简单了. 假设测试集是不包含在已知数据集的独立集合, 在每次每个分类器对全部的数据进行预测, 假设是5折交叉验证产生的分类器, 则对5个分类器得到的预测结果直接取平均, 得到同样测试集大小的测试集数据, 然后递交给下一层.</p><p>如果上述文字描述还不够直观, 那么下面这个图很好的说明了Stacking的工作过程(出自<a href="https://blog.csdn.net/ztf312/article/details/98852775" target="_blank" rel="noopener">【机器学习】Stacking方法详解</a>):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/stacking1.png" style="zoom:67%;" />]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 集成学习 </tag>
            
            <tag> Boosting </tag>
            
            <tag> Stacking </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之K邻近</title>
      <link href="/posts/40015.html"/>
      <url>/posts/40015.html</url>
      
        <content type="html"><![CDATA[<h1 id="K邻近KNN-K-Nearest-Neighbor"><a href="#K邻近KNN-K-Nearest-Neighbor" class="headerlink" title="K邻近KNN K-Nearest Neighbor"></a>K邻近KNN K-Nearest Neighbor</h1><p>K邻近是一种非常简单的监督学习分类方法. KNN指的是每个样本都可以通过它最近的K个样本来代表. 比方说在下述图片中, 若K=3, 找到距离未知样本即绿色圆圈最近的3个样本, 在该范围内红色三角占$\frac 2 3$, 则绿色圆圈被认为是红色三角的类别. 若K=5, 则蓝色方块所占的比例为$\frac 3 5$, 绿色圆圈被认为是蓝色方块. 如果K的取指不同, 则未知样本的类别也会产生改变, 所以结果很大程度取决于<strong>K的选择</strong>.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/knn.jpg" style="zoom:50%;" /><p>当然, 在这个过程中距离不一定是欧氏距离, 还可以选择曼哈顿距离.<br>$$<br>d(x, y) = \sqrt{\sum_{k=1}^n |x_k - y_k|}<br>$$<br>在实际应用过程中, 还可以基于距离的远近进行加权平均或投票, 距离越近的样本权重越大.</p><p>KNN是一种Lazy learner, 也就是懒惰学习算法. 它不需要训练, 只是单纯的记住所有的训练样本, 在进行预测时根据已经记住的训练集去寻找临近, 从而获得结果.</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之决策树</title>
      <link href="/posts/11302.html"/>
      <url>/posts/11302.html</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>2020.09.08</strong>: 更新了剪枝.</p></blockquote><h1 id="决策树DT-Desicion-Tree"><a href="#决策树DT-Desicion-Tree" class="headerlink" title="决策树DT Desicion Tree"></a>决策树DT Desicion Tree</h1><p>决策树(Decision Tree) 是在已知各种情况发生概率的基础上, 通过构成决策树来求取净现值的期望值大于等于零的概率, 评价项目风险, 判断其可行性的决策分析方法, 是直观运用概率分析的一种图解法. 由于这种决策分支画成图形很像一棵树的枝干, 故称决策树. 在机器学习中, 决策树是一个预测模型, 他代表的是对象属性与对象值之间的一种映射关系. Entropy = 系统的凌乱程度, 使用算法ID3, C4.5和C5.0生成树算法使用熵. 这一度量是基于信息学理论中熵的概念.  决策树是一种<strong>树形结构</strong>, 其中每个内部节点表示一个属性上的测试, 每个分支代表一个测试输出, 每个叶节点代表一种类别. </p><h2 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h2><p>决策树分为两种, 也就是按照它们功能进行区分的, 回归树和分类树. 决策树作为树类模型, 不依赖于特征的离散或连续, 树的分裂仅取决于分裂时的判断条件, 与特征的离散或连续性无关. </p><blockquote><ol><li><strong>根结点</strong>(Root Node): 它表示整个样本集合, 并且该节点可以进一步划分成两个或多个子集. </li><li><strong>拆分</strong>(Splitting): 表示将一个结点拆分成多个子集的过程. </li><li><strong>决策结点</strong>(Decision Node): 当一个子结点进一步被拆分成多个子节点时, 这个子节点就叫做决策结点. </li><li><strong>叶子结点</strong>(Leaf/Terminal Node): 无法再拆分的结点被称为叶子结点. </li><li><strong>剪枝</strong>(Pruning): 移除决策树中子结点的过程就叫做剪枝, 跟拆分过程相反. </li><li><strong>分支/子树</strong>(Branch/Sub-Tree): 一棵决策树的一部分就叫做分支或子树. </li><li><strong>父结点和子结点</strong>(Paren and Child Node): 一个结点被拆分成多个子节点, 这个结点就叫做父节点；其拆分后的子结点也叫做子结点. </li></ol></blockquote><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/决策树.png" style="zoom:67%;" /><h2 id="构造过程"><a href="#构造过程" class="headerlink" title="构造过程"></a>构造过程</h2><p>决策树的构造过程一般分为3个部分, 分别是特征选择, 决策树生产和决策树裁剪. </p><ol><li><p><strong>特征选择</strong></p><p>特征选择表示从众多的特征中选择一个特征作为当前节点分裂的标准, 如何选择特征有不同的量化评估方法, 从而衍生出不同的决策树, 如ID3(通过信息增益选择特征) , C4.5(通过信息增益比选择特征) , CART(通过Gini指数选择特征) 等. </p><p>目的(准则) : 使用某特征对数据集划分之后, 各数据子集的纯度要比划分钱的数据集D的纯度高(也就是不确定性要比划分前数据集D的不确定性低.</p></li><li><p><strong>决策树的生成</strong></p><p>根据选择的特征评估标准, 从上至下递归地生成子节点, 直到数据集不可分则停止决策树停止生长. 这个过程实际上就是使用满足划分准则的特征不断的将数据集划分成纯度更高, 不确定行更小的子集的过程. 对于当前数据集的每一次划分, 都希望根据某个特征划分之后的各个子集的纯度更高, 不确定性更小. </p></li><li><p><strong>决策树的裁剪</strong></p><p>决策树容易过拟合, 一般需要剪枝来缩小树结构规模, 缓解过拟合. </p></li></ol><h2 id="划分选择"><a href="#划分选择" class="headerlink" title="划分选择"></a>划分选择</h2><p>划分选择是决策树进行学习的关键. 我们希望决策树的分支节点包含的样本尽可能属于同一类别, 即节点的纯度越来越高, 选择方法都是基于<strong>最大熵原理</strong>的. 最大熵原理是一种选择随机变量统计特性最符合客观情况的准则, 也称为最大信息原理, 在已知一些知识的情况下, 将其他所有未知的事件全部当做等概率事件来处理. <strong>万物趋近于无序, 当事件越不确定(等事件概率发生)时, 熵就最大</strong>.</p><h3 id="信息增益-Information-Gain"><a href="#信息增益-Information-Gain" class="headerlink" title="信息增益 Information Gain"></a>信息增益 Information Gain</h3><p>信息熵是度量样本集合纯度的一种常用指标. 对于当前样本集合$D$中第$k$类样本所占的比例$p_k$, 其信息熵定义为:<br>$$<br>{\rm Ent}(D) = - \sum_k^{K}p_k \log_2{p_k}<br>$$<br><strong>熵越小, 未知的信息就越少</strong>, 即纯度越高. 这里约定如果$p=0$, 则$p\log_2p=0$ .</p><p>而在节点划分时, 可能特征$a$有多个可以取到的可能性$V$, 对数据集进行划分, 这时候可以用条件熵是用来表示在这个特征下某值的不确定性. 这里的${\rm Ent}(D^v)$ 其实就是${\rm Ent}(D|a=a_v)$. 为了保证后续和西瓜书上的公式形式一致采取了前者.<br>$$<br>{\rm Ent}(D|a)=\sum_{v=1}^V\frac{|D^v|}{|D|}{\rm Ent}(D^v)<br>$$<br>信息增益就是在按照特征的某值进行划分后的熵的变化, 条件熵越大, 证明划分效果越差, 对应的信息增益就越少:<br>$$<br>{\rm Gain}(D, a) ={\rm Ent}(D)-{\rm Ent}(D|a)<br>$$<br>使用信息增益作为节点划分依据的算法称为<strong>ID3算法</strong>.</p><p>以周志华老师的西瓜书中的西瓜数据集为例:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/西瓜数据集.png" style="zoom: 33%;" /><p>明显西瓜只有好瓜和坏瓜两种, 好瓜$p_1=\frac{8}{17}$. 坏瓜$p_2=\frac{9}{17}$首先计算出根节点的信息熵:<br>$$<br>{\rm Ent}(D) = - \sum_{k=1}^{2}p_k \log_2{p_k}=-(\frac{8}{17}\log_2\frac{8}{17}+\frac{9}{17}\log_2\frac{9}{17})=0.998<br>$$<br>然后要遍历当前的属性集合$\{色泽, 根蒂, 敲声, 纹理, 脐部, 触感\}$. 以色泽为例, 有三个可能取值$\{青绿, 乌黑, 浅白\}$. 将这个属性进行划分, $D^1(色泽=青绿)$, $D^2(色泽=乌黑)$, $D^3(色泽=浅白)$. 对于子集$D^1, p_1=\frac{3}{6}, p_2=\frac{3}{6}$, 对于子集$D^2, p_1=\frac{4}{6}, p_2=\frac{2}{6}$, 对于子集$D^3, p_1=\frac{1}{5}, p_2=\frac{4}{5}$, 以色泽为划分依据之后得到三个节点的信息熵为:<br>$$<br>\begin{aligned}<br>{\rm Ent}(D^1)&amp;=-(\frac{3}{6}\log_2\frac{3}{6}+\frac{3}{6}\log_2\frac{3}{6})=1 \\<br>{\rm Ent}(D^2)&amp;=-(\frac{4}{6}\log_2\frac{4}{6}+\frac{2}{6}\log_2\frac{2}{6})=0.918 \\<br>{\rm Ent}(D^3)&amp;=-(\frac{1}{5}\log_2\frac{1}{5}+\frac{4}{5}\log_2\frac{4}{5})=0.722 \\<br>\end{aligned}<br>$$<br>因此能够根据信息熵计算出信息增益为:<br>$$<br>\begin{aligned}<br>{\rm Gain}(D, 色泽)&amp;={\rm Ent}(D)-\sum_{v=1}^3\frac{|D^v|}{|D|}{\rm Ent}(D^v)\\<br>&amp;=0.998-(\frac{6}{17}\times 1 + \frac{6}{17} \times 0.918 + \frac{5}{17}\times 0.722) \\<br>&amp;=0.109<br>\end{aligned}<br>$$</p><h3 id="信息增益比-Information-Gain-Ratio"><a href="#信息增益比-Information-Gain-Ratio" class="headerlink" title="信息增益比 Information Gain Ratio"></a>信息增益比 Information Gain Ratio</h3><p>不要忘记西瓜数据集中, 仍然含有”编号”这一特征, 如果将其作为特征纳入决策树的选择中, 那岂不是决策树可以完美拟合编号这一特征, 而完全丧失了泛化能力? 其实对于其他取值较多的特征亦是如此, 当特征的可能性较多时, 每次做一次节点划分, <strong>信息增益会偏爱那些取值多的特征</strong>, 因为原本特征的不确定性就比较大, 所以当选择一个值作为划分时, 消除的熵也很多, 信息增益就会变得大.</p><p>这时就需要用某种熵的定义来平衡掉这个偏好, 也就是”信息增益率”.<br>$$<br>{\rm Gain\_ratio}(D, a) = \frac{ {\rm Gain}(D, a)} { {\rm IV}(a)}<br>$$<br>其中有:<br>$$<br>{\rm IV}(a) = -\sum_{v=1}^V\frac{|D^v|}{|D|}\log_2 \frac{|D^v|}{|D|}<br>$$<br>$a$ 的可能取值数目越多, 则${\rm IV}(a)$就越大(其实它完全就是按照熵来定义的, 不确定性越多熵越大). 但是信息增益率可能对取值数目少的特征有所偏好, 因此是先从候选划分属性中找出信息增益高于平均水平的属性, 然后再从中选择增益率最高的.</p><p>使用信息增益比作为节点划分依据的算法称为<strong>C4.5算法</strong>.</p><h3 id="基尼指数-Gini-Index"><a href="#基尼指数-Gini-Index" class="headerlink" title="基尼指数 Gini Index"></a>基尼指数 Gini Index</h3><p>基尼值表示了<strong>数据集的纯度</strong>, 因此划分准则可以用基尼指数:<br>$$<br>\begin{aligned}<br>{\rm Gini}(D)&amp;=\sum_{k=1}^K\sum_{k’\neq k}p_kp_{k’} \\<br>&amp;=1-\sum_{k=1}^Kp_k^2<br>\end{aligned}<br>$$<br>某个属性的基尼指数定义为:<br>$$<br>{\rm Gini\_index}(D,a)=\sum_{v=1}^{V}\frac{|D^v|}{|D|}{\rm Gini}(D^v)<br>$$<br>其实和之前的条件熵的形式如出一辙. 因此我们在选择候选属性时, 选择划分后基尼指数最小的属性为最优划分属性.</p><p>使用信息增益比作为节点划分依据的算法称为<strong>CART算法</strong>.</p><h2 id="连续值处理"><a href="#连续值处理" class="headerlink" title="连续值处理"></a>连续值处理</h2><p>对于连续属性, 可取值书目不再有限, 因此采用最简单的<strong>二分法</strong>. 对于给定样本中连续属性的所有不同取值, 从小到大进行排序, 然后对每个值进行一次二分, 每次都产生比划分点$t$ 的值不大的集合$D_t^-$和比它大的集合$D_t^+$, 然后再进行计算信息增益, 以信息增益最大的划分点作为划分依据即可.</p><h2 id="决策树剪枝"><a href="#决策树剪枝" class="headerlink" title="决策树剪枝"></a>决策树剪枝</h2><p>剪枝是决策树缓解过拟合的重要手段, 当决策树对节点划分过于细致时候就容易发生过拟合. 决策树有预剪枝和后剪枝两种剪枝策略, 即分别在决策树构建划分节点时和决策树生长完成后剪枝.</p><h3 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h3><p>预剪枝基于分支划分的准则, 在局部子树生成完成后, 依据<strong>验证集</strong>计算节点划分后能否带来收益, 如果没有性能上的提升或者导致性能下降, 则禁止该节点的划分.</p><p>除了依据划分准则进行预剪枝外, 还可以设定树的生长高度或导致叶子节点分裂的最小样本数, 禁止节点划分的最小阈值等参数, 达到防止过拟合的效果.</p><p>因为预剪枝是与决策树构建并行的, 所以可能会因为局部的性能提升而剪掉更有潜力的节点, 可能过早的停止决策树构造. 常常效果也没有后剪枝好.</p><h3 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h3><p>等决策树完整生成后, 后剪枝才开始工作. 后剪枝将从决策树的底部往上进行剪枝, 如果剪枝能够提高验证集的精度, 那么就将其裁去. 后剪枝还有许多其他的剪枝方法, 通过采用不同的性能标准来决定节点的保留与否. </p><p>后剪枝因为不具有视野上的问题, 保留的分支常比预剪枝要多, 泛化能力也更好一些.</p><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p>优点: 可解释性好, 分类速度快.</p><p>缺点: 如果不采用剪枝或随机森林很容易发生过拟合, 体现在决策树结构中就是树划分的过细, 或者深度过深.</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 决策树 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之朴素贝叶斯</title>
      <link href="/posts/63092.html"/>
      <url>/posts/63092.html</url>
      
        <content type="html"><![CDATA[<h1 id="朴素贝叶斯NB-Naive-Bayes"><a href="#朴素贝叶斯NB-Naive-Bayes" class="headerlink" title="朴素贝叶斯NB Naive Bayes"></a>朴素贝叶斯NB Naive Bayes</h1><p>朴素贝叶斯有一个非常Naive的假设: 所有特征都是相互独立的, 因此所有特征总的条件概率总是每个特征条件概率的乘积. 这个算法的核心就在于贝叶斯公式.</p><h2 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h2><p>条件概率是贝叶斯定理的铺垫. 指的是事件A在另外一个事件B已经发生条件下的发生概率, 记为$P(A|B)$. </p><p>比方说有某个布袋, 其中的有2个蓝球3个红球. 问每次不放回的随机从布袋中取出一个球, 连续两次拿到蓝球概率是多少? 第一次拿到蓝球记为$P(A)$, 概率是$\frac{2}{5}$, 假设第一次拿到了蓝球, 那么第二次布袋里一定只有1个蓝球和3个红球. 第二次拿到蓝球记为$P(B|A)$, 概率为$\frac{1}{4}$, 记为, 所以连续两次拿到蓝球的概率为$\frac{2}{5}*\frac{1}{4}=\frac{1}{10}$. 如果把连续两次拿到蓝球记为$P(A, B)$, 那么就得到了条件概率公式:<br>$$<br>P(A, B) = P(B|A)\cdot P(A)<br>$$<br>反推一下B在A的条件下的概率$P(B|A)$:<br>$$<br>P(B|A) = \frac{P(A, B)}{P(A)}<br>$$</p><h2 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h2><p>根据条件概率, 可以类似的推出A在B的条件下的概率$P(A|B)$:<br>$$<br>P(A|B) = \frac{P(A, B)}{P(B)}<br>$$<br>将两侧相同的$P(A,B)$联立消去, 就得到了贝叶斯定理:<br>$$<br>P(B|A) = \frac{P(A|B)P(B)}{P(A)}<br>$$<br>在这个式子中, 称$P(B)$为<strong>先验概率</strong>, 即在不知道A事件条件下, 对B事件发生做出的判断. $P(B|A)$称为<strong>后验概率</strong>, 即在A事件发生后对B事件的重新评估. $\frac{P(A|B)}{P(A)}$称为调整因子或者<strong>似然概率</strong>, 它对先验概率进行调整, 使其变为后验概率. </p><h2 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h2><p>朴素贝叶斯正是建立在贝叶斯定理的基础上, 对条件概率分布做条件独立性假设, 即特征之间相互独立, 并且他们同等重要. 虽然影响精度, 但仍然在某些问题上取得良好的效果.</p><p>在贝叶斯定理中, 对于多特征的情况, 将特征写成一个向量$\textbf x$, 对于该样本属于第$k$的个类别的事件记为$C_k$, 将其转化为向量形式有:<br>$$<br>P(C_k|\textbf x) = \frac{P(\textbf x|C_k)P(C_k)}{P(\textbf x)}<br>$$<br>写成文字也就是:<br>$$<br>P(类别|特征) = \frac{P(特征|类别)P(类别)}{P(特征)}<br>$$<br>基于各个假设独立的特征, 分母$P(\textbf x)$是一个常数, 将分子$P(\textbf x|C_k)P(C_k)$基于链式法则, 在特征独立的情况下, 将分子重新写为:<br>$$<br>P(C_k| x_1, x_2, \dots, x_n)=P(C_k)\prod_{i=1}^nP(x_i|C_k)<br>$$<br>这样根据样本中的特征出现频率, 就能得到各个先验概率, 从而推断出后验概率了.</p><h2 id="分类准则"><a href="#分类准则" class="headerlink" title="分类准则"></a>分类准则</h2><p>分母$P(\textbf x)$是一个常数, 从优化目标中略去. 只要满足分类器基于特征独立下, 预测目标的概率最大即可.<br>$$<br>\hat{y}=\arg\max\limits_{c \in y} P(c)\prod_{i=1}^dP(x_i|c)<br>$$</p><h2 id="拉普拉斯平滑"><a href="#拉普拉斯平滑" class="headerlink" title="拉普拉斯平滑"></a>拉普拉斯平滑</h2><p>如何处理概率为0的情况? 由于概率在分子上和分母上都是连乘形式, 如果其中一个为0则导致整个分类错误, 这显然不合理. 根据拉普拉斯平滑(也称为拉普拉斯修正), 对类别概率$P(c)$和预测中使用的先验概率$P(x_i|c)$进行修正, 如下:<br>$$<br>\begin{aligned}<br>\hat{P}(c) &amp;= \frac{|D_c|+1}{|D|+N} \\<br>\hat{P}(x_i|c) &amp;= \frac{|D_{c, x_i}|+1}{|D_c|+N_i}<br>\end{aligned}<br>$$<br>其中$D$代表数据集, $N$为可能的类别总数, $N_i$为第$i$个属性可能取值数, </p><h2 id="离散和连续"><a href="#离散和连续" class="headerlink" title="离散和连续"></a>离散和连续</h2><p>如果是离散特征, 直接用特征的频率除以样本总数作为概率即可. </p><p>如果是连续特征, 必须要结合概率密度函数, 假设特征服从正态:<br>$$<br>G(x, \mu, \sigma) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}<br>$$<br>因此$P(x_i|C_k)=G(x_i,\mu_{c,i}, \sigma_{c,i})$. $\mu_{c,i}$和$\sigma^2_{c,i}$分别是第$c$类样本在第$i$个特征上的均值和方差.</p><h2 id="优缺点分析"><a href="#优缺点分析" class="headerlink" title="优缺点分析"></a>优缺点分析</h2><p>优点:</p><ul><li>既简单又快速，预测表现良好；</li><li>如果变量独立这个条件成立，相比Logistic回归等其他分类方法，朴素贝叶斯分类器性能更优，且只需少量训练数据；</li><li>相较于数值变量，朴素贝叶斯分类器在多个分类变量的情况下表现更好。若是数值变量，需要正态分布假设。</li></ul><p>缺点:</p><ul><li>如果分类变量的类别（测试数据集）没有在训练数据集总被观察到，那这个模型会分配一个0（零）概率给它，同时也会无法进行预测。这通常被称为“零频率”。为了解决这个问题，我们可以使用平滑技术，拉普拉斯估计是其中最基础的技术。</li><li>朴素贝叶斯也被称为<strong>bad estimator</strong>，所以它的概率输出predict_proba不应被太认真对待。</li><li>朴素贝叶斯的另一个限制是独立预测的假设。在现实生活中，这几乎是不可能的，各变量间或多或少都会存在相互影响。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之逻辑回归与线性回归</title>
      <link href="/posts/39405.html"/>
      <url>/posts/39405.html</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>2020.08.22</strong>: 附加了后续的逻辑回归部分. </p></blockquote><h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>想要了解逻辑回归，必须了解线性回归. </p><h2 id="线性回归-Logistcs-Regression"><a href="#线性回归-Logistcs-Regression" class="headerlink" title="线性回归 Logistcs Regression"></a>线性回归 Logistcs Regression</h2><p>线性回归是监督学习中最简单的模型了, 它具有非常好的<strong>可解释性</strong>, 也有一种简洁的典雅美. 在机器学习中, 不再区分一元和多元线性回归. 线性回归可以用来解决回归问题, 当然也可以通过一个函数来解决<strong>分类</strong>问题.<br>$$<br>\hat{y} = w_1x_1 + w_2x_2+\cdots + w_nx_n + b<br>$$<br>每个$x$ 代表一维度的特征, $w$ 代表这一维特征对预测目标$\hat{y}$ 影响所占的权重. $b$ 是一个偏置. </p><p>将以上方程用矩阵形式描述:</p><p>$$<br>Y = W^TX +b<br>$$<br>我们要求解的就是权重矩阵$W$和偏置$b$. 假设损失函数为均方误差$\rm MSE$, 代入损失函数有:<br>$$<br>\ell(w_i, b) =\frac{1}{n}\sum\limits_{i=1}^n(w_ix_i+b-\hat{y})^2<br>$$<br>那么为了最小化损失函数, 最终目标为:<br>$$<br>(w_i’, b) = \mathop{\arg\min}_{w_i, b}\ell(w_i, b)<br>$$<br>也就是求出最终能使损失最小化的解$(w_i’, b)$. 如何求出这个最优解呢? 我们需要借助优化方法.</p><h2 id="最小二乘法-Least-Squares-Method"><a href="#最小二乘法-Least-Squares-Method" class="headerlink" title="最小二乘法  Least Squares Method"></a>最小二乘法  Least Squares Method</h2><p>最小二乘法其实是线性回归最早的调整权重的方法(估计高中都接触过). 最小二乘法的想法非常朴实, 既然是<strong>线性问题</strong>, 想要让目标函数最小, 直接对损失函数求导, 令其导数为0, 利用和特征数量相等的样本数, 解一个多元方程组, 自然能够得到所有参数的解. 那样本不够呢? 函数不是线性的呢? 而且看起来样本数量很大时直接解这个方程组也不是很现实. 这时候就必须换一种思路, 不是一步登天, 而是通过某种迭代的方式, 慢慢逼近结果.</p><h2 id="梯度下降-Gradient-Descent"><a href="#梯度下降-Gradient-Descent" class="headerlink" title="梯度下降 Gradient Descent"></a>梯度下降 Gradient Descent</h2><p>沿梯度的方向前进, 函数值上升的速度最快. 那么沿着<strong>负梯度</strong>的方向前进, 就能让函数值以最快的速度下降. 对于每个参数, 只需要求出损失函数对它的偏导数, 就能调整参数值.</p><p>$$<br>\nabla f(x, y)=(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y})<br>$$<br>我们可以手动设置一个学习率$\alpha$, 来分别调整每个$w_i$和每个$b$的参数值, 经过多次迭代使得其值收敛与某个能够成功拟合问题的值. 这里直接使用梯度下降, 快速调整参数.<br>$$<br>\begin{aligned}<br>w_i &amp;\leftarrow w_i - \alpha\frac{\partial\ell(w_i, b)}{\partial{w_i}}\\<br>b&amp;\leftarrow b - \alpha\frac{\partial\ell(w_i, b)}{\partial{b}}<br>\end{aligned}<br>$$</p><h3 id="批量梯度下降-Batch-gradient-descent"><a href="#批量梯度下降-Batch-gradient-descent" class="headerlink" title="批量梯度下降 Batch gradient descent"></a>批量梯度下降 Batch gradient descent</h3><p><strong>逐个地</strong>在每个数据点应用均方(或绝对)误差, 并重复这一流程很多次. 它对所有样本进行迭代实现了并行, 同时用总体样本指明了下降的方向, 很有可能找到全局最优. 缺点就是当样本很多时, 每次都要对所有样本迭代导致训练缓慢.</p><h3 id="随机梯度下降-Stochastic-gradient-descent"><a href="#随机梯度下降-Stochastic-gradient-descent" class="headerlink" title="随机梯度下降 Stochastic gradient descent"></a>随机梯度下降 Stochastic gradient descent</h3><p><strong>同时</strong>在每个数据点应用均方(或绝对)误差, 并重复这一流程很多次. 这样每次只对一个样本进行迭代, 训练速度会加快, 但不易于并行, 且由于个体样本的差异不容易学习到总体样本的特性.</p><h3 id="小批次梯度下降法-Mini-batch-gradient-descent"><a href="#小批次梯度下降法-Mini-batch-gradient-descent" class="headerlink" title="小批次梯度下降法 Mini batch gradient descent"></a>小批次梯度下降法 Mini batch gradient descent</h3><p>批量梯度下降和随机梯度下降的<strong>折中</strong>, 上述二者都是极端情况, 一个使用所有样本, 一次只使用一个样本. 所以为什么不去取折中的办法, 取一个合适的迭代样本数量<strong>batch_size</strong> , 每次都迭代batch_size个样本. 它具有上述两种算法的优点平均, 缺点仅有batch_size设置不当会导致问题.</p><ul><li>如果batch_size在合理范围内增大, 有以下好处<ol><li>内存利用率提高.</li><li>每个epoch所用的时间少了, 也就是训练时间减少.</li><li>准确率增加, 震荡情况减少.</li></ol></li><li>盲目增大batch_size的坏处:<ol><li>内存可能扛不住.</li><li>花费时间变多, 逐渐变向批次梯度下降.</li><li>batch_size当增大到一定阈值时, 下降的方向已经不发生变化, 这时盲目增大batch_size是无用功.</li></ol></li></ul><h2 id="逻辑回归-Logistics-Regression"><a href="#逻辑回归-Logistics-Regression" class="headerlink" title="逻辑回归 Logistics Regression"></a>逻辑回归 Logistics Regression</h2><p>逻辑回归是在线性回归基础上的延伸. 逻辑回归虽然叫做回归，可是却是一个<strong>分类模型</strong>. 线性模型虽然简单, 但却含有丰富的变化. 应该考虑用某个函数来使得线性回归来适应分类问题. 不然分类会变得十分困难，比如在拟合单位阶跃函数时，回归问题就不能迁移到分类问题上来.<br>$$<br>y=<br>\begin{cases}<br>0, \quad y&lt;z; \\<br>0.5, \quad z=0; \\<br>1, \quad z &gt;0;<br>\end{cases}<br>$$<br>线性模型虽然简单，但是在预测$y$ 的衍生物时却显得有些无力. 比如在学习$y$ 的对数作为目标时就显得有些无力. 但是可以对线性回归做相应的变化，使得其容易逼近.<br>$$<br>\ln y=W^TX+b<br>$$<br>这就是<strong>对数几率回归</strong>，在形式上仍然是线性回归，但实际上已经是在求输入到输出的<strong>非线性映射</strong>了. 这里的对数起到了关联输出和输入的作用. 对于更普遍的输出，只需要考虑单调可微的函数$g(\cdot)$ :<br>$$<br>y = g^{-1}(W^TX+b)<br>$$<br>这样称为<strong>广义线性模型</strong>，对任意要逼近的目标都是适用的.<br>阶跃函数是不连续，不可微的. 使用对数几率函数来代替这个函数，使得其满足可微性质.<br>$$<br>y=\frac{1}{1+e^{-z}}<br>$$<br>对数几率函数也是神经网络中常说的$\sigma$ 激活函数，在神经网络中起到至关重要的作用.<br>它能够将任意输入放缩到$(0, 1)$ 之间，视为概率.<br>将$\sigma$ 函数带入广义线性模型中，能够得到:<br>$$<br>y=\frac{1}{1+e^{-(W^TX+b)}}<br>$$<br>那么将其转换为线性回归的形式:<br>$$<br>\ln\frac{y}{1-y}=W^TX+b<br>$$<br>如果预测值$y$ 是样本$x$ 为正例的可能性，那么$1-y$ 即是其反例可能性. 二者的比值称为<strong>几率</strong>，再对其取对数，对数几率函数的名字便由此诞生了. 想要训练这个分类模型，就不能单单再用之前定义过的MSE作为损失函数了. 根据极大似然，只要以类别概率的对数作为损失函数，通过迭代更新参数就行了. 设数据集中样本数为$m$损失函数是:<br>$$<br>\ell(w,b)=\sum_{i=1}^m \ln p(y_i\mid x_i;w, b)<br>$$<br>如果想要实现多分类，有多种方法可以实现. </p><ol><li>将Sigmoid函数替换为Softmax函数即可，然后对应的改变损失函数. </li><li>采用<code>One vs All</code>的方法，假设要为三类，则要分别训练三个分类器，每个都将自己要识别的类视为正例，其余两个类全部视为反例. 最终预测时从三个分类器中取最高概率作为该样本的类别. 对于$k$ 类，只需要$k$个分类器. 这么做会导致天然的训练集样本不平衡. 当选择其中一个类为正类时，其余两个类的样本都是反类. </li><li>采用<code>One vs One</code>的方法，假设要分三类，则像车轮战一样，每次只训练两个类的样本，最后通过三个分类器的投票来决定样本点的类别. 最终需要$C_k^2$个分类器. 这样一定程度上改善了训练集不均衡的问题，但可能会增加训练资源开销. </li></ol>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线性回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算机网络-自顶向下</title>
      <link href="/posts/40485.html"/>
      <url>/posts/40485.html</url>
      
        <content type="html"><![CDATA[<h1 id="计算机网络"><a href="#计算机网络" class="headerlink" title="计算机网络"></a>计算机网络</h1><p>计网复习笔记, 参考书籍为<a href="[https://baike.baidu.com/item/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E8%87%AA%E9%A1%B6%E5%90%91%E4%B8%8B%E6%96%B9%E6%B3%95/9362103?fr=aladdin](https://baike.baidu.com/item/计算机网络自顶向下方法/9362103?fr=aladdin)">自顶向下</a>.</p><h2 id="计算机网络体系结构"><a href="#计算机网络体系结构" class="headerlink" title="计算机网络体系结构"></a>计算机网络体系结构</h2><p>这里的概念都比较散, 大多是一些计网的基础概念和整体知识的框架.</p><p><strong>计算机网络的功能:</strong></p><ol><li>数据通信</li><li>资源共享</li><li>分布式处理</li><li>提高可靠性</li><li>负载均衡</li></ol><p><strong>计算机网络的分类(距离分):</strong></p><ul><li>广域网 - 长距离通信, 几十千米到几千千米</li><li>城域网 - 覆盖几个街区或城市</li><li>局域网 - 短距离通信, 几十米到几千米</li><li>个人区域网 - WPAN, 直径十米</li></ul><p><strong>传输类型分类</strong>: 广播式, 点对点</p><p><strong>主机间的通信方式</strong>: C/S, P2P</p><p><strong>协议</strong>: 定义了在两个或多个通信实体之间交换的<strong>报文格式</strong>和<strong>次序</strong>, 以及在报文传输和接收或其它事件方面所采取的<strong>动作</strong>.</p><p><strong>三种交换方式的区别?</strong></p><blockquote><p>电路交换: 整个报文的比特流从源点连续的直达终点, 像在一个管道中传输. 包括建立连接, 传输数据和断开连接三个阶段. 最典型的电路交换网络是传统电话网络.  </p><p>报文交换: 将整个报文转发到相邻节点, 全部存储下来, 查找转发表, 转发到下一个节点. 是<strong>存储-转发</strong>类型的网络.  </p><p>分组交换: 将报文分组转发到相邻节点, 查找转发表, 转发到下一个节点. 也是<strong>存储-转发</strong>类型的网络. 网络核心是分组交换.</p></blockquote><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/交换方式.jpg" style="zoom:50%;" /><p><strong>接入网络的三种方式</strong>:</p><ul><li><p>住宅接入: modem, xDSL, HFC</p></li><li><p>公司: LAN</p></li><li><p>无线接入: Wireless LAN, Wireless WAN</p></li></ul><p><strong>物理媒体</strong>:</p><ul><li>引导型: 带线的, 光纤, 双绞线, 同轴电缆</li><li>非引导性: 波类的, 无线, 卫星</li></ul><p><strong>主要性能指标:</strong></p><ol><li><p><strong>带宽(Bandwidth)</strong>: 本来表示通信线路允许通过的信号频带范围, 但在计算机网络中, 带宽表示网络的通信线路所能 传送数据的能力, 是数字信道所能传送的”最高数据率”的同义词, 单位是比特/秒( b/s). </p></li><li><p><strong>时延(Delay)</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/延时.jpg" style="zoom:50%;" /></li><li><p><strong>时延带宽积</strong>: 指发送端发送的第一个比特即将到达终点时, 发送端已经发送了多少个比特, 因此又称以比特为单位的链路长度, 即<strong>时延带宽积 = 传播时延 * 信道带宽</strong>.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/时延带宽积.jpg" style="zoom:50%;" /><p><strong>流量强度约为0时, 平均排队时延小, 趋于1时时延变大, 大于1时平均时延无穷大</strong>.</p></li></ol><p><strong>计算机网络提供的服务的三种分类</strong>:</p><ol><li><p><strong>面向连接服务与无连接服务</strong></p><p>在面向连接服务中,  通信前双方必须先建立连接,  分配相应的资源( 如缓冲区) ,  以保证通信能正常进行,  传输结束后释放连接和所占用的资源. 因此这种服务可以分为连接建立, 数据传输和连接释放三个阶段. 例如TCP就是一种面向连接服务的协议.  在无连接服务中,  通信前双方不需要先建立连接,  需要发送数据时可直接发送, 把每个带有目的地址的包( 报文分组)  传送到线路上,  由系统选定路线进行传输. 这是一种不可靠的服务.  这种服务常被描述为”尽最大努力交付”(Best-Effort-Delivery), 它并不保证通信的可靠性. 例如 IP, UDP就是一种无连接服务的协议. </p></li><li><p><strong>可靠服务和不可靠服务</strong></p><p>可靠服务是指网络具有纠错, 检错, 应答机制,  能保证数据正确, 可靠地传送到目的地.  不可靠服务是指网络只是尽晕正确, 可靠地传送,  而不能保证数据正确, 可靠地传送到目的地,  是一种尽力而为的服务.  对于提供不可靠服务的网络,  其网络的正确性, 可靠性要由应用或用户来保障. 例如,  用户收到信息后要判断信息的正确性,  如果不正确,  那么用户要把出错信息报告给信息的发送者,  以便发送者采取纠正措施. 通过用户的这些措施,  可以把不可靠的服务变成可靠的服务. </p></li><li><p><strong>有应答服务和无应答服务</strong></p><p>有应答服务是指接收方在收到数据后向发送方给出相应的应答, 该应答由传输系统内部自动实现,  而不由用户实现. 所发送的应答既可以是肯定应答,  也可以是否定应答,  通常在接收到的数据有错误时发送否定应答. 例如,  文件传输服务就是一种有应答服务.  无应答服务是指接收方收到数据后不自动给出应答. 若需要应答,  则由高层实现. 例如, 对于WWW服务,  客户端收到服务器发送的页面文件后不给出应答. </p></li></ol><p><strong>网络分层</strong>:</p><p><strong>层次划分是根据功能划分的, 与实现的方法无关</strong>. 譬如RIP协议是网络层协议, 但却是通过应用层和传输层来实现的.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/iso_osi.jpg" alt=""></p><p><strong>五层协议</strong></p><blockquote><p>应用层 : 为特定应用程序提供数据传输服务, 例如 HTTP, DNS 等协议. 数据单位为报文. </p><p>传输层 : 为进程提供通用数据传输服务. 由于应用层协议很多, 定义通用的传输层协议就可以支持不断增多的应用层协议. 运输层包括两种协议: 传输控制协议 TCP, 提供面向连接, 可靠的数据传输服务, 数据单位为报文段, 用户数据报协议 UDP, 提供无连接, 尽最大努力的数据传输服务, 数据单位为用户数据报. TCP 主要提供完整性服务, UDP 主要提供及时性服务. ( 流量控制, 差错控制, 服务质量, 数据传输管理, 端到端)  </p><p>网络层 : 为主机提供数据传输服务. 而传输层协议是为主机中的进程提供数据传输服务. 网络层把传输层传递下来的报文段或者用户数据报封装成分组. ( 流量控制, 拥塞控制, 差错控制, 网 际互联)  </p><p>链路层 : 网络层针对的还是主机之间的数据传输服务, 而主机之间可以有很多链路, 链路层协议就是为同一链路的主机提供数据传输服务. 数据链路层把网络层传下来的分组封装成帧.  ( 封装成帧, 差错控制, 流量控制, 传输管理)  </p><p>物理层 : 考虑的是怎样在传输媒体上传输数据比特流, 而不是指具体的传输媒体. 物理层的作用是尽可能屏蔽传输媒体和通信手段的差异, 使数据链路层感觉不到这些差异. </p></blockquote><p><strong>OSI(七层协议)</strong></p><blockquote><p>表示层 : 数据压缩, 加密以及数据描述, 这使得应用程序不必关心在各台主机中数据内部格式不同的问题.  </p><p>会话层 : 建立及管理会话.  </p><p>五层协议没有表示层和会话层, 而是将这些功能留给应用程序开发者处理. </p></blockquote><p><strong>端到端</strong>: 端到端通信建立在点到点通信的基础上, 它是由一段段的点到点通信信道构成的, 是比点到 点通信更高一级的通信方式, 以完成应用程序( 进程)  之间的通信. “端” 是指<strong>用户程序的端口</strong>,  端口号标识了应用层中<strong>不同的进程</strong>. </p><p><strong>点到点</strong>: 直接相连的结点之间的通信称为点到点通信,  它只<strong>提供一台机器到另一台机器之间的通信</strong>,  不涉及程序或进程的概念. 同时, 点到点通信并不能保证数据传输的可靠性, 也不能说明源主机与目的主机之间是哪两个进程在通信, 这些工作都是由网络层来完成的. </p><h2 id="应用层"><a href="#应用层" class="headerlink" title="应用层"></a>应用层</h2><p>应用层讲解了与用户距离最近的网络应用, 网络应用才是计算机网络存在的原因. 应用层传输数据的单位是<strong>报文(message)</strong>.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/应用层框架.jpg" style="zoom:67%;" /><h3 id="HTTP-超文本传输协议"><a href="#HTTP-超文本传输协议" class="headerlink" title="HTTP 超文本传输协议"></a>HTTP 超文本传输协议</h3><p> WEB网页是由一些<strong>对象</strong>该组成的, HTML文件涵盖了网页源码, 其中就包括其他媒体流文件对象的链接. 每个对象都能通过<code>URL</code>进行标识.</p><blockquote><p><a href="http://www.someschool.edu/someDept/pic.gif" target="_blank" rel="noopener">www.someschool.edu/someDept/pic.gif</a></p></blockquote><p>HTTP(HyperText Transfer Protocol, 超文本传输协议): HTTP定义了浏览器(万维网客户进程) 怎样向万维网服务器请求万维网文档, 以及服务器怎样把文档传送给浏览器. 从层次的角度看,  HTTP 是面向事务的(Transaction-oriented) 应用层协议, 它规定了在浏览器和服务器之间的请求和响应的格式与规则, 是万维网上能够可靠地交换文件( 包括文本, 声音, 图像等各种多媒体文件) 的重要基础. </p><p>HTTP使用TCP, 端口号80, 也是无状态的(不保存用户过去的使用信息, Cookie可以解决). 有持久和非持久连接方式. 不同连接方式下所需传输文件的时间必定是不一样的. 定义了<strong>往返时延</strong>作为衡量标准.</p><p><strong>往返时延RRT的定义</strong>: 从客户机到服务器发送一个小分组并返回所历经的时间.</p><p>假设当前网页有1个HTML对象, 3个JPG对象, 1个Audio对象, 共计5个对象.</p><p><strong>非持久</strong>: 每个对象都建立连接, 然后再传输. </p><p><strong>持久 - 非流水线</strong>: 先建立一次连接, 之后串行地传输每个对象, 超时关闭.</p><p><strong>持久 - 流水线</strong>: 建立连接后, 并行的传输每个对象, 并认为所有对象发送时间约为1个RTT. 但是必须先传输HTML对象, 所以多个对象传输也可以认为总共是3个RTT. 超时关闭.</p><table><thead><tr><th>连接方式</th><th>所需RTT</th><th>公式</th></tr></thead><tbody><tr><td>非持久</td><td>5 * 2 RTT  = 10 RTT</td><td>2n RTT</td></tr><tr><td>持久 - 非流水线</td><td>(5 + 1)RTT = 6 RTT</td><td>(n + 1) RTT</td></tr><tr><td>持久 - 流水线</td><td>2 / 3 RTT</td><td>2 / 3 RTT</td></tr></tbody></table><p>请求头:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/http请求头.jpg" style="zoom: 50%;" /><p>响应头:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/http响应头.jpg" style="zoom: 50%;" /><h3 id="FTP-文件传输协议"><a href="#FTP-文件传输协议" class="headerlink" title="FTP 文件传输协议"></a>FTP 文件传输协议</h3><p>FTP(File Transfer Protocol, 文件传输协议): C/S模式的文件传输协议, 因为控制端口和数据端口不是在一起, 所以实现了<strong>带外传输</strong>. 采用的是<strong>TCP协议</strong>.</p><p><strong>控制端口</strong>: 21</p><p><strong>数据端口</strong>: 20</p><p>过程:</p><ol><li>客户机通过控制连接获得授权</li><li>客户机经控制连接通过发送命令浏览远程目录</li><li>当服务器接收到一个文件传输命令时, 该服务器打开到客户机的一个数据连接</li><li>在传输一个文件后, 服务器关闭连接</li></ol><h3 id="电子邮件协议"><a href="#电子邮件协议" class="headerlink" title="电子邮件协议"></a>电子邮件协议</h3><p>电子邮件协议主要有<strong>SMTP, POP3, IMAP</strong>.</p><p>电子邮件主要由三个部分组成: <strong>用户代理</strong>, <strong>邮件服务器</strong>, <strong>邮件传输协议</strong>.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/电子邮件.jpg" style="zoom: 50%;" /><p><strong>用户代理</strong>: 也叫作邮件阅读器. 用户查看电子邮件就用的是这种应用程序.</p><p><strong>邮件服务器</strong>: 用户邮件的信息托管在邮件服务器上, 也同样负责用户邮件的发送(通过一个发送队列实现). </p><p><strong>邮件传输协议</strong>: 邮件服务器见传输邮件通过邮件传输协议来规定格式.</p><h4 id="SMTP"><a href="#SMTP" class="headerlink" title="SMTP"></a>SMTP</h4><p>简单邮件传输协议(Simple Mail Transfer Protocol, SMTP) 是一种提供可靠且有效的电子邮件传输的协议, 它控制两个相互通信的SMTP 进程交换信息. 由于SMTP 使用客户／服务器方式, 因此负责发送邮件的SMTP 进程就是SMTP 客户, 而负责接收邮件的SMTP 进程就是SMTP 服务器. <strong>SMTP 用的是TCP 连接, 端口号为25.</strong></p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/smtp.jpg" style="zoom: 80%;" /><ol><li>Alice使用UA写作报文并向 <a href="mailto:bob@someschool.edu">bob@someschool.edu</a>发送.</li><li>Alice的UA向其邮件服务器发送报文, 报文放置在报文队列中.</li><li>SMTP的客户机侧打开与Bob的邮件服务器的TCP连接.</li><li>SMTP通过TCP连接发送Alice的报文.</li><li>Bob的邮件服务器将该报文放入Bob的邮箱.</li><li>Bob调用其用户代理来读报文.</li></ol><p><strong>通信三阶段</strong>:</p><ul><li>握手</li><li>报文传输</li><li>关闭</li></ul><p>交互实例:</p><blockquote><p>S: 220 hamburger.edu<br>C: HELO crepes.fr<br>S: 250  Hello crepes.fr, pleased to meet you<br>C: MAIL FROM: <a href="mailto:&#97;&#x6c;&#105;&#x63;&#x65;&#64;&#99;&#114;&#x65;&#112;&#101;&#115;&#46;&#102;&#114;">&#97;&#x6c;&#105;&#x63;&#x65;&#64;&#99;&#114;&#x65;&#112;&#101;&#115;&#46;&#102;&#114;</a><br>S: 250 <a href="mailto:alice@crepes.fr">alice@crepes.fr</a>… Sender ok<br>C: RCPT TO: <a href="mailto:&#98;&#111;&#98;&#x40;&#104;&#97;&#109;&#x62;&#x75;&#114;&#103;&#x65;&#114;&#x2e;&#x65;&#100;&#x75;">&#98;&#111;&#98;&#x40;&#104;&#97;&#109;&#x62;&#x75;&#114;&#103;&#x65;&#114;&#x2e;&#x65;&#100;&#x75;</a><br>S: 250 <a href="mailto:bob@hamburger.edu">bob@hamburger.edu</a> … Recipient ok<br>C: DATA<br>S: 354 Enter mail, end with “.” on a line by itself<br>C: Do you like ketchup?<br>C: How about pickles?<br>C: .<br>S: 250 Message accepted for delivery<br>C: QUIT<br>S: 221 hamburger.edu closing connection</p></blockquote><p>由于SMTP比较早, 不能兼容国内的文字和其他多媒体流. <strong>报文必须以7比特ASCII格式</strong>.</p><p><strong>SMTP与HTTP的异同</strong>:</p><table><thead><tr><th>属性</th><th>HTTP</th><th>SMTP</th></tr></thead><tbody><tr><td>协议</td><td>TCP</td><td>TCP</td></tr><tr><td>端口号</td><td>80</td><td>25</td></tr><tr><td>推拉</td><td>拉</td><td>推</td></tr><tr><td>对象封装</td><td>每个对象用响应报文分别封装</td><td>所有对象封装成一个响应报文</td></tr><tr><td>限制</td><td>没有报文限制</td><td>必须用7位ASCII格式</td></tr></tbody></table><p><strong>邮件报文格式</strong>:</p><blockquote><p>—首部行—</p><p>TO:</p><p>From:</p><p>Subject:</p><p>—主体—</p><p>Body:</p></blockquote><p><strong>MIME</strong>: 多媒体邮件扩展, 能采用多种编码格式和编码数据的方法.</p><h4 id="POP3"><a href="#POP3" class="headerlink" title="POP3"></a>POP3</h4><p>POP3直接看一个例子:</p><blockquote><p>— 特许阶段 — 对用户信息进行认证</p><p>S: +OK POP3 服务器 ready<br>C: user bob<br>S: +OK<br>C: pass hungry<br>S: +OK user successfully logged on</p><p>— 事务阶段 — 用简单的命令执行操作</p><p>C: list<br>S: 1 498  -&gt; content length<br>S: 2 912<br>S: .<br>C: retr 1<br>S: &lt;message 1 contents&gt;<br>S: .<br>C: dele 1<br>C: retr 2<br>S: &lt;message 1 contents&gt;<br>S: .<br>C: dele 2<br>C: quit<br>S: +OK POP3 服务器 signing off</p><p>— 更新阶段 — 用户离线后, 服务器进行内容更新</p></blockquote><p>POP3有<code>Download and delete</code>和<code>Download and keep</code>两种模式, 当<code>Download and delete</code>时, 阅读完内容后会在服务器中删除. <code>Download and keep</code>时, 每次阅读都会下载所有的邮件到本地, 不删除邮箱内容.</p><h4 id="IMAP"><a href="#IMAP" class="headerlink" title="IMAP"></a>IMAP</h4><p>IMAP把所有的报文信息都<strong>保存在服务器</strong>上, 并<strong>允许用户组织文件夹</strong>. 保存文件夹名和报文ID和文件夹名之间的映射.</p><h3 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a>DNS</h3><p>DNS(Domain Name System): 域名解析是指把<strong>域名映射成为IP 地址或把IP 地址映射成域名的过程</strong>. 前者称为正向解析, 后者称为反向解析. 当客户端需要域名解析时, 通过本机的DNS 客户端构造一个DNS 请求报文, 以UDP数据报方式发往本地域名服务器. DNS是由分布式数据库实现的, 集中式DNS容易产生<strong>单点故障</strong>, 而且不利于维护, 当查询量很大的时候也不能进行负载分配. <strong>端口号53</strong>, <strong>书中唯一一个UDP协议的应用</strong>.</p><p><strong>等级制数据库的DNS查询过程</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/dns分布式.jpg" style="zoom:67%;" /><p>假设客户机要求<a href="http://www.amazon.com" target="_blank" rel="noopener">www.amazon.com</a> 的IP地址:</p><ol><li>客户机请求根服务器以发现com DNS服务器</li><li>客户机请求com DNS服务器以得到 amazon.com DNS 服务器</li><li>客户机请求amazon.com DNS服务器以得到对 <a href="http://www.amazon.com的ip/" target="_blank" rel="noopener">www.amazon.com</a><a href="http://www.amazon.com的ip/" target="_blank" rel="noopener">的</a><a href="http://www.amazon.com的ip/" target="_blank" rel="noopener">IP</a>地址</li></ol><p><strong>分布式DNS服务器</strong>:</p><ul><li>根名字服务器: 当本地名字服务器不能分解名字时联系它, 如果名字映射未知, 联系权威名字服务器.</li><li>顶级域服务器: 负责com, org, net, edu等, 以及所有顶级国家域 uk, fr, ca, jp.</li><li>权威DNS服务器: 组织的DNS 服务器为组织的服务器(如Web和电子邮件)提供对IP映射的权威主机名. 能够由组织或服务提供商维护.</li><li>本地名字服务器: 并不严格属于等级结构, 当主机发出DNS请求时, 请求被发送到其本地名字服务器, 作为代理, 将请求转发到等级结构.</li></ul><p><strong>DNS查询方式</strong>:</p><ul><li>递归请求: 搜索上类似与<strong>DFS</strong>, “我不知道该名字, 但我可以帮你问问别的服务器”. 会带来沉重的查询负担.</li><li>迭代请求: 搜索上类似与<strong>BFS</strong>, “我不知道该名字, 但你应该问问这个服务器”.</li></ul><p>应用层APP的协议和端口号:</p><table><thead><tr><th>Protocol</th><th>Port  number</th><th>Transport  layer protocol</th></tr></thead><tbody><tr><td>HTTP</td><td>80</td><td>TCP</td></tr><tr><td>FTP</td><td>21(control)/20(data)</td><td>TCP</td></tr><tr><td>SMTP</td><td>25</td><td>TCP</td></tr><tr><td>POP3</td><td>110</td><td>TCP</td></tr><tr><td>IMAP</td><td>143</td><td>TCP</td></tr><tr><td>DNS</td><td>53</td><td>UDP</td></tr></tbody></table><h2 id="传输层"><a href="#传输层" class="headerlink" title="传输层"></a>传输层</h2><p>传输层已经对上一层, 也就是应用层传输过来的报文进行第二层封装(抽象), 为<strong>进程</strong>提供通用数据传输服务. 从通信和信息处理的角度看, 传输层向它上面的应用层提供通信服务, 它属于面向通信部分的最高层, 同时也是用户功能中的最低层. 传输层位于网络层之上, 它为运行在不同主机上的进程之间提供了逻辑通信, 而网络层提供主机之间的逻辑通信. 显然, 即使网络层协议不可靠( 网络层协议使分组丢失, 混乱或重复) ,  传输层同样能为应用程序提供可靠的服务. 对应的传输层协议就只有两个, TCP和UDP. 在传输层, 传输数据所用的单位是<strong>报文段(segment)</strong>.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/传输层框架.jpg" style="zoom: 67%;" /><p><strong>传输层的功能</strong>:</p><ol><li>传输层提供应用进程之间的逻辑通信( 即<strong>端到端</strong>的通信) . 与网络层的区别是, 网络层提供的是主机之间的逻辑通信. 从网络层来说, 通信的双方是两台主机,  IP 数据报的首部给出了这两台主机的IP地址. 但”两台主机之间的通信”实际上是两台主机中的应用进程之间的通信, 应用进程之间的通信又称端到端的逻辑通信. </li><li><strong>复用和分用</strong>. 复用是指发送方不同的应用进程都可使用同一个传输层协议传送数据, 分用是指接收方的传输层在剥去报文的首部后能够把这些数据正确交付到目的应用进程. </li><li>传输层还要对收到的报文进行<strong>差错检测</strong>( 首部和数据部分) . 而网络层只检查IP 数据报的首部, 不检验数据部分是否出错. </li><li>提供两种不同的传输协议, 即面向连接的TCP 和无连接的UDP . 而网络层无法同时实现两种协议( 即在网络层要么只提供面向连接的服务, 如虚电路, 要么只提供无连接服务, 如数据报, 而不可能在网络层同时存在这两种方式) . </li></ol><p><strong>多路复用和多路分解</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/多路复用和分解.jpg" style="zoom: 50%;" /><ul><li><strong>多路复用</strong>: 从源主机的不同socket中收集数据块, 并为每个数据块封装上首部信息从而生成报文段, 将报文段传递到网络层的工作. </li><li><strong>多路分解</strong>: 将运输层报文段中的数据交付到正确的socket的工作. </li></ul><p>套接字就像<strong>门</strong>一样, 不同的进程有不同的套接字. 进程从/来自它的套接字发送/接收报文.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/socket.jpg" style="zoom: 50%;" /><p><strong>标识套接字唯一的方法</strong>:</p><p><strong>UDP套接字</strong>: 目的地IP地址, 目的地端口号组成的二元组.</p><p><strong>TCP套接字</strong>: 源IP地址, 源端口号, 目的IP地址, 目的端口号组成的四元组.</p><p>无论是UDP还是TCP, 报文段结构都有16位源端口, 16位目的端口.</p><p><strong>IP地址从在哪获取呢? 报文段中并没有包含IP相关的内容. 但不要忘记多路复用和多路分解, 在多路分解时数据是由网络层以数据报形式往上传输的, 在去掉封装的头部后传输层一定能够知道源IP和目标IP.</strong></p><h3 id="UDP"><a href="#UDP" class="headerlink" title="UDP"></a>UDP</h3><p>UDP(用户数据报协议): </p><ol><li>UDP 无须建立连接. 因此UDP 不会引入建立连接的时延. 试想如果DNS 运行在TCP 而非UDP 上, 那么DNS 的速度会慢很多. HTTP 使用TCP 而非UDP, 是因为对于基于文本数据的Web网页来说可靠性是至关重要的</li><li>无连接状态. TCP 需要在端系统中维护连接状态. 此连接状态包括接收和发送缓存, 拥塞控制参数和序号与确认号的参数. 而UDP 不维护连接状态, 也不跟踪这些参数. 因此, 某些专用应用服务器使用UDP 时, 一般都能支持更多的活动客户机.</li><li>分组首部开销小. <strong>TCP 有20B 的首部开销, 而UDP 仅有8B(32*2bit/8)的开销</strong>. </li><li>应用层能更好地控制要发送的数据和发送时间. UDP 没有拥塞控制, 因此网络中的拥塞不会影响主机的发送效率. 某些实时应用要求以稳定的速度发送, 能容忍一些数据的丢失, 但不允许有较大的时延, 而UDP 正好满足这些应用的需求. UDP 常用于一次性传输较少数据的网络应用如 DNS , SNMP 等, 因为对千这此应用, 若采用TCP, 则将为连接创建, 维护和拆除带来不小的开销. UDP 也常用于多媒体应用( 如IP 电话, 实时视频会议, 流媒体等) , 显然, 可靠数据传输对 这些应用来说并不是最重要的, 但TCP的拥塞控制会导致数据出现较大的延迟, 这是它们不可容忍的. </li></ol><p>UDP 提供尽最大努力的交付, 即不保证可靠交付, 但这并不意味着应用对数据的要求是不可靠的, 因此所有维护传输可靠性的工作需要用户在应用层来完成. 应用实体可以根据应用的需求来灵活设计自己的可靠性机制. 所以UDP常应用在对丢包率可以容忍, 对速率敏感的应用, 即使发生很小一部分的丢包, 仍然不影响用户的体验.</p><h4 id="报文段结构"><a href="#报文段结构" class="headerlink" title="报文段结构:"></a>报文段结构:</h4><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/udp报文段.jpg" style="zoom: 50%;" /><p>除了TCP/UDP必备的源端口和目的端口号, UDP有检查和. 检查和能够在传输的报文段中检测一位比特的差错. 发送方将报文段所有内容分批处理多个16位的整数序列, <strong>逐一加和, 进行回卷, 最后取反.</strong> </p><h4 id="检查和"><a href="#检查和" class="headerlink" title="检查和:"></a>检查和:</h4><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/udp检查和.jpg" style="zoom:50%;" /><p>接收方也计算和, 看与发送方告诉自己的检查和累加后是否为<code>1111 1111 1111 1111</code>, 如果是的话说明<strong>没有检测出错误(检查和并不能检测到所有错误)</strong>, 如果不是则说明存在错误.</p><h3 id="可靠数据传输"><a href="#可靠数据传输" class="headerlink" title="可靠数据传输"></a>可靠数据传输</h3><p>可靠数据传输是后面TCP协议能够支持可靠性的设计依据. IP在网络层一定是不可靠的, 为什么说UDP/IP是不可靠的, 而TCP/IP就可靠了? 正是因为下层的传输的不可靠, 所以我们可以通过上层的可靠传输协议来淘汰甚至纠正不可靠的数据. 当然可靠一定伴随着代价, 也就是时间开销和资源开销.</p><p>可靠象征着接收到的字节流是<strong>完整</strong>, <strong>无差错</strong>, <strong>有序</strong>的. (TCP和UDP都没有保证最低的传输速率!)</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/可靠数据传输.jpg" style="zoom: 67%;" /><h4 id="Rdt1-0"><a href="#Rdt1-0" class="headerlink" title="Rdt1.0"></a>Rdt1.0</h4><p>仅考虑理想化的, 无比特错误, 无丢包. 接收方完全不需要其他额外数据.</p><h4 id="Rdt2-0"><a href="#Rdt2-0" class="headerlink" title="Rdt2.0"></a>Rdt2.0</h4><p>假设在分组的传输, 传播或缓存过程中具有比特差错, 但无丢包.</p><ul><li>解决方法: ARQ协议( 自动重传请求) , 用以下3种协议来处理<ol><li>差错检测( 接收方) : 检验和</li><li>反馈<ul><li>接收方—&gt;确认: 告诉发送方数据包有无错误, 返回肯定应答ACK 与否定应答NAK</li><li>发送方—&gt;决定下一步动作</li></ul></li><li>重传( 发送方) : 发送方重发</li></ol></li><li>与之类似的协议被称为: 停止等待协议(只有发送成功, 才发送给下一个分组, 否则一直重复)</li></ul><h4 id="Rdt2-1"><a href="#Rdt2-1" class="headerlink" title="Rdt2.1"></a>Rdt2.1</h4><p>在Rdt2.0的基础上, 考虑ACK与NAK本身出现错误的情况.</p><ul><li>无其它动作: 无法解决. </li><li>增加足够的比特, 发送方可检错与纠错. </li><li>当不能确认是ACK还是NAK时, 发送方重发: 引入冗余分组, 接收方混淆？</li><li>解决方法: 加入1bit序号(sequence number), 因为发送是交替进行的, 所以引入0和1两个数字就能说明发包的顺序.</li></ul><h4 id="Rdt2-2"><a href="#Rdt2-2" class="headerlink" title="Rdt2.2"></a>Rdt2.2</h4><p>无NAK的可靠数据传输协议, 用冗余的ACK来代替了多余的NAK. 从逻辑角度来想, 冗余ACK就代表了发送方没有收到接收方接收的响应, 那么一定是出了问题, 也就是NAK的作用.</p><ul><li>接收方必须包括由一个ACK报文确认的分组序号, 即对上次正确接收的分组的ACK.</li></ul><h4 id="Rdt3-0"><a href="#Rdt3-0" class="headerlink" title="Rdt3.0"></a>Rdt3.0</h4><p>不光考虑Rdt2.0比特差错的基础上, 还考虑底层信道的丢包.</p><ul><li>定时器( 倒计数器) <ul><li>时间值: RTT+时延</li><li>发送后, 启动定时器</li><li>响应定时器中断</li><li>终止定时器</li></ul></li><li>功能正确的协议</li><li>使用检验和, 序号, 肯定和否定确认, 定时器技术</li><li>效率低下(停止等待协议需要等待对方应答才能发下一个, 这个空闲时间很长, 就导致了时间上的利用率非常低, 后面通过流水线协议提升了效率)</li></ul><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/停等协议.jpg" style="zoom:50%;" /><h5 id="无丢包时的运行"><a href="#无丢包时的运行" class="headerlink" title="无丢包时的运行"></a>无丢包时的运行</h5><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/rdt3无丢包.jpg" style="zoom:50%;" /><h5 id="分组丢失"><a href="#分组丢失" class="headerlink" title="分组丢失"></a>分组丢失</h5><p>因为发送分组1时发生了丢失, 在计时器期间没收到接收方的应答. 计时器超时了, 所以发送方再次发送相同的分组1.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/rdt3分组丢失.jpg" style="zoom:50%;" /><h5 id="ACK丢失"><a href="#ACK丢失" class="headerlink" title="ACK丢失"></a>ACK丢失</h5><p>因为接收方对发送方分组1的ACK丢失了, 发送方在计时器时间内迟迟等不来响应, 超时后发送方重发.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/rdt3ack丢失.jpg" style="zoom:50%;" /><h5 id="过早超时"><a href="#过早超时" class="headerlink" title="过早超时"></a>过早超时</h5><p>在发送分组1后, 可能因为网速的原因, 接收方给发送方的ACK比较晚的到达了. 接收方接收到两次相同的冗余分组, 也就是说发送方有不必要的重传. 如果某个报文段的发送过程被延迟较长时间但并未丢失, 这种情况叫做过早超时.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/rdt3过早超时.jpg" style="zoom:50%;" /><h4 id="流水线协议"><a href="#流水线协议" class="headerlink" title="流水线协议"></a>流水线协议</h4><p>在Rdt3.0中知道, 其效率是十分低下的. 原因就是发送方必须等待接收方的响应才能继续发送下一个分组. 为什么必须要等待响应才能发下一个分组呢? 如果连续发一串分组出去就能提升效率了, 但是要增加一些资源开销, 比如说要区分分组发出去的先后顺序必须要扩大序号的范围, 而且考虑到接收方处理分组的速度之间的差异, 必须在接收方和发送方之间引入缓冲区. 还要考虑发送方可能发送非按序到达的数据的问题.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/流水线协议.jpg" style="zoom:50%;" /><p>为解决上述基本问题, 引入了两种方法, 分别是<strong>GBN(Go-Back-N, 滑动窗口协议)</strong>和<strong>SR(选择重传)</strong>.</p><h5 id="Go-Back-N"><a href="#Go-Back-N" class="headerlink" title="Go-Back-N"></a>Go-Back-N</h5><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/gbn.jpg" style="zoom:50%;" /><ul><li>允许发送多个分组而不需要等待确认, 受限于窗口长度N.</li><li><strong>累积确认</strong>: 接收方返回的ACK号代表的含义是”<strong>该号和该号之前的分组已经成功收到</strong>“.</li><li>窗口共享计时器, 当最早发出去未被响应的分组超时则重传所有分组.</li><li>数据按序交付, <strong>失序则丢弃</strong>.</li><li><strong>回退机制</strong>:<ul><li>表示需要再退回来重传已发送过的 N 个分组</li><li>当通信线路质量不好和N过大时, 连续 ARQ 协议会带来负面影响</li></ul></li></ul><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/gbn执行.jpg" style="zoom:50%;" /><p>当pkt2发生丢失时, 接收方一直收到了乱序的pkt, 所以一直在不停的返回ACK1, 即”ACK1和ACK1以前的分组我已经收到了!”, 希望接收方发送ACK2. 之后收到的乱序pkt3, pkt4, pkt5全都被丢弃, 直到pkt2被发送方重新补上, 才继续发送下去.</p><h5 id="Selective-Repeat"><a href="#Selective-Repeat" class="headerlink" title="Selective Repeat"></a>Selective Repeat</h5><p>GBN改善了信道效率, 但仍然有不必要重传问题. 与GBN风格不同, 针对每个不同的分组都有不同的计时器和确认.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/sr.jpg" style="zoom:50%;" /><ul><li>窗口长度必须小于或等于序号空间大小的一半</li><li>逐一确认</li><li>只重发未被确认的分组(发送方定时器对每个没有确认的分组计时)</li><li>失序缓存, 但最终仍是按序交付</li></ul><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/sr执行.jpg" style="zoom:50%;" /><h4 id="可靠数据传输机制及用途总结"><a href="#可靠数据传输机制及用途总结" class="headerlink" title="可靠数据传输机制及用途总结"></a>可靠数据传输机制及用途总结</h4><table><thead><tr><th>机制</th><th>用途和说明</th></tr></thead><tbody><tr><td>检验和</td><td>用于检测在一个传输分组中的比特错误.</td></tr><tr><td>确认</td><td>接收方用于告诉发送方一个分组或一组分组已被正确地接收到了. 确认报文通常携带着被确认的分组或多个分组的序号. 确认可以是逐个的或累积的, 这取决于协议.</td></tr><tr><td>序号</td><td>用于为发送的数据分组进行按序编号. 所接收分组的序号间的空隙可使接收方检测出丢失的分组. 具有相同序号的分组可使接收方检测出一个分组的冗余拷贝.</td></tr><tr><td>定时器</td><td>用于检测超时/重传一个分组, 可能因为该分组( 或其ACK) 在信道中丢失了. 由于当一个分组被时延但未丢失( 过早超时) , 或当一个分组已被接收方收到但从接收方到发送方的ACK丢失时, 可能产生超时事件, 所以接收方可能会收到一个分组的多个冗余拷贝.</td></tr><tr><td>窗口, 流水线  GO-BACK-N  Selective Repeat</td><td>发送方也许被限制仅发送那些序号落在一个指定范围内的分组. 通过允许一次发送多个分组但未被确认, 发送方的利用率可在停等操作模式的基础上得到增加. 我们很快将会看到, 窗口长度可根据接收方接收和缓存报文的能力或网络中的拥塞程度, 或两者情况来进行设置.</td></tr></tbody></table><h3 id="TCP"><a href="#TCP" class="headerlink" title="TCP"></a>TCP</h3><p>在看完可靠数据传输后, 再去理解TCP就不困难了. TCP 是在不可靠的IP 层之上实现的可靠的数据传输协议, 它主要解决传输的可靠, 有序, 无丢失和不重复问题. TCP 是TCP/IP 体系中非常复杂的一个协议, 它更像是GBN和SR的混合体, 主要特点如下:</p><ol><li><p>TCP 是<strong>面向连接</strong>的传输层协议</p></li><li><p>每条TCP 连接只能有两个端点, 每条TCP 连接只能是<strong>点对点</strong>的( 一对一) . </p></li><li><p>TCP 提供可靠的交付服务, 保证传送的数据无差错, 不丢失, 不重复且有序. </p></li><li><p>TCP 提供<strong>全双工</strong>通信, 允许通信双方的应用进程在任何时候都能发送数据, 为此TCP 连接的两端都设有发送缓存和接收缓存, 用来临时存放双向通信的数据. </p><p>发送缓存用来暂时存放以下数据: </p><ul><li><p>发送应用程序传送给发送方TCP 准备发送的数据</p></li><li><p>TCP 已发送但尚未收到确认的数据</p></li></ul><p>接收缓存用来暂时存放以下数据:</p><ul><li>按序到达但尚未被接收应用程序收取的数据</li><li>不按序到达的数据</li></ul></li></ol><p>重传在TCP中, 当产生超时事件或者重复ACK才会被触发. 并像GBN一样采用单个重传计时器.</p><h4 id="序号和确认号"><a href="#序号和确认号" class="headerlink" title="序号和确认号"></a>序号和确认号</h4><p><strong>序号(seq)</strong>: 报文段中第1个数据字节在字节流中的位置编号.</p><p><strong>确认号(ACK)</strong>: 期望从对方收到下一个字节的序号, 且是累计应答. 即”<strong>我希望你下一次发从这个号开始的分组</strong>“.</p><p><strong>捎带确认</strong>: 确认号被装载在服务器到客户机的数据的报文段中.</p><p>对于失序的报文段, TCP没有明确指出, 应该是根据不同情况而定的.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/tcpseq和ack.jpg" style="zoom:50%;" /><h4 id="报文段结构-1"><a href="#报文段结构-1" class="headerlink" title="报文段结构"></a>报文段结构</h4><p>头部一共32*5/8 = 20bytes, 比UDP的8bytes冗长很多.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/tcp报文段.jpg" style="zoom: 50%;" /><h4 id="传输过程"><a href="#传输过程" class="headerlink" title="传输过程"></a>传输过程</h4><ol><li>从应用层接收数据:<ul><li>根据序号创建报文段</li><li>序号是报文段中第一个数据字节的数据流编号</li><li>如果未启动, 启动计时器 (考虑计时器用于最早的没有确认的报文段)</li></ul></li><li>超时<ul><li>重传导致超时的报文段</li><li>重新启动计时器</li></ul></li><li>收到确认<ul><li>如果确认了先前未被确认的报文段, 更新被确认的报文段序号, 如果还有未被确认的报文段, 重新启动计时器</li></ul></li></ol><h4 id="ACK产生"><a href="#ACK产生" class="headerlink" title="ACK产生"></a>ACK产生</h4><table><thead><tr><th>接收方事件</th><th>TCP接收方行为</th></tr></thead><tbody><tr><td>所期望序号的报文段按序到达. 所有在期望序号及以前的数据都已经被确认</td><td>延迟的ACK. 对另一个按序报文段的到达最多等待500 ms. 如果下一个按序报文段在这个时间间隔内没有到达, 则发送一个ACK</td></tr><tr><td>有期望序号的报文段按序到达. 另一个按序报文段等待发送ACK</td><td>立即发送单个累积ACK, 以确认两个按序报文段</td></tr><tr><td>比期望序号大的失序报文段到达, 检测出数据流中的间隔.</td><td>立即发送冗余ACK, 指明下一个期待字节的序号( 也就是间隔的低端字节序号)</td></tr><tr><td>部分或者完全填充已接收到, 数据间隔的报文段到达</td><td>倘若该报文段起始于间隔的低端, 则立即发送ACK</td></tr></tbody></table><h4 id="重传情况"><a href="#重传情况" class="headerlink" title="重传情况"></a>重传情况</h4><h5 id="丢失确认"><a href="#丢失确认" class="headerlink" title="丢失确认"></a>丢失确认</h5><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/tcp丢失确认.jpg" style="zoom:50%;" /><p>主机B发送的ACK丢失, 超时后主机A重传丢失的Seq=92分组.</p><h5 id="过早超时-1"><a href="#过早超时-1" class="headerlink" title="过早超时"></a>过早超时</h5><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/tcp过早超时.jpg" style="zoom:50%;" /><p>主机A在重传Seq=92分组后, 分别收到了ACK=100和ACK=120, 累计确认, 更新了应该发送的起始序号. 下次发送从Seq=120开始.</p><h5 id="累计确认"><a href="#累计确认" class="headerlink" title="累计确认"></a>累计确认</h5><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/tcp累计确认.jpg" style="zoom:50%;" /><p>主机B响应的ACK=100丢失, 但是主机B已经收到了120以前的所有分包. 当主机A收到ACK=120时, 代表120号之前的分组主机B已经收到, 虽然没有ACK=100的响应但也无妨(累计确认). 所以下次发送新分组应该是Seq=120.</p><h4 id="快速重传"><a href="#快速重传" class="headerlink" title="快速重传"></a>快速重传</h4><p>因为超时间隔常常相对较长, 重传丢失报文段以前有长时延. 所以引入快速重传, 如果对相同数据, 发送方收到<strong>3个重复ACK</strong>, 假定被确认的报文段以后的报文段丢失了. 在定时器超时之前就能重传.</p><h4 id="流量控制"><a href="#流量控制" class="headerlink" title="流量控制"></a>流量控制</h4><p>为了解决发送方和接收方在速度上的不匹配, 在发送方和接收方都各自设计了缓冲区. 发送方不能发送太多, 太快的数据让接收方缓冲区溢出. 接收方在报文段接收窗口字段中通告其接收缓冲区的剩余空间.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/tcp缓冲区.jpg" style="zoom: 67%;" /><p>缓冲区的剩余空间(也叫接收窗口 <code>RcvWindow</code>):<br>$$<br>\rm RcvWindow = RcvBuffer-[LastByteRcvd - LastByteRead]<br>$$<br>只要接收方的总Buffer的承载能力超过接收方已经确认的数据就能保证接收方缓冲区不溢出.</p><h4 id="连接管理"><a href="#连接管理" class="headerlink" title="连接管理"></a>连接管理</h4><p>三次握手, 四次挥手. 面试巨高频考点. 连接管理就是TCP为什么是面向连接的协议的原因.</p><h5 id="三次握手"><a href="#三次握手" class="headerlink" title="三次握手"></a>三次握手</h5><ol><li>客户机的TCP 首先向服务器的TCP 发送一个连接请求报文段. 这个特殊的报文段中不含应用层数据, 其首部中的SYN标志位被置为1 . 另外, 客户机会随机选择一个起始序号seq = x( 连接请求报文不携带数据, 但要消耗一个序号) . </li><li>服务器的TCP 收到连接请求报文段后, 如同意建立连接, 就向客户机发回确认, 并为该 TCP 连接分配TCP 缓存和变量. 在确认报文段中,  SYN 和ACK 位都被置为1, 确认号字段的值为 x + 1, 并且服务器随机产生起始序号seq= y( 确认报文不携带数据, 但也要消耗一个序号) . 确认 报文段同样不包含应用层数据. <ul><li>这里大写的ACK和小写的ack, 其中一个是<strong>确认值</strong>, 当ACK为1时代表确定连接. 还有一个ack是<strong>确认编号</strong>, 响应seq = x + 1.</li></ul></li><li>当客户机收到确认报文段后, 还要向服务器给出确认, 并且也要给该连接分配缓存和变量. 这个报文段的ACK 标志位被置1, 序号字段为x+ 1, 确认号字段ack=y+ 1 . 该报文段可以携带数据, 若不携带数据则不消耗序号. </li></ol><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/tcp三次握手.jpg" style="zoom: 50%;" /><p>成功进行以上三步后, 就建立了TCP 连接, 接下来就可以传送应用层数据. TCP 提供的是全双工通信, 因此通信双方的应用进程在任何时候都能发送数据. 另外, 值得注意的是, 服务器端的资源是在完成第二次握手时分配的, 而客户端的资源是在完成第三次握手时分配的, 这就使得服务器易于受到SYN 洪泛攻击. </p><p><strong>为什么不采用”两次握手”建立连接呢？</strong></p><p>这主要是为了防止两次握手情况下已失效的连接请求报文段突然又传送到服务器而产生错误. 考虑下面这种情况. 客户A 向服务器B 发出TCP 连接请求, 第一个连接请求报文在网络的某个结点长时间滞留,  A 超时后认为报文丢失, 千是再重传一次连接请求,  B 收到后建立连接. 数据传输完毕后双方断开连接. 而此时, 前一个滞留在网络中的连接请求到达服务器B, 而B 认为A 又发来连接请求, 此时若使用”三次握手”, 则B 向A 返回确认报文段, 由于是一个失效的请求, 因此A 不予理睬, 建立连接失败. 若采用的是”两次握手”, 则这种情况下B 认为传输连接已经建立, 并一直 等待A 传输数据, 而A 此时并无连接请求, 因此不予理睬, 这样就造成了B的资源白白浪费. </p><h5 id="四次挥手"><a href="#四次挥手" class="headerlink" title="四次挥手"></a>四次挥手</h5><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/tcp四次挥手.jpg" style="zoom:50%;" /><ol><li>客户机打算关闭连接时, 向其TCP 发送一个连接释放报文段, 并停止发送数据, 主动关闭TCP 连接, 该报文段的FIN 标志位被置1, seq= u, 它等于前面已传送过的数据的最后一个字节的序号加1 (FIN 报文段即使不携带数据, 也要消耗一个序号) . TCP 是全双工的, 即可以想象为一条TCP 连接上有两条数据通路. 发送FIN 报文时, 发送FIN 的一端不能再发送数据, 即关闭了其中一条数据通路, 但对方还可以发送数据. </li><li>服务器收到连接释放报文段后即发出确认, 确认号是ack = u + 1, 而这个报文段自己的序号是V, 等于它前面已传送过的数据的最后一个字节的序号加1 . 此时, 从客户机到服务器这个方向的连接就释放了, TCP 连接处于半关闭状态. 但服务器若发送数据, 客户机仍要接收, 即从服务器到客户机这个方向的连接并未关闭. </li><li>若服务器已经没有要向客户机发送的数据, 就通知TCP 释放连接, 此时其发出FIN= 1 的连接释放报文段. </li><li>客户机收到连接释放报文段后, 必须发出确认. 在确认报文段中,  ACK 字段被置为1, 确认号ack= w + 1, 序号seq= u + 1 . 此时TCP 连接还未释放, 必须经过时间等待计时器设置的时间 2MSL 后,  A 才进入连接关闭状态. </li></ol><p><strong>为何不采用”三次握手”释放连接, 且发送最后一次握手报文后要等待2MSL 的时间呢？</strong></p><p>原因有两个: </p><ol><li>保证A 发送的最后一个确认报文段能够到达B . 如果A 不等待2MSL, 若A 返回的最后确认报文段丢失, 则B 不能进入正常关闭状态, 而A 此时已经关闭, 也不可能再重传. </li><li>防止出现”已失效的连接请求报文段”. A 在发送最后一个确认报文段后, 再经过2MSL可保证本连接持续的时间内所产生的所有报文段从网络中消失. </li></ol><p>服务器结束TCP 连接的时间要比客户机早一些, 因为客户机最后要等待2MSL 后才可进入CLOSED状态. </p><h5 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h5><p>三次握手:</p><blockquote><ol><li>SYN = 1, seq = x</li><li>SYN = 1, ACK = 1, seq = y, ack = x + 1</li><li>ACK = 1, seq = x + 1, ack = y + 1</li></ol></blockquote><p>四次挥手:</p><blockquote><ol><li>FIN = 1, seq = u</li><li>ACK = 1, seq = v, ack = u + 1</li><li>FIN = 1, ACK = 1, seq = w, ack = u + 1</li><li>ACK = 1, seq = u + 1, ack = w + 1</li></ol></blockquote><h4 id="拥塞控制"><a href="#拥塞控制" class="headerlink" title="拥塞控制"></a>拥塞控制</h4><p>某段时间, 若对网络中某资源的需求超过了该资源所能提供的可用部分, 即现有的负荷超过了网络能够承受的最大负荷, 太多的源发送太多太快的数据, 使网络来不及处理, 产生拥塞(congestion). 不同于流量控制. 经常导致丢包(路由器缓冲区溢出)和高延迟(路由器缓冲区中排队). 拥塞控制是一个<strong>全局性</strong>的过程, 涉及到所有的主机, 所有的路由器, 以及与降低网络传输性能有关的所有因素.</p><p>控制拥塞有两种方法:</p><ul><li><strong>端到端的拥塞控制</strong>: 不能从网络得到明确的反馈, 从端系统根据观察到的时延和丢失现象推断出拥塞.</li><li><strong>网络辅助的拥塞控制</strong>: 路由器为端系统提供反馈.</li></ul><p>TCP采用的是端到端的拥塞控制.</p><p>设置一个拥塞窗口<code>ConWin</code>, 通过拥塞窗口来限制发送方的传输:<br>$$<br>\rm LastByteSent-LastByteAcked \leq CongWin<br>$$<br>当然这个拥塞窗口是动态的, 应该根据感知到的网络拥塞情况进行调整. 也就是当发生<strong>丢失时间, 即超时或3次冗余确认</strong>时降低拥塞窗口大小.</p><p>通过三个机制来调整拥塞窗口:</p><ul><li>AIMD - 加增倍减</li><li>慢启动</li><li>超时事件后的保守机制</li></ul><h5 id="AIMD-乘性减-加性增"><a href="#AIMD-乘性减-加性增" class="headerlink" title="AIMD 乘性减, 加性增"></a>AIMD 乘性减, 加性增</h5><ul><li>乘性减: 丢包事件后, 拥塞窗口值减半</li><li>加性增: 如没有检测到丢包事件, 每个RTT时间拥塞窗口值增加一个MSS (最大报文段长度)</li></ul><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/tcpaimd.jpg" style="zoom:50%;" /><h5 id="慢启动"><a href="#慢启动" class="headerlink" title="慢启动"></a>慢启动</h5><p>当连接开始的时候, 速率呈指数式上升, 直到第1次报文丢失事件发生为止.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/tcp慢启动.jpg" style="zoom:50%;" /><h5 id="超时事件后的保守机制"><a href="#超时事件后的保守机制" class="headerlink" title="超时事件后的保守机制"></a>超时事件后的保守机制</h5><p>基本思想是3个冗余ACK指示网络还具有某些传送报文段的能力, 那么3个冗余ACK以前的<strong>超时</strong>,则<strong>更为”严重”</strong>.</p><p>收到3个冗余确认后:</p><ul><li>$\rm CongWin$减半</li><li>窗口再线性增加</li></ul><p>超时事件以后:</p><ul><li>$\rm CongWin$值设置为1 MSS</li><li>窗口再指数增长</li><li>到达一个阈值 (Threshold) 后, 再线性增长</li></ul><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/tcp超时保守.jpg" style="zoom: 67%;" /><p>在丢包事件发生时, 阈值$\rm Threshold$设置为发生丢包以前的拥塞窗口的一半, 这样能够减轻网络压力, 到达一个相对原来比较低的阈值后线性增加.</p><p>图中先是慢启动, 在$\rm TTR$为4时到达阈值, 从指数型增长变为线性增长, 在$\rm TTR$为9时下方蓝线超时事件, 拥塞窗口置1, 阈值减半, 重新指数增加, 到达阈值后线性增加. 上方黑线发生的是3次冗余确认, 拥塞窗口减半, 之后线性增加.</p><p>拥塞控制小结:</p><ul><li><p>当$\rm CongWin &lt; Threshold$时, 发送者处于慢启动阶段, $\rm CongWin$指数增长.</p></li><li><p>当$\rm CongWin &gt; Threshold$时, 发送者处于拥塞避免阶段, $\rm CongWin$线性增长.</p></li><li><p>当出现3个冗余确认时, 阈值Threshold设置为$\rm CongWin/2$, 且$\rm CongWin$设置为$\rm Threshold$.</p></li><li><p>当超时发生时, 阈值$\rm Threshold$设置为$\rm CongWin/2$, 并且$\rm CongWin$设置为1 MSS. </p></li></ul><h3 id="TCP和UDP比较"><a href="#TCP和UDP比较" class="headerlink" title="TCP和UDP比较"></a>TCP和UDP比较</h3><table><thead><tr><th>比较项</th><th>TCP</th><th>UDP</th></tr></thead><tbody><tr><td>套接字</td><td>四元组</td><td>二元组</td></tr><tr><td>连接及状态</td><td>需要建立, 有状态</td><td>无需建立连接, 无状态</td></tr><tr><td>拥塞控制, 流量控制</td><td>有</td><td>无, 流量不可调节</td></tr><tr><td>分组首部开销</td><td>20字节(大)</td><td>8字节(小)</td></tr><tr><td>应用层对发送的数据和发送时间</td><td>不可控制</td><td>可控制</td></tr><tr><td>服务原则</td><td>可靠交付</td><td>尽力交付</td></tr><tr><td>传送的数据单位</td><td>报文段, 大小可变</td><td>报文, 大小不变</td></tr><tr><td>协议类型</td><td>点对点协议</td><td>可支持一对一, 一对多, 多对一和多对多</td></tr><tr><td>差错检测手段</td><td>校验和: 出错重传</td><td>校验和: 丢弃</td></tr><tr><td>应用层协议</td><td>SMTP,   TELNET,   FTP,   HTTP, VoIP</td><td>DNS, SNMP, RIP, VoIP</td></tr></tbody></table><h2 id="网络层"><a href="#网络层" class="headerlink" title="网络层"></a>网络层</h2><p>网络层是对传输层协议的数据进行进一步封装(抽象). 无视传输层的具体内容, 以<strong>数据报(datagram)</strong>作为单位传输. 网络层研究的就是, 我知道A到B主机要传信息, 但是怎么传最好的问题. 涉及转发, 路径选择等算法. 网络层负责完成的是<strong>主机到主机间</strong>, 也就是<strong>点到点</strong>之间的通信. 实现也全是在网络内部实现的, 当IP数据报通过路由器时, 路由器检查只所有数据报的首部字段.</p><p><strong>转发</strong>: 微观上的, 一个分组从一条入链路到一台路由器中的出链路的传送, 是路由器的本地动作.</p><p><strong>选路</strong>: 宏观上的, 分组从发送方流向接收方时, 网络层决定分组从源到目的地节点所采用的路径. 涉及到一个网络中的所有路由器, 它们经选路协议共同交互才能完成.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/网络层框架.jpg" style="zoom:67%;" /><p>Internet采用的是数据报网络, 其网络层服务模型是尽力而为的. 没有任何的顺序, 带宽, 丢失保证. 但其他协议譬如ATM, 帧中继, X.25因为使用了虚电路, 从一定程度上保证了传输的稳定性.</p><h3 id="虚电路和数据报网络"><a href="#虚电路和数据报网络" class="headerlink" title="虚电路和数据报网络"></a>虚电路和数据报网络</h3><p>数据报网络能提供网络层的无连接服务, 而虚电路可以提供连接服务. </p><h4 id="虚电路"><a href="#虚电路" class="headerlink" title="虚电路"></a>虚电路</h4><p>虚电路名字的由来是”源到目的地路径与电话电路行为非常相似”, 其性能是明确的, 并且一定是沿着源到目的地路径的网络动作.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/虚电路.jpg" style="zoom: 67%;" /><p>在数据流动之前, 先建立呼叫, 然后在结束时拆除. 在源到目的地路径上的每台路由器为每条经过的连接维护维护状态, 链路, 路由器资源(带宽, 缓存)可能分配给VC.</p><p>VC组成:</p><ul><li>源和目的主机之间的路径(由一系列链路和路由器组成)</li><li>VC号(VC号是标识沿路径每条链路的号码)</li><li>沿路径路由器中转发表中的项</li></ul><p>虚电路的分组的首部会携带一个VC号, 因为一条虚电路在每条链路上的VC号可能不同, 所以每次经过新的路由器, 都会分配一个新的VC号来代替原来的VC号, 新的VC号是从新的转发表中获得的.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/vc号.jpg" style="zoom: 67%;" /><p>虚电路路由器维护的转发表形式和下表类似, 上图网络中西北处路由器的转发表为:</p><table><thead><tr><th>入接口</th><th>入VC#</th><th>出接口</th><th>出VC#</th></tr></thead><tbody><tr><td>1</td><td>12</td><td>2</td><td>22</td></tr><tr><td>2</td><td>63</td><td>1</td><td>18</td></tr><tr><td>3</td><td>7</td><td>2</td><td>17</td></tr><tr><td>1</td><td>97</td><td>3</td><td>87</td></tr><tr><td>…</td><td>…</td><td>…</td><td>…</td></tr></tbody></table><p>这个转发表是路由器一定会维护的. 路由器通过存放接入口的VC#和出口的VC#之间的映射, 来完成VC#的转换.</p><h4 id="数据报网络"><a href="#数据报网络" class="headerlink" title="数据报网络"></a>数据报网络</h4><p>数据报网络的数据传输不依赖于”连接”, 路由器也没有端到端连接的状态.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/数据报网络.jpg" style="zoom: 67%;" /><p>路由器也不需要维护像虚电路内容的转发表. 只需要根据数据报的目的地信息就能知道数据报应该往路由器的哪个端口发送. 所以它的转发表仅仅包含了目的地址相关的信息和链路接口两个项.</p><h5 id="最长前缀匹配"><a href="#最长前缀匹配" class="headerlink" title="最长前缀匹配"></a>最长前缀匹配</h5><p>因为转发表中含有的项非常多, 查找起来比较慢, 肯定不能挨个去查, 这样效率太低了. 所以它的转发表不再存储每条目的地址, 而是存储一个前缀匹配法则. 做最长的前缀匹配, 就能确定出口.</p><table><thead><tr><th>目的地址范围</th><th>链路接口</th></tr></thead><tbody><tr><td>11001000 00010111 00010000 <strong>00000000</strong> 到 11001000 00010111 00010000 <strong>11111111</strong></td><td>0</td></tr><tr><td>11001000 00010111 00011000 <strong>00000000</strong> 到 11001000 00010111 00011000 <strong>11111111</strong></td><td>1</td></tr><tr><td>11001000 00010111 00011<strong>001 00000000</strong> 到 11001000 00010111 00011<strong>111 11111111</strong></td><td>2</td></tr><tr><td>其他</td><td>3</td></tr></tbody></table><p>但是上面这个表只能确定IP范围, 还没有做到最长的前缀匹配, 上面存储的内容有些过于冗杂了.</p><table><thead><tr><th>前缀匹配</th><th>链路接口</th></tr></thead><tbody><tr><td>11001000 00010111 00010</td><td>0</td></tr><tr><td>11001000 00010111 00011000</td><td>1</td></tr><tr><td>11001000 00010111 00011</td><td>2</td></tr><tr><td>其他</td><td>3</td></tr></tbody></table><p>只要根据最符合前缀匹配式的接口进行匹配, 就达到了和上面储存地址范围相同的效果.</p><p>例如:</p><p><code>11001000 00010111 00010110 10100001</code>和第一个式子匹配度最高, 所以导向接口0.</p><p><code>11001000 00010111 00011000 10101010</code>和第二个式子匹配度最高(第三个式子没有000, 但是第二个式子有), 导向接口1.</p><h4 id="虚电路服务与数据报服务的对比"><a href="#虚电路服务与数据报服务的对比" class="headerlink" title="虚电路服务与数据报服务的对比"></a>虚电路服务与数据报服务的对比</h4><table><thead><tr><th>对比的方面</th><th>虚电路服务</th><th>数据报服务</th></tr></thead><tbody><tr><td>思路</td><td>可靠通信应当由网络来保证</td><td>可靠通信应当由用户主机来保证</td></tr><tr><td>网络层连接</td><td>提供主机到主机的连接服务</td><td>不提供主机到主机的连接服务</td></tr><tr><td>终端与网络</td><td>终端简单, 网络复杂</td><td>终端复杂, 网络简单</td></tr><tr><td>终点地址</td><td>仅在连接建立阶段使用, 每个分组使用短的虚电路号</td><td>每个分组都有终点的完整地址</td></tr><tr><td>分组的转发</td><td>属于同一条虚电路的分组均按照同一路由进行转发</td><td>每个分组独立选择路由进行转发</td></tr><tr><td>当结点出故障时</td><td>所有通过出故障的结点的虚电路均不能工作</td><td>出故障的结点可能会丢失分组, 一些路由可能会发生变化</td></tr><tr><td>分组的顺序</td><td>总是按发送顺序到达终点</td><td>到达终点时不一定按发送顺序</td></tr><tr><td>端到端的差错处理和流量控制</td><td>可以由网络负责, 也可以由用户主机负责</td><td>由用户主机负责</td></tr></tbody></table><h3 id="路由器"><a href="#路由器" class="headerlink" title="路由器"></a>路由器</h3><p>其实之前说的网络层实现的功能, 转发与路径选择, 也是路由器实现的, 换句话说网络层的核心硬件其实就是路由器. 路由器实现了选路算法, 并将数据报转发到另一个路由器上.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/router.jpg" style="zoom:50%;" /><p>路由器由四部分组成: </p><ul><li><strong>输入端口</strong>:  物理层, 链路层的接入功能及查找功能, 以”线速”完成输入端口处理</li><li><strong>交换结构</strong>: 完成输入端与输出端的交换</li><li><strong>输出端口</strong>: 执行与输入端口完全相反的流程.</li><li><strong>选路处理器</strong>: 执行选路协议.</li></ul><p>在输入端口, 如果数据报到达的速度比交换结构转发速率快, 就需要排队. 同理, 在输出端口, 如果交换结构的数据报到达速度比数据报发送的速率更快时, 也要发生排队. 当队列满时, 就会发生溢出, 无论是在输入还是输出端口都有可能发生溢出. 排队时就会产生时延.</p><p><strong>线头(HOL)阻塞</strong>: 排队的数据报在队列的前面阻碍队列中的其他数据报转发.</p><h3 id="IP"><a href="#IP" class="headerlink" title="IP"></a>IP</h3><p>互联网服务被定义成不可靠的, 尽力而为, 无连接分组交付系统. </p><ul><li>服务是<strong>不可靠</strong>的, 因为分组可能丢失, 重复, 延迟或不按序交付等, 但服务不检测这些情况, 也不提醒发送方和接收方. </li><li>服务是<strong>尽力而为</strong>的, 互联网并不随意地丢弃分组, 只有当资源用完或底层网络出现故障时才可能出现不可靠性. </li><li>服务是<strong>无连接</strong>的, 因为每个分组都是独立对待的. 分组序列可能经过不同的传输路径或者有的丢失有的到达. </li></ul><h4 id="数据报格式"><a href="#数据报格式" class="headerlink" title="数据报格式"></a>数据报格式</h4><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/ip数据报.jpg" style="zoom:50%;" /><p>在加上IP的头之后, TCP的开销就显得非常大了. IP一共是32bit*5/8=20bytes, 加上TCP报文段的20B信息一共是40B. </p><p>其中的寿命(Time to live, TTL)为<strong>0</strong>时数据报结束传输.</p><p>网络链路有MTU (Maximum Transmission Unit, 最大传输单元) – 最大可能的链路级帧, 大的IP数据报会被拆成很多个小段分别发送, 并在最终的目的地进行整合.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/ip分片.jpg" style="zoom:50%;" /><p>分片后头部信息的长度, ID, 偏移量, 段标识需要进行改变. 同一组分片的ID都是相同的, 偏移量总是依次向上加$\frac{MTU-20}{8}$. 每组最后一个片的段标识为0. 长度指的是包含IP头部所有的长度. 最终分片出来的个数应该是$\lceil \frac{\rm datagram-20}{\rm MTU-20}\rceil$片.</p><p>假设有4000B数据报, MTU为1500B. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/ip分片例子.jpg" style="zoom:50%;" /><p>因为分片前的数据报ID为X, 所以分片后也都是X. 一共分了$\lceil \frac{4000-20}{1500-20}\rceil=3$片, 前两片的长度都是MTU, 段标识都是1. 偏移量依次向上增加$\frac{1500-20}{8}=185$. 最后一片因为不是正好凑齐, 所以长度为$(4000-20) - (1500-20)\times 2+20=1040$, 段标识为0代表是分片的末尾.</p><p>再举个例子:</p><blockquote><p>datagram: 3000B</p><p>MTU: 500</p><p>ID: 422</p></blockquote><p>应该一共有$\lceil \frac{3000-20}{500-20}\rceil=7$片, 所有ID均为422, 偏移量依次向上增加$\frac{500-20}{8}=60$. 前6片的长度为500, 段标识为1. 最后一片的段标识为0, 长度为$(3000-20)-(500-20)\times6+20=120$.</p><h4 id="IPv4编址"><a href="#IPv4编址" class="headerlink" title="IPv4编址"></a>IPv4编址</h4><p>IP地址是对主机, 路由器接口的32-bit 标识符. 一般情况下, 路由器通常具有多个接口, 主机可能具有多个接口, IP编址与每个接口相联系. 在早期, 32位的IP地址被严格划分为8位一段, 即主机号和子网号必须严格是8的倍数. 子网号是高位地址, 主机号是低位地址. 直到有了CIDR协议, 打破了这种限制.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/五类地址.jpg" style="zoom: 67%;" /><p>当然有一些关键的IP地址是要保留的, 因为他们很重要, 不能被占用.</p><p>主机号全为0的地址表示网络本身. </p><p>主机号全为1的保留作为定向广播, 是子网内的广播地址. </p><p><code>127.xxx.xxx.xxx</code>的任意IP保留作为环路测试TCP/IP以及本机进程之间的通信, 它不是一个网络地址. 如果还不理解, 它就是我们常见的<strong>localhost</strong>. </p><p>子网掩码也是一个32位的地址, 它将所有子网位置1, 主机位置0.</p><h5 id="子网"><a href="#子网" class="headerlink" title="子网"></a>子网</h5><p>子网: 具有IP地址相同的子网部分的设备接口, 能够物理上互相到达而没有中间路由器的网络叫做子网.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/子网.jpg" style="zoom:50%;" /><p>图中有三个子网, 三个子网通过一个路由器相连接. 与子网之外的主机互相通信必须借助路由器.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/子网2.jpg" style="zoom:50%;" /><p>图中除去最上方, 最下方一共三个子网外, 由于路由器和路由器不同接口的通信也满足子网定义, 所以一共有六个子网.</p><h5 id="CIDR"><a href="#CIDR" class="headerlink" title="CIDR"></a>CIDR</h5><p>CIDR(Classless InterDomain Routing, 无类型域间选路): 子网部分长度可以为任意位, 地址格式修改为<code>a.b.c.d/x</code>, 其中x为子网部分的长度.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/cidr.jpg" style="zoom:50%;" /><h5 id="IP地址计算"><a href="#IP地址计算" class="headerlink" title="IP地址计算"></a>IP地址计算</h5><blockquote><p>假设有一个已知的IP地址204.110.113.86/22, 请分别计算:</p><ol><li>子网掩码</li><li>网络本身地址</li><li>广播地址</li><li>网络中主机号的数量</li><li>IP地址范围</li></ol></blockquote><p>要计算出上述问题, 我们只关心主机和子网的一部分, 所以我们没必要将其全部化为二进制形式, 只需要将所需的段化为二进制即可.</p><ol><li>子网掩码: <code>255.255.252.0</code></li><li>网络本身地址: <code>204.110.112.0</code></li><li>广播地址: <code>204.110.115.255</code></li><li>网络中主机号的数量: $1022$</li><li>IP地址范围: <code>204.110.112.0 ~ 204.110.115.254</code></li></ol><p>一共有22位子网号, 所以所有分割一定以第三段IP的二进制为基础. 113是$64+32+16+1$, 转为二进制为<code>01110001</code>. 前6位被子网号所占, 后2位被主机号所占. </p><p>子网掩码将子网号全置1, 所以是<code>255.255.255-2-1.0</code>, 即<code>255.255.252.0</code>.</p><p>将<code>01110001</code>后两位置0, 即<code>01110000</code>, 也就是$113-1=112$, 故网络本身地址为<code>204.110.112.0</code>.</p><p>广播地址同理, 后两位置1, 即<code>01110011</code>, $112+2+1=115$, 广播地址为<code>204.110.112.255</code>.</p><p>因为网络主机号有$32-22=10$位, 除去网络本身和广播地址保留, 一共主机有$2^{10}-2=1022$个.</p><p>IP地址范围沿着上面写, 就是去除掉广播地址和网络本身的所有IP地址, 即<code>204.110.112.0 ~ 204.110.115.254</code>.</p><p>再来一个例子:</p><blockquote><p>假设有一个已知的IP地址190.168.100.5/20, 请分别计算:</p><ol><li>子网掩码</li><li>网络本身地址</li><li>广播地址</li><li>网络中主机号的数量</li><li>IP地址范围</li></ol></blockquote><p>还是与上面一样, 直接放答案:</p><ol><li>子网掩码: <code>255.255.240.0</code></li><li>网络本身地址: <code>190.168.96.0</code></li><li>广播地址: <code>190.168.111.0</code></li><li>网络中主机号的数量: $4094$</li><li>IP地址范围: <code>190.168.96.1 ~ 190.168.111.254</code></li></ol><h4 id="DHCP"><a href="#DHCP" class="headerlink" title="DHCP"></a>DHCP</h4><p>动态主机配置协议(Dynamic Host Configuration Protocol, DHCP) 常用于给主机<strong>动态</strong>地分配IP 地址, 它提供了<strong>即插即用</strong>联网的机制, 这种机制允许一台计算机加入新的网络和获取IP 地址而不用手工参与. DHCP 是应用层协议, 它是基于UDP 的.<br>DHCP 的工作原理如下: </p><p>使用客户／服务器方式. 需要IP 地址的主机在启动时就向DHCP 服务器广播发送发现报文, 这时该主机就成为DHCP 客户. 本地网络上所有主机都能收到此广播报文, 但只有DHCP 服务器(静态IP)才回答此广播报文. DHCP 服务器先在其数据库中查找该计算机的配置信息. 若找到, 则返回找到的信息. 若找不到, 则从服务器的IP 地址池中取一个地址分配给该计算机. DHCP 服务器的回答报文称为提供报文. </p><p>DHCP 服务器聚合DHCP 客户端的交换过程如下: </p><ol><li>DHCP 客户机广播”<strong>DHCP 发现</strong>“消息, 试图找到网络中的DHCP 服务器, 以便从DHCP服务器获得一个IP 地址. </li><li>DHCP 服务器收到”DHCP 发现消息后, 向网络中单播”<strong>DHCP 提供</strong>“消息, 其中包括提供DHCP客户机的IP 地址和相关配置信息. </li><li>DHCP 客户机收到”DHCP 提供”消息, 如果确认接收DHCP 服务器所提供的相关参数, 那么通过广播”<strong>DHCP 请求</strong>“消息向DHCP 服务器请求提供IP 地址. </li><li>DHCP 服务器收到”DHCP 请求”消息后, 单播一个”<strong>DHCP响应</strong>“.</li></ol><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/dhcp.jpg" style="zoom: 67%;" /><h4 id="NAT"><a href="#NAT" class="headerlink" title="NAT"></a>NAT</h4><p>NAT(Network Address Translation, 网络地址转换): 因为外部关注本地网络只使用的一个IP地址, 所以对于某个局域网下的设备完全可以对外共用一个IP地址. 通俗的说就是将网络划分为了<strong>公网</strong>和<strong>私网</strong>, 当多台私网内设备访问外部时, 都是通过一个中心设备做一次IP地址映射, 以同一个公网IP访问外部. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/nat.jpg" style="zoom:50%;" /><p>当私网内的IP发生变化时, 根本不用通知外网, 并且增强了私网设备的安全性, 大大的节省了IPv4数量.</p><h4 id="ICMP"><a href="#ICMP" class="headerlink" title="ICMP"></a>ICMP</h4><p>为了提高IP 数据报交付成功的机会, 在网络层使用了网际控制报文协议(Internet Control Message Protocol, ICMP) 来让主机或路由器<strong>报告差错和异常</strong>情况. ICMP 报文作为<strong>网络层</strong>数据报的数据, 加上数据报的首部, 组成IP 数据报发送出去. ICMP 是IP 层协议. ICMP 报文的种类有两种, 即ICMP 差错报告报文和ICMP 询问报文. ICMP 差错报告报文用于目标主机或到目标主机路径上的路由器向源主机报告差错和异常情况. </p><ol><li>终点不可达. 当路由器或主机不能交付数据报时, 就向源点发送终点不可达报文. </li><li>源点抑制. 当路由器或主机由于拥塞而丢弃数据报时, 就向源点发送源点抑制报文, 使源点知道应当把数据报的发送速率放慢. </li><li>时间超过. 当路由器收到生存时间(TTL) 为零的数据报时, 除丢弃该数据报外, 还要向源点发送时间超过报文. 当终点在预先规定的时间内不能收到一个数据报的全部数据报片时, 就把已收到的数据报片都丢弃, 并向源点发送时间超过报文. </li><li>参数问题. 当路由器或目的主机收到的数据报的首部中有的字段的值不正确时, 就丢弃该数据报, 并向源点发送参数问题报文. </li><li>改变路由(重定向) . 路由器把改变路由报文发送给主机, 让主机知道下次应将数据报发送给另外的路由器(可通过更好的路由) . </li></ol><h4 id="IPv6"><a href="#IPv6" class="headerlink" title="IPv6"></a>IPv6</h4><p>IPv6开始是为解决32位IPv4地址很快分配完而诞生的. 稍作了解即可.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/ipv6.jpg" style="zoom:50%;" /><ul><li>128位的IP地址</li><li><strong>固定</strong>长度 40 字节首部</li><li><strong>只能在源与目的上进行分片与重组装</strong></li><li>没有检查和</li><li>选项部分被移到首部之外</li></ul><p>IPv6和IPv4目前只能通过<strong>隧道</strong>的方式混用, 也就是在IPv路由器之间IPv6数据报作为IPv4数据报的负载.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/隧道.jpg" style="zoom: 67%;" /><p>所谓的双栈不是一个很好的解决方法, 在一台设备上同时启用IPv4协议栈和IPv6协议栈, 同时分别配置了IPv4地址和IPv6地址, 对于IP地址耗尽的问题却没有任何帮助, 反而增加了网络的复杂度.</p><h3 id="选路算法"><a href="#选路算法" class="headerlink" title="选路算法"></a>选路算法</h3><p>选路是为了决定从源到目的地通过网络的”好的路径”, 选路问题可以抽象成图论问题.</p><p>全局的或分散的信息?</p><p>分散的: </p><ul><li>路由器知道物理相连的邻居, 到邻居的链路费用</li><li>计算的迭代过程, 与邻居交换信息</li><li>“距离矢量(DV)” 算法</li></ul><p>全局的:</p><ul><li>所有路由器具有完全的拓扑, 链路费用信息</li><li>“链路状态(LS)”算法</li></ul><p>静态的或动态的?</p><p>静态(非自适应): 路由随时间缓慢变化</p><p>动态(自适应): 路由更快地变化, 周期的更新, 适应链路费用变化</p><h4 id="Link-State-Routing-Algorithm"><a href="#Link-State-Routing-Algorithm" class="headerlink" title="Link State Routing Algorithm"></a>Link State Routing Algorithm</h4><p>LS采用的图论算法是<strong>Dijkstra</strong>算法, 所有节点知道网络拓扑, 链路费用, 经”链路状态广播”完成, 所有节点具有相同信息, 因此LS是<strong>全局</strong>的算法. 然后利用Dijkstra算法迭代得到最短路径.</p><p>$c(x,y)$: 从节点$x$到$y$的链路费用, 如果不是直接邻居则为$\infty$.</p><p>$D(v)$: 从源到目的地$v$路径的代价.</p><p>$p(v)$: 从源到$v$沿路径的前驱节点.</p><p>$N’$: 已知最小代价的节点集合.</p><blockquote><p>1  初始化:<br>2    N’ = {u}<br>3    对所有节点v<br>4      if v 临近 u<br>5          then D(v) = c(u,v)<br>6      else D(v) = ∞<br>7<br>8   Loop<br>9     找出w不在N‘中使得D(w)最小<br>10    将w加入N’<br>11    对于所有v临近w并不在N’中, 更新D(v):<br>12       D(v) = min( D(v), D(w) + c(w,v) )<br>13    /<em> 到v的新费用或是到v的老费用或到w加上从w到v的已知最短路费用</em>/<br>15  until 所有节点在 N‘中 </p></blockquote><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/ls.jpg" style="zoom: 67%;" /><blockquote><p>本题从u出发, 步骤0初始化先写好所有与u节点相邻的节点距离, 选择一个最小的, 即节点x, 加入已知集合N’.</p><p>步骤1对距离进行更新, 选择节点y(此处选择v会有不同的顺序, 但会有相同的结果)加入已知集合. 反复迭代, 最后得到结果.</p></blockquote><h4 id="Distance-Vector-Algorithm"><a href="#Distance-Vector-Algorithm" class="headerlink" title="Distance Vector Algorithm"></a>Distance Vector Algorithm</h4><p>DV算法是一个<strong>分布式</strong>的, <strong>可迭代</strong>的<strong>异步动态</strong>算法. 它采用的是<strong>贝尔曼 - 福特方程</strong>(动态规划里的一种解法). 每个路由器将自身的路由信息发送给邻居, 每个路由器将邻居发送来的信息更新自己的路由表, 若路由表更新则发送信息更新信息给邻居, 否则不发送. 但距离向量算法具有路由自环的缺点, 会导致好消息传的快, 坏消息传的慢的现象, 且可能会导致无穷计算的问题. 发生这种情况的话, 可以采用<strong>毒性逆转</strong>, 即当一条路径信息变为无效之后, 路由器并不立即将它从路由表中删除, 而是用不可达的度量值将它广播出去.</p><p>$d_x(y)$: x到y的最低费用.</p><p>$c(x, v)$: 节点x到邻居v的代价.</p><p>$D_x$: 节点x到其他所有节点距离的向量, $D_x=[d_x(y): y\in N]$.</p><p>$S_x$:  节点x到其他所有节点后继的向量, $S_x=[s_x(y): y\in N]$. $s_x(y)$为节点x到y的后继节点.</p><p>当节点x接收到来自邻居的新距离向量估计, 它使用B-F方程更新其自己的距离向量:<br>$$<br>D_x(y) \leftarrow \min_v\{c(x, y)+D_v(y): y\in N\}<br>$$<br><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/dv.jpg" style="zoom: 67%;" /></p><p>在节点u计算完到达其他所有节点的$d_u(x)$后, 产生一个距离向量$D_u$, 和一个后继向量$S_u$.</p><p>因此, 每个节点要做的只是<strong>等待通知 -&gt; 重新计算 -&gt; 通知邻居</strong>的循环. 所以说它是一个能够自适应的分布式动态异步算法.</p><h4 id="LS和DV的对比"><a href="#LS和DV的对比" class="headerlink" title="LS和DV的对比"></a>LS和DV的对比</h4><table><thead><tr><th></th><th>LS 算法</th><th>DV 算法</th></tr></thead><tbody><tr><td>原理</td><td>使用全局信息的算法, 网络拓扑和所有的链路费用是已知的, 通过让每个节点向网络中的所有其它路由器广播链路状态分组来完成.</td><td>每个节点的选路表包括了它的距离向量和它的每个邻居的距离向量, 并不时地向它的每个邻居发送它的距离向量拷贝.</td></tr><tr><td>算法</td><td>Dijkstra算法,  是迭代算法</td><td>是一种迭代的, 异步的和分布式的算法</td></tr><tr><td>计算方法</td><td>计算从源节点到网络中所有其它节点的最低费用路径  经算法迭的第K次迭代后, 可知道到K个目的节点的最低费用.</td><td>从邻居接收更新距离向量, 重新计算选路表项和通知邻居到目的地的最低费用路径的费用已经变化的过程继续下去, 直到无更新报文发送为止.</td></tr><tr><td>特性</td><td>报文复杂, 收敛速度快</td><td>报文少, 收敛速度慢</td></tr></tbody></table><h4 id="层次路由"><a href="#层次路由" class="headerlink" title="层次路由"></a>层次路由</h4><p>由于网络规模增大到一定程度时候, 路由表不可能存放下所有的信息. 将某区域的路由器聚合成为 “自治系统” (AS), 在相同AS中的路由器运行相同的选路协议, 在不同AS中可以不用运行相同的选路协议. 连接AS间的路由器称为<strong>网关</strong>.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/等级选路.jpg" style="zoom: 67%;" /><p>假设AS1要将接收到的数据报转发到外部, 此时网关不唯一, 那么AS1必须要知道AS2能到达哪些目的地, AS3能到达哪些目的地, 还必须将这些信息传播到AS1内部的所有路由器. 这些内容都由<strong>AS之间的选路协议</strong>负责. 层次路由应运而生, 如何管理跨越AS之间的路由情况, 就是AS间选路协议要解决的问题.</p><p>在AS1中选择网关时, 是由AS间路由协议完成, 采用<strong>热土豆路由</strong>, 也就是直接发送给离自己最近的网关即可.</p><h3 id="互联网中的选路"><a href="#互联网中的选路" class="headerlink" title="互联网中的选路"></a>互联网中的选路</h3><p>RIP和OSPF都属于IGP(Interior Gateway Protocol, 内部网关协议), 即AS内的协议, 而BGP属于EGP(Exterior Gateway Protocol, 外部网关协议), 也就是AS间的协议.</p><h4 id="RIP"><a href="#RIP" class="headerlink" title="RIP"></a>RIP</h4><ul><li>是最早的<strong>AS内部</strong>因特网选路协议, 是一种<strong>距离向量</strong>协议. </li><li>使用跳数(hop) 做为其费用测度, 一条路径的最大费用被限制为<strong>15</strong>(防止无穷计算). </li><li>RIP中, 选路信息在<strong>邻居之间</strong>通过RIP响应报文交换, 约<strong>30</strong>秒交换一次.</li><li>每台路由器维护一张RIP表, 称为选路表, 包括该路由器的距离向量和该路由器的转发表. <ul><li>选路表的更新</li></ul></li><li>RIP使用IP协议之上的UDP协议来实现网络层功能, 该UDP报文段在标准IP数据报中承载在路由器之间. </li><li>RIP被设置在较低层ISP和企业网中. </li><li>RIP协议通过应用层进程进行管理, 周期性通过UDP数据报发送.</li></ul><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/rip.jpg" style="zoom: 67%;" /><h4 id="OSPF"><a href="#OSPF" class="headerlink" title="OSPF"></a>OSPF</h4><p>OSPF(Open Shortest Path First)是开放最短路径优先算法, 开放指协议是公开发表, 公众可用的, 最短路径优先指的是使用了Dijkstra提出的最短路径算法(采用LS分组扩散). 也是一个作用域<strong>AS内部</strong>的算法.</p><ul><li>OSPF通常被设置在较顶层的ISP中. </li><li>它的核心是一个使用<strong>洪泛</strong>链路状态信息的链路状态协议和一个Dijkstra 最低费用路径算法. </li><li>一台路由器构建了一幅关于整个AS的<strong>完整拓扑图</strong>.</li><li>使用OSPF时, 路由向自治系统内所有其它路由器广播选路信息. OSPF报文直接封装在<strong>IP数据报</strong>中, 必须实现可靠传输.</li><li>仅当链路状态发生变化时, 路由器才向所有路由器洪泛发送信息.</li></ul><p><strong>OSPF优点</strong>: </p><ul><li>安全: 交换都是经过鉴别的</li><li>多条费用相同的路径: 费用相同时, 可以使用多条(负载均衡, RIP中只能用一条)</li><li>对单播选路与多播选路的综合支持</li><li>支持在单个选路域内的层次结构</li><li>AS内的一个OSPF区域配置成主干区域, 区域间选路要求分组首先路由到一个区域边界路由器, 再通过主干路由器到目的区域的区域边界路由器, 最后到达目的. </li></ul><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/层次ospf.jpg" style="zoom: 50%;" /><p>有<strong>局部</strong>(Area)和<strong>主干</strong>(Backbone)两级分层, 链路的状态通告仅在区内, 每个节点具有详细的区域拓扑, 仅知道到其他区域网络的方向(最短路径). 区边界路由器(Area border routers)汇总了到在自己区域网络的距离, 向其他区域边界路由器通告. 主干路由器(Backbone Routers)在主干区运行OSPF算法, 边界路由器(Boundary routers)要连接其他的AS.</p><h4 id="BGP"><a href="#BGP" class="headerlink" title="BGP"></a>BGP</h4><p>边界网关协议(Border Gateway Protocol, BGP) 是不同自治系统的路由器之间交换路由信息的协议, 是一种外部网关协议. 边界网关协议常用于互联网的网关之间. 路由表包含已知路由器的列表, 路由器能够达到的地址及到达每个路由器的路径的跳数. 内部网关协议主要设法使数据报在一个AS 中尽可能有效地从源站传送到目的站. 在一个AS内部不需要考虑其他方面的策略. 然而BGP 使用的环境却不同, 主要原因如下: </p><ol><li>因特网的规模太大, 使得自治系统之间路由选择非常困难.</li><li>对于自治系统之间的路由选择, 要寻找最佳路由是很不现实的. </li><li>自治系统之间的路由选择必须考虑有关策略. </li></ol><p>边界网关协议(BGP) 只能力求寻找一条能够到达目的网络且<strong>比较好的路由</strong>(不能兜圈子) , 而并非寻找一条最佳路由. BGP 采用的是<strong>路径向量</strong>路由选择协议, 它与距离向量协议和链路状态协议有很大的区别. BGP 的实现是<strong>应用层</strong>协议(BGP本身肯定是网络层协议), 基于<strong>TCP</strong> (AS间的策略比性能更加重要), 端口号179. 支持CIDR.<br>BGP 的工作原理如下: 每个AS的管理员要选择至少一个路由器(可以有多个) 作为该自治系统的”<strong>BGP 发言人</strong>“. 一个BGP 发言人与其他自治系统中的BGP 发言人要交换路由信息, 就要先建立TCP 连接(可见BGP 报文是通过TCP 传送的, 也就是说<strong>BGP 报文是TCP 报文的数据部分</strong>) , 然后在此连接上交换BGP 报文以建立BGP 会话, 再利用BGP 会话交换路由信息. 当所有BGP 发言人都相互交换网络可达性的信息后, 各BGP 发言人就可找出到达各个自治系统的较好路由. 每当可达性信息发生变化时, 才进行交换, 否则不交换.</p><ul><li><strong>eBGP</strong>: 跨越两个AS的BGP会话(外部) </li><li><strong>iBGP</strong>: 同一个AS中的两台路由器之间的BGP会话(内部) </li></ul><h3 id="广播和多播选路"><a href="#广播和多播选路" class="headerlink" title="广播和多播选路"></a>广播和多播选路</h3><p>广播和多播是什么?</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/广播和多播.jpg" style="zoom:50%;" /><p><strong>广播选路</strong>: 从一个源节点到网络中的所有其它节点交付分组.</p><p><strong>多播选路</strong>: 使单个源节点能够向其它网络节点的一个子集发送分组的拷贝.</p><p>当向多个接收方发送相同的数据时, 为了更好的带宽利用率, 较少的主机/路由器处理和更快的参与, 就需要多播.</p><h4 id="广播选路算法"><a href="#广播选路算法" class="headerlink" title="广播选路算法"></a>广播选路算法</h4><p><strong>无控制洪泛</strong>: 要求源节点向它的所有邻居发送该分组拷贝, 产生广播风暴. </p><p><strong>受控洪泛</strong>: </p><ul><li>序号控制洪泛: 加入控制序号, 重复时, 直接丢弃. 否则, 保存并广播.</li><li>反向路径转发RPF: 考虑是否为最短路径上转发的分组. 仅当该分组到达的链路正好是位于源节点到该节点的最短单播路径上, 它才向其所有出链路(除了它接收的那个) 传输报文. 否则, 丢弃入分组. </li></ul><p><strong>生成广播树</strong>: 避免环的产生, 因此可以完全避免冗余广播. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/生成广播树.jpg" style="zoom: 67%;" /><p>这是一种基于中心的生成树构造过程:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/生成广播树2.jpg" style="zoom:67%;" /><p>定义一个中心节点, 其他节点都向中心节点进行单播, 并将路径加入这棵树, 如果路径的一部分已经在树中了, 那么取剩下未加入的部分加入到树中.</p><h4 id="多播选路算法"><a href="#多播选路算法" class="headerlink" title="多播选路算法"></a>多播选路算法</h4><p>在多播通信中, 面临两个问题, 即怎样标识多播分组的接收方, 以及怎样为发送到这些接收方的分组编址. 在因特网体系结构中, 多播数据报使用<strong>间接地址</strong>来编址. 表示一组接收方的单一标识就是一个 D 类多播地址. 与一个 D 类地址相关联的多个接收方称为一个多播组. </p><p><strong>IGMP</strong>(Internet Group Management Protocol, 互联网组管理协议) 运行在一台主机与其直接相连的路由器之间.  IGMP 为一台主机提供了手段, 可让它通知与其相连的路由器, 在本主机上运行的一个应用程序如何加入一个特定的多播组. </p><p>在实践中有两种方法用于确定多播选路树: </p><p><strong>使用一棵组共享树进行多播选路</strong>. 像在生成树广播的场合中一样, 跨越组的共享树多播选路基于一棵多播树, 该树包括所有具有属干该多播组的相连主机的边缘路由器. 在使用基于中心方法来构造多播选路树, 具有属于多播组相连主机的边缘路由器向中心节点(经单播)发送加入消息. 像在广播情况下一样, 一个加入报文使用单播选路朝着中心转发, 直到它到达已经属干多播树的一台路由器或到达这个中心. </p><p><strong>使用一棵基于源的树进行多播选路</strong>. 为多播组中的每个源构建一棵多播选路树. 在实践中, 使用 RPF 算法来构造一棵多播转发树, 以用于转发来自源节点的多播数据报. </p><p>第一个用于因特网中的多播选路协议是<strong>距离向量多播选路协议</strong>(Distance Vector Multicast Routing Protocol, <strong>DVMRP</strong>). </p><ul><li>洪泛与剪枝:  反向路径转发, 基于源的树</li><li>基于DVMRP自己的选路表的RPF 树, 通过DVMRP路由器的通信构造</li><li>向多播组发起的数据报经RPF洪泛到各处 </li><li>无组成员的路由器: 发送上游剪枝报文</li></ul><h2 id="链路层和局域网"><a href="#链路层和局域网" class="headerlink" title="链路层和局域网"></a>链路层和局域网</h2><p>因为网络层考虑的是解决路径选择和转发的问题, 链路层更加的贴近于物理层, 解决的是<strong>同一链路</strong>主机的数据如何在连路上进行传输的问题. 比如说解决在链路传播比特级的差错和纠错, 半双工全双工, 链路上的流量控制, 链路冲突问题等. 链路层把网络层传下来的分组封装成<strong>帧</strong>(frame). 不同的链路完全可以提供<strong>不同的链路协议</strong>而不互相干扰, 在链路上的传输控制免不了物理设备的配合, 即<strong>网卡</strong>(NIC).</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/链路层.jpg" style="zoom: 80%;" /><h3 id="差错检测"><a href="#差错检测" class="headerlink" title="差错检测"></a>差错检测</h3><p>EDC(Error Detection and Correction bits), 实际上差错检测是<strong>差错的检测和纠错</strong>. 差错检测可能保护各层的头信息. 但是差错检测也不是完全可靠, 因为协议可能会漏掉某些差错.</p><h4 id="奇偶校验"><a href="#奇偶校验" class="headerlink" title="奇偶校验"></a>奇偶校验</h4><p>奇偶校验比较简单, 能够检测出错误, 但是不能对错误进行纠正.</p><p><strong>奇校验</strong>: 添加冗余位使得整个序列中1的个数为奇数.</p><p><strong>偶校验</strong>: 添加冗余位使得整个序列中1的个数为偶数.</p><p>比如<code>0111000110101011|0</code>添加了一位冗余, 使得整个序列中1的个数为9, 即奇数, 所以这是奇校验. 添加一位冗余的奇偶校验只能检测出有1比特的错误, 也不能确定位置.</p><p>二维比特奇偶校验不但能检测出1比特差错, 还能将其纠正. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/二维奇偶校验.png" style="zoom: 33%;" /><p>图中所示的是二维偶校验, 将原始序列按照行列排好, 每行每列都产生一位冗余位, 最后再按照同样奇偶规则将产生的冗余位再加一位校验冗余位. 最下面左侧的图是还没有发送前保存的数据, 右侧是接收方接收到的数据. 将冗余位进行比对, 由于第二行不满足1个数为偶数, 所以说第二行有错误. 第二列也不满足, 所以错误定位到第二行第二列. 这样就完成了错误差错检测.</p><h4 id="检查和-1"><a href="#检查和-1" class="headerlink" title="检查和"></a>检查和</h4><p>检查和就是之前传输层的检查和, 只能做到差错检测, 如果有错自己做其他处理.</p><h4 id="CRC循环冗余校验"><a href="#CRC循环冗余校验" class="headerlink" title="CRC循环冗余校验"></a>CRC循环冗余校验</h4><p>将数据比特$\rm D$视为一个二进制数, 选择$r$个CRC比特$\rm R$, 使得接收方通过某个生成多项式转换为$r+1$位二进制数$\rm G$能整除$\rm &lt;D, R&gt;$. 如果有非零余数即检测到差错.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/crc格式.jpg" style="zoom: 67%;" /><p>$\rm R$就是我们最终要计算出来的东西. 在写出$\rm &lt;D,R&gt;$ 前, 需要根据多项式位数对数据$\rm D$进行移位, 多项式$\rm G$ 是$r+1$位, 就将数据$\rm D$ 后添加$r$ 个0(也就是下述式子所述的$\rm D\cdot 2^r$), 从而满足求出来的余数一定是$r$ 位.<br>$$<br>\rm R = 余数[\frac{D\cdot2^r}{G}]<br>$$<br>如何将生成多项式转化为二进制数? 其实就是取多项式系数的过程. 假设生成多项式为$x^5+x^4+x+1$, 那么对应的二进制数就是<code>110011</code>, 从$1\times x^5+1\times x^4+0\times x^3+0\times x^2+1\times x+1\times x^0$ 提取系数得来. </p><p>看一个例子:</p><blockquote><p>假设多项式为$\rm G(x)=x^3 + 1$, 数据$\rm D=101110$, 求CRC位$\rm R$.</p></blockquote><p>先将多项式转化为二进制, 即<code>1001</code>, 是4位, 则将D的右侧补3个0. 然后进行二进制除法. <strong>二进制除法的本质是异或(XOR)</strong>, 因为每一位除的结果不影响其他位, 加法不进位，减法不借位, 所以实际上就是异或.</p><p>只要遵循够$r+1$位商1, 否则商0的原则, 就能得到结果. 如下所示:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/crc.jpg" style="zoom:67%;" /><p>除到最后时, 如果不满足$r$ 位不要忘记<strong>添0</strong>.</p><p>再来一个例子检验一下:</p><blockquote><p>假设多项式为$\rm G(x)=x^3 + 1$, 数据$\rm D=100101110$, 求CRC位$\rm R$.</p></blockquote><p>结果为<code>11010</code>.</p><h3 id="多路访问协议"><a href="#多路访问协议" class="headerlink" title="多路访问协议"></a>多路访问协议</h3><p>对于链路, 往往有两种, 一种是点对点类型的, 还有一种市广播类型的. 多路访问协议能够一定程度上解决两个节点的并行传输问题, 也称为<strong>碰撞</strong>. 在这个过程中, 被共享的信道必须通过其自身解决, 不增加外来信道进行协调, 否则也失去了多路访问协议存在的意义.</p><p>在理想的多路访问协议中, 一个节点传输时, 应该达到信道的最大传输速率, 多个节点传输时应该达到平均最大传输速率. 并且没有其他的特殊节点协调, 也没有同步时钟和间隙. </p><p>MAC协议分为三大类:</p><ul><li>信道划分: 将信道划分为较小的“段” (时隙，频率，编码), 为节点分配一部分专用.</li><li>随机访问: 不划分信道，<strong>允许碰撞</strong>, 但想方设法从“碰撞”恢复.</li><li>轮转: 节点轮转，但有更多信息要发送的能够轮转的较长时间.</li></ul><h4 id="信道划分"><a href="#信道划分" class="headerlink" title="信道划分"></a>信道划分</h4><p>信道划分实际上有四个算法, 分别是时分多路访问TDMA, 频分多路访问FDMA, 波分多路复用WDM, 码分多路访问CDMA.</p><h5 id="TDMA"><a href="#TDMA" class="headerlink" title="TDMA"></a>TDMA</h5><p>TDMA(Time division multiple access, 时分多路访问): </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/tdma.jpg" style="zoom:67%;" /><p>每个站点在每个循环中获得<strong>固定长度时隙</strong>, 循环访问信道, 不占用空闲的时隙. 上图中时隙2, 5, 6空闲.</p><h5 id="FDMA"><a href="#FDMA" class="headerlink" title="FDMA"></a>FDMA</h5><p>FDMA(Frequency division multiple access, 频分多路访问):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/fdma.jpg" style="zoom:67%;" /><p>为信道频谱划分为多个频带, 每个站点分配固定的频带, 每个频带上分别传输, 也不占用空闲的频带. 上图中频带2, 5, 6空闲.</p><h5 id="CDMA"><a href="#CDMA" class="headerlink" title="CDMA"></a>CDMA</h5><p>CDMA(Code Division Multiple Access, 码分多路访问):</p><p>码分多路访问下, 所有用户共享相同的频道, 也共享相同的时间. 每个比特时间被划分为m个更短的时间槽, 称为”<strong>码片</strong>“. 每个用户用自己的”码片序列”对数据进行编码, 在输出端进行解码, 这些序列应该是相互”正交”的, 所以不会相互干扰. 在<strong>WIFI</strong>经常用到这种技术.</p><h5 id="WDM"><a href="#WDM" class="headerlink" title="WDM"></a>WDM</h5><p>WDM(Wavelength Division Multiplexing, 波分多路复用):</p><p>波分多路复用就是光的频分多路复用，在一根光纤中传输多种不同波长(频率) 的光信号，由于波长(频率) 不同，所以各路光信号互不干扰，最后再用波长分解复用器将各路波长分解出来.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/wdm.png" style="zoom: 50%;" /><h4 id="随机访问协议"><a href="#随机访问协议" class="headerlink" title="随机访问协议"></a>随机访问协议</h4><h5 id="时隙ALOHA"><a href="#时隙ALOHA" class="headerlink" title="时隙ALOHA"></a>时隙ALOHA</h5><p>一句话概括: <strong>时隙 + 概率重传</strong>.</p><p><strong>假定</strong>:</p><ul><li>所有帧有相同长度</li><li>时间划分为等长时隙，能够传输1个帧</li><li>节点是同步的, 且节点仅在时隙开始时开始传输帧</li><li>如果2个或多个节点在时隙中传输，所有节点检测碰撞</li></ul><p><strong>操作</strong>:</p><ul><li>当节点获得新帧，将在下一个时隙中传输</li><li>无碰撞，节点能够在下一个时隙中发送新帧</li><li>如果碰撞，节点在每个后继时隙中以<strong>概率p重传帧</strong>直到成功</li></ul><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/时隙aloha.jpg" style="zoom:67%;" /><p>这么搞看起来是没啥问题, 但是实际上时隙空闲的概率非常高.</p><p>假定$N$个有许多帧要发送节点，每个时隙以概率$p$发送.</p><p>某个节点在一个时隙中成功发送的概率为$p(1-p)^{N-1}$, 即除自己外的其他节点都重传失败. 那么任何节点成功发送的概率为$Np(1-p)^{N-1}$. </p><p>下面来求一下这个概率的最大值$p_{max}$,<br>$$<br>\begin{aligned}<br>E(p)&amp;=Np(1-p)^{N-1} \\<br>E’(p)&amp;=N(1-p)^{N-1} -Np(1-p)^{N-2} \\<br>&amp;=N(1-p)^{N-2}((1-p)-p(N-1))\\<br>E’(p)&amp;=0 \Rightarrow p_{max}=\frac{1}{N}<br>\end{aligned}<br>$$</p><p>当有无穷多个节点, 即$N\rightarrow \infty$时,<br>$$<br>\displaylines{<br>\lim_{N\to\infty}E(p_{max})=N\frac{1}{N}(1-\frac{1}{N})^{N-1} = \frac{(1-\frac{1}{N})^N}{1-\frac{1}{N}} \\<br>\lim_{N\to \infty}{1-\frac{1}{N}}=1  \qquad\lim_{N\to \infty}({1-\frac{1}{N}})^N=\frac{1}{e} \\<br>\lim_{N\to \infty}{E(p_{max})}=\frac{1}{e}=37\%<br>}<br>$$<br>即在节点无穷多时, 效率最终只有$37\%$.</p><p><strong>优点</strong>:</p><ul><li>单个活跃节点能够连续地以信道的全速传输</li><li>仅节点中的时隙需要同步, 即高速分散</li><li>简单</li></ul><p><strong>缺点</strong>:</p><ul><li>碰撞后可能浪费时隙</li><li>空闲时隙无法利用</li><li>节点可能能够以小于传输分组的时间检测到碰撞</li><li>所有节点时钟同步</li></ul><h5 id="纯ALOHA"><a href="#纯ALOHA" class="headerlink" title="纯ALOHA"></a>纯ALOHA</h5><p>一句话概括: <strong>想发就发 + 随机重传</strong>.</p><p>也就是非时隙的ALOHA, 更简单, 没有同步的要求. 当帧到达时立即进行传输. 增加了碰撞的概率. 如果接收方在一定时间内没有收到, 那就判断发生了冲突.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/非时隙aloha.jpg" style="zoom:67%;" /><p>从效率角度考虑一下呢? 仍然采用帧传输时间为时间单元. 在任意给定时间, 某节点传输一个帧的概率为$p$. 假设该帧在$t_0$ 时刻开始传输, 为了使得这帧传输成功, 则在$[t_0-1, t_0]$不能有其他节点进行传输. 所有其他节点在这个时间间隔不开始传输的概率是$(1-p)^{N-1}$, 同样在$t_0$时刻也不能有其他节点进行传输, 概率也是$(1-p)^{N-1}$. 所以对于$t_0$时刻一个节点传输成功的概率是$(1-p)^{2(N-1)}$.</p><p>仍然先求出发送成功概率的最大值$p_{max}$.<br>$$<br>\begin{aligned}<br>E(p)&amp;=Np(1-p)^{2(N-1)} \\<br>E’(p)&amp;=N(1-p)^{2(N-1)} -Np2(N-1)(1-p)^{2(N-2)} \\<br>&amp;=N(1-p)^{2N-3}((1-p)-p2(N-1))\\<br>E’(p)&amp;=0 \Rightarrow p_{max}=\frac{1}{2N-1}<br>\end{aligned}<br>$$<br>然后再求出当节点区域无穷时的极限.<br>$$<br>\displaylines{<br>\lim_{N\to\infty}E(p_{max})=\frac{N}{2N-1}(1-\frac{1}{2N-1})^{2(N-1)} \\<br>\lim_{N\to \infty}{\frac{N}{2N-1}}=\frac{1}{2} \qquad\lim_{N\to \infty}({1-\frac{1}{2N-1}})^{2N-1}=\frac{1}{e} \\<br>\lim_{N\to \infty}{E(p_{max})}=\frac{1}{2} \cdot \frac{1}{e} = 18\%}<br>$$<br>实际上纯ALOHA的效率还不如时隙ALOHA, 也就是时隙ALOHA的一半.</p><h5 id="CSMA"><a href="#CSMA" class="headerlink" title="CSMA"></a>CSMA</h5><p>CSMA(Carrier Sense Multiple Access, 载波侦听多路访问). 前面的信道划分协议都是不管别人有没有在传输, 都会直接传输试试, 再判断是否有人传输, 这从逻辑上就有点反常. 在别人已经进行传输时, 自己再进行传输, 岂不是白费功夫? 别人在说话时候你打岔, 就大大增加了冲突的概率. CSMA就是一个在<strong>传输前侦听</strong>的访问协议. 如果侦听到信道正忙, 则推迟传输. 先听听有没有人在说话, 如果有就等别人说完了再说. 碰撞仍然会出现, 但是概率会减小.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/csma.jpg" style="zoom: 67%;" /><p>但是考虑到传播时延, 也许节点已在传播数据, 但还在路上, 有些节点可能还没有检测到信道忙信号. 这当然会有一些问题, 叫最小帧长问题, 后面会说.</p><p>CSMA实际上有三种:</p><ul><li><strong>1-坚持 CSMA:</strong> 节点发送数据时先侦听信道是否空闲, 如果空闲则立即发送, 如果忙就一直侦听, 等到空闲时再发送. 发生冲突时, 随机等待一段时间后再重新侦听. “1-坚持”指的是信道忙时继续坚持侦听信道, 信道空闲时发送的概率为1, 立刻发送数据.</li><li><strong>非坚持 CSMA</strong>: 节点发送数据时先侦听信道是否空闲, 如果空闲则立即发送, 如果忙就放弃侦听, 等待随机一段时间后再侦听, 发送, 重复上述过程.</li><li><strong>p-坚持 CSMA</strong>: 用于时分信道, 发送数据前先侦听信道, 如果信道忙就继续侦听, 直到信道空闲. 如果信道空闲, 有概率p发送数据, 有概率1-p推迟到下个时隙. 如果下个时隙信道仍然空闲, 仍然重复上述过程, 直到信道忙为止. 如果信道忙则等到下个时隙再重新开始侦听.</li></ul><h5 id="CSMA-CD"><a href="#CSMA-CD" class="headerlink" title="CSMA/CD"></a>CSMA/CD</h5><p>CSMA/CD(Carrier Sense Multiple Access with Collision Detection, 载波监听多点接入/碰撞检测):</p><ul><li>CS: <strong>载波监听/侦听</strong>，每一个站在<strong>发送数据之前</strong>以及<strong>发送数据中</strong>都要检测一下总线上是否有其他计算机在发送数据。</li><li>MA: <strong>多点接入</strong>，说明这是总线型网络，许多计算机以多点接入的方式连接在一根总线上。</li><li>CD: <strong>碰撞检测(冲突检测) <em><em>, </em></em>边发送边监听</strong>，适配器边发送数据边检测信道上信号电压的变化情况，以便判断自己在发送数据时其他站是否也在发送数据。如果一个站发现了总线上出现了碰撞，其适配器就要<strong>立即停止</strong>发送，免得继续进行无效的发送，白白浪费网络资源，然后等待一段时间随机时间后再次发送, <strong>半双工网络</strong>.</li></ul><p>如果几个发生碰撞的站都在监听信道，那么都会同时发现信道变成了空闲，如果此时都同时发送，那么肯定在原来的地方又发生了碰撞，所以碰撞后使用<strong>二进制指数退避算法</strong>, 往往等待时间再发送.</p><blockquote><p>先听后发, 边听边发 </p><p>冲突停发, 随机重发</p></blockquote><h5 id="最小帧长问题"><a href="#最小帧长问题" class="headerlink" title="最小帧长问题"></a>最小帧长问题</h5><p>A节点发了一个很短的帧, 但是发生了碰撞. 这个帧是在发送完毕的时候才检测到发生碰撞, 没法停止发送, 已经发完了. CSMA/CD希望能及时控制局面, 就必须有一个<strong>最小帧长</strong>来限制最短帧的发送, 最起码要在帧没发送完的时候检测到碰撞. 下面来探究一下这个问题.</p><p>假设A与B两个节点都需要传输数据, 单程传播时延为$\tau$. 在$t=0$时刻A开始发送数据, B检测到信道空闲. 在$t=\tau-\delta$ 时刻A的数据还没到达B, B检测到信道空闲而发送数据. 但发送数据$\delta/2$ 后, 即$t=\tau-\delta/2$ 时刻,  A和B发生的数据发生碰撞, 但返回的信息还没到达A和B, 所以A和B都不知道. 在$t=\tau$时, B检测到碰撞, 于是停止发送数据. 在$t=2\tau-\delta$ 时刻, A检测到碰撞, 于是也停止发数据. 因此明显在CSMA/CD协议下只能进行半双工通信.</p><p>A在发送帧后最多经过$2\tau(\delta \to 0)$ 的时间就能知道发送的帧是否发生碰撞. 因此将这个$2\tau$ 叫做<strong>争用期(</strong>也称冲突窗口或碰撞窗口). 只有经过这段争用期后, 才能确定这次不会产生发送冲突.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/最小帧长.jpg" style="zoom: 50%;" /><p>因此, 为确保发送站在发送数据的同时能检测到可能存在的冲突, 需要在<strong>发送完帧之前</strong>就能接收到自己发送出去的数据, 即<strong>帧的传输时延最少要两倍于信号在总线中的传播时延</strong>. 所以必须要有一个最小帧长来约束, 任何小于最小帧长的帧都是被打断的异常帧.<br>$$<br>最小帧长=总线传播时延\times数据传输率\times2<br>$$</p><blockquote><p>以太网规定取$51.2\mu s$为争用期的长度. 对于10Mb/s的以太网, 在争用期内可发送512bit, 即64B. 如果前64B未发生冲突, 那么剩下的后续数据肯定不会发生冲突(表示已经抢占信道). 因此因特网规定最小帧长为64B, 小于64B的都是冲突而终止的无效帧.</p><p>如果只发送小于64B的帧, 需要在MAC子层中于数据字段后面加入填充字段保证帧长不小于64B.</p></blockquote><h4 id="轮转协议"><a href="#轮转协议" class="headerlink" title="轮转协议"></a>轮转协议</h4><p>轮转协议兼顾了信道划分和随机访问的优点, 使用一个”令牌”进行传递, 使得大家轮流进行传输.</p><h5 id="轮询"><a href="#轮询" class="headerlink" title="轮询"></a>轮询</h5><ul><li><p>指定一个主节点，主节点轮询每个节点.</p></li><li><p>主节点依次通知每个节点它被允许的传输量，在该节点传输完毕后(传完最大传输量或无更多待传输数据) ，主节点再通知下一个节点.</p></li><li><p>引入了轮询时延，即主节点通知从节点的时延.</p></li><li><p>主节点故障会导致信道崩溃.</p></li></ul><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/轮询.jpg" style="zoom: 67%;" /><h5 id="令牌传递"><a href="#令牌传递" class="headerlink" title="令牌传递"></a>令牌传递</h5><ul><li><p><strong>令牌</strong> 小特殊帧，以某种次序在所有节点间传播.</p></li><li><p>当节点收到令牌时，当且仅当它有数据要传输时，它才持有该令牌并传输一个最大限制内的数据而后将令牌传递给下一个节点. 否则，它立即向下一个节点传输令牌.</p></li><li><p>一个节点的故障(如不肯释放令牌) 可能导致信道崩溃.</p></li></ul><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/令牌.png" style="zoom: 67%;" /><h3 id="链路层编址"><a href="#链路层编址" class="headerlink" title="链路层编址"></a>链路层编址</h3><p>虽然高层程序希望设备用IP地址进行通信, 但是传输起来的实际通信必须采用设备的物理地址.</p><h4 id="MAC地址"><a href="#MAC地址" class="headerlink" title="MAC地址"></a>MAC地址</h4><p>MAC地址(Media Access Control Address)是<strong>48bit</strong>的<strong>物理地址</strong>. 用于使数据报从一个接口到达另一个物理连接的接口, 在网络设备出厂时MAC地址就已经烧入了设备, <strong>全球唯一</strong>.</p><p>广播地址: <code>FF-FF-FF-FF-FF-FF</code></p><h4 id="ARP"><a href="#ARP" class="headerlink" title="ARP"></a>ARP</h4><p>ARP(Address Resolution Protocol, 地址解析协议) 完成了从局域网IP到MAC物理地址的映射. 每个主机, 路由器都有ARP表, 存放了<code>&lt;IP, MAC, TTL&gt;</code>, 是一张动态表.. TTL是地址映射能存活的时间, 一般是20分钟. </p><h5 id="相同网络的ARP协议"><a href="#相同网络的ARP协议" class="headerlink" title="相同网络的ARP协议"></a>相同网络的ARP协议</h5><p>在相同的局域网中, A要向B发送数据报, 但B的MAC地址不在A的ARP表中, 需要执行下述步骤:</p><p>在相同的局域网中, A要向B发送数据报, 但B的MAC地址不在A的ARP表中, 需要执行下述步骤:</p><ol><li>A通过<code>FF-FF-FF-FF-FF-FF</code>广播ARP查询分组, 包括了B的IP, 在局域网内的所有机器都收到了ARP请求.</li><li>B接收ARP分组, 用自身的MAC地址通过单播回答了A. </li><li>A在ARP表中缓存了B的IP到MAC地址映射, 直到TTL为0.</li></ol><p>上述ARP查询和ARP响应分组为:</p><table><thead><tr><th>ARP query packet</th><th>ARP response packet</th></tr></thead><tbody><tr><td>Sip: 137.196.7.78</td><td>Sip: 137.196.7.14</td></tr><tr><td>Dip: 137.196.7.14</td><td>Dip: 137.196.7.78</td></tr><tr><td>Smac: 1A-2F-BB-76-09-AD</td><td>Smac: 58-23-D7-FA-20-B0</td></tr><tr><td>Dmac: FF-FF-FF-FF-FF-FF</td><td>Dmac: 1A-2F-BB-76-09-AD</td></tr></tbody></table><h5 id="不同网络的ARP协议"><a href="#不同网络的ARP协议" class="headerlink" title="不同网络的ARP协议"></a>不同网络的ARP协议</h5><p>仍然是A给B发送数据报, 但这回A和B属于两个不同的网络. 假定A知道B的IP地址.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/arp不同局域网.jpg" style="zoom: 80%;" /><p>在连接局域网LAN1和LAN2的路由器R中有两张ARP表, 每张表对应了一个局域网.</p><ol><li>A生成具有源A, 目的地B的数据报.</li><li>A使用ARP从<code>111.111.111.110</code>得到R的MAC地址 .</li><li>A生成以R的MAC地址作为目的地的链路层帧,帧包含A-to-B IP 数据报.</li><li>A的适配器发送帧, B的适配器接收帧.</li><li>R从以太网帧取出IP数据报，看到它目的地是B.</li><li>R使用ARP得到B的MAC地址.</li><li>R生成包含A-to-B IP数据报的帧向B发送.</li></ol><p>其分组如下:</p><table><thead><tr><th>Frame A to R</th><th>Frame R to B</th></tr></thead><tbody><tr><td>Sip: 111.111.111.111</td><td>Sip: 111.111.111.111</td></tr><tr><td>Dip: 222.222.222.222</td><td>Dip: 222.222.222.222</td></tr><tr><td>Smac: 74-29-9C-E8-FF-55</td><td>Smac: 1A-23-F9-CD-06-9B</td></tr><tr><td>Dmac: E6-E9-00-17-BB-4B</td><td>Dmac: 49-BD-D2-C7-56-2A</td></tr></tbody></table><p>可以这样理解, 网络层不牵扯到物理地址, 所以IP始终没发生变化. 但是实际上在下层传输的链路层用到了物理地址, IP和MAC的地址映射工作完全交给<strong>中间人</strong>路由器完成了.</p><h3 id="以太网"><a href="#以太网" class="headerlink" title="以太网"></a>以太网</h3><p>以太网是全球使用最广泛的局域网技术，以至于好多人都以为以太网就是我们所说的网络。我们平时所说的交换机，其实专业说法叫以太网交换机。而一般的光纤交换机其实也是采用以太网技术，只是传输介质由网线改成光纤. 一般以太网的<strong>MTU</strong>设为<strong>1500</strong>字节，加上以太帧首部的长度14字节，也就是一个以太帧不会超过1500+14 = 1514字节. 最小帧长以太网的<strong>最小帧长</strong>是<strong>64</strong>字节, 凡是长度小于64字节的都是发生冲突而异常的无效帧(详见最小帧长).</p><p>网络的拓扑结构也是岁时代更新. 20世纪90年代总线拓扑流行, 现在已经流行星型拓扑结构, 中心连接通过集线器或交换机完成. 当然也有其他类型的网络:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/局域网拓扑.jpg" style="zoom: 50%;" /><p><strong>无连接</strong>: 在发送和接收适配器之间没有握手</p><p><strong>不可靠</strong>: 接收适配器不向发送适配器发送应答或否定应答. 传送给网络层的数据报流可能有间隙, 如果应用程序使用TCP, 间隙将能弥补, 否则应用程序将看到该间隙.</p><h4 id="以太网帧结构"><a href="#以太网帧结构" class="headerlink" title="以太网帧结构"></a>以太网帧结构</h4><p>发送适配器在以太网帧(或其他网络层协议分组)中封装IP数据报.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/以太网帧.jpg" style="zoom: 50%;" /><p><strong>前导码</strong>(Preamable): 用于同步接收方，发送方时钟速率. 前7个字节为10101010 的重复(前同步码, 像踏步走的121, 121循环), 最后一个字节是10101011(帧开始界定符), 结束后就说明接收方可以开始接收数据了. 但是<strong>前导码并不是链路层插入的, 而是物理层插入的</strong>!</p><p><strong>地址</strong>: <strong>6</strong>字节, 如果适配器接收具有匹配的目的地址或广播地址(如ARP分组)的帧, 它将帧中的数据提交给网络层协议</p><p><strong>类型</strong>: <strong>2</strong>字节, 指示较高层协议(网络层协议).</p><p><strong>数据</strong>: <strong>最小46B, 最大1500B</strong>. 链路层的MTU是1500字节. 以太网的最小帧长是64字节, 那么$64-6-6-2-4=46$字节.</p><p><strong>CRC</strong>: <strong>4</strong>字节, CRC循环冗余校验码.</p><h4 id="以太网的CSMA-CD"><a href="#以太网的CSMA-CD" class="headerlink" title="以太网的CSMA/CD"></a>以太网的CSMA/CD</h4><ol><li>适配器从网络层接收数据报并生成帧.</li><li>如果适配器侦听到信道空闲，开始传输帧. 如果侦听到信道忙，等待信道空闲再传输.</li><li>如果适配器传输整个帧而没有检测到另一个帧的传输，该适配器已经处理完帧.</li><li>如果适配器传输过程中检测到另一次传输, 中止并发送<strong>强化冲突信号</strong>, 放大冲突, 确保所有的其他传输方都知道发生了碰撞.</li><li>中止后, 适配器进入<strong>指数退避</strong>.</li></ol><h5 id="指数退避"><a href="#指数退避" class="headerlink" title="指数退避"></a>指数退避</h5><ul><li><p>比特时间: 对10 Mbps 以太网是 0.1 μs; 对K=1023, 等待时间约为50 msec</p></li><li><p>首次碰撞: 从$\{0,1\}$中选择$K$；时延是$K \cdot 512$bit 的传输时间</p></li><li><p>多次碰撞: 从$[0, 2^n] n \in Z$中选择$K$.</p></li></ul><h5 id="CSMA-CD的效率"><a href="#CSMA-CD的效率" class="headerlink" title="CSMA/CD的效率"></a>CSMA/CD的效率</h5><p>$t_{prop}$: LAN中的2站点之间的最大传播时间.</p><p>$t_{trans}$: 传输最长帧的时间.<br>$$<br>\eta = \frac{1}{1+5t_{prop/t_{trans}}}<br>$$<br>这样来看, 传播时间越短, 传输时间越长, 效率都越高.</p><h4 id="以太网技术"><a href="#以太网技术" class="headerlink" title="以太网技术"></a>以太网技术</h4><p>主要是10 BASE-5, 10 BASE-2, 10 BASE-T, 100 BASE-T, 100 BASE-T. </p><p>每个BASE前都有一个数字, 代表速度, 单位Mbps. BASE代表基带传输, 即baseband transmission. </p><p>10 BASE-5, 10 BASE-2最后的数字代表最大网段长度分别是500m和200m.</p><p>10 BASE-T, 100 BASE-T, 100 BASE-T最后的字母代表的是双绞线, 即twisted pair.</p><p>10 BASE-T, 100 BASE-T, 是速度为10/100Mbps的快速以太网, 使用了星型拓扑结构. 所有节点连接到中心的一台集线器, 节点和<strong>集线器</strong>最大的距离为100m.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/星型拓扑.jpg" style="zoom: 50%;" /><h4 id="曼切斯特编码"><a href="#曼切斯特编码" class="headerlink" title="曼切斯特编码"></a>曼切斯特编码</h4><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/曼彻斯特编码.jpg" style="zoom: 67%;" /><p>曼彻斯特编码在每个周期中都会发生一次<strong>突变</strong>, 以判断是否还有新的数据在传输. 这就是以太网帧结构前要加上前导码, 但却没有结束码的原因. 这种编码是由<strong>物理层</strong>实现的.</p><h3 id="集线器和交换机"><a href="#集线器和交换机" class="headerlink" title="集线器和交换机"></a>集线器和交换机</h3><h4 id="集线器"><a href="#集线器" class="headerlink" title="集线器"></a>集线器</h4><p>集线器(Hub)可以看做是<strong>物理层</strong>中继器, 只对物理电信号放大中继，所有端口<strong>同属一个冲突域</strong>，主要用来延伸网络访问距离，扩展终端数量:</p><ul><li>来自一条链路的比特从其他所有链路出去(广播).</li><li>链路上以相同的速率传输.</li><li>无帧缓存(<strong>直通</strong>).</li><li>在集线器中<strong>无CSMA/CD</strong> : 适配器检测碰撞.</li><li>将多个结点连接成一个共享式 的局域网.</li></ul><h4 id="交换机"><a href="#交换机" class="headerlink" title="交换机"></a>交换机</h4><p>交换机(Switch)是<strong>链路层</strong>设备, 它的每个端口相当于一个集线器，原理是根据数据帧头的MAC地址转发帧到合适的端口，每个端口是一个<strong>独立的冲突域</strong>:</p><ul><li>存储, 转发以太网帧.</li><li>检查帧首部并基于MAC目的地址<strong>选择性</strong>地转发帧.</li><li>当帧在网段上转发时，使用<strong>CSMA/CD</strong> 访问网段.</li><li>无碰撞, 全双工.</li></ul><p>透明: 主机不知道交换机的存在.</p><p><strong>即插即用</strong>, 自学习: 交换机不必配置.</p><h5 id="转发"><a href="#转发" class="headerlink" title="转发"></a>转发</h5><p>交换机连接了三个LAN, 怎样决定向哪个LAN段转发帧? </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/交换机.jpg" style="zoom:67%;" /><p>每个交换机都有一个交换机表, 表项为<code>&lt;MAC地址, 接口, 时间戳&gt;</code>. 时间戳用于淘汰陈旧项. 交换机知道通过哪个接口可以到达哪个主机, 因为当帧收到时, 交换机记录下发送方的MAC和进入时的接口.</p><h5 id="过滤后转发"><a href="#过滤后转发" class="headerlink" title="过滤后转发"></a>过滤后转发</h5><p>如果目的站点所属LAN和源站点所属LAN相同, 则丢弃该帧. 如果目的站点所属LAN和源站点所属LAN不同, 则转发该帧. 如果目的站点所属的LAN未知, 则进行洪泛. 交换机减少了在不必要LAN内的重复扩散, 节约了资源.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/交换机隔离.jpg" style="zoom: 67%;" /><p>即相同LAN段的帧通常不在其他LAN段上转发, 分割成了不同的碰撞域.</p><h4 id="集线器-交换机-路由器"><a href="#集线器-交换机-路由器" class="headerlink" title="集线器, 交换机, 路由器"></a>集线器, 交换机, 路由器</h4><table><thead><tr><th></th><th align="center">集线器</th><th align="center">路由器</th><th align="center">交换机</th></tr></thead><tbody><tr><td>流量隔离</td><td align="center">×</td><td align="center">√</td><td align="center">√</td></tr><tr><td>即插即用</td><td align="center">√</td><td align="center">×</td><td align="center">√</td></tr><tr><td>优化选路</td><td align="center">×</td><td align="center">√</td><td align="center">×</td></tr><tr><td>直通</td><td align="center">√</td><td align="center">×</td><td align="center">√</td></tr><tr><td>隔离广播</td><td align="center">×</td><td align="center">√</td><td align="center">×</td></tr><tr><td>所属层级</td><td align="center">物理层</td><td align="center">网络层</td><td align="center">链路层</td></tr></tbody></table><h3 id="PPP"><a href="#PPP" class="headerlink" title="PPP"></a>PPP</h3><p>PPP(Point to Point Protocol，点到点协议) 是为在同等单元之间传输数据包这样的<strong>简单链路设计</strong>的链路层协议。这种链路提供全双工操作，并按照顺序传递数据包。设计目的主要是用来通过拨号或专线方式建立<strong>点对点连接</strong>发送数据，使其成为各种主机, 网桥和路由器之间简单连接的一种共通的解决方案。</p><h4 id="PPP设计要求"><a href="#PPP设计要求" class="headerlink" title="PPP设计要求"></a>PPP设计要求</h4><ul><li><strong>分组成帧</strong>: 在数据链路帧中封装网络层数据报, 在相同时间承载任何网络层协议(不止是IP)的网络层数据.</li><li><strong>比特透明性</strong>: 在数据字段必须承载任何比特模式.</li><li><strong>连接活跃性</strong>: 对网络层检测, 通知链路故障.</li><li><strong>差错检测</strong>: 仅进行差错的检测而不纠正.</li><li><strong>网络层地址协商</strong>: 端点能学习/配置每个其他网络地址.</li></ul><p>不要求纠错/恢复, 流量控制, 数据重排序, 多点链路支持, 这些所有额外功能全部<strong>移交到高层</strong>处理.</p><h4 id="PPP数据帧"><a href="#PPP数据帧" class="headerlink" title="PPP数据帧"></a>PPP数据帧</h4><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/ppp.jpg" style="zoom: 33%;" /><p><strong>标志</strong>: 定界符(成帧).</p><p><strong>地址</strong>: 不起作用 (仅是一个选项).</p><p><strong>控制</strong>:不起作用, 以后可能多控制字段.</p><p><strong>协议</strong>: 该帧交付的高层协议 (如 PPP-LCP, IP, IPCP等).</p><p><strong>信息</strong>: 高层承载的数据.</p><p><strong>校验</strong>: 对差错检测的冗余循环校验.</p><h5 id="比特填充-字节填充"><a href="#比特填充-字节填充" class="headerlink" title="比特填充/字节填充"></a>比特填充/字节填充</h5><p>有时候起始和结束的Flag<code>01111110</code> 也会出现在数据中啊, 可能是无意出现的. 处理方法其实和我们编程时字符串中用到的<strong>转义</strong>方法如出一辙. 如果数据中无意出现了<code>01111110</code>, 就在这个字节旁边额外加一个<code>01111110</code>, 是不是有转义内味了?</p><p>在接收方视角中, 在一排中出现<code>01111110 01111110</code> , 丢弃第一个字节, 继续数据接收即可. 如果是单个<code>01111110</code>, 那它就是起始和结束的Flag.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/ComputerNetwork/比特填充.jpg" style="zoom:67%;" />]]></content>
      
      
      <categories>
          
          <category> 计算机基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 面试 </tag>
            
            <tag> 计算机网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之特征缩放</title>
      <link href="/posts/46329.html"/>
      <url>/posts/46329.html</url>
      
        <content type="html"><![CDATA[<h1 id="特征缩放-Feature-scaling"><a href="#特征缩放-Feature-scaling" class="headerlink" title="特征缩放 Feature scaling"></a>特征缩放 Feature scaling</h1><p>特征缩放还有另外一个名字, 叫做标准化. 标准化能够尽可能的使得模型快速收敛, 如果某个特征的方差比别的特征大几个数量级的话, 用<strong>距离度量的算法</strong>就会受到非常大的影响, 比如神经网络, SVM, 逻辑回归线性回归等, 但是基于树类选择的模型不会受到缩放的影响. 当特征尺度不同时, 需要用一个统一的量纲来衡量他们. 比方说身高和体重的分布, 就常常不是同一个量级的.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/归一化.png" style="zoom: 80%;" /><h2 id="均值标准化-Standardization"><a href="#均值标准化-Standardization" class="headerlink" title="均值标准化 Standardization"></a>均值标准化 Standardization</h2><p>假设数据服从正态分布, 这个标准化是最好的. 也叫Z-score 标准化. 度量相似性等涉及到距离的算法用它效果比较好. 处理后的数值服从均值为0, 方差为1的标准正态分布.会改变原数据的数据分布, <strong>不适用于对稀疏数据</strong>做处理.<br>$$<br>x’  = \frac{x - \mu}{\sigma}<br>$$</p><h2 id="最大最小值缩放-Min-Max-Scaling"><a href="#最大最小值缩放-Min-Max-Scaling" class="headerlink" title="最大最小值缩放 Min-Max Scaling"></a>最大最小值缩放 Min-Max Scaling</h2><p>其实就是用最小值和最大值把数据固定到$[0, 1]$ 区域内. 但是当有新数据加入时, 可能导致$X_{max}$和$X_{min}$ 发生变化. 所以这种算法<strong>对数据非常敏感</strong>, 如果有许多异常值可能会导致整个数据集的分布发生变化. 但是它没法消除量纲对方程和协方差的影响, 所以涉及距离的算法计算结果会发生变化.<br>$$<br>x’ = \frac{x - x_{min}}{x_{max} - x_{min}}<br>$$</p><h2 id="最大绝对值缩放-MaxAbs-Scaling"><a href="#最大绝对值缩放-MaxAbs-Scaling" class="headerlink" title="最大绝对值缩放 MaxAbs Scaling"></a>最大绝对值缩放 MaxAbs Scaling</h2><p>最大值绝对值标准化, 和Min-Max方法类似，将数据落入$[-1,1]$区间内, 但是MaxAbs具有<strong>不破坏数据结构</strong>的特点, 可以用于<strong>稀疏数据</strong>.<br>$$<br>x’ = \frac{x}{|\max {x}|}<br>$$</p><h2 id="鲁棒性缩放-Robust-Scaling"><a href="#鲁棒性缩放-Robust-Scaling" class="headerlink" title="鲁棒性缩放 Robust Scaling"></a>鲁棒性缩放 Robust Scaling</h2><p>均值标准化在具有<strong>很多异常值</strong>的情况下是效果不好的, 用中位数和四分位距<strong>代替均值和方差</strong>效果会好很多.<br>$$<br>\displaylines{<br>x’ = \frac{x - x_{median}}{IQR}  \\<br>IQR = x_{q3} - x_{q1}<br>}<br>$$<br>$IQR$(interquartile range)为四分位距, 是第三四分位数$Q_3$和第一四分位数$Q_1$的差. $x_{median}$是特征的中位数. </p><p>假设有很大的离群值在特征中, 因为离群值在数轴最大侧的加入, 这个离群值使得$x_{q3}$变大了(虽然$x_{q1}$也变大了, 但是因为分布的缘故可能影响没有$x_{q3}$大), 中位数也变大了, 分母更大, 分子更小, 整体变小. 缩放后的数据更倾向于将离群值平衡到正常范围之内. 很小的离群值同理, 分母变得更小了, 分子更大, 整体变大.</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 特征缩放 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之模型选择</title>
      <link href="/posts/63889.html"/>
      <url>/posts/63889.html</url>
      
        <content type="html"><![CDATA[<h1 id="模型选择-Model-Selection"><a href="#模型选择-Model-Selection" class="headerlink" title="模型选择 Model Selection"></a>模型选择 Model Selection</h1><h2 id="过拟合和欠拟合-Overfitting-and-Underfitting"><a href="#过拟合和欠拟合-Overfitting-and-Underfitting" class="headerlink" title="过拟合和欠拟合 Overfitting and Underfitting"></a>过拟合和欠拟合 Overfitting and Underfitting</h2><p>这个其实非常好解释, 就放在一起说了.</p><p>过拟合就像是平时做很多作业题但是却不会考试的学生, 一到考试就拉胯, 但是平时作业写得很完美. 过拟合导致了过度的学习了作业中的内容, 甚至是单纯错误的把作业题背下来了, 导致<strong>失去了泛化能力</strong>.</p><p>欠拟合就像是平时不怎么学习的学渣. 考试题也不会, 作业也不会写. 欠拟合根本没有对作业中的内容进行学习, 对考试题型的拟合程度根本不够.</p><p>欠拟合对应着高偏差, 过拟合对应着高方差.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/偏差和方差.png" style="zoom:67%;" /><ol><li>左上的模型偏差最大, 右下的模型偏差最小.</li><li>左上的模型方差最小, 右下的模型方差最大. 这样去理解, 如果两个训练集的分布有一丝丝差异, 由于采用了复杂的模型拟合, 会导致分类结果或回归结果截然不同. 就导致新样本点散落在原样本的两侧, 方差极高.</li></ol><p>用CV代表在验证集上的训练误差, 而Training error代表在训练集上的训练误差.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/learning-curves.png" alt=""></p><p>欠拟合的训练误差和验证误差都很大, 过拟合的训练误差很小, 但是测试误差很大. 只有在中间的模型刚刚好, 具有泛化能力, 也同时具有判断问题的能力.</p><h2 id="交叉验证-Cross-Validation"><a href="#交叉验证-Cross-Validation" class="headerlink" title="交叉验证 Cross Validation"></a>交叉验证 Cross Validation</h2><p>先解释一下数据的分割问题. 我们常常将数据分割为三类, 分别是<strong>训练集, 验证集, 和测试集</strong>. 一般都将一份数据集按照比例划分成训练集和验证集, 测试集一般取样于实际问题中, 这样才能够检验模型真正的泛化能力. 但是在后面说的”全部数据集” 都是在不考虑测试集情况下说的, 也就是只划分为训练集和测试集的情况.</p><table><thead><tr><th align="center">划分</th><th align="left">意义</th></tr></thead><tbody><tr><td align="center">训练集</td><td align="left">专门用来训练模型的, 在训练过程中, 模型参数会随着训练而改变, 就像是上课一样.</td></tr><tr><td align="center">验证集</td><td align="left">用来检测当前模型对数据集的拟合程度, 此时模型参数不会更新, 它像作业题.</td></tr><tr><td align="center">测试集</td><td align="left">模型从来没有见过的, 真正的考试题, 能够判决模型能力的终极手段.</td></tr></tbody></table><p><strong>测试集只能用一次, 就是最后对模型参数调好后进行终极测试的时候</strong>.</p><p>对于一份数据, 如果我们既将它全部用来训练, 也用于验证, 那么就相当于用作业题去上课, 造成了<strong>标签泄露</strong>. 如果只将训练集拿去训练呢, 又会损失一部分训练的数据, 并且模型最终的结果严重取决于训练集和验证集的划分. 基于这个矛盾, 提出了交叉验证.</p><h3 id="K折交叉验证-K-Fold-Cross-Validation"><a href="#K折交叉验证-K-Fold-Cross-Validation" class="headerlink" title="K折交叉验证 K-Fold Cross Validation"></a>K折交叉验证 K-Fold Cross Validation</h3><p>将含有$m$个样本的数据集划分为$k$个不相交的子集, 每个子集有$\frac{m}{k}$ 个样本. 从每次分好的样本中, 选一个子集作为验证集, 其他$k-1$个子集全部作为训练集, 每次训练都能得到一个评估值(每次都用同样的参数训练一个新的分类器), 重复进行$k$ 次最后取平均, 就是k折交叉验证所得到的验证结果. K-Fold CV 能反映出当前模型真正的拟合能力, 经验值常取k为5, 10… </p><h3 id="留一法-Leave-one-out-cross-validation"><a href="#留一法-Leave-one-out-cross-validation" class="headerlink" title="留一法 Leave-one-out cross-validation"></a>留一法 Leave-one-out cross-validation</h3><p>当$k=m$的时候, 每个子集只有一个样本, 每次只取一个样本作为验证集, 用其余所有样本进行训练, K折交叉验证就变成了留一法. 留一法能够最大程度的利用样本, 但是也同时牺牲了时间. 如果有k个样本, 则需要训练k次, 测试k次. <strong>小样本适用</strong>.</p><h2 id="网格搜索-Grid-Search"><a href="#网格搜索-Grid-Search" class="headerlink" title="网格搜索 Grid Search"></a>网格搜索 Grid Search</h2><p>当我们不知道模型的参数如何选择时, 可以采用网格搜索. 网格搜索其实就是<strong>暴力穷举</strong>, 将模型所有可能的参数取值全都试一遍, 这能保证不遗漏模型组合参数造成的影响, 但是同时这样做的时间成本也非常高. 所以一般情况下都是先进行参数范围的缩小, 最后再使用网格搜索. 网格搜索也常和交叉验证一起使用, 用来更精确的评价模型.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gridSearch.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 交叉验证 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之损失函数</title>
      <link href="/posts/49440.html"/>
      <url>/posts/49440.html</url>
      
        <content type="html"><![CDATA[<h1 id="损失函数-Loss-function"><a href="#损失函数-Loss-function" class="headerlink" title="损失函数 Loss function"></a>损失函数 Loss function</h1><p>损失函数是用来度量模型当前预测状况与损失目标的差距的函数.</p><h2 id="均方误差-MSE"><a href="#均方误差-MSE" class="headerlink" title="均方误差 MSE"></a>均方误差 MSE</h2><p>也称为平方损失, L2损失, 均方误差(Mean Squared error). 有时候人们也直接将其开根号称为RMSE, 这样能更好的反映出模型和真实值之间的差异, 以至于不像MSE看起来的那么大. <strong>有时候分母上多挂个2是为了求导方便</strong>.<br>$$<br>MSE = \frac{\sum\limits_{i=1}^n(y_i-\hat{y_i})^2}{n}<br>$$</p><h2 id="平均绝对值误差-MAE"><a href="#平均绝对值误差-MAE" class="headerlink" title="平均绝对值误差 MAE"></a>平均绝对值误差 MAE</h2><p>平均绝对值误差(Mean Absolute Error) 也称之为L1损失, 是另一种损失函数, 它表示预测值的平均误差幅度而不考虑误差的方向.<br>$$<br>MAE = \frac{\sum \limits_{i=1}^n \left| y_i-\hat{y_i}\right|}{n}<br>$$<br>当训练数据中含有较多的异常值时MAE更为有效. 当我们对所有观测值进行处理时, 如果利用MSE进行优化则我们会得到所有观测的均值, 而使用MAE则能得到所有观测的中值. 与均值相比, 中值对于异常值的鲁棒性更好, 这就意味着平均绝对误差对于异常值有着比均方误差更好的鲁棒性. 但是MAE它的梯度在极值点处会有很大的跃变, 所以<strong>不利于梯度下降相关算法</strong>. MSE却能很好的收敛.</p><h2 id="R2-score"><a href="#R2-score" class="headerlink" title="R2 score"></a>R2 score</h2><p>$R^2\ score$ 也称为决定系数, 越大越好, 解决了不同量纲下模型的效果好坏.<br>$$<br>R^2 \ score = 1 - \frac{\sum \limits_{i=1}^n (y_i-\hat{y_i})^2}{\sum \limits_{i=1}^n (y_i-\bar{y_i})^2} = 1 - \frac{\sum \limits_{i=1}^n (y_i-\hat{y_i})^2 / n}{\sum \limits_{i=1}^n (y_i-\bar{y_i})^2/ n} = 1 - \frac{MSE}{Var}<br>$$<br>第二项的分子代表我们实际作出的回归曲线和真实值之间的均方误差, 分母代表了在所有样本中取平均所获得的均方误差, 回归曲线越好, 那么二者之间的比值就越小, $R^2\ score$也就越趋近于1.</p><h2 id="交叉熵-Cross-Entropy"><a href="#交叉熵-Cross-Entropy" class="headerlink" title="交叉熵 Cross Entropy"></a>交叉熵 Cross Entropy</h2><p>其实大多数情况下, 在<strong>分类问题</strong>中使用的损失函数是交叉熵. 很明显上述几种损失函数对于一个分类问题来说, 都不是好的损失函数. 而交叉熵从信息的角度, 从概率入手, 很好的衡量模型的损失. 熵本身就是系统混乱度的度量, 熵越大, 则代表信息的不确定性越大. 在这里, 熵越大, 则代表模型预测的结果越差, 也就是损失值越大. 因为伴随着概率, 所以二分类交叉熵经常伴随<code>sigmoid</code>, 多分类伴随<code>softmax</code>, 这两个函数能将神经网络的输出映射到概率范围内.</p><h3 id="二分类交叉熵"><a href="#二分类交叉熵" class="headerlink" title="二分类交叉熵"></a>二分类交叉熵</h3><p>假设$y_i$是预测样本$i$的实际概率(或者说标签), $p_i$是第$i$个样本的类别预测概率, 则有:<br>$$<br>Entropy = -\sum^m_{i=1}\left[y_i\log{(1-p_i)} + (1-y_i)\log {p_i}\right]<br>$$<br>加入负号是为了保证熵为正, 能够让它的规律符合损失函数定义.</p><h3 id="多分类交叉熵"><a href="#多分类交叉熵" class="headerlink" title="多分类交叉熵"></a>多分类交叉熵</h3><p>多分类就是将二分类扩展到一般情况. 共有$m$个类, $n$个样本, $y_{ij}$是第$i$个样本对应第$j$个类别的标签, $p_{ij}$是第$i$个样本对应第$j$个类别的预测概率.<br>$$<br>Entropy = -\sum^n_{i=1}\sum^m_{j=1}y_{ij}\log {p_{ij}}<br>$$</p><h2 id="正则化-Regularization"><a href="#正则化-Regularization" class="headerlink" title="正则化 Regularization"></a>正则化 Regularization</h2><p>正则化也可以看做是损失函数的惩罚项. 正则化是对模型优化方向加以限制, 在损失函数后面添加的项. 这样在计算损失函数时, 也会将人为优化的方向考虑进去. 如果模型没有朝着人们想要的方向发展, 会导致损失很高, 模型则不会向着该方向更新. 对于原来的损失函数$E(w; X, y)$ 和仅与权重相关的正则项$\Omega(w)$, 正则项超参数$\alpha$, 新的损失函数$E_r(w; X, y)$:<br>$$<br>E_r(w; X, y)=E(w; X, y) + \alpha\Omega(w)<br>$$</p><h3 id="L1正则化-L1-Regularization"><a href="#L1正则化-L1-Regularization" class="headerlink" title="L1正则化 L1 Regularization"></a>L1正则化 L1 Regularization</h3><p>使用L1正则化的模型也叫作Lasso回归模型. L1正则化是将模型权重的个向量绝对值之和加到一起. 这样模型自动的将不重要的特征系数变小, 逐渐趋于0. 因此L1正则化可以产生稀疏权值矩阵, 即产生一个稀疏模型, 用于特征选择.<br>$$<br>\Omega(w)=\sum_i^W|w_i|<br>$$<br>相比于后面提到的L2正则化来说, L1具有更好的鲁棒性, 当出现异常值时, 异常值不会被放大. 但L1因为含有绝对值, 非常难以计算.</p><h4 id="L1正则化为什么易于产生稀疏解"><a href="#L1正则化为什么易于产生稀疏解" class="headerlink" title="L1正则化为什么易于产生稀疏解?"></a>L1正则化为什么易于产生稀疏解?</h4><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/L1正则化.jpg" style="zoom: 67%;" /><p>在寻找最优解时, L1正则由于是权重的绝对值, 做出的图像如图中的正方形所示, 更容易在某方向权重为0时找到解, 所以能够将整个权重矩阵稀疏化.而且L1的导数因带有绝对值而容易发生突变, 所以在0处时取到极小值的几率很大. 或者说L1因为涉及绝对值, 如果想最小化惩罚, 就是让对应的$w_i$取0.</p><h3 id="L2正则化-L2-Regularization"><a href="#L2正则化-L2-Regularization" class="headerlink" title="L2正则化 L2 Regularization"></a>L2正则化 L2 Regularization</h3><p>使用L2正则化的模型也叫作Ridge回归模型(岭回归模型). L2具有更均匀的输出. 也非常便于计算.<br>$$<br>\Omega(w)=\sum_i^Ww_i^2<br>$$<br>当模型权重过多, 或者某个权重向量值过大时, 都会得到严厉的惩罚. 但是因为L2是权重的平方, 因此当模型遇到异常值时, 对权重进行大幅度调整, 这个误差会被放大.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/L2正则化.jpg" style="zoom: 50%;" />]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 损失函数 </tag>
            
            <tag> 交叉熵 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习之评估指标</title>
      <link href="/posts/18683.html"/>
      <url>/posts/18683.html</url>
      
        <content type="html"><![CDATA[<h1 id="评估指标-Metrics"><a href="#评估指标-Metrics" class="headerlink" title="评估指标 Metrics"></a>评估指标 Metrics</h1><h2 id="混淆矩阵-Confusion-Matrix"><a href="#混淆矩阵-Confusion-Matrix" class="headerlink" title="混淆矩阵 Confusion Matrix"></a>混淆矩阵 Confusion Matrix</h2><p>对于简单的二分类情况, 混淆矩阵就是如下形式:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5.png" alt=""></p><p>混淆矩阵把数据和模型预测情况分为四类:</p><p>真实值是positive, 模型认为是positive的数量(True Positive=TP)<br>真实值是positive, 模型认为是negative的数量(False Negative=FN):这就是统计学上的第二类错误(Type II Error).<br>真实值是negative, 模型认为是positive的数量(False Positive=FP):这就是统计学上的第一类错误(Type I Error).<br>真实值是negative, 模型认为是negative的数量(True Negative=TN).</p><h2 id="一级指标"><a href="#一级指标" class="headerlink" title="一级指标"></a>一级指标</h2><p>一级指标就是对于混淆矩阵来说的, 那肯定是处于对角线上的数字越大越好, 越大说明分类正确的越多. 因此混淆矩阵也常用热图进行绘制, 能够一眼看出分类情况.</p><h2 id="二级指标"><a href="#二级指标" class="headerlink" title="二级指标"></a>二级指标</h2><p>但是,混淆矩阵里面统计的是个数,有时候面对大量的数据,光凭算个数,很难衡量模型的优劣。因此混淆矩阵在基本的统计结果上又延伸了如下4个指标,我称他们是二级指标(通过最底层指标加减乘除得到的):</p><ul><li>准确率(Accuracy) - 针对分类正确</li><li>精确率(Precision) - 所有阳性中分类正确的</li><li>灵敏度(Sensitivity) - 就是召回率(Recall), 也叫查全率.</li><li>特异度(Specificity)</li></ul><table><thead><tr><th>指标</th><th>公式</th><th>含义</th></tr></thead><tbody><tr><td>准确率</td><td>$Accuracy= \frac{TP+TN}{TP+TN+FP+FN}$</td><td>所有分类正确的样本占样本总数的百分比.</td></tr><tr><td>精度</td><td>$Precision=\frac{TP}{TP+FP}$</td><td>所有预测阳性的样本中真实为阳性的百分比.</td></tr><tr><td>召回率</td><td>$Recall=\frac{TP}{TP+FN}$</td><td>所有真实阳性的结果中预测为阳性的百分比.</td></tr><tr><td>特异度</td><td>$Specificity=\frac{TN}{TN+FP}$</td><td>所有真实阴性的结果中预测为阴性的百分比.</td></tr></tbody></table><h2 id="三级指标"><a href="#三级指标" class="headerlink" title="三级指标"></a>三级指标</h2><p>三级指标主要是$F_1\ score$ 和 $F_\beta \ score$.<br>$$<br>F_1 \ score = 2 \cdot \frac{Precision \cdot Recall} {Precision + Recall}<br>$$<br>即$F_1 \ score$ 是 $Precision$ 和 $Recall$ 的调和平均数. 将这个情况扩展到一般情况, 称之为$F_\beta \ score$.<br>$$<br>F_\beta \ score = (1+N^2) \cdot \frac{Precision \cdot Recall}{N^2\cdot Precision + Recall} = \frac{Precision \cdot Recall}{\frac{N^2}{1+N^2}\cdot Precision + \frac{1}{1+N^2}Recall}<br>$$<br>$\beta$ 的取值范围是$[0, +\infty)$, 有以下特殊情况:</p><ul><li>${\lim\limits_{N \to 0}}F_\beta \ score= Precision$</li><li>$\beta = 1, F_\beta = F_1\ score$</li><li>${\lim\limits_{N \to +\infty}}F_\beta \ score= Recall$</li><li>能够从不同的$\beta$ 值看出, 如果$\beta$ 值越小, $Precision$ 越重要, 反之$\beta$ 值越大, $Recall$ 越重要.</li></ul><h2 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h2><p>ROC(Receiver Operating Characteristic) 受试者特性曲线. 纵坐标是true positive rate(TPR), 指的是所有实际为阳性的样本中被正确判定为阳性的比例. 横坐标是false positive rate(FPR), 指的是所有实际为阴性的样本中被错误判定为阳性的比例.<br>$$<br>\displaylines{<br>TPR = \frac{TP}{TP+FN} \\<br>FPR = \frac{FP}{FP+TN}<br>}<br>$$<br>曲线下方的面积成为AUC(Area Under roc Curve), 值通常在0.5到1之间, 认为面积越大的模型越好. 小于0.5的补救方法就是按照模型说的反着做.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/roc.png" style="zoom: 50%;" />]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 评估指标 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>循环神经网络小结</title>
      <link href="/posts/60202.html"/>
      <url>/posts/60202.html</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>2020.09.07</strong>: 重写了LSTM和GRU的描述.</p><p><strong>2020.08.22</strong>: 对部分内容进行了更新.</p></blockquote><h1 id="循环神经网络-Recurrent-Neural-Network"><a href="#循环神经网络-Recurrent-Neural-Network" class="headerlink" title="循环神经网络 Recurrent Neural Network"></a>循环神经网络 Recurrent Neural Network</h1><p>面对时序型数据, 如自然语言, 乐谱, 金融数据等包含隐含的时间信息在内的数据, 不能采用原始的稠密神经网络, 会产生很多问题. 比如, 很容易就产生海量的参数, 或者输入输出的神经元很有可能是不确定的, 最主要的是无法将时序的信息传递给稠密神经网络. 采用循环神经网络可以很好的处理时序问题.</p><h2 id="RNN的基本结构"><a href="#RNN的基本结构" class="headerlink" title="RNN的基本结构"></a>RNN的基本结构</h2><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rnn.jpg" alt=""></p><p>RNN因为满足<strong>循环递归</strong>的结构, 很多时候也画成左边折叠的样子. 能够看到, 下方的$X_i$ 对应着$t$时刻的输入. 在图中没有画出来的是, 对于图中的$A$ 实际上是一个含有若干个神经元的与输入输出相连的神经网络(其实对应数学结构就是一个矩阵), 只不过<strong>隐藏层之间相互有了连接</strong>, 下面这个图可能更好解释一些:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rnn6.png" style="zoom:50%;" /><p>在新的时刻$t$ ,输入视为是$t-1$时刻的神经元结果和$t$时刻的时序序列输入$X_t$的合成结果. 假设激活函数$f$, 当前时刻为$t$有:<br>$$<br>\displaylines{<br>A_t = f(UX_t + WA_{t-1} + b_a) \\<br>h_t = f(VA_t + b_y)}<br>$$<br><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rnn2.jpg" alt=""></p><p>在进行计算时, 既然是循环使用的神经元, 那么<strong>在时刻尺度上</strong>它的参数一定是<strong>共享</strong>的, 即对于同一组RNN, 它们的$U, W, V, b$ 采用的都是<strong>相同的值</strong>, 这也符合我们对<strong>递归</strong>的理解, RNN每个时间步都在做相同的事情, 只是输入不同. 并且在实际计算的过程中, 经常将$U$和$W$合并为同一个矩阵, 将$X_t, A_{t-1}$也合并成一个矩阵做矩阵乘.</p><p>其实上述过程就是RNN的前向传播, 非常的符合逻辑, 也同时隐含了它<strong>只能进行串行计算</strong>的弊病. 其实反向传播只要把前向传播过程反过来就行了, 损失函数为所有时刻的损失平均值.</p><h2 id="RNN的几种结构"><a href="#RNN的几种结构" class="headerlink" title="RNN的几种结构"></a>RNN的几种结构</h2><p>RNN也可以分为很多种, 有一对一, 一对多, 多对一, 多对多, 具体采用哪种需要结合具体的任务目标而定.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84rnn.jpg" alt=""></p><p>经典RNN由于结构输入和输出必须是一一对应的,应用范围受限很大. 图中所示的第一种多对多经常用于机器翻译, 前一段没有输出的神经网络对应的称为<code>encoder</code>即编码器, 后一段有输出的神经网络称为<code>decoder</code>即解码器, 机器在经过编码器读完整个句子后从解码器获取输出. 第二种多对多经常用于多对多经常用于序列生成. </p><h3 id="RNN的采样"><a href="#RNN的采样" class="headerlink" title="RNN的采样"></a>RNN的采样</h3><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rnn%E9%87%87%E6%A0%B7.jpg" alt=""></p><p>假设有个已经训练好的模型, 那么我们对输入$x^{&lt;1&gt;}$和$a^{&lt;0&gt;}$都置为零向量, 然后让RNN自己预测第一个单词的概率向量$\hat y^{&lt;1&gt;}$, 根据这个概率利用例如<code>np.random.choice</code>获取一个单词的index, 将这个单词的one-hot形式作为下时刻的输入. 当然不一定是单词级别的RNN, 字符级别也可以用, 但是计算成本非常大, 一般只有很多专业词汇才用.</p><h2 id="RNN的梯度爆炸和梯度消失"><a href="#RNN的梯度爆炸和梯度消失" class="headerlink" title="RNN的梯度爆炸和梯度消失"></a>RNN的梯度爆炸和梯度消失</h2><p>RNN因为是循环的结构, 循环多次很容易导致网络层数加深, 这样前面的网络参数很难被反向传播影响. 比如说RNN可能很难记住一个长句子里的时态语态信息, 其每个时刻的输出主要由临近的几个时刻所影响, 导致<strong>不善于处理长期依赖</strong>问题. 必须引入一些结构来传递需要被长期记忆的信息.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rnn长期依赖.png" style="zoom: 33%;" /><h3 id="长短期记忆神经网络LSTM-Long-Short-Term-Memory"><a href="#长短期记忆神经网络LSTM-Long-Short-Term-Memory" class="headerlink" title="长短期记忆神经网络LSTM Long Short-Term Memory"></a>长短期记忆神经网络LSTM Long Short-Term Memory</h3><p>LSTM于1997年在<a href="(https://www.bioinf.jku.at/publications/older/2604.pdf)">Long Short-Term Memory</a>中出现, 是一种尝试保存长期记忆避免梯度消失的RNN. LSTM主要有四个部分, <strong>遗忘门</strong>, <strong>输入门</strong>, <strong>输出门</strong>, <strong>细胞状态</strong>. 因为引入了循环神经网络的记忆机制, 常形象地将结构单位称为<strong>记忆细胞</strong>(memory cell).</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lstm.png" alt=""></p><p>LSTM中所有的向量操作如下:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lstm%E6%93%8D%E4%BD%9C.png" alt=""></p><p>黄框框代表<strong>神经层</strong>(注意是一层神经层, 不单单是图中标注的激活函数), 粉圈圈代表某种向量的操作, 单箭头代表向量的流向, 汇聚箭头代表向量的concat, 分离箭头代表向量拷贝成了两份.</p><p>LSTM最上面的那条水平线代表<strong>细胞状态</strong>, 细胞存储的状态实际上就是需要<strong>长期记忆</strong>的信息. 这条路上只有遗忘门和输入门能够对细胞状态进行更改, 只有一些少量的线性交互, 实际上这条线是非常<strong>容易不发生任何转变</strong>而传递到下一个时刻的. 如下所示:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lstm%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3.png" alt=""></p><p>下面来看数据在LSTM中从输入到输出的完整过程.</p><p>数据输入后, 数据首先经过LSTM的<strong>遗忘门</strong>. 遗忘门接收了当前时刻$t$的输入$x_t$和上个时刻的隐藏状态 $h_{t-1}$, 经过$\sigma$函数的处理, 能够决定到底<strong>留下</strong>多少在细胞状态中. $f_t=1$表示<strong>完全记住</strong>这个信息, $f_t=0$代表<strong>完全忘记</strong>这个值. 基于上文预测下文词的语言模型中, 可能细胞状态会包含前文的主题, 那么最好记住$h_{t-1}$, 如果得到一个新的语言主题, 则希望遗忘掉过去的信息.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lstm%E9%81%97%E5%BF%98%E9%97%A8.png" alt=""></p><p>第二步决定在细胞状态中<strong>储存</strong>什么样的信息, 与之对应的结构称为<strong>输入门</strong>. 输入门有两条并行的向量流向. 左侧线路用$\sigma$函数决定有哪些位置上的信息是需要更新的, 右侧线路利用$tanh$为等待细胞状态更新时的使用的候选值创建一个新的向量.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lstm%E8%BE%93%E5%85%A5%E9%97%A8.png" alt=""></p><p>第三步便是对<strong>细胞状态更新</strong>的过程. 细胞状态先通过遗忘门所给出的遗忘程度对先前状态遗忘, 然后再将输入门的左右两条并行路线的结果计算出来, 与细胞状态相加, 就完成了细胞状态的更新. 从下述式子中可以看到, 对于旧信息的遗忘和新信息的输入, 是让它们自己决定到底留下哪些, 加入哪些. 这点与GRU的风格是不同的, 后面会提到.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lstm%E7%BB%86%E8%83%9E%E7%8A%B6%E6%80%81.png" alt=""></p><p>最后决定要输出的东西, 所对应的结构就是<strong>输出门</strong>. 输出基于细胞当前的状态, 但会经过一次过滤才输出. 与输入门对应, 先利用$\sigma$函数得到一个输出的系数, 再将细胞状态过一次$tanh$将信息压到$[-1, 1]$之间, 与系数相乘就得到了结果. 也正是因为输出门的设置, 导致LSTM的输出$h_t$ 其实只是经过输出门过滤后的$C_t$ 的一部分信息.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lstm%E8%BE%93%E5%87%BA%E9%97%A8.png" alt=""></p><p>LSTM的遗忘门和输入门都对细胞状态进行了更改, 先遗忘再存储. 输出门发生作用的时候是不会对细胞状态再次更改的.</p><p>在这三种门控结构中, 不难观察门控作为一种关键的结构, 能起到让<strong>信息选择性通过</strong>的作用, 根据$\sigma$函数的特性, 向量的每个维度都能够得到一个介于$[0, 1]$ 之间的值, 在与其他向量相乘时, 可以作为保留或遗忘程度的依据.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/lstm门控.png" style="zoom:50%;" /><h3 id="门控循环单元-GRU-Gate-Recurrent-Unit"><a href="#门控循环单元-GRU-Gate-Recurrent-Unit" class="headerlink" title="门控循环单元 GRU Gate Recurrent Unit"></a>门控循环单元 GRU Gate Recurrent Unit</h3><p>GRU是一个LSTM的一个变种, 于<a href="https://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="noopener">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</a>中提出. 它比LSTM参数更少, 结构更简单, 具有更高的<strong>效率</strong>, 达到的精度和LSTM相近. GRU和LSTM同样借鉴了<strong>门控</strong>的思想, 在RNN中添加不同的门控, 从而决定是否要更新信息. GRU结构比较简单, 所以用一张图就完全可以说得清.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/gru.jpg" alt=""></p><p>GRU分别有两个门控结构, 分别是<strong>重置门</strong>$r_t$ (reset gate)和<strong>更新门</strong>$z_t$ (update gate). </p><blockquote><p>重置门: 重置门用于决定丢弃<strong>先前信息</strong>的程度.</p><p>更新门: 更新门能决定在当前时刻要丢弃哪些信息和要添加哪些<strong>新信息</strong>. 作用类似于LSTM中的遗忘门和输入门.</p></blockquote><p>$\tilde h_t$表示的是相较于普通RNN的隐藏状态输出$h_t$的<strong>候选</strong>, 最终不会使用它, 仅作为最终输出$h_t$的<strong>依据</strong>. </p><p>下面描述一下数据在GRU的传播过程:</p><ol><li><p>对于重置门, 上一个时刻$t-1$的隐藏状态$h_{t-1}$和当前时刻的输入$x_t$传入GRU, 通过重置门后将得到一个将$h_{t-1}$和$x_t$ 丢弃的程度系数向量$r_t$. </p></li><li><p>而对于更新门, 同样将是上一个时刻$t-1$的隐藏状态$h_{t-1}$和当前时刻的输入$x_t$传入, 获得表示一个更新程度的系数向量$z_t$. </p></li><li><p>重置门和更新门(或者说重置系数和更新系数)的值已经求出来了, 接着利用重置门的丢弃系数和上一时刻的隐藏状态$h_{t-1}$ 相乘, 并结合当前时刻输入, 经过激活函数为$tanh$ 的神经层, 求出当前时刻的候选隐藏状态$\tilde {h_t}$. </p></li><li><p>根据重置和更新的互斥关系, 求出当前时刻隐藏状态$h_t$ . 当更新门$z_t$的值趋近于1时, 就更倾向于更新记忆细胞的信息, $h_t$的值会和$\tilde{h_{t}}$相仿, 反之不更新, 即仍然沿用上个时刻的信息$h_{t-1}$. </p></li></ol><p>两个门的作用在进行运算中十分明显, <strong>重置门决定了要丢弃多少先前信息</strong>, 直接影响了$\tilde{h_t}$, 间接影响了结果$h_t$, <strong>更新门决定了细胞要替换多少新信息</strong>, 直接影响最终输出的隐藏状态 $h_t$.</p><h3 id="LSTM与GRU对比及个人理解"><a href="#LSTM与GRU对比及个人理解" class="headerlink" title="LSTM与GRU对比及个人理解"></a>LSTM与GRU对比及个人理解</h3><p>首先要明确细胞状态和隐藏状态的区别, 细胞状态$C_t$ 代表的是初始时刻一直到$t$ 时刻的全局信息, 而$h_t$ 代表的是初始时刻到$t-1$ 时刻的全局信息影响下, 当前$t$ 时刻的上下文表示.</p><p>LSTM中, 有细胞状态这个概念, GRU只有隐藏状态. LSTM对信息的保留更加<strong>细腻</strong>, 但对下一个时刻只暴露<strong>部分</strong>信息, 因为在LSTM中$h_t$ 才是真正的输出, $C_t$ 只作为一个信息载体继续传递下去. 相比于LSTM, GRU则更加<strong>简单粗暴</strong>, 对下个时刻暴露<strong>全部</strong>信息.</p><p>LSTM对细胞状态的更新过程中, 经过遗忘门和输入门后求出细胞状态$C_t$ 是<strong>相互独立</strong>的, 即$f_t$ 和 $i_t$ 之间没有关联, 由遗忘门和输入门分别控制遗忘和存储. 而对于GRU来说, 既然$h_t$ 被包含在$C_t$ 中了, 干脆将细胞状态与隐藏状态合并, 在求出最终隐藏状态$h_t$ 时, 而去除了细胞状态后, 当前信息与全局信息是此消彼长的, 即对于写入新信息和保留旧信息是<strong>互相制约</strong>的, 故令$f_t$ 和 $i_t$ 的总和1, 直接用一个更新门$z_t$ 来代替原有的遗忘门和输入门. 同样因为输出的调整, 重置门本质上是输出门的一种变化.</p><h2 id="双向RNN和RNN的堆叠"><a href="#双向RNN和RNN的堆叠" class="headerlink" title="双向RNN和RNN的堆叠"></a>双向RNN和RNN的堆叠</h2><p>双向RNN解决了网络不知道下文信息的问题, 使网络不光会结合前文进行判断, 还会结合后文信息进行预测. 如下图所示, 对于给定的$x^{&lt;1&gt;}, x^{&lt;2&gt;}, x^{&lt;3&gt;}, x^{&lt;4&gt;}$, 每个时刻都增加一个反向链接的神经元. 这样RNN就构成了一个无环图. 当进行前向传播时, 信息会从左到右, 再从右逆着传回来, 最后再做出预测, 即对于$t$时刻的预测值$y_t$, 是由$a^{&lt;t\rightarrow&gt;}$和$a^{&lt;t\leftarrow&gt;}$, $x^{&lt;t&gt;}$共同决定的.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/%E5%8F%8C%E5%90%91rnn.jpg" alt=""></p><p>和下面这张图是一样的:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/双向rnn2.jpg" style="zoom: 67%;" /><p>RNN的堆叠方式其实也非常简单, 就是按照层数往上传递隐藏状态, 最终得到预测值.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/rnn%E5%A0%86%E5%8F%A0.png" alt=""></p><p>如果是若干双向RNN, 堆叠起来也和普通RNN大同小异:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/双向堆叠rnn.png" style="zoom: 67%;" />]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RNN </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>卷积神经网络小结</title>
      <link href="/posts/28799.html"/>
      <url>/posts/28799.html</url>
      
        <content type="html"><![CDATA[<h1 id="卷积神经网络-Convolutional-Neural-Network"><a href="#卷积神经网络-Convolutional-Neural-Network" class="headerlink" title="卷积神经网络 Convolutional Neural Network"></a>卷积神经网络 Convolutional Neural Network</h1><p>卷积神经网络是一种含有空间信息的数据表示方法. 它与普通的DNN不同, 它包含了数据的位置信息, 以保证每次看到的是数据矩阵的一个区域, 而不是单纯的矩阵某一维. 卷积神经网络里所说的”卷积”并非真正意义上的卷积运算, 而是互相关运算. 下面这个是VGG16, 算早期CNN的先驱之一了. 用了很多小的卷积核, 网络也比较深(相对之前的来说).</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/vgg16.png" style="zoom: 33%;" /><p>从图中能看到, 卷积神经网络就是将数据从短而粗, 变为长而细. 也就是从原始数据表征, 通过复杂的神经元连接转换为人们难以理解, 高度抽象的特征. 这类特征往往是通过多次学习得来的, 但效果也非常好.</p><h2 id="卷积层-Convolutional-Layer"><a href="#卷积层-Convolutional-Layer" class="headerlink" title="卷积层 Convolutional Layer"></a>卷积层 Convolutional Layer</h2><p>每个卷积层由若干个卷积核和一个偏置$b$组成, 每一个卷积核和输入数据相互作用可以得到一张特征图. 卷积层的反向传播方式与DNN反向传播完全一致.</p><h3 id="卷积核-Filter"><a href="#卷积核-Filter" class="headerlink" title="卷积核 Filter"></a>卷积核 Filter</h3><p>以二维为例子, 卷积核(也叫滤波器)就是一个含有过滤信息的滑动窗口, 在二维平面上不断滑动, 卷积核内的权重是学习得来的, 它与原数据进行”卷积”运算(其实叫<strong>点积求和</strong>运算更合适), 就是对应位置相乘最后加到一起, 形成这个卷积核在这个位置上得到的数据, 将数据仍然以矩阵的形式拼接, 这个矩阵就称为特征图. 在卷积过程之中, <strong>卷积核的参数不会发生改变</strong>, 这叫做<strong>权值共享</strong>. 也就是一个卷积核提取了原图不同位置的相同特征. 所以引入多个卷积核就能提取原图中的不同特征.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/卷积运算.png" style="zoom: 50%;" /><p>在这里左上角的元素是$[10\times 1 + 10\times 0 + 10\times(-1)]\times3=0$, 中间下方的元素是$[10\times 1+0\times0+0\times-1]\times3=10$. 如果写成一个抽象的公式, 在二维情况下:<br>$$<br>C(x, y)=\sum_{t=-\infty}^\infty\sum_{s=-\infty}^\infty F(s, t)\times G(x-s, y-t)\Delta s\Delta t<br>$$<br>每个卷积核进行运算时, 是<strong>贯穿所有维的</strong>.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/filter贯穿.png" style="zoom:50%;" /><p>比如图中的运算, 上方的输入的RGB图像数据大小为$4\times4\times3$, 卷积核所对应的最后一维也就是深度一定会和输入数据一致, 也就是$2\times2\times3$. 所以但是<strong>深度上卷积核并不共享权重</strong>, 每层深度用的是自己的权重. 对于$R, G, B$三个通道第$i$个位置上的输入数据分别为$x_{ri}, x_{gi}, x_{bi}$, 卷积核中的对应权重有$w_{ri}, w_{gi}, w_{bi}$, 有如下形式:<br>$$<br>\begin{bmatrix}<br>w_{r1} &amp; w_{r2} \\<br>w_{r3} &amp; w_{r4}<br>\end{bmatrix},<br>\begin{bmatrix}<br>w_{g1} &amp; w_{g2} \\<br>w_{g3} &amp; w_{g4}<br>\end{bmatrix},<br>\begin{bmatrix}<br>w_{b1} &amp; w_{b2} \\<br>w_{b3} &amp; w_{b4}<br>\end{bmatrix}<br>$$<br>和深度神经网络一样, 每个卷积核都有一个权重. 那么当计算特征图的时候, 第$k$ 个卷积核对应的特征图每个位置的输出$y_k$ 其实是将深度维上的对应位置的所有结果加起来(别忘了偏置$b_k$, 因为前面卷积运算计算完以后是一个数, 而非一个矩阵, 所以$b_k$ 也是一个数).<br>$$<br>y_k = \begin{bmatrix}w_{r1} &amp; w_{r2} &amp;w_{r3} &amp; w_{r4}\end{bmatrix}\cdot\begin{bmatrix}x_{r1} \\ x_{r2} \\x_{r3} \\ x_{r4}\end{bmatrix}+\begin{bmatrix}w_{g1} &amp; w_{g2} &amp;w_{g3} &amp; g_{r4}\end{bmatrix}\cdot\begin{bmatrix}x_{g1} \\ x_{g2} \\x_{g3} \\ x_{g4}\end{bmatrix}+\begin{bmatrix}w_{b1} &amp; w_{b2} &amp;w_{b3} &amp; w_{b4}\end{bmatrix}\cdot\begin{bmatrix}x_{b1} \\ x_{b2} \\x_{b3} \\ x_{b4}\end{bmatrix}+b_k<br>$$<br>在计算完后, 将filters的所有结果一层层的<strong>叠加</strong>起来, 就使得新的数据又拥有了深度.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/filter深度.jpg" style="zoom: 33%;" /><p>图中经过四个卷积核扫描输入空间后, 得到了四个$3\times3\times1$的输出. 也就是$3\times3\times4$的输出. 由此可见<strong>卷积层有多少卷积核, 就有多深</strong>. 我们当然不能忘记激活函数, 加上激活函数:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/filter个数.png" style="zoom: 33%;" /><p>这里就有个特例, <strong>$1\times1$卷积</strong>. 它能够通过任意的卷积核数量在不改变特征图长和宽的情况下, 改变深度, 在Inception系列模型中, 大量的使用了这个技巧来控制信息.</p><h3 id="步长-Stride"><a href="#步长-Stride" class="headerlink" title="步长 Stride"></a>步长 Stride</h3><p>步长是每滑动一次所前进的距离, 也就是每次向前拖动多少个单位. 即$x’=x+p$.</p><h3 id="填充-Padding"><a href="#填充-Padding" class="headerlink" title="填充 Padding"></a>填充 Padding</h3><p>卷积一步步计算后, 会让图片越来越小. 但是我们可以在图片边缘<strong>补上一圈数据</strong>, 使得卷积能够继续运算. Tensorflow中有两种padding模式, 在<strong>Stride=1</strong>的情况下, SAME指的是在输入图片的边缘补0, 输出图像大小与原来是<strong>相同</strong>的, 并且$p=\frac{f-1}{2}$, 大小为VALID就是不补. VALID可能导致在某些情况下卷积运算的数据丢失. 输出图像为$(n-f+1)\times(n-f+1)$.</p><blockquote><p>关于空间中的位置信息泄露是否来源于Padding, 这个一直都有争议.</p></blockquote><h3 id="参数计算"><a href="#参数计算" class="headerlink" title="参数计算"></a>参数计算</h3><p>对于输入数据的大小$W_{in}\times H_{in}\times D_{in}$, 卷积核个数$K$, 卷积核大小$F$, 步长$S$, 填充$P$, 有输出:<br>$$<br>\displaylines{<br>W_{out}=H_{out}= \lfloor\frac{W_{in}-F+2P}{S}\rfloor+1 \\<br>D_{out}=K}<br>$$<br>该层卷积核的参数个数是(本层的卷积核体积+偏置)*本层卷积核个数:<br>$$<br>N=(F\times F\times D_{in} + 1)\times K<br>$$</p><h2 id="池化层-Pooling-Layer"><a href="#池化层-Pooling-Layer" class="headerlink" title="池化层 Pooling Layer"></a>池化层 Pooling Layer</h2><p>池化层也叫下采样层, 其实也是一种滑动的操作, 池化层里没有需要训练的参数. 池化也分为最大池化和平均池化两种. 池化的<strong>步长和窗口大小相同</strong>, 这保证了每次池化时都能取到<strong>不相交</strong>的区域. 池化增大了每个元素单元对应的感受野, 更利于抽取更抽象而有效的特征, 减少了过拟合. 池化时, 池化操作发生在<strong>每个通道上</strong>, 而不是像CNN一样将各通道输入加在一起. 也就是说, 池化后的输出深度和输入深度相等.</p><h3 id="平均池化层-AveragePooling"><a href="#平均池化层-AveragePooling" class="headerlink" title="平均池化层 AveragePooling"></a>平均池化层 AveragePooling</h3><p>平均池化取的是窗口内所有元素的<strong>平均值</strong>. 平均池化在反向传播时, 将梯度平均分为$n$份, 平均分配到原来对应的位置上, 这样保持池化前后梯度之和不变.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/平均池化.png" style="zoom: 80%;" /><h3 id="最大池化层-MaxPooling"><a href="#最大池化层-MaxPooling" class="headerlink" title="最大池化层 MaxPooling"></a>最大池化层 MaxPooling</h3><p>最大池化层取的是窗口内所有元素的<strong>最大值</strong>. 有一更为极端的例子是全局最大池化层, 它将每一张特征图取最大值, 最终只得到一个值. 最大池化层在前向传播时会记录最大值的位置, 反向传播时只对对应位置的参数进行调整.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/全局池化.png" style="zoom: 50%;" /><h2 id="批量标准化层-BatchNormalization-Layer"><a href="#批量标准化层-BatchNormalization-Layer" class="headerlink" title="批量标准化层 BatchNormalization Layer"></a>批量标准化层 BatchNormalization Layer</h2><p>批量标准化能把输入直接强行拉$(0, 1)$分布上, 使得激活函数输入值落在对输入相对来说敏感的区域, 由小的输入变化导致更大的梯度变化, 从而加快收敛速度. 如果不加BN, 输入分布会经常发生变化, 后续网络在学习时总会因为前面层的分布变化而变化, BN减少了层和层之间的耦合度.</p><h2 id="全连接层-Fully-Connected-Layer"><a href="#全连接层-Fully-Connected-Layer" class="headerlink" title="全连接层 Fully Connected Layer"></a>全连接层 Fully Connected Layer</h2><p>FC层其实就是一层普通的神经网络, 它与上一层是全连接的. 用于分类所以激活函数为Softmax, 一般只用在最后一层或两层, 或预训练网络后添加的几层. 如果是在预训练网络后添加了几层FC进行调参, 那么这个预训练网络在做的事情也是<strong>特征抽取或特征提取</strong>, 它通过预训练的知识抽取了对事物的一般看法, 加上FC层后能够较好地完成我们指定的分类. 这种学习方式也叫作<strong>迁移学习</strong>(Transfer Learning). 但是过多的FC层会导致参数过多, 并且提高过拟合的可能性. 现在一般直接用<strong>全局最大池化层代替全连接层</strong>能够降低模型的参数, 并且表现稳定.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/fc.png" style="zoom:33%;" /><h2 id="残差块-Residual-Block"><a href="#残差块-Residual-Block" class="headerlink" title="残差块 Residual Block"></a>残差块 Residual Block</h2><p>VGG在研究时候发现, 有时深度神经网络的层数越多, 起到的效果却不是很好. 这是因为在网络加深的过程中, 因为特征不断地被高度抽象, 导致最低阶时的原始特征已经被逐渐的消磨掉了. 假设已有一个最优化的18层网络结构, 当我们设计网络结构时, 假设设计了34层, 多出来的16层实际上是完全冗余的, 那么经过冗余层时的输入和输出实际上是和最优层的输入输出完全一致的. 那么实际模型训练的效果不一定能比最优化模型的效果好, 这称为<strong>退化问题</strong>. 理论上来说, 网络越深, 模型的表达能力越强. 如何在维持网络深度的情况下提升性能?</p><p>基于这个思想, 如果能通过某种方式实现低阶信息到高阶信息的直接传递, 应该可以在很深的神经网络的情况下, 达到很好的效果. 这种跳跃式的做法也可以形象的叫Shortcut connection, 一条捷径. 可以看到下述两个图中的信息传递都是跨层的. 我们把下述结构称为残差块:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/残差块.png" style="zoom:67%;" /><p>残差网络就是由一系列的残差块组成的. 残差块分为<strong>直接映射部分$h(x_l)$和残差部分</strong>$\mathcal{F}(x_l, W_l)$. 一般情况下在卷积神经网络中, 很有可能输入$x_l$和输出$x_{l+1}$的特征图尺寸不同, 这时候就必须要用$1\times 1$卷积核进行升维和降维, $h(x)$描述的就是这个升维和降维的过程. 映射中有多重方式, 但根据数学证明和事实证明直接映射是最好的选择, 效果最好.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/简单残差块.png" style="zoom:50%;" /><p>它的数学表达:<br>$$<br>x_{l+1} = h(x_l) + \mathcal{F}(x_l,W_l)<br>$$<br>在上图中, 两层权重都是冗余的, 那么神经网络最终拟合的结果应该是函数$H(x)=x$这个恒等映射, 因为非线性映射导致<strong>直接学习恒等映射函数十分困难</strong>, 网络学习的肯定是不恒等映射. 如果将网络设计为$H(x)=F(x)+x$, 从而转化为学习一个残差函数$F(x)=H(x)-x$, 当$F(x)=0$时, 就能得到恒等映射函数, 对残差的拟合肯定更容易. 也就是说, <strong>对于冗余层, 模型最差程度也能学习到和原来一样的结果</strong>.</p><p>除了解决了网络的退化问题, 还解决了梯度消失和梯度爆炸. 即使上图中的$\mathcal{F}(x)$的部分为0, 也仍然可以由直接加过来的$\mathcal x$. 进行良性的梯度传播, 而不至于因为连乘而导致梯度消失或爆炸.</p><p>点这里看<a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">ResNet论文</a>和<a href="https://arxiv.org/pdf/1603.05027.pdf" target="_blank" rel="noopener">恒等映射起到作用的分析</a>.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度神经网络小结</title>
      <link href="/posts/50630.html"/>
      <url>/posts/50630.html</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>2020.09.09</strong>: 叙述调整.</p></blockquote><h1 id="深度神经网络-Deep-Neural-Network"><a href="#深度神经网络-Deep-Neural-Network" class="headerlink" title="深度神经网络 Deep Neural Network"></a>深度神经网络 Deep Neural Network</h1><p>在本文中的神经网络是指的<strong>深度神经网络 Deep Neural Network</strong>. 进入到深度学习领域后, 除去DL计算量大, 参数多的特点外, 还会发现深度学习中有一个很大的特点: 不需要特征工程. 特征提取直接由模型自动端到端的完成. DL大多是用仿生学的结构, 来获得信息的新的表示, 所以我们也常说, <strong>深度学习的本质是表示学习</strong>. 其强大的功能我认为是由神经网络本身对事物的表征方式和神经网络的万能逼近定理决定的. </p><p>在错杂的神经网络结构中, 对数据进行非线性拟合. 由于其结构的复杂性, DL也<strong>没有很好的解释性</strong>, 但是研究人员常常将其逐步拆开, 发现其中的规律, 看看神经网络究竟学习到了什么.</p><h2 id="神经元-Neuron"><a href="#神经元-Neuron" class="headerlink" title="神经元 Neuron"></a>神经元 Neuron</h2><p>神经元的概念这个比较简单, 神经网络是由神经元构成的. 一定都看过一张类似的图:</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/DNN.png" alt=""></p><p>这就是神经网络的结构, 两端的分别称为<strong>输入层</strong>和<strong>输出层</strong>, 中间的称为<strong>隐藏层</strong>. 最左侧的输入层大小与输入的特征维数是保持一致的. 最右侧的输出层大小与输出的维数保持一致, 如果任务是分类任务, 输出层的神经元个数就是要分类的类别数, 预测的是归属于每类的类别概率. 如果是回归任务, 最后一层的神经元只有一个, 预测的是回归值. </p><p>在不对神经网络注入灵魂的情况下, 每个神经层之间是做最简单的<strong>线性回归</strong>, 只不过我们形象的将这个过程拆解为神经元的描述(或者说二者互相表示).</p><p>从神经元和神经层的角度来看, 在第$l$ 层每个神经元$j$, 都有一个与上一层神经元$i$ 连接的权重$w_{ij}^{(l)}$, 以及每个神经元的偏置项$b^{(l)}_j$, 把权重矩阵记作$W^{(l)}$, 上一层$l-1$ 的输出$Y^{(l-1)}$ 给进当前层$l$ 记作本层输入$X^{(l-1)}$, 偏置向量记为$b^{(l)}$, 那么该层的输出$Y^{(l)}$ 为:<br>$$<br>Y^{(l)} = W^{(l)}X^{(l-1)} +b^{(l)}<br>$$<br>它的形式上就是单纯的线性回归. 如果只是这样, 神经网络不论有多少层, 输入输出都是线性的, 加不加隐藏层都一样, 是无法拟合非线性关系的. 这时候必须加入一些非线性元素使得神经网络具有非线性拟合的能力, 必须用某种方式给神经网络注入灵魂.</p><h2 id="激活函数-Activation-Function"><a href="#激活函数-Activation-Function" class="headerlink" title="激活函数 Activation Function"></a>激活函数 Activation Function</h2><p>激活函数是作用在神经元输出上的<strong>非线性函数</strong>, 最开始是被单独作为一个神经层而存在的, 后来被集成到神经元身上. 它使得神经网络具有了逼近任意函数的潜力, 只要神经元够多, 数据足够健壮, 那么它可以拟合任意的情况. 激活函数常用的只有几种, 现在最常见的是<code>Relu</code>, 解决了梯度爆炸和梯度消失的难题(后面提到). 在有了激活函数$f(x)$后, 每个神经元的输出变为了:<br>$$<br>Y^{(l)} = f(W^{(l)}X^{(l-1)} + b^{(l)})<br>$$</p><h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><p>它是早期很常见的激活函数, 但是因为存在一些缺陷, 近些年使用的人数越来越少. 公式如下:<br>$$<br>\sigma(x) = \frac{1}{1+e^{-x}}<br>$$<br>这个函数的导数很好求. 并且它能够将任意的输入都放缩到$[0, 1]$的区间内.<br>$$<br>\sigma’(x) = \sigma(x)(1-f(x))<br>$$<br><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/sigmoid.png" style="zoom: 33%;" /></p><p>它的缺点也很明显, 当输入过大或过小时, 所对应的的导数近乎为0, 这种现象称为梯度消失. 这对于之后梯度下降的链式求导是极为不利的. 因为我们对神经网络的权值初始化范围在$[0, 1]$之间, 当发生反向传播时, 如果隐藏层特别多, 就很容易发生梯度消失, 使链式求导趋于0. 还有一个问题就是, <code>Sigmoid</code>每次输出的数据都不是Zero-centered, 其输出值全是正数, 会在收敛的路上越走越远, 导致收敛慢.</p><h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3><p>Tanh不是很了解, 但是现在用在NLP里很多(RNN经常用).<br>$$<br>tanh(x) =\frac{e^x-e^{-x}}{e^x+e^{-x}} \<br>tanh’(x) = 1-tanh^2(x)<br>$$<br><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/tanh.png" style="zoom:33%;" /></p><h3 id="Relu"><a href="#Relu" class="headerlink" title="Relu"></a>Relu</h3><p>为了改善梯度消失和梯度爆炸的问题, 计算也非常简单. 当时的<code>Relu</code>是非常具有统治力的(现在也是).<br>$$<br>ReLu(x) = max(0, x)<br>$$<br>其实就是一个取最大值的函数, 如果输入是负数直接取0. 不是全区间可导, 但是可以取次梯度.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/relu.jpg" style="zoom: 33%;" /><p>它的优点就是收敛快, 在输入大于0的情况下, 导数恒为1, 改善了梯度消失问题. 但是<code>Relu​</code>的输出同样不是Zero-centered. 而且还有可能会造成<code>Dead neuron</code>. 因为有些输入是小于0的, 或者有些节点因为不幸的初始化或者Learning rate设的太大, 导致整个权重矩阵的分布发生变化, 若矩阵分布中心在负区域, 则负输入的梯度为0, 导致神经元可能永远不被更新. </p><p>基于这个问题, 人们提出了许多别的解决方案, 都是在负输入上做一些手脚. 比如说LeakyRelu, 在负输入区域上的值就不全为零, 而是用一个可以调整的参数$\alpha$(通常取0.01)乘上输入.即:<br>$$<br>LeakyReLu(x)=max(\alpha x, x)<br>$$<br>参数$\alpha$可以通过反向传播学习到. 理论上来说<code>LeakyRelu</code>会继承<code>Relu</code>的所有优点, 并改善它的缺点. 但是实际上并没有相关实验证明它会在所有情况下比Relu好.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/leakyrelu.png" style="zoom:33%;" /><h3 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h3><p>在处理多分类问题时, 最后一层上会使用<code>Softmax</code>函数作为激活函数. <code>Softmax</code>保证了最后神经元输出是以概率形式输出. 对于分$C$类的问题, 最后一层第$i$个神经元在未激活的情况下的输出$y_i$, 有:<br>$$<br>Softmax(y_i) = \frac{e^i}{\sum\limits_i^Ce^i}<br>$$</p><h2 id="反向传播BP-Back-Propagation"><a href="#反向传播BP-Back-Propagation" class="headerlink" title="反向传播BP Back Propagation"></a>反向传播BP Back Propagation</h2><p>深度神经网络也被称为BP神经网络, 就是指的反向传播. 反向传播可以说是神经网络中最重要的部分之一了. 这种方式告诉我们如何调整神经元的相关权重. 首先, BP是基于梯度下降来调整权重的. 对于第$l$层, 第$i$个神经元对下一层第$j$个神经的权重$w_{ij}$, 损失函数为$E$, 人为设定学习率$\eta$ , 更新$w_{ij}$有:</p><p>$$<br>w^{(l)}_{ij} = w^{(l)}_{ij} - \eta\frac{\partial E(W, b)}{\partial w^{(l)}_{ij}}<br>$$<br>对于每层的偏置$b^{(l)}$ 同理. 这其中涉及到链式求导, 因为在计算时, 是通过输出层的最终复合函数逐渐向输入层求导, 所以就叫反向传播.</p><h2 id="随机失活-Dropout"><a href="#随机失活-Dropout" class="headerlink" title="随机失活 Dropout"></a>随机失活 Dropout</h2><p>当某些神经元过于强势时, 导致其他某些神经元会不被得到训练, 从而增大过拟合的几率, 当强大神经元对应输入的部分数据出现问题时, 就会出现单点故障. 所以需要一个方法使得其他神经元也得到训练, 并避免某些神经元过于强大. 此时, 采用神经元的随机失活策略, 使得每个神经元在训练时都有一定的概率权重不被更新, 能够保证绝大多数的神经元都处于活跃状态.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/dropout.png" alt=""></p><h2 id="参数计算"><a href="#参数计算" class="headerlink" title="参数计算"></a>参数计算</h2><p>DNN的参数计算比较简单, 假设第$l$层有$m$个神经元, 第$l+1$层有$n$个神经元, 那么$l$层的每个神经元都对应$l+1$层的$n$个权重, 外加一个偏置$b$, 第$l$层需训练的参数个数是$m\times n + 1$.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>操作系统常见问题整理</title>
      <link href="/posts/57255.html"/>
      <url>/posts/57255.html</url>
      
        <content type="html"><![CDATA[<h1 id="操作系统"><a href="#操作系统" class="headerlink" title="操作系统"></a>操作系统</h1><p>操作系统问的比较多的就是进程线程的区别, 作业调度算法, 分页分段, 死锁很关键, 假脱机这部分也需要重视起来. 文中有部分算法的细节没有提到, 只是列出来名字, 需要自己补充. 当时整理的排版可能比较乱, 以后有空再整理.</p><h2 id="计算机系统概述"><a href="#计算机系统概述" class="headerlink" title="计算机系统概述"></a>计算机系统概述</h2><h3 id="操作系统的基本概念"><a href="#操作系统的基本概念" class="headerlink" title="操作系统的基本概念"></a>操作系统的基本概念</h3><p><strong>操作系统</strong>: 是指控制和管理整个计算机系统的硬件与软件, 合理地组织, 调度计算机的工作与资源的分配, 进而为用户和其他软件提供方便接口与环境的程序集合. 操作系统是计算机系统中最基本的系统软件.</p><p><strong>操作系统的特征</strong>: </p><ul><li>并发</li><li>共享</li><li>虚拟: 物理资源的虚拟化.</li><li>异步</li></ul><p><strong>操作系统的目标和功能</strong>:</p><ol><li>计算机系统资源的管理者</li><li>用户与计算机硬件系统之间的接口</li><li>用作扩充机器</li></ol><h3 id="操作系统的发展与分类"><a href="#操作系统的发展与分类" class="headerlink" title="操作系统的发展与分类"></a>操作系统的发展与分类</h3><ol><li><p>手工操作阶段: 无操作系统</p></li><li><p>批处理阶段: 用户脱机使用计算机. 单道批处理 - 顺序执行, 多道批处理 - 多道程序并行, 共享资源, 不利于人机交互 但效率高</p></li><li><p>分时操作系统: 时间片轮转, 多用户通过终端共享一台主机, 具有可交互性, 响应时间短. 且用户之间彼此独立互不干扰.</p></li><li><p>实时操作系统: 嵌入式和工业界, 紧急状态常用. 注重响应时间.</p></li><li><p>网络操作系统和分布式计算机操作系统</p></li><li><p>个人计算机操作系统: 现在用, Widnows等.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/OS/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B.jpg" alt=""></p></li></ol><h3 id="操作系统运行的环境"><a href="#操作系统运行的环境" class="headerlink" title="操作系统运行的环境"></a>操作系统运行的环境</h3><p>CPU执行两种程序: 操作系统的内核程序,用户APP.</p><p>因此CPU执行时常分为用户态和核心态, 一些与硬件关联比较紧密或者权限比较大的都是处于核心态执行.</p><p><strong>内核主要包括四方面</strong>:</p><ul><li><p>时钟管理</p></li><li><p>中断机制</p></li><li><p>原语: 底层可以被调用的一些小程序, 距离硬件很近, 运行具有原子性, 执行时间短, 调用频繁.</p></li><li><p>系统控制的数据结构及处理: 进程管理, 存储器管理, 设备管理. 分别对应不同的数据结构, 需要进行有效管理和操作.</p></li></ul><p><strong>中断和异常</strong>:</p><ul><li><p><strong>异常</strong>: 内中断(指令中断或强迫中断). 异常通常出现立即处理, 依赖于当前程序的运行现场. 一般是出现意想不到的错误, 如程序非法操作码, 地址越界等.</p></li><li><p><strong>中断</strong>: 外中断(强迫中断). 来自CPU指令以外的事件发生, 主要是设备的I/O处理完成,希望能向下一个设备发出I/O请求.</p></li></ul><p><strong>中断处理流程</strong>:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/OS/中断.jpg" style="zoom:50%;" /><p><strong>系统调用</strong>: 当用户在程序中调用操作系统的一些特殊子功能(包括异常), 必须切换到内核态, 比如设备管理, 内存管理, 文件管理, 进程控制, 进程通信. 用户执行”陷入指令trap”将CPU使用权交给操作系统内核. 等到执行完成后, 内核会将使用权交还给用户, 即返回.</p><h3 id="操作系统体系结构"><a href="#操作系统体系结构" class="headerlink" title="操作系统体系结构"></a>操作系统体系结构</h3><p><strong>大内核和微内核</strong>: 大内核将操作系统功能作为紧密的整体放倒内核, 性能高, 各种信息共享, 但耦合度高, 管理复杂. 微内核架构下, 操作系统的一部分功能被移出内核降低内核复杂度, 移出去的分层分成若干相互独立的服务. 操作系统被划分为若干个小的定义良好的木块. 只有一个模块在内核态, 其余在用户态. 操作系统因需要频繁在用户态和核心态进行切换有性能损失.</p><p><strong>并行性和并发性</strong>: 并行性是多个事件在同一时刻发生, 并发性指多个事件在同一时间间隔内发生. 多道程序环境下, 宏观上程序同时运行, 微观上是交替进行, 即并发性. 若想满足并行性, 可以分配到多个处理器上并行执行.</p><h2 id="进程管理"><a href="#进程管理" class="headerlink" title="进程管理"></a>进程管理</h2><h3 id="进程与线程"><a href="#进程与线程" class="headerlink" title="进程与线程"></a>进程与线程</h3><p>在多道程序环境下, 允许多个进程并发执行, 此时他们将失去封闭性, 并具有间断性及不可再现 性的特征. 为此引入了进程的概念, 以便更好地描述和控制程序的并发执行, 实现操作系统的并发性和共享性.  进程是程序的运行过程, 是系统进行资源分配和调度的一个独立单位. </p><p>PCB是进程存在的<strong>唯一标志</strong>.</p><p><strong>进程的五个状态</strong>: 运行态, 就绪态, 阻塞态, 创建态, 结束态.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/OS/%E8%BF%9B%E7%A8%8B%E5%88%9B%E5%BB%BA.jpg" alt=""></p><p>阻塞也就是挂起, 即使处理器空闲, 没有触发挂起结束的信号, 就不能运行. 就绪态是缺资源.</p><p><strong>进程的通信</strong>: </p><ol><li><strong>共享存储</strong>: 有一块直接访问的共享空间, 直接读写, 互斥访问.</li><li><strong>消息传递</strong>: 通过消息来通知另一个进程要做什么. 有直接和间接, 间接就是邮箱通信, 消息队列.</li><li><strong>管道通信</strong>: 读写一个共享的pip文件(也可以视为缓冲区), 以字符流形式写入和取出.</li></ol><p><strong>线程和进程的比较</strong>:</p><ol><li>进程是系统进行资源分配和调度的基本单位, 线程是CPU调度和分配资源的基本单位.</li><li>线程依赖于进程存在. 每个进程至少有一个线程.</li><li>进程有自己的独立地址空间, 线程共享进程的地址空间.</li><li>进程是用于系统资源的独立单位, 线程基本自己不拥有系统资源, 只有一些运行必不可少的资源, 其他的资源由线程共享进程的资源.</li><li>进程切换开销大, 涉及CPU环境的保存和设置. 线程保存少量寄存器内容.</li><li>线程通信直接共享数据即可, 进程通信需要通过进程间通信.</li><li>多线程程序若有一个线程崩溃则整个程序崩溃. 多进程程序进程崩溃不影响其他进程.</li></ol><h3 id="处理机调度"><a href="#处理机调度" class="headerlink" title="处理机调度"></a>处理机调度</h3><p>调度往往分为三个层次:</p><ul><li>作业调度(高级调度): 按照某个原则从后备状态的作业中选一个, 分配资源建立进程.</li><li>内存调度(中级调度): 将暂时不运行的进程调到外存等待, 挂起. 提高内存使用率和吞吐量.</li><li>进程调度(低级调度): 操作系统中最基本的调度, 选取进程分配.</li></ul><p>进程调度方式: 抢占和非抢占.</p><p><strong>经典调度算法</strong>:</p><ol><li><p>先来先服务 FCFS: 效率低, 非抢占 长作业有利, CPU密集型有利</p></li><li><p>短作业优先 SJF: 非抢占, 长作业不利, 容易触发死锁. 作业运行时间是估计的, 不一定真正最短. 而且未考虑作业的紧迫程度.</p></li><li><p>优先级调度: 设立一个优先级, 每当当前进程让出处理机时(可以是主动或被动的, 也就是抢占和非抢占), 把处理机分配给更紧迫的进程. 优先级也可以是动态的和静态的, 静态的在创建进程时就已经被确定, 动态的可以根据情况调整.</p></li><li><p>高响应比优先级调度: 计算响应比, 将响应比最高的作业投入运行.</p><p>响应比为(1+等待时间/要求服务时间).</p><ul><li>等待时间相同, 有利于短作业.</li><li>要求服务时间相同, 等待时间越长响应比越高, 是先来先服务.</li><li>长作业可以通过等待时间增加而提高.</li></ul></li><li><p>时间片轮转调度: 将系统所有就绪进程按到达时间分为先后次序排成一个队列, 每次都选队列中第一个进程执行, 仅能运行一个时间片, 即使未运行完成也要强制释放处理机给下一个就绪进程, 自身回到就绪队列尾端. 性能严重依赖于时间片大小的选取.</p></li><li><p>多级队列: 设置多个就绪队列1、2、3…, 优先级递减, 时间片递 增. 只有等到优先级更高的队列为空时才会调度当前队列中的进程. 如果进程用完了当前队列的时间片还未执行, 则会被移到下一队列. 抢占式(时间片用完时), 开销可能较大, 对IO型进程有利, 可能会出现饥饿问题. </p></li></ol><h3 id="进程同步"><a href="#进程同步" class="headerlink" title="进程同步"></a>进程同步</h3><ul><li>同步: 串行执行的先后顺序不能乱.</li><li>互斥: 共享的临界资源只能同时由一个进程访问.</li></ul><p>信号量 P为申请, V为释放, 互斥即信号量为1</p><p><strong>同步问题</strong>:</p><ol><li>生产者-消费者</li><li>读者-写者</li><li>哲学家进餐</li><li>吸烟者</li></ol><h3 id="死锁"><a href="#死锁" class="headerlink" title="死锁"></a>死锁</h3><p><strong>死锁</strong>: 多进程的并发执行, 进行资源竞争导致的僵局, 无外力作用则每个进程都无法向前执行.</p><p><strong>原因</strong>:</p><ol><li>系统资源竞争</li><li>进程推进顺序非法(释放资源顺序不当)</li><li>死锁产生的必要条件: 互斥, 非抢占, 请求并保持(要了还要), 循环等待(资源分配图有环, )</li></ol><p><strong>死锁避免</strong></p><p><strong>安全状态</strong>: 按照某个序列分配资源能够使得当前所有进程全部执行完.</p><p>只要系统处于安全状态, 就不会进入死锁. 处于不安全不一定死锁.</p><p>算法: 银行家算法</p><p><strong>解决死锁三个方法</strong>: 死锁避免, 死锁检测, 死锁解除</p><p><strong>饥饿和死锁的区别</strong>:</p><p>等待时间给进程推进和响应带来明显影响时成为进程饥饿.  饥饿并不代表系统已经死锁, 但至少有一个程序的执行被无限期地推迟.  差别:  ① 进入饥饿的进程可以只有一个, 但是死锁必须大于等于两个;  ② 出于饥饿状态的进程可以是一个就绪进程, 但是死锁状态的进程必定是阻塞进程. </p><h2 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h2><h3 id="内存管理概念"><a href="#内存管理概念" class="headerlink" title="内存管理概念"></a>内存管理概念</h3><p>由源程序变为内存中可以执行的程序通常需要: </p><ol><li>编译: 由编译程序将用户源代码编译成若干目标模块</li><li>链接: 由链接程序将编译后形成的一组目标模块及所需的库函数链接在一起, 形成一个完整的装入模块.</li><li>装入: 由装入程序将装入模块装入内存中运行</li></ol><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/OS/%E7%A8%8B%E5%BA%8F%E6%89%A7%E8%A1%8C%E4%B8%89%E6%AD%A5%E9%AA%A4.jpg" alt=""></p><p>链接可以分为静态链接, 装入时动态链接, 运行时动态链接.</p><ol><li>静态链接: 在程序运行之前, 先把各个目标模块及所需库链接为一个完整的可执行程序, 以后不再拆开.  </li><li>装入时动态链接: 将应用程序编译后所得到的一组目标模块在装入内存时采用边装入边链接的 链接方式.  </li><li>运行时动态链接: 知道程序运行过程中需要一些模块时, 才对这些模块进行链接. </li></ol><p>内存装入分为绝对装入, 可重定位装入,动态运行时装入.</p><ol><li>绝对装入: 在编译时就知道程序将要驻留在内存的物理地址, 编译程序产生含有物理地址的目 标代码,  不适合多道程序设计. </li><li>可重定位装入: 根据内存当前情况, 将装入模块装入到内存的适当位置, 地址变换通常在装入时一次完成, 之后不再改变, 也称静态重定位. 当操作系统为程序分配一个以某地址为起始地址的连续主存区域后, 重定位时将程序中指令或操作数的逻辑地址加上这个起始地址就得到了物理地址.  在作业装入内存时, 必须分配全部的内存空间, 如果没有足够的内存, 则不能装入该作业. 相应的, 因为采用了相对地址, 一旦进入内存, 作业在运行期间也不能移动和申请新的内存空间.</li><li>动态运行装入: 允许程序运行时在内存中移动位置, 把装入模块装入到内存后的所有地址都是 相对地 址, 在程序执行过程中每当访问到相应指令或数据时, 才将要访问的程序或数据的相对地址转换为物理地址. 动态重定位的实现要依靠硬件地址变换机构. </li></ol><p>物理地址转换成逻辑地址的过程称为<strong>地址重定位</strong>.</p><p><strong>内存保护</strong>: 主要目标还是为了保护程序地址不越界, 需要用额外寄存器维护.(不常)</p><p><strong>覆盖与交换</strong></p><ul><li><p>覆盖: 把一个大的程序划分为一系列覆盖, 每个覆盖是一个相对独立的程序单位, 把程序执行时并不要求同时装入内存的覆盖组成一组, 成为覆盖段, 这个覆盖段分配到同一个存储区域, 这个存储区域成为覆盖区, 它与覆盖段一一对应. 覆盖段的大小由覆盖段中最大的覆盖来确定. (为了解决内存容量太小的问题, 打破了必须将一个程序全部信息装入内存后才能运行的限制) </p></li><li><p>交换: 把暂时不用的某个程序及数据部分从内存移到外存中去, 以便腾出必要的内存空间; 或者把指定 的程序或数据从外存读到相应的内存中, 并将控制权交给他, 让其在系统上运行的一种内存扩充技术. 处理器的中级调度就是采用交换技术</p></li></ul><p>区别: </p><ol><li>与覆盖技术相比, 交换技术不要求程序员给出的 程序段之间的覆盖结构;  </li><li>交换技术主要在进程和作业之间进行, 覆盖技术主要在同一个进程或作业中进行; </li><li>覆盖技术只能覆盖于覆盖程序段无关的程序段, 交换进程由换出和换入两个过程组成. </li></ol><p><strong>连续分配管理</strong></p><ol><li><p><strong>单一连续分配</strong>: 内存在此方式下分为系统区和用户区, 系统区仅提供给操作系统使用, 通常在低地址部分; 用户区是为用户提供的、除系统区之外的内存空间. 这种方式无需进行内存保护.  这种方式的优点是简单、无外部碎片, 可以釆用覆盖技术, 不需要额外的技术支持. 缺点是只能用于单用户、单任务的操作系统中, 有内部碎片, 存储器的利用率极低. </p></li><li><p><strong>固定连续分配</strong>: 固定分区分配是最简单的一种多道程序存储管理方式, 它将用户内存空间划分为若干个固定大小 的区域, 每个分区只装入一道作业. 当有空闲分区时, 便可以再从外存的后备作业队列中,选择适 当大小的作业装入该分区, 如此循环.  固定分区分配在划分分区时, 有两种不同的方法.  (1) 分区大小相等: 用于利用一台计算机去控制多个相同对象的场合, 缺乏灵活性.  (2) 分区大小不等: 划分为含有多个较小的分区、适量的中等分区及少量的大分区. </p></li><li><p><strong>动态分区分配</strong>: 动态分区分配又称为可变分区分配, 是一种动态划分内存的分区方法. 这种分区方法不预先将内存划分, 而是在进程装入内存时, 根据进程的大小动态地建立分区, 并使分区的大小正好适合进 程的需要. 因此系统中分区的大小和数目是可变的. 由于大内存的进程被换出, 若小内存进程被换入, 则容易产生更小内存的碎片.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/OS/%E5%8A%A8%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86.jpg" alt=""></p><p>所以必须考虑下面<strong>四种内存管理算法</strong>:</p><ol><li><strong>首次适应算法</strong>(First fit): 空闲分区地址递增查找, 找第一个大小能满足要求的空闲分区. UNIX采用了这种, 不花里胡哨, 性能也不错.</li><li><strong>最佳适应算法</strong>(Best fit): 空闲分区按容量递增排列, 找第一个大小满足要求的. 内存空间不一定每次都完美契合, 每次都产生很小的外部碎片, 根本无法利用.</li><li><strong>最坏适应算法</strong>(Worst fit): 与最佳相反, 按照容量递减排列. 把大的连续内存划分开了, 导致没有大的内存块.</li><li><strong>邻近适应</strong>(Next fit): 是首次适应的升级版, 在上次查找结束的位置继续查找.</li></ol></li></ol><p><strong>非连续分配管理</strong></p><p>将作业要求的内存空间分散地分配开</p><p><strong>基本分页存储管理</strong></p><p>分页思想: 主存空间划分为大小相等且固定的块, 块相对小, 作为主存的基本单位. 进程也按照块为单位进行划分, 进程在执行的时候, 也以块为单位逐个申请主存空间.</p><p>分页不会产生外部碎片, 但容易产生内部碎片(很小, 也称页内碎片).</p><p>进程中的块称为页, 内存中的块称为页框或页帧. 外存也按照块划分, 直接称为块. 为了方便地址转换, 页面大小设置为$2^k$. 页面大小也应该适中, 过大会导致页内碎片过多, 过小会导致交换频繁从而降低页面换入换出的效率.</p><p>地址结构分为两部分: 页号和页内偏移量. 地址长度为32位, 则0<del>11为页内地址, 即每页大小4KB, 12</del>31位页号, 即地址空间最多允许$2^{20}$页.</p><p>地址结构决定了虚拟内存的寻址空间.</p><p>页表是便于在内存中找到指定进程所对应的每个页面的物理块, 系统为每个进程建立一个页表, 记录页面在内存中的物理块号(页帧号).</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/OS/%E9%A1%B5%E8%A1%A8.jpg" alt=""></p><p>页表由页表项组成. 页表项和物理地址都由两部分组成, 第一部分都是页号, 页表项第二部分是物理内存的块号, 地址的第二部分是页内偏移量.</p><p>一页对应多个地址单元, 所以地址结构是由页内偏移量和页号组成的. 页表的作用是实现页号到物理块号的地址映射.</p><p><strong>基本地址变换机构</strong></p><p>系统中的页表寄存器PTR, 有页表存放在内存的起始位置F, 页表长度M. 进程执行前F和M放在PCB中, 执行时将其调入页表寄存器. 设页面大小L., 逻辑地址A到物理地址E的变化如下:</p><ol><li>计算页号P, P = A / L, 页内偏移量W, W = A % L.</li><li>检查P和M是否满足P &lt; M, 否则产生越界中断.</li><li>然后找P对应的页表项地址 = F + P * 页地址长度. 取出该地址对应的物理块号, 在内存中找到内容</li><li>E = b * L + W, 就得到了物理地址E. 然后取E中的数据或指令.</li></ol><p>这个过程中只需要给出逻辑地址就能确定物理地址, 地址结构是页号和页内偏移量组成的, 不同的页号会映射到不同的位置, 所以说页面管理的地址是一维的.</p><p><strong>快表</strong>: 上述过程访问了两次内存, 第一次是访问页表, 第二次是根据地址取数据或指令. 加一个高速缓存, 也叫相联存储器TLB, 用来存放当前访问的若干页表项, 只访问一次主存.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/OS/%E5%BF%AB%E8%A1%A8.jpg" alt=""></p><p><strong>两级页表</strong>: 继续延伸页表映射的思想, 页表项过多时, 存在内存中十分浪费空间. 于是引入二级页表的结构来完成空间压缩. 地址结构变为一级页号, 二级页号, 页内偏移量. 每次进程执行时, 只需要调入一级页号其中的一页就能完成对应的地址转换. 大大降低了内存使用量(实际上就是构造页表的页表). 多级页表同理.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/OS/%E4%BA%8C%E7%BA%A7%E9%A1%B5%E8%A1%A8.jpg" alt=""></p><p><strong>基本分段存储管理</strong></p><p>分段是为了满足用户和程序员方便编程, 信息保护等需要. 不是通过硬件实现的.</p><p>分段将进程分为多个段, 段间可不连续, 段内连续. 每段从0开始编址, 分配一段    连续的地址空间. 分段的逻辑地质结构是段号和段内偏移量. 段表的地址结构为段号, 段长, 本段在主存的起始地址. 在访问某地址时, 需要给出段号和段内偏移量, 结合段表的本段起始地址, 就能找到对应的物理地址单元.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/OS/%E6%AE%B5%E8%A1%A8%E6%98%A0%E5%B0%84.jpg" alt=""></p><p>分段管理不能根据给出的一个整数确定物理地址, 因为每段的段长是不固定的, 所以无法求出段内偏移, 因此说分段管理的地址空间是二维的.</p><p><strong>段页式管理</strong></p><p>分页能提高内存利用率, 分段能反映程序的罗结构, 有利于段的共享.</p><p>在分段管理的基础上给每个段添加一个页表.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/OS/%E6%AE%B5%E9%A1%B5%E7%AE%A1%E7%90%86.jpg" alt=""></p><p>段页式的逻辑地质结构为段号, 页号, 页内偏移量. 段表只有一个, 页表可能有多个.</p><p>进行地址转换时, 先从段表查询到页表的起始地址, 再从页表中找到页帧号, 最后结合页内偏移量能找到对应的物理地址. 访问三次主存. 段页式的地址空间是二维的, 主要通过段号和页内偏移量就能访问对应的物理单元.</p><h3 id="虚拟内存"><a href="#虚拟内存" class="headerlink" title="虚拟内存"></a>虚拟内存</h3><p>传统内存有一次性和驻留性, 必须一次装入内存, 且装入后就一直留在内存直到作业运行结束.</p><p><strong>局部性原理</strong></p><ul><li>时间局部性: 某条指令一旦被执行, 不久后可能被再次执行, 数据同理.</li><li>空间局部性: 程序访问某存储单元, 不久后 附近的存储单元也会被访问, 集中在一定范围内.</li></ul><p>虚拟存储器是基于局部性原理设计的, 程序装入时只将一部分装入内存, 就可以执行程序, 访问内容不在内存时将那部分调入内存然后继续执行. 还有一个就是把暂时不用的调出内存.</p><p><strong>请求分页管理方式</strong></p><p><strong>页表机制</strong></p><p>请求分页的页表和普通也表不同, 因为不需要一次性将程序调入内存. 所以就一定会有缺页的情况, 因此必须给页表项加上标志位.</p><p>因此页表项变为: 页号, 物理块号, 状态位P, 访问字段A, 修改位M, 外存地址.</p><p>状态位P: 是否已调入内存.</p><p>访问字段A: 记录本页在一段时间内被访问的次数, 供页面置换算法参考.</p><p>修改位M: 调入内存后是否被修改过.</p><p>外存地址: 用于指出该页在外存上的地址.</p><p>缺页中断: 当要访问的页面不在内存时产生缺页中断, 操作系统将缺的页调入内存, 如果有空闲块则分配块, 如果没有则淘汰某页. 并阻塞进程, 在完成调页后唤醒.</p><p>地址变换: 在分页地址变换基础上加了一些功能.</p><p>地址变换先找快表, 找到后修改访问位, 然后利用物理块号和页内地址形成物理地址. 如果没找到就去内存中找页表, 对比状态位P, 看是否已调入内存, 未调入则调入.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/OS/%E8%AF%B7%E6%B1%82%E5%88%86%E9%A1%B5.jpg" alt=""></p><p><strong>页面置换算法</strong></p><ol><li><p><strong>最佳置换算法</strong>: 从主存中移出永远不再需要的页面; 如无这样的页面存在, 则选择最长时间不需要访问的页面.  于所选择的被淘汰页面将是以后永不使用的, 或者是在最长时间内不再被访问的页面, 这样可以 保证获得最低的缺页率.  即被淘汰页面是以后永不使用或最长时间内不再访问的页面. (往后看) </p></li><li><p><strong>先进先出</strong>(FIFO) 置换算法: 是最简单的页面置换算法. 这种算法的基本思想是: 当需要淘汰一个页面时, 总是选择驻留主存 时间最长的页面进行淘汰, 即先进入主存的页面先淘汰. 其理由是: 最早调入主存的页面不再被 使用的可能性最大.  即优先淘汰最早进入内存的页面. (往前看)  但容易产生Belady异常, 即当物理块数增大时, 故障数不减反增.</p></li><li><p><strong>最近最久未使用</strong>(LRU) 算法: 这种算法的基本思想是: 利用局部性原理, 根据一个作业在执行过程中过去的页面访问历史来推 测未来的行为. 它认为过去一段时间里不曾被访问过的页面, 在最近的将来可能也不会再被访 问. 所以, 这种算法的实质是: 当需要淘汰一个页面时, 总是选择在最近一段时间内最久不用的 页面予以淘汰.  即淘汰最近最长时间未访问过的页面. (往前看) </p></li><li><p><strong>时钟(CLOCK)置换</strong>算法: LRU算法的性能接近于OPT,但是实现起来比较困难, 且开销大; FIFO算法实现简单, 但性能差.  所以操作系统的设计者尝试了很多算法, 试图用比较小的开销接近LRU的性能, 这类算法都是 CLOCK算法的变体.  简单的CLOCK算法是给每一帧关联一个附加位, 称为使用位. 当某一页首次装入主存时, 该帧的 使用位设置为1;当该页随后再被访问到时, 它的使用位也被置为1. 对于页替换算法, 用于替换的 候选帧集合看做一个循环缓冲区, 并且有一个指针与之相关联. 当某一页被替换时, 该指针被设 置成指向缓冲区中的下一帧. 当需要替换一页时, 操作系统扫描缓冲区, 以查找使用位被置为0的 一帧. 每当遇到一个使用位为1的帧时, 操作系统就将该位重新置为0; 如果在这个过程开始时,  缓冲区中所有帧的使用位均为0, 则选择遇到的第一个帧替换; 如果所有帧的使用位均为1,则指针 在缓冲区中完整地循环一周, 把所有使用位都置为0, 并且停留在最初的位置上, 替换该帧中的 页. 由于该算法循环地检查各页面的情况, 故称为CLOCK算法, 又称为最近未用(Not Recently Used, NRU)算法. </p></li></ol><p><strong>地址翻译</strong>: TLB-&gt;页表(TLB不命中) -&gt;Cache-&gt;主存(Cache不命中) -&gt;外存</p><h2 id="文件管理"><a href="#文件管理" class="headerlink" title="文件管理"></a>文件管理</h2><p><strong>文件基本操作</strong></p><p>文件属于抽象数据类型. 为了恰当地定义文件, 就需要考虑有关文件的操作. 操作系统提供系统 调用, 它对文件进行创建、写、读、定位和截断. </p><ol><li>创建文件: 创建文件有两个必要步骤, 一是在文件系统中为文件找到空间; 二是在目录中为新 文件创建条目, 该条目记录文件名称、在文件系统中的位置及其他可能信息. </li><li>写文件: 为了写文件, 执行一个系统调用, 指明文件名称和要写入文件的内容. 对于给定文件 名称, 系统搜索目录以查找文件位置. 系统必须为该文件维护一个写位置的指针. 每当发生写操 作, 便更新写指针.  </li><li>读文件: 为了读文件, 执行一个系统调用, 指明文件名称和要读入文件块的内存位置. 同样,  需要搜索目录以找到相关目录项, 系统维护一个读位置的指针. 每当发生读操作时, 更新读指 针. 一个进程通常只对一个文件读或写, 所以当前操作位置可作为每个进程当前文件位置指针.  由于读和写操作都使用同一指针, 节省了空间也降低了系统复杂度. </li><li>文件重定位(文件寻址) : 按某条件搜索目录, 将当前文件位置设为给定值, 并且不会读、写 文件. </li><li>删除文件: 先从目录中找到要删除文件的目录项, 使之成为空项, 然后回收该文件所占用的存 储空间.  </li><li>截断文件: 允许文件所有属性不变, 并删除文件内容, 即将其长度设为0并释放其空间.  这6个基本操作可以组合执行其他文件操作. 例如, 一个文件的复制, 可以创建新文件、 从旧文 件读出并写入到新文件. </li></ol><p><strong>磁盘调度算法</strong></p><p><strong>寻道时间</strong>: 跨越磁道数为n, 启动磁臂时间s, m为磁盘驱动器相关的常数, 约为0.2ms<br>$$<br>T_s = m \times n + s<br>$$<br><strong>延迟时间</strong>: 磁头定位到某一磁道的扇区所需的时间. 磁盘旋转速度为r.<br>$$<br>T_r = \frac{1}{2r}<br>$$<br><strong>传输时间</strong>: b为每次读写的字节数和磁盘旋转速度, r为磁盘每秒转数, N为一个磁道上的字节数.<br>$$<br>T_t = \frac{b}{rN}<br>$$<br><strong>平均存取时间</strong>:<br>$$<br>T_a = T_s+T_r+T_t<br>$$<br><strong>磁盘调度算法</strong>:</p><ol><li><strong>先来先服务</strong>算法(FCFS) First Come First Service 这是一种比较简单的磁盘调度算法. 它根据进程请求访问磁盘的先后次序进行调度. 此算法的优 点是公平、简单, 且每个进程的请求都能依次得到处理, 不会出现某一进程的请求长期得不到满 足的情况. 此算法由于未对寻道进行优化, 在对磁盘的访问请求比较多的情况下, 此算法将降低 设备服务的吞吐量, 致使平均寻道时间可能较长, 但各进程得到服务的响应时间的变化幅度较 小. </li><li><strong>最短寻道时间优先</strong>算法(SSTF)  Shortest Seek Time First 该算法选择这样的进程, 其要求访问的磁道与当前磁头所在的磁道距离最近, 以使每次的寻道时 间最短, 该算法可以得到比较好的吞吐量, 但却不能保证平均寻道时间最短. 其缺点是对用户的 服务请求的响应机会不是均等的, 因而导致响应时间的变化幅度很大. 在服务请求很多的情况 下, 对内外边缘磁道的请求将会无限期的被延迟, 有些请求的响应时间将不可预期. </li><li><strong>扫描算法(SCAN) 电梯调度扫描</strong>算法不仅考虑到欲访问的磁道与当前磁道的距离, 更优先考虑的是磁头的当前移动方向. 例 如, 当磁头正在自里向外移动时, 扫描算法所选择的下一个访问对象应是其欲访问的磁道既在当 前磁道之外, 又是距离最近的. 这样自里向外地访问, 直到再无更外的磁道需要访问才将磁臂换 向, 自外向里移动. 这时, 同样也是每次选择这样的进程来调度, 即其要访问的磁道, 在当前磁 道之内, 从而避免了饥饿现象的出现. 由于这种算法中磁头移动的规律颇似电梯的运行, 故又称 为电梯调度算法. 此算法基本上克服了最短寻道时间优先算法的服务集中于中间磁道和响应时间 变化比较大的缺点, 而具有最短寻道时间优先算法的优点即吞吐量较大, 平均响应时间较小, 但 由于是摆动式的扫描方法, 两侧磁道被访问的频率仍低于中间磁道. </li><li><strong>循环扫描算法(CSCAN)  循环扫描</strong>算法是对扫描算法的改进. 如果对磁道的访问请求是均匀分布的, 当磁头到达磁盘的一 端, 并反向运动时落在磁头之后的访问请求相对较少. 这是由于这些磁道刚被处理, 而磁盘另一 端的请求密度相当高, 且这些访问请求等待的时间较长, 为了解决这种情况, 循环扫描算法规定 磁头单向移动. 例如, 只自里向外移动, 当磁头移到最外的被访问磁道时, 磁头立即返回到最里 的欲访磁道, 即将最小磁道号紧接着最大磁道号构成循环, 进行扫描. </li></ol><h2 id="I-O管理"><a href="#I-O管理" class="headerlink" title="I/O管理"></a>I/O管理</h2><p>IO的控制方式:</p><ol><li><p>程序直接控制方式</p><p>计算机从外设读取数据到存储器, 每次读一个字, 每读入一个字都对外设进行循环检查, 直到确定该字已在IO控制器的数据寄存器中. CPU利用率很低, 大多数时间都在检查.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/OS/程序式IO.jpg" style="zoom: 50%;" /></li><li><p>中断驱动方式</p><p>当某进程要启动某个 I/O 设备工作时, 便由 CPU 向相应的设备控制器发出一条 I/O 命令, 然后立 即返回继续执行原来的任务. 仅当输完一个数据时, 才需 CPU 花费极短的时间去做些中断处理. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/OS/中断驱动式io.jpg" style="zoom:50%;" /></li><li><p>DMA方式</p><p>通过在I/O设备和内存之间开启一个可以直接传输数据的通路, 采用DMA控制器来控制一个数据块 的传输, CPU只需在一个数据块传输开始阶段设置好传输所需的控制信息, 并在传输结束阶段做 进一步处理. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/OS/DMAIO.jpg" style="zoom:50%;" /></li></ol><p><strong>IO子系统层次结构</strong></p><ol><li>用户层IO软件</li><li>设备独立性软件</li><li>设备驱动程序</li><li>中断处理程序</li><li>硬件</li></ol><p><strong>假脱机技术</strong></p><p>虚拟性是OS的四大特性之一. 如果说可以通过多道程序技术将一台物理CPU虚拟为多台逻辑 CPU, 从而允许多个用户共享一台主机, 那么, 通过SPOOling技术便可将一台物理I/O设备虚拟 为多台逻辑I/O设备, 同样允许多个用户共享一台物理I/O设备.  SPOOLing技术是对脱机输入、输出系统的模拟. 相应地, SPOOLing系统必须建立在具有多道程 序功能的操作系统上, 而且还应有高速随机外存的支持, 这通常是采用磁盘存储技术.  SPOOLing系统主要有以下三部分:  </p><ol><li>输入井和输出井. 这是在磁盘上开辟的两个大存储空间. 输入井是模拟脱机输入时的磁盘设 备, 用于暂存I/Q设备输入的数据; 输出井是模拟脱机输出时的磁盘, 用于暂存用户程序的输出数据.  </li><li>输入缓冲区和输出缓冲区. 为了缓和和CPU和磁盘之间速度不匹配的矛盾, 在内存中要开辟 两个缓冲区; 输入缓冲区和输出缓冲区. 输入缓冲区用于暂存由输入设备送来的数据, 以后再传送到输入井. 输出缓冲区用与暂存从输出井送来的数据, 以后在传送给输出设备.  </li><li>输入进程SPi 和输入进程SP0. 这里利用两个进程来模拟脱机I/O时的外围控制机. 其中, 进 程SPi模拟脱机输入时的外围控制机, 将用户要求的数据从输入机通过输入缓冲区再送到输入井,  当CPU需要输入数据时, 直接从输入井读入内存; 进程SP0模拟脱机输出时的外围控制机, 把用户 要求输出的数据从先内存送到输出井, 待输出设备空闲时, 在将输出井中的数据经过输出缓冲区 送到输出设备上.  </li></ol><p>SPOOLing技术的特点:  </p><ol><li>提高了I/O速度. 从对低速I/O设备进行的I/O操作变为对输入井或输出井的操作, 如同脱机操作 一样, 提高了I/O速度, 缓和了CPU与低速I/O设备速度不匹配的矛盾. </li><li>将独占设备改造为共享设备. 因为在SPOOLing系统的系统中, 实际上并没为任何进程分配设 备, 而知识在输入井或输出井中为进程分配一个存储区和建立一张I/O请求表. 这样, 便把独占设 备改造为共享设备. </li><li>实现了虚拟设备功能. 多个进程同时使用一独享设备, 而对每一进程而言, 都认为自己独占这 一设备, 从而实现了设备的虚拟分配. 不过, 该设备是逻辑上的设备. </li></ol>]]></content>
      
      
      <categories>
          
          <category> 计算机基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 操作系统 </tag>
            
            <tag> 面试 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo博客优化</title>
      <link href="/posts/42790.html"/>
      <url>/posts/42790.html</url>
      
        <content type="html"><![CDATA[<h1 id="博客优化"><a href="#博客优化" class="headerlink" title="博客优化"></a>博客优化</h1><p>当东西多了以后, 博客的运行速度就会被拖慢. 所以优化访问速度是非常有必要的. 本文参考了<strong>Yafine</strong>的<a href="https://yafine-blog.cn/posts/4ab2.html" target="_blank" rel="noopener">这篇博客</a>和<strong>Sky03</strong>的<a href="https://blog.sky03.cn/posts/42790.html#toc-heading-5" target="_blank" rel="noopener">博客</a>对访问进行了优化.</p><h2 id="图片懒加载"><a href="#图片懒加载" class="headerlink" title="图片懒加载"></a>图片懒加载</h2><blockquote><p>经过多次尝试, 懒加载虽然带来了访问速度的提升, 但即使加大了懒加载的范围, 还是能够看到Loading图, 总是会给人产生一种网速慢的感觉. 对于现在大家的网速来说, <strong>似乎体验的重要性要大于性能的重要性</strong>, 建议不开启懒加载.</p></blockquote><h3 id="预加载和懒加载"><a href="#预加载和懒加载" class="headerlink" title="预加载和懒加载"></a>预加载和懒加载</h3><blockquote><p>图片预加载：顾名思义，图片预加载就是在网页全部加载之前，提前加载图片。当用户需要查看时可直接从本地缓存中渲染，以提供给用户更好的体验，减少等待的时间。否则，如果一个页面的内容过于庞大，没有使用预加载技术的页面就会长时间的展现为一片空白，这样浏览者可能以为图片预览慢而没兴趣浏览，把网页关掉，这时，就需要图片预加载。当然这种做法实际上牺牲了服务器的性能换取了更好的用户体验。<br>图片懒加载（缓载）：延迟加载图片或符合某些条件时才加载某些图片。这样做的好处是减少不必要的访问数据库或延迟访问数据库的次数，因为每次访问数据库都是比较耗时的即只有真正使用该对象的数据时才会创建。懒加载的主要目的是作为服务器前端的优化，减少请求数或延迟请求数。</p></blockquote><p>预加载会在用户没看到图片之前, 就将图片显示加载好. 而懒加载恰恰相反, 当用户快要看到图片之前(或满足某个触发条件时), 才会进行加载. 图片是一种非常吃流量的内容, 所以当点入一篇新文章时, 预加载会导致瞬间流量过大, 用户等待的时间会增加. 其实完全没必要让图片在用户看不见的时候就去加载, 最好是用户即将看到图片之前, 再对图片进行加载. 这样可以减缓访问的压力, 提高用户体验.</p><h3 id="加入懒加载"><a href="#加入懒加载" class="headerlink" title="加入懒加载"></a>加入懒加载</h3><p>懒加载需要安装<a href="https://github.com/Troy-Yang/hexo-lazyload-image" target="_blank" rel="noopener">hexo-lazyload-image</a>插件. 在blog根目录命令行输入:</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> hexo-lazyload-image --save<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>然后在Hexo配置文件末尾加入:</p><pre><code>lazyload:  enable: true  # 是否开启图片懒加载  onlypost: false  # 是否只对文章的图片做懒加载  loadingImg: # eg ./images/loading.gif</code></pre><p>这里的<code>loadingImg</code>路径起始就从主题的<code>source</code>下开始算的.</p><p>最后执行<code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo s</code>就能看到懒加载效果了.</p><h3 id="Matery的bug"><a href="#Matery的bug" class="headerlink" title="Matery的bug"></a>Matery的bug</h3><p>针对Matery主题这个插件有两个小bug, 感谢原博主的解决.</p><ol><li><p>查看大图, 发现全是loading加载图:</p><p>原因是因为懒加载插件与 lightgallery 插件冲突. 在 <code>blog\themes\hexo-theme-matery\source\jsmatery.js</code>中，在 108 行左右添加以下代码</p><pre class="line-numbers language-js"><code class="language-js"><span class="token function">$</span><span class="token punctuation">(</span>document<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">find</span><span class="token punctuation">(</span><span class="token string">'img[data-original]'</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">each</span><span class="token punctuation">(</span><span class="token keyword">function</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token function">$</span><span class="token punctuation">(</span><span class="token keyword">this</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">parent</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">attr</span><span class="token punctuation">(</span><span class="token string">"href"</span><span class="token punctuation">,</span> <span class="token function">$</span><span class="token punctuation">(</span><span class="token keyword">this</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">attr</span><span class="token punctuation">(</span><span class="token string">"data-original"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p>点击首页 logo 不是跳转到首页, 而是查看 logo 图片:</p><p>修改<code>blog\themes\hexo-theme-matery\layout\ header.ejs</code>. </p><pre class="line-numbers language-js"><code class="language-js"><span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"brand-logo"</span><span class="token operator">></span>    <span class="token operator">&lt;</span>a href<span class="token operator">=</span><span class="token string">"&lt;%- url_for() %>"</span> <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"waves-effect waves-light"</span><span class="token operator">></span>         <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>theme<span class="token punctuation">.</span>logo <span class="token operator">!==</span> undefined <span class="token operator">&amp;&amp;</span> theme<span class="token punctuation">.</span>logo<span class="token punctuation">.</span>length <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span> <span class="token operator">%</span><span class="token operator">></span>         <span class="token operator">&lt;</span>img src<span class="token operator">=</span><span class="token string">"&lt;%= theme.logo %>"</span> <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"logo-img"</span> alt<span class="token operator">=</span><span class="token string">"LOGO"</span><span class="token operator">></span>         <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token punctuation">}</span> <span class="token operator">%</span><span class="token operator">></span>         <span class="token operator">&lt;</span>span <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"logo-span"</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> config<span class="token punctuation">.</span>title <span class="token operator">%</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>span<span class="token operator">></span>    <span class="token operator">&lt;</span><span class="token operator">/</span>a<span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>修改为:</p><pre class="line-numbers language-js"><code class="language-js"><span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"brand-logo"</span><span class="token operator">></span>    <span class="token operator">&lt;</span>a href<span class="token operator">=</span><span class="token string">"&lt;%- url_for() %>"</span> <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"waves-effect waves-light"</span><span class="token operator">></span>        <span class="token operator">&lt;</span>div<span class="token operator">></span>            <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>theme<span class="token punctuation">.</span>logo <span class="token operator">!==</span> undefined <span class="token operator">&amp;&amp;</span> theme<span class="token punctuation">.</span>logo<span class="token punctuation">.</span>length <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span> <span class="token operator">%</span><span class="token operator">></span>            <span class="token operator">&lt;</span>img src<span class="token operator">=</span><span class="token string">"&lt;%= theme.logo %>"</span> <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"logo-img"</span> alt<span class="token operator">=</span><span class="token string">"LOGO"</span><span class="token operator">></span>            <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token punctuation">}</span> <span class="token operator">%</span><span class="token operator">></span>            <span class="token operator">&lt;</span>span <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"logo-span"</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> config<span class="token punctuation">.</span>title <span class="token operator">%</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>span<span class="token operator">></span>        <span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span>    <span class="token operator">&lt;</span><span class="token operator">/</span>a<span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><h3 id="懒加载优化"><a href="#懒加载优化" class="headerlink" title="懒加载优化"></a>懒加载优化</h3><p>每次加载完后本地应该都会有图片缓存, 但是还是会显示loading的logo. 所以需要对插件进行修改, 提前显示出图片. 打开<code>blogtest\node_modules\hexo-lazyload-image\lib\simple-lazyload.js</code>, 第九行修改为:</p><pre class="line-numbers language-js"><code class="language-js"><span class="token operator">&amp;&amp;</span> rect<span class="token punctuation">.</span>top <span class="token operator">&lt;=</span> <span class="token punctuation">(</span>window<span class="token punctuation">.</span>innerHeight <span class="token operator">+</span> <span class="token number">360</span> <span class="token operator">||</span> document<span class="token punctuation">.</span>documentElement<span class="token punctuation">.</span>clientHeight <span class="token operator">+</span> <span class="token number">360</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>能够在图片前360个pix加载图片, 我这里试了试360效果比较好, 可以自行修改.</p><h2 id="代码压缩"><a href="#代码压缩" class="headerlink" title="代码压缩"></a>代码压缩</h2><p>其实Hexo生成的所有html, css, js全是有空行和空格的, 但是空格和空行是需要占用空间的. 这就是为什么js总是发布一个完整版和一个min版, <strong>min版是压缩后的</strong>, 不利于代码的阅读, 但是使用起来会比原版快. 我这里网络问题很大, cnpm安装这个库<strong>有各种权限问题</strong>, 所以就没有采用<code>gulp</code>的方式. 如果没有条件建议尝试<code>hexo-neat</code>, 比较简单快捷. 我没有对比过二者的压缩效率, 但后者对某些比较大的文件能够压缩<strong>38.13%</strong>左右的空间, 效果已经很不错了.</p><h3 id="gulp"><a href="#gulp" class="headerlink" title="gulp"></a>gulp</h3><p>利用gulp进行代码的压缩. 先在博客根目录下安装好<code>gulp</code>:</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># 全局安装gulp模块</span><span class="token function">npm</span> <span class="token function">install</span> gulp -g<span class="token comment" spellcheck="true"># 安装各种小功能模块  执行这步的时候，可能会提示权限的问题，最好以管理员模式执行</span><span class="token function">npm</span> <span class="token function">install</span> gulp gulp-htmlclean gulp-htmlmin gulp-minify-css gulp-uglify gulp-imagemin --save<span class="token comment" spellcheck="true"># 额外的功能模块</span><span class="token function">npm</span> <span class="token function">install</span> gulp-debug gulp-clean-css gulp-changed gulp-if gulp-plumber gulp-babel babel-preset-es2015 del @babel/core --save<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在博客根目录下创建<code>gulpfile.js</code>(必须是这个文件名), 并复制以下内容:</p><pre class="line-numbers language-js"><code class="language-js"><span class="token keyword">var</span> gulp <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"gulp"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">var</span> debug <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"gulp-debug"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">var</span> cleancss <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"gulp-clean-css"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//css压缩组件</span><span class="token keyword">var</span> uglify <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"gulp-uglify"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//js压缩组件</span><span class="token keyword">var</span> htmlmin <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"gulp-htmlmin"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//html压缩组件</span><span class="token keyword">var</span> htmlclean <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"gulp-htmlclean"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//html清理组件</span><span class="token keyword">var</span> imagemin <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"gulp-imagemin"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//图片压缩组件</span><span class="token keyword">var</span> changed <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"gulp-changed"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//文件更改校验组件</span><span class="token keyword">var</span> gulpif <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"gulp-if"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//任务 帮助调用组件</span><span class="token keyword">var</span> plumber <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"gulp-plumber"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//容错组件（发生错误不跳出任务，并报出错误内容）</span><span class="token keyword">var</span> isScriptAll <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//是否处理所有文件，(true|处理所有文件)(false|只处理有更改的文件)</span><span class="token keyword">var</span> isDebug <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//是否调试显示 编译通过的文件</span><span class="token keyword">var</span> gulpBabel <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"gulp-babel"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">var</span> es2015Preset <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"babel-preset-es2015"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">var</span> del <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"del"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">var</span> Hexo <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"hexo"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">var</span> hexo <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Hexo</span><span class="token punctuation">(</span>process<span class="token punctuation">.</span><span class="token function">cwd</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 初始化一个hexo对象</span><span class="token comment" spellcheck="true">// 清除public文件夹</span>gulp<span class="token punctuation">.</span><span class="token function">task</span><span class="token punctuation">(</span><span class="token string">"clean"</span><span class="token punctuation">,</span> <span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">return</span> <span class="token function">del</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"public/**/*"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 下面几个跟hexo有关的操作，主要通过hexo.call()去执行，注意return</span><span class="token comment" spellcheck="true">// 创建静态页面 （等同 hexo generate）</span>gulp<span class="token punctuation">.</span><span class="token function">task</span><span class="token punctuation">(</span><span class="token string">"generate"</span><span class="token punctuation">,</span> <span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">return</span> hexo<span class="token punctuation">.</span><span class="token function">init</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">then</span><span class="token punctuation">(</span><span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">return</span> hexo            <span class="token punctuation">.</span><span class="token function">call</span><span class="token punctuation">(</span><span class="token string">"generate"</span><span class="token punctuation">,</span> <span class="token punctuation">{</span>                watch<span class="token punctuation">:</span> <span class="token boolean">false</span>            <span class="token punctuation">}</span><span class="token punctuation">)</span>            <span class="token punctuation">.</span><span class="token function">then</span><span class="token punctuation">(</span><span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">return</span> hexo<span class="token punctuation">.</span><span class="token function">exit</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span><span class="token punctuation">)</span>            <span class="token punctuation">.</span><span class="token keyword">catch</span><span class="token punctuation">(</span><span class="token keyword">function</span> <span class="token punctuation">(</span>err<span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">return</span> hexo<span class="token punctuation">.</span><span class="token function">exit</span><span class="token punctuation">(</span>err<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 启动Hexo服务器</span>gulp<span class="token punctuation">.</span><span class="token function">task</span><span class="token punctuation">(</span><span class="token string">"server"</span><span class="token punctuation">,</span> <span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">return</span> hexo        <span class="token punctuation">.</span><span class="token function">init</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">then</span><span class="token punctuation">(</span><span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">return</span> hexo<span class="token punctuation">.</span><span class="token function">call</span><span class="token punctuation">(</span><span class="token string">"server"</span><span class="token punctuation">,</span> <span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token keyword">catch</span><span class="token punctuation">(</span><span class="token keyword">function</span> <span class="token punctuation">(</span>err<span class="token punctuation">)</span> <span class="token punctuation">{</span>            console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span>err<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 部署到服务器</span>gulp<span class="token punctuation">.</span><span class="token function">task</span><span class="token punctuation">(</span><span class="token string">"deploy"</span><span class="token punctuation">,</span> <span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">return</span> hexo<span class="token punctuation">.</span><span class="token function">init</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">then</span><span class="token punctuation">(</span><span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">return</span> hexo            <span class="token punctuation">.</span><span class="token function">call</span><span class="token punctuation">(</span><span class="token string">"deploy"</span><span class="token punctuation">,</span> <span class="token punctuation">{</span>                watch<span class="token punctuation">:</span> <span class="token boolean">false</span>            <span class="token punctuation">}</span><span class="token punctuation">)</span>            <span class="token punctuation">.</span><span class="token function">then</span><span class="token punctuation">(</span><span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">return</span> hexo<span class="token punctuation">.</span><span class="token function">exit</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span><span class="token punctuation">)</span>            <span class="token punctuation">.</span><span class="token keyword">catch</span><span class="token punctuation">(</span><span class="token keyword">function</span> <span class="token punctuation">(</span>err<span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">return</span> hexo<span class="token punctuation">.</span><span class="token function">exit</span><span class="token punctuation">(</span>err<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 压缩public目录下的js文件</span>gulp<span class="token punctuation">.</span><span class="token function">task</span><span class="token punctuation">(</span><span class="token string">"compressJs"</span><span class="token punctuation">,</span> <span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">return</span> gulp        <span class="token punctuation">.</span><span class="token function">src</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"./public/**/*.js"</span><span class="token punctuation">,</span> <span class="token string">"!./public/libs/**"</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span><span class="token operator">/</span>排除的js        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">gulpif</span><span class="token punctuation">(</span><span class="token operator">!</span>isScriptAll<span class="token punctuation">,</span> <span class="token function">changed</span><span class="token punctuation">(</span><span class="token string">"./public"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">gulpif</span><span class="token punctuation">(</span>isDebug<span class="token punctuation">,</span> <span class="token function">debug</span><span class="token punctuation">(</span><span class="token punctuation">{</span> title<span class="token punctuation">:</span> <span class="token string">"Compress JS:"</span> <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">plumber</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span>            <span class="token function">gulpBabel</span><span class="token punctuation">(</span><span class="token punctuation">{</span>                presets<span class="token punctuation">:</span> <span class="token punctuation">[</span>es2015Preset<span class="token punctuation">]</span> <span class="token operator">/</span><span class="token operator">/</span> es5检查机制            <span class="token punctuation">}</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">uglify</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span><span class="token operator">/</span><span class="token function">调用压缩组件方法uglify</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>对合并的文件进行压缩        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span>gulp<span class="token punctuation">.</span><span class="token function">dest</span><span class="token punctuation">(</span><span class="token string">"./public"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token operator">/</span><span class="token operator">/</span>输出到目标目录<span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token operator">/</span><span class="token operator">/</span> 压缩<span class="token keyword">public</span>目录下的css文件gulp<span class="token punctuation">.</span><span class="token function">task</span><span class="token punctuation">(</span><span class="token string">"compressCss"</span><span class="token punctuation">,</span> <span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">var</span> option <span class="token operator">=</span> <span class="token punctuation">{</span>        rebase<span class="token punctuation">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span>        <span class="token operator">/</span><span class="token operator">/</span>advanced<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token operator">/</span><span class="token operator">/</span>类型：Boolean 默认：<span class="token boolean">true</span> <span class="token punctuation">[</span>是否开启高级优化（合并选择器等）<span class="token punctuation">]</span>        compatibility<span class="token punctuation">:</span> <span class="token string">"ie7"</span> <span class="token operator">/</span><span class="token operator">/</span>保留ie7及以下兼容写法 类型：String 默认：<span class="token string">''</span>or<span class="token string">'*'</span> <span class="token punctuation">[</span>启用兼容模式； <span class="token string">'ie7'</span>：IE7兼容模式，<span class="token string">'ie8'</span>：IE8兼容模式，<span class="token string">'*'</span>：IE9<span class="token operator">+</span>兼容模式<span class="token punctuation">]</span>        <span class="token operator">/</span><span class="token operator">/</span>keepBreaks<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token operator">/</span><span class="token operator">/</span>类型：Boolean 默认：<span class="token boolean">false</span> <span class="token punctuation">[</span>是否保留换行<span class="token punctuation">]</span>        <span class="token operator">/</span><span class="token operator">/</span>keepSpecialComments<span class="token punctuation">:</span> <span class="token string">'*'</span> <span class="token operator">/</span><span class="token operator">/</span>保留所有特殊前缀 当你用autoprefixer生成的浏览器前缀，如果不加这个参数，有可能将会删除你的部分前缀    <span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> gulp        <span class="token punctuation">.</span><span class="token function">src</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"./public/**/*.css"</span><span class="token punctuation">,</span> <span class="token string">"!./public/**/*.min.css"</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">//排除的css</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">gulpif</span><span class="token punctuation">(</span><span class="token operator">!</span>isScriptAll<span class="token punctuation">,</span> <span class="token function">changed</span><span class="token punctuation">(</span><span class="token string">"./public"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">gulpif</span><span class="token punctuation">(</span>isDebug<span class="token punctuation">,</span> <span class="token function">debug</span><span class="token punctuation">(</span><span class="token punctuation">{</span> title<span class="token punctuation">:</span> <span class="token string">"Compress CSS:"</span> <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">plumber</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">cleancss</span><span class="token punctuation">(</span>option<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span>gulp<span class="token punctuation">.</span><span class="token function">dest</span><span class="token punctuation">(</span><span class="token string">"./public"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 压缩public目录下的html文件</span>gulp<span class="token punctuation">.</span><span class="token function">task</span><span class="token punctuation">(</span><span class="token string">"compressHtml"</span><span class="token punctuation">,</span> <span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">var</span> cleanOptions <span class="token operator">=</span> <span class="token punctuation">{</span>        protect<span class="token punctuation">:</span> <span class="token regex">/&lt;\!--%fooTemplate\b.*?%-->/g</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">//忽略处理</span>        unprotect<span class="token punctuation">:</span> <span class="token operator">/</span><span class="token operator">&lt;</span>script <span class="token punctuation">[</span><span class="token operator">^</span><span class="token operator">></span><span class="token punctuation">]</span><span class="token operator">*</span>\btype<span class="token operator">=</span><span class="token string">"text\/x-handlebars-template"</span><span class="token punctuation">[</span>\s\S<span class="token punctuation">]</span><span class="token operator">+</span><span class="token operator">?</span><span class="token operator">&lt;</span>\<span class="token operator">/</span>script<span class="token operator">></span><span class="token operator">/</span>gi <span class="token comment" spellcheck="true">//特殊处理</span>    <span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token keyword">var</span> minOption <span class="token operator">=</span> <span class="token punctuation">{</span>        collapseWhitespace<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">//压缩HTML</span>        collapseBooleanAttributes<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">//省略布尔属性的值 &lt;input checked="true"/> ==> &lt;input /></span>        removeEmptyAttributes<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">//删除所有空格作属性值 &lt;input id="" /> ==> &lt;input /></span>        removeScriptTypeAttributes<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">//删除&lt;script>的type="text/javascript"</span>        removeStyleLinkTypeAttributes<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">//删除&lt;style>和&lt;link>的type="text/css"</span>        removeComments<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">//清除HTML注释</span>        minifyJS<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">//压缩页面JS</span>        minifyCSS<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">//压缩页面CSS</span>        minifyURLs<span class="token punctuation">:</span> <span class="token boolean">true</span> <span class="token comment" spellcheck="true">//替换页面URL</span>    <span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> gulp        <span class="token punctuation">.</span><span class="token function">src</span><span class="token punctuation">(</span><span class="token string">"./public/**/*.html"</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">gulpif</span><span class="token punctuation">(</span>isDebug<span class="token punctuation">,</span> <span class="token function">debug</span><span class="token punctuation">(</span><span class="token punctuation">{</span> title<span class="token punctuation">:</span> <span class="token string">"Compress HTML:"</span> <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">plumber</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">htmlclean</span><span class="token punctuation">(</span>cleanOptions<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">htmlmin</span><span class="token punctuation">(</span>minOption<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span>gulp<span class="token punctuation">.</span><span class="token function">dest</span><span class="token punctuation">(</span><span class="token string">"./public"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 压缩 public/medias 目录内图片</span>gulp<span class="token punctuation">.</span><span class="token function">task</span><span class="token punctuation">(</span><span class="token string">"compressImage"</span><span class="token punctuation">,</span> <span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">var</span> option <span class="token operator">=</span> <span class="token punctuation">{</span>        optimizationLevel<span class="token punctuation">:</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">//类型：Number 默认：3 取值范围：0-7（优化等级）</span>        progressive<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">//类型：Boolean 默认：false 无损压缩jpg图片</span>        interlaced<span class="token punctuation">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">//类型：Boolean 默认：false 隔行扫描gif进行渲染</span>        multipass<span class="token punctuation">:</span> <span class="token boolean">false</span> <span class="token comment" spellcheck="true">//类型：Boolean 默认：false 多次优化svg直到完全优化</span>    <span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> gulp        <span class="token punctuation">.</span><span class="token function">src</span><span class="token punctuation">(</span><span class="token string">"./public/medias/**/*.*"</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">gulpif</span><span class="token punctuation">(</span><span class="token operator">!</span>isScriptAll<span class="token punctuation">,</span> <span class="token function">changed</span><span class="token punctuation">(</span><span class="token string">"./public/medias"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">gulpif</span><span class="token punctuation">(</span>isDebug<span class="token punctuation">,</span> <span class="token function">debug</span><span class="token punctuation">(</span><span class="token punctuation">{</span> title<span class="token punctuation">:</span> <span class="token string">"Compress Images:"</span> <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">plumber</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span><span class="token function">imagemin</span><span class="token punctuation">(</span>option<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span><span class="token function">pipe</span><span class="token punctuation">(</span>gulp<span class="token punctuation">.</span><span class="token function">dest</span><span class="token punctuation">(</span><span class="token string">"./public"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 执行顺序： 清除public目录 -> 产生原始博客内容 -> 执行压缩混淆 -> 部署到服务器</span>gulp<span class="token punctuation">.</span><span class="token function">task</span><span class="token punctuation">(</span>    <span class="token string">"build"</span><span class="token punctuation">,</span>    gulp<span class="token punctuation">.</span><span class="token function">series</span><span class="token punctuation">(</span>        <span class="token string">"clean"</span><span class="token punctuation">,</span>        <span class="token string">"generate"</span><span class="token punctuation">,</span>        <span class="token string">"compressHtml"</span><span class="token punctuation">,</span>        <span class="token string">"compressCss"</span><span class="token punctuation">,</span>        <span class="token string">"compressJs"</span><span class="token punctuation">,</span>        <span class="token string">"compressImage"</span><span class="token punctuation">,</span>        gulp<span class="token punctuation">.</span><span class="token function">parallel</span><span class="token punctuation">(</span><span class="token string">"deploy"</span><span class="token punctuation">)</span>    <span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 默认任务</span>gulp<span class="token punctuation">.</span><span class="token function">task</span><span class="token punctuation">(</span>    <span class="token string">"default"</span><span class="token punctuation">,</span>    gulp<span class="token punctuation">.</span><span class="token function">series</span><span class="token punctuation">(</span>        <span class="token string">"clean"</span><span class="token punctuation">,</span>        <span class="token string">"generate"</span><span class="token punctuation">,</span>        gulp<span class="token punctuation">.</span><span class="token function">parallel</span><span class="token punctuation">(</span><span class="token string">"compressHtml"</span><span class="token punctuation">,</span> <span class="token string">"compressCss"</span><span class="token punctuation">,</span> <span class="token string">"compressJs"</span><span class="token punctuation">,</span><span class="token string">"compressImage"</span><span class="token punctuation">)</span>    <span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//Gulp4最大的一个改变就是gulp.task函数现在只支持两个参数，分别是任务名和运行任务的函数</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>最后在根目录下命令行输入<code>gulp</code>或者<code>gulp default</code>, 相当于输入了<code>hexo cl &amp;&amp; hexo g</code>, 然后再压缩图片和代码.</p><h3 id="hexo-neat"><a href="#hexo-neat" class="headerlink" title="hexo-neat"></a>hexo-neat</h3><p>使用<a href="https://github.com/rozbo/hexo-neat" target="_blank" rel="noopener">hexo-neat</a>更为简单, 美中不足的是这个插件有俩小bug:</p><ul><li>压缩<code>.md</code>文件会使 markdown 语法的代码块消失.</li><li>会删除全角空格.</li></ul><p>但是它避免了国内<code>npm</code>的使用问题. 所以我推荐这种方式.</p><p>在博客根目录命令行输入:</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> hexo-neat --save<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>然后在Hexo配置文件末尾加入(已针对matery的bug优化):</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token comment" spellcheck="true">#hexo-neat 优化提速插件（去掉HTML、css、js的blank字符）</span><span class="token key atrule">neat_enable</span><span class="token punctuation">:</span> <span class="token boolean important">true</span><span class="token key atrule">neat_html</span><span class="token punctuation">:</span>  <span class="token key atrule">enable</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">exclude</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token string">'**/*.md'</span><span class="token key atrule">neat_css</span><span class="token punctuation">:</span>  <span class="token key atrule">enable</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">exclude</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token string">'**/*.min.css'</span><span class="token key atrule">neat_js</span><span class="token punctuation">:</span>  <span class="token key atrule">enable</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">mangle</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">output</span><span class="token punctuation">:</span>  <span class="token key atrule">compress</span><span class="token punctuation">:</span>  <span class="token key atrule">exclude</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token string">'**/*.min.js'</span>    <span class="token punctuation">-</span> <span class="token string">'**/**/instantpage.js'</span>    <span class="token punctuation">-</span> <span class="token string">'**/matery.js'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="CDN加速"><a href="#CDN加速" class="headerlink" title="CDN加速"></a>CDN加速</h2><p>因为Github在国内访问速度比较慢, 所以用CDN加速来优化网站访问速度. jsDelivr + Github就能免费实现博客网站的访问加速.</p><blockquote><p>CDN 的全称是 Content Delivery Network，即内容分发网络。CDN 是构建在网络之上的内容分发网络，依靠部署在各地的边缘服务器，通过中心平台的负载均衡、内容分发、调度等功能模块，使用户就近获取所需内容，降低网络拥塞，提高用户访问响应速度和命中率。</p></blockquote><p>在Matery主题的配置文件末尾已经有相应的配置:</p><pre class="line-numbers language-ejs"><code class="language-ejs">jsDelivr:  url: #https://cdn.jsdelivr.net/gh/<github用户名>/<github仓库名><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>配置文件中有关于<code>jsDelivr</code>相应的注释, 即在内容没有被部署到github仓库之前, 是没法使用cdn在本地查看效果的, 只有部署之后才能看到效果.</p><blockquote><p>CDN访问加速<br>第一次使用本功能，一定要先配置url，再<code>hexo cl &amp;&amp; hexo g &amp;&amp; hexo d</code>部署到GitHub的仓库，注意！必须是GitHub的仓库！<br>如果必须要使用国内的coding或者gitee，可以采用双部署，同时将网站部署到两个仓库（其中一个必须是GitHub的仓库）<br>URL配置规则（例子如下）： <a href="https://cdn.jsdelivr.net/gh/你的GitHub用户名/你的仓库名" target="_blank" rel="noopener">https://cdn.jsdelivr.net/gh/你的GitHub用户名/你的仓库名</a><br>如果想关闭此功能，将 url地址 注释或删除即可！</p><p>注：配置了此项，就代表着本地调试的时候，网站依然会去GitHub请求资源（原来的资源），本地调试的时候记得将 此项配置 注释或者删除掉.</p></blockquote><p><strong>jsDelivr 不支持加载超过 20M 的资源.</strong></p>]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> gulp </tag>
            
            <tag> 图片懒加载 </tag>
            
            <tag> CDN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CSS样式覆盖</title>
      <link href="/posts/42309.html"/>
      <url>/posts/42309.html</url>
      
        <content type="html"><![CDATA[<h1 id="CSS样式覆盖"><a href="#CSS样式覆盖" class="headerlink" title="CSS样式覆盖"></a>CSS样式覆盖</h1><h2 id="覆盖"><a href="#覆盖" class="headerlink" title="覆盖"></a>覆盖</h2><p>今天下午想给博客配个在线Markdown的小工具, 按照教程搭配完了, 结果发现这行高简直欺骗感情. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200725212325.png" style="zoom:50%;" /><p>明明<a href="https://hehung.top/MyMarkdown/" target="_blank" rel="noopener">这位博主</a>的在线Markdown和editormd官方的行距就是正常的, 怎么到我这就开始变宽了呢?</p><p>在一轮乱搞<code>editormd.css</code>之后, 发现仍然不能改变前端显示的效果. 用Chrome按下F12看, 发现右侧很多css的样式都被线划掉了.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200725213113.png" style="zoom:67%;" /><p>按过F12查看元素实际大小, 发现就是padding和margin出现了问题, 导致行距很宽. 我上网查了很久, 不知道到底是哪个css覆盖了它. </p><h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>我把那位博主的css文件都下载下来, 发现<code>matery.css</code>的大小和我本地的大小居然不一样! 比对后果然发现了端倪. 他将直接将<code>matery.css</code>中同名的<code>pre</code>中<code>margin</code>和<code>padding</code>后的<code>!important</code>注释掉了.</p><p>其实只要<strong>按照关键词进行查找</strong>, 找到同名的所有样式, 没被线划掉的就是没被覆盖的样式, 根据后面的文件位置就能快速定位. 样式被覆盖的原因是<strong>优先级不够</strong>.</p><h3 id="优先级规则"><a href="#优先级规则" class="headerlink" title="优先级规则"></a>优先级规则</h3><p>内容来自<a href="https://blog.csdn.net/cuishizun/article/details/81979809" target="_blank" rel="noopener">CSDN</a>, 我这里只遇到过前三个.</p><ul><li>优先级就近原则, 同权重的样式谁离标签内容近谁就优先级高.</li><li>载入样式以最后载入的定位为准(覆盖)</li><li>!important优先级最高.</li><li>按照类别进行区分:<ul><li>内联, 如style=””——1000</li><li>id, 如#content——100</li><li>类、伪类和属性选择器, 如.content——10，</li><li>标签选择器和伪元素选择器, 如div, p——1</li><li>通配符、子选择器和相邻选择器, 如*, &gt;, +——0</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 前端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CSS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>嵌入在线Markdown编辑器</title>
      <link href="/posts/46204.html"/>
      <url>/posts/46204.html</url>
      
        <content type="html"><![CDATA[<h1 id="嵌入在线Markdown编辑器"><a href="#嵌入在线Markdown编辑器" class="headerlink" title="嵌入在线Markdown编辑器"></a>嵌入在线Markdown编辑器</h1><p>因为Hexo是经过静态编译的, 所以其实不太需要在线编辑的Markdown, 因为平常都用<code>Typora</code>. 但是这仍然可以作为一个小功能放在自己的网站中. 在它的文档还有其他功能, 比如做在线代码编译器等.</p><h2 id="下载Markdown"><a href="#下载Markdown" class="headerlink" title="下载Markdown"></a>下载Markdown</h2><p>这里<a href="http://editor.md.ipandao.com/" target="_blank" rel="noopener">这里</a>找到下载并安装, 下载<code>editormd</code>到本地并解压. 因为写起来比较麻烦所以我就把解压后的点去掉了.</p><h2 id="添加文件和新页面"><a href="#添加文件和新页面" class="headerlink" title="添加文件和新页面"></a>添加文件和新页面</h2><p>将刚解压的<code>editormd</code>整体放到<code>blog\themes\hexo-theme-matery\source\libs</code>下.</p><p>在blog文件夹下命令行敲入:</p><pre class="line-numbers language-bash"><code class="language-bash">hexo new page markdown<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>并为<code>blog\source\markdown\index.md</code>添加类别和布局:</p><pre><code>---title: markdowndate: 2020-07-25 16:41:26type: &quot;markdown&quot;layout: &quot;markdown&quot;---</code></pre><h2 id="添加css和js路径"><a href="#添加css和js路径" class="headerlink" title="添加css和js路径"></a>添加css和js路径</h2><p>在主题配置文件<code>_config.yml</code>中的<code>libs</code>找到<code>css</code>和<code>js</code>, 将新的相对文件路径加到最后.</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">libs</span><span class="token punctuation">:</span>  <span class="token key atrule">css</span><span class="token punctuation">:</span>      <span class="token comment" spellcheck="true"># ...</span>    editormd<span class="token punctuation">:</span>/libs/editormd/css/editormd.css  <span class="token key atrule">js</span><span class="token punctuation">:</span>      <span class="token comment" spellcheck="true"># ...</span>    <span class="token key atrule">editormd</span><span class="token punctuation">:</span> /libs/editormd/editormd.min.js<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="添加ejs文件"><a href="#添加ejs文件" class="headerlink" title="添加ejs文件"></a>添加ejs文件</h2><p>虽然刚才已经新起一个页面, 但是还没有为这个页面提供样式和内容.</p><p>找到<code>blog\themes\hexo-theme-matery\layout</code>, 创建一个与刚才新建页面的<code>layout</code>同名的<code>markdown.ejs</code>文件. 内容如下:</p><pre class="line-numbers language-html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>link</span> <span class="token attr-name">rel</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>stylesheet<span class="token punctuation">"</span></span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>&lt;%- theme.libs.css.editormd %<span class="token punctuation">></span><span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>style</span> <span class="token attr-name">type</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>text/css<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token style language-css">    <span class="token comment" spellcheck="true">/* don't remove. */</span>    <span class="token selector"><span class="token class">.page-cover</span> </span><span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">/* height: 75vh; */</span>        <span class="token property">height</span><span class="token punctuation">:</span> <span class="token number">960</span>px<span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token selector"><span class="token class">.editormd</span> </span><span class="token punctuation">{</span>        <span class="token property">top</span><span class="token punctuation">:</span> <span class="token number">22</span>px<span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token selector">pre </span><span class="token punctuation">{</span>        <span class="token property">padding</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token selector"><span class="token class">.editormd-menu</span>>li<span class="token class">.divider</span> </span><span class="token punctuation">{</span>        <span class="token property">overflow</span><span class="token punctuation">:</span> inherit<span class="token punctuation">;</span>        <span class="token property">padding</span><span class="token punctuation">:</span> <span class="token number">5</span>px <span class="token number">0</span>px<span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token selector">header <span class="token class">.nav-transparent</span> </span><span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">/*修改导航栏颜色*/</span>        <span class="token property">background-image</span><span class="token punctuation">:</span> <span class="token function">linear-gradient</span><span class="token punctuation">(</span>to right, <span class="token hexcode">#16b182</span> <span class="token number">0%</span>, <span class="token hexcode">#058044</span> <span class="token number">100%</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token selector"><span class="token class">.editormd-form</span> input<span class="token attribute">[type="text"]</span>,    <span class="token class">.editormd-form</span> input<span class="token attribute">[type="number"]</span> </span><span class="token punctuation">{</span>        <span class="token property">height</span><span class="token punctuation">:</span> <span class="token number">15</span>px<span class="token punctuation">;</span>        <span class="token property">margin</span><span class="token punctuation">:</span> <span class="token number">0</span>px<span class="token punctuation">;</span>        <span class="token property">font-size</span><span class="token punctuation">:</span> <span class="token number">14</span>px<span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token selector"><span class="token class">.editormd-form</span> input<span class="token attribute">[type="text"]</span> </span><span class="token punctuation">{</span>        <span class="token property">display</span><span class="token punctuation">:</span> inline-block<span class="token punctuation">;</span>        <span class="token property">width</span><span class="token punctuation">:</span> <span class="token number">246</span>px<span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token selector"><span class="token class">.editormd-dialog-container</span> label </span><span class="token punctuation">{</span>        <span class="token property">font-size</span><span class="token punctuation">:</span> <span class="token number">14</span>px<span class="token punctuation">;</span>        <span class="token property">color</span><span class="token punctuation">:</span> <span class="token hexcode">#444</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token selector"><span class="token class">.editormd-dialog-container</span> select </span><span class="token punctuation">{</span>        <span class="token property">display</span><span class="token punctuation">:</span> inline-block<span class="token punctuation">;</span>        <span class="token property">background-color</span><span class="token punctuation">:</span> <span class="token function">rgba</span><span class="token punctuation">(</span><span class="token number">255</span>, <span class="token number">255</span>, <span class="token number">255</span>, <span class="token number">0.9</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token property">width</span><span class="token punctuation">:</span> <span class="token number">182</span>px<span class="token punctuation">;</span>        <span class="token property">border-radius</span><span class="token punctuation">:</span> <span class="token number">2</span>px<span class="token punctuation">;</span>        <span class="token property">height</span><span class="token punctuation">:</span> <span class="token number">25</span>px<span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token selector"><span class="token class">.navbar-fixed</span> nav </span><span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">/* 固定导航栏不动 */</span>        <span class="token property">position</span><span class="token punctuation">:</span> static<span class="token punctuation">;</span>    <span class="token punctuation">}</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>style</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>pd-header page-cover<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>editormd<span class="token punctuation">"</span></span> <span class="token attr-name">id</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>my-editormd<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>textarea</span><span class="token style-attr language-css"><span class="token attr-name"> <span class="token attr-name">style</span></span><span class="token punctuation">="</span><span class="token attr-value"><span class="token property">display</span><span class="token punctuation">:</span>none<span class="token punctuation">;</span></span><span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>textarea</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>&lt;script src="<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>%-</span> <span class="token attr-name">theme.libs.js.jquery</span> <span class="token attr-name">%</span><span class="token punctuation">></span></span><span class="token script language-javascript">"<span class="token operator">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>script</span><span class="token punctuation">></span></span>&lt;script src="<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>%-</span> <span class="token attr-name">theme.libs.js.editormd</span> <span class="token attr-name">%</span><span class="token punctuation">></span></span><span class="token script language-javascript">"<span class="token operator">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>script</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>script</span> <span class="token attr-name">type</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>text/javascript<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token script language-javascript">    <span class="token keyword">var</span> myEditor<span class="token punctuation">;</span>    <span class="token function">$</span><span class="token punctuation">(</span><span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        myEditor <span class="token operator">=</span> <span class="token function">editormd</span><span class="token punctuation">(</span><span class="token string">"my-editormd"</span><span class="token punctuation">,</span> <span class="token punctuation">{</span>            width<span class="token punctuation">:</span> <span class="token string">"98%"</span><span class="token punctuation">,</span>            height<span class="token punctuation">:</span> <span class="token number">840</span><span class="token punctuation">,</span>            syncScrolling<span class="token punctuation">:</span> <span class="token string">"single"</span><span class="token punctuation">,</span>            path<span class="token punctuation">:</span> <span class="token string">"/libs/editormd/lib/"</span><span class="token punctuation">,</span>            <span class="token comment" spellcheck="true">// theme: "dark",</span>            <span class="token comment" spellcheck="true">//  previewTheme: "dark",</span>            <span class="token comment" spellcheck="true">//  editorTheme: "pastel-on-dark",</span>            markdown<span class="token punctuation">:</span> <span class="token string">''</span><span class="token punctuation">,</span>            codeFold<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span>            searchReplace<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span>            emoji<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span>            taskList<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span>            tocm<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span>         <span class="token comment" spellcheck="true">// Using [TOCM]</span>            tex<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span>                   <span class="token comment" spellcheck="true">// 开启科学公式TeX语言支持，默认关闭</span>            flowChart<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span>             <span class="token comment" spellcheck="true">// 开启流程图支持，默认关闭</span>            sequenceDiagram<span class="token punctuation">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span>       <span class="token comment" spellcheck="true">// 开启时序/序列图支持，默认关闭,</span>            htmlDecode<span class="token punctuation">:</span> <span class="token string">"style,script,iframe|on*"</span><span class="token punctuation">,</span>            <span class="token comment" spellcheck="true">// 开启 HTML 标签解析，为了安全性，默认不开启   </span>        <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>script</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果使用<code>Matery1.31</code>出现了行距很大的问题, 是出现了css的样式覆盖, 可以参考之前的博客. 直接将<code>matery.css</code>中同名的<code>pre</code>中<code>margin</code>和<code>padding</code>后的<code>!important</code>注释掉即可.</p>]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Markdown </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据库常见问题整理</title>
      <link href="/posts/64558.html"/>
      <url>/posts/64558.html</url>
      
        <content type="html"><![CDATA[<h1 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h1><p>总体来说, 出现的频率很高但是问题问的不是很深, 并且内容少.</p><h2 id="数据库系统概述"><a href="#数据库系统概述" class="headerlink" title="数据库系统概述"></a>数据库系统概述</h2><p>数据管理技术的发展主要经历<strong>三个阶段</strong>:人工管理阶段, 文件系统阶段, 数据库系统阶段. </p><p><strong>数据库的完整性</strong>: 数据库的完整性是指防止数据库中存在不正确的数据. </p><h3 id="数据库系统结构"><a href="#数据库系统结构" class="headerlink" title="数据库系统结构"></a>数据库系统结构</h3><p><strong>数据库系统结构</strong>: 外模式, 概念模式(模式), 内模式</p><p><strong>物理数据独立性</strong>: 更改模式/内模式映像</p><p><strong>逻辑数据独立性</strong>: 更改外模式/模式映像</p><p><strong>数据处理三阶段</strong>: 概念设计, 逻辑设计, 物理设计.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/DB/三级模式两级映像.png" style="zoom:50%;" /><p><strong>概念模型(E - R模型)</strong>: 信息模型, 语言描述. </p><p><strong>E - R模型三要素</strong>: 实体, 属性, 联系(一对一, 一对多, 多对多)</p><p><strong>逻辑模型</strong>: 数据的逻辑存储方式, 比如层次模型(有向树), 网状模型(有向图), 关系模型(二维表), 面向对象模型等.</p><p><strong>物理模型</strong>: 物理存储介质上怎么存的.</p><h2 id="关系数据库的基本理论"><a href="#关系数据库的基本理论" class="headerlink" title="关系数据库的基本理论"></a>关系数据库的基本理论</h2><p>关系模式, 关系子模式, 存储模式分别与外模式, 模式, 内模式相对应.</p><h3 id="键"><a href="#键" class="headerlink" title="键"></a>键</h3><p><strong>超键</strong>: 能唯一标识元组的属性或属性组称为关系的超键.</p><p><strong>候选键</strong>: 如果一个属性组能唯一标识元组, 且又不含有多余的属性, 那么这个属性组称为关系的候选键.</p><p><strong>主键</strong>: 若一个关系中有多个候选键, 则选其中的一个为关系的主键.</p><p><strong>外键</strong>: 另一个关系的参照关系.</p><p>范围: 超键 &gt; 候选键 &gt; 主键.</p><h3 id="关系的完整性约束"><a href="#关系的完整性约束" class="headerlink" title="关系的完整性约束"></a>关系的完整性约束</h3><p><strong>实体完整性</strong>: 主属性不为空.</p><p><strong>参照完整性</strong>: 关系之间的引用, 参照关系的外键必须是被参照关系的主键或空.</p><p><strong>用户自定义完整性</strong>: 用户自己定义的完整性.</p><h3 id="关系代数"><a href="#关系代数" class="headerlink" title="关系代数"></a>关系代数</h3><h4 id="集合运算"><a href="#集合运算" class="headerlink" title="集合运算"></a>集合运算</h4><p>交, 并, 差, 笛卡尔积.</p><p>笛卡尔积: 互相组合, 表A有3个元组, 表B有2个元组, 那么$A \times B$为2x3个元组.</p><h4 id="关系运算"><a href="#关系运算" class="headerlink" title="关系运算"></a>关系运算</h4><p>选择, 投影, 连接, 除(没学)</p><p><strong>选择</strong> $\sigma$:  $\sigma_{condition}(R)$ 表示对表R进行condition的条件筛选. 根据条件选若干元组组成新的关系.</p><p><strong>投影</strong> $\pi$: $\pi_A(R)$ 表示从R表中选名为A的列, 并去除重复元组.</p><p><strong>连接</strong> $A\underset{C\theta D}⋈B$ : 连接符号下方添加C和D的关系条件, 表示从连接的结果中选出符合条件的元组.</p><p><strong>自然连接</strong>$A⋈B$: 从两表中取相同属性列等值的元组, 并去掉重复列. </p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/DB/%E8%87%AA%E7%84%B6%E8%BF%9E%E6%8E%A5.png" alt=""></p><p><strong>自然连接和等值连接的区别?</strong></p><p>自然连接中相等的分量必须是<strong>相同的属性组</strong>, 并且要在结果中<strong>去掉重复</strong>的属性, 而等值连接则不必.</p><p><strong>外连接是什么?</strong></p><p>外连接是指两个表在进行操作时, 不仅返回符合连接和查询条件的元组, 还返回不符合条件的一些元组;<br>左外连接是指返回左表中仅符合连接条件不符合查询条件的元组;<br>右外连接是指返回右表中仅符合连接条件不符合查询条件的元组;<br>全外连接是左外连接和右外连接去掉重复项的元组集并集. </p><h3 id="关系模式规范化"><a href="#关系模式规范化" class="headerlink" title="关系模式规范化"></a>关系模式规范化</h3><h4 id="函数依赖"><a href="#函数依赖" class="headerlink" title="函数依赖"></a>函数依赖</h4><p><strong>函数依赖</strong>: 对于X的每一个值, Y都有唯一值与之对应. 即$x \rightarrow y$ 称y函数依赖于x, 即y被x决定. 也称为x函数决定y.</p><p><strong>完全函数依赖</strong>: 依赖关系中的键没有冗余属性.</p><p><strong>部分函数依赖</strong>: 有冗余属性.</p><p><strong>传递函数依赖</strong>: 依赖之间存在传递关系.</p><h4 id="范式"><a href="#范式" class="headerlink" title="范式"></a>范式</h4><p><strong>第一范式(1NF)</strong>: 对于关系模式所有属性都是不可分的基本数据项.</p><p><strong>第二范式(2NF)</strong>: 是第一范式, 且每个非主属性都完全依赖<strong>候选键</strong>(这个时候可以是传递依赖).</p><p><strong>第三范式(3NF)</strong>: 是第二范式, 且每个非主属性也不传递依赖于候选键(属性不能传递依赖于主属性).</p><h2 id="数据库设计"><a href="#数据库设计" class="headerlink" title="数据库设计"></a>数据库设计</h2><h3 id="ER图向关系模式转换"><a href="#ER图向关系模式转换" class="headerlink" title="ER图向关系模式转换"></a>ER图向关系模式转换</h3><p><strong>一对一</strong>: 直接合并成一张表.</p><p><strong>一对多</strong>: 合并成一张表, 合并到多的那端.</p><p><strong>多对多</strong>: 将两张表的主键拿出来单独形成一张表, 并储存与该关系相关的属性.</p><p><strong>一对一对多</strong>: 单独形成一张表.</p><h3 id="基本步骤"><a href="#基本步骤" class="headerlink" title="基本步骤"></a>基本步骤</h3><p>需求分析: 了解和分析用户需求;<br>概念结构设计: 对用户需求进行抽象和归纳, 形成一个独立于DBMS的概念模型（E-R图）;<br>逻辑结构设计: 将概念结构转换为数据模型, 通常为关系模型;<br>物理结构设计: 为逻辑数据模型选取一个最适合存储结构和存取方法;<br>数据库实施阶段: 编写数据库, 编写和调试应用程序;<br>数据库运行和维护: 正式投入运行. </p><h2 id="SQL语法"><a href="#SQL语法" class="headerlink" title="SQL语法"></a>SQL语法</h2><p>我估计应该不会考写SQL, 所以就整理一部分.</p><h4 id="表管理操作"><a href="#表管理操作" class="headerlink" title="表管理操作"></a>表管理操作</h4><pre class="line-numbers language-sql"><code class="language-sql"><span class="token comment" spellcheck="true"># 创建表</span><span class="token keyword">Create</span> <span class="token keyword">table</span> BR<span class="token punctuation">(</span> RNo char<span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">)</span> <span class="token operator">not</span> <span class="token boolean">null</span><span class="token punctuation">,</span>  ISBN char<span class="token punctuation">(</span><span class="token number">13</span><span class="token punctuation">)</span> <span class="token operator">not</span> <span class="token boolean">null</span><span class="token punctuation">,</span>  BDate <span class="token keyword">date</span><span class="token punctuation">,</span>  <span class="token keyword">primary</span> <span class="token keyword">key</span> <span class="token punctuation">(</span>RNo<span class="token punctuation">,</span>ISBN<span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token keyword">foreign</span> <span class="token keyword">key</span> <span class="token punctuation">(</span>RNo<span class="token punctuation">)</span> <span class="token keyword">references</span> R<span class="token punctuation">(</span>RNo<span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token keyword">foreign</span> <span class="token keyword">key</span> <span class="token punctuation">(</span>ISBN<span class="token punctuation">)</span> <span class="token keyword">references</span> B<span class="token punctuation">(</span>ISBN<span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 修改表</span><span class="token keyword">ALTER</span> <span class="token keyword">TABLE</span> S  <span class="token keyword">ALTER</span> <span class="token keyword">COLUMN</span> SEX CHAR<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token boolean">NULL</span> <span class="token keyword">ALTER</span> <span class="token keyword">TABLE</span> S  <span class="token keyword">DROP</span> <span class="token keyword">COLUMN</span>  AGE<span class="token comment" spellcheck="true"># 删除表  </span><span class="token keyword">DROP</span> <span class="token keyword">TABLE</span> <span class="token operator">&lt;</span>表名<span class="token operator">></span><span class="token keyword">DROP</span> <span class="token keyword">TABLE</span> S<span class="token comment" spellcheck="true"># 插入数据</span><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> <span class="token operator">&lt;</span>表名<span class="token operator">></span>         <span class="token punctuation">[</span>（<span class="token operator">&lt;</span>属性列<span class="token number">1</span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token punctuation">,</span> <span class="token operator">&lt;</span>属性列<span class="token number">2</span><span class="token operator">></span>……<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token keyword">VALUES</span>   <span class="token punctuation">(</span> <span class="token operator">&lt;</span>常量<span class="token number">1</span><span class="token operator">></span>  <span class="token punctuation">[</span><span class="token punctuation">,</span> <span class="token operator">&lt;</span>常量<span class="token number">2</span><span class="token operator">></span><span class="token punctuation">]</span>……<span class="token punctuation">)</span><span class="token keyword">Insert</span> <span class="token keyword">into</span> sc <span class="token punctuation">(</span>Cno<span class="token punctuation">,</span>Sno<span class="token punctuation">)</span><span class="token keyword">Values</span> <span class="token punctuation">(</span><span class="token string">'c2'</span><span class="token punctuation">,</span><span class="token string">'s4'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 修改表数据</span><span class="token keyword">UPDATE</span> <span class="token operator">&lt;</span>表名<span class="token operator">></span>   <span class="token keyword">SET</span> <span class="token operator">&lt;</span>列名<span class="token operator">>=</span><span class="token operator">&lt;</span>表达式<span class="token operator">></span><span class="token punctuation">[</span><span class="token punctuation">,</span>… n<span class="token punctuation">]</span>   <span class="token punctuation">[</span><span class="token keyword">WHERE</span> <span class="token operator">&lt;</span>逻辑表达式<span class="token operator">></span><span class="token punctuation">]</span><span class="token keyword">UPDATE</span> S<span class="token keyword">SET</span> SNAME<span class="token operator">=</span><span class="token string">'姜芸'</span><span class="token punctuation">,</span>AGE<span class="token operator">=</span><span class="token number">22</span><span class="token keyword">WHERE</span> SNO<span class="token operator">=</span><span class="token string">'S2'</span><span class="token comment" spellcheck="true"># 删除表数据</span><span class="token keyword">DELETE</span> <span class="token keyword">FROM</span> <span class="token operator">&lt;</span>表名<span class="token operator">></span><span class="token punctuation">[</span><span class="token keyword">WHERE</span> <span class="token operator">&lt;</span>逻辑表达式<span class="token operator">></span><span class="token punctuation">]</span><span class="token keyword">DELETE</span> <span class="token keyword">from</span> S<span class="token keyword">WHERE</span> SNAME<span class="token operator">=</span><span class="token string">'张丽'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="基本查询"><a href="#基本查询" class="headerlink" title="基本查询"></a>基本查询</h4><pre class="line-numbers language-sql"><code class="language-sql"><span class="token comment" spellcheck="true"># 简单查询</span><span class="token keyword">SELECT</span> SNo<span class="token punctuation">,</span>SName<span class="token keyword">FROM</span>   S<span class="token keyword">SELECT</span> SName<span class="token punctuation">,</span> Year<span class="token punctuation">(</span>Getdate<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">-</span>Age<span class="token keyword">FROM</span>   S<span class="token keyword">SELECT</span> SName<span class="token punctuation">,</span>           <span class="token string">'Year of Birth:'</span><span class="token punctuation">,</span>       Year<span class="token punctuation">(</span>Getdate<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">-</span>Age<span class="token punctuation">,</span>       Lower<span class="token punctuation">(</span>Sdept<span class="token punctuation">)</span><span class="token keyword">FROM</span>   S<span class="token keyword">SELECT</span> SName<span class="token punctuation">,</span>           <span class="token string">'Year of Birth:'</span> <span class="token keyword">AS</span> <span class="token string">'YEAR OF BIRTH'</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true"># 可以给列赋别名</span>       Year<span class="token punctuation">(</span>Getdate<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">-</span>Age <span class="token string">'Birthday'</span><span class="token punctuation">,</span>       Lower<span class="token punctuation">(</span>Sdept<span class="token punctuation">)</span> Department<span class="token keyword">FROM</span>   S<span class="token comment" spellcheck="true"># 去重</span><span class="token keyword">SELECT</span> <span class="token keyword">Distinct</span> SNo<span class="token keyword">FROM</span>   SC<span class="token comment" spellcheck="true"># 多条件查询</span><span class="token keyword">Select</span> SName<span class="token punctuation">,</span>Sdept<span class="token keyword">From</span>   S<span class="token keyword">Where</span>  Sdept<span class="token operator">=</span><span class="token string">'CS'</span> <span class="token operator">OR</span> Sdept<span class="token operator">=</span><span class="token string">'MA'</span> <span class="token operator">OR</span> Sdept<span class="token operator">=</span><span class="token string">'IS'</span><span class="token comment" spellcheck="true"># 范围查询</span><span class="token keyword">Select</span> SName<span class="token punctuation">,</span>Sdept<span class="token punctuation">,</span>Age<span class="token keyword">From</span>   S<span class="token keyword">Where</span>  Age <span class="token operator">BETWEEN</span> <span class="token number">20</span> <span class="token operator">AND</span> <span class="token number">23</span><span class="token comment" spellcheck="true"># 确定集合</span><span class="token keyword">Select</span> SName<span class="token punctuation">,</span>Sdept<span class="token keyword">From</span>   S<span class="token keyword">Where</span>  Sdept <span class="token operator">IN</span> <span class="token punctuation">(</span> <span class="token string">'CS'</span><span class="token punctuation">,</span><span class="token string">'MA'</span><span class="token punctuation">,</span><span class="token string">'IS'</span> <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>语句执行顺序</strong>: <code>From -&gt; Where -&gt; Group by -&gt; Having -&gt; Select -&gt; Order by</code></p><h2 id="视图与索引"><a href="#视图与索引" class="headerlink" title="视图与索引"></a>视图与索引</h2><h3 id="视图"><a href="#视图" class="headerlink" title="视图"></a>视图</h3><p><strong>视图</strong>: 包含了一系列带有名称的列和数据行, 这些列和数据行来自由定义视图的查询所引用的表, 并且在引用视图时动态生成. </p><p><strong>视图的作用?</strong> </p><ol><li>能够简化用户的操作</li><li>使用户能以多种角度看待同一数据</li><li>在一定程度上提供了数据的逻辑独立性</li><li>能够对秘密数据提供安全保护</li><li>利用可以更清晰的表达查询</li></ol><p><strong>视图机制是如何对数据库实现安全性控制的？</strong></p><p>可以对不同的用户定义不同的视图, 也就是说, 通过视图机制把要保密的数据对无权存取的用户隐藏起来. </p><h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><p>当表的数据量比较大时, 查询操作比较耗时, 建立索引可以加快查询速度.<br>优点: 加速查询速度; 缺点: 索引需要占一定的存储空间, 且基本表更新时需要维护索引表. </p><p>索引表是与基本表关联的一种数据结构, 它包含由基本表中的一列或多列生成的索引键和基本表中包含各个索引键的行所在的存储位置. 不论基本表中是否按索引键有序, 但索引中总是按索引键有序的.  </p><p><strong>优点</strong>:</p><ol><li>创建唯一性索引, 可以保证每一行数据的唯一性; </li><li>可以大大加快数据的检索速度; </li><li>可以加速表和表之间的连接; </li><li>在使用分组和排序子句进行数据检索时, 同样可以显著减少查询中分组和排序的时间; </li><li>通过使用索引, 可以在查询的过程中, 使用优化隐藏器, 提高系统的性能. </li></ol><p><strong>缺点</strong>:</p><ol><li>创建索引和维护索引要耗费时间; </li><li>索引需要占用物理空间; </li><li>当对表中的数据进行增加、删除和修改的时候, 索引也要动态的维护, 这样就降低了数据的维护速度. </li></ol><p><strong>聚集索引</strong>: 会对基本表进行物理排序, 所以这种索引对查询非常有效, 在每一张基本表中只能有一个聚集索引. </p><p><strong>非聚集索引</strong>: 不对基本表进行物理排序.</p><p><strong>基本的索引类型</strong>:</p><p>唯一性索引和复合索引. </p><p>唯一性索引保证在索引列中的全部数据是唯一的, 不会包含冗余数据. </p><p>复合索引就是一个索引创建在两个列或者多个列上, 可以减少一在一个表中所创建的索引数量. </p><p><strong>应该哪些列建立索引</strong>: </p><p>在作为主键的列上, 强制该列的唯一性和组织表中数据的排列结构. </p><p>在经常用在连接的列上, 这些列主要是一些外键, 可以加快连接的速度. </p><p>在经常需要根据范围进行搜索的列上创建索引, 因为索引已经排序, 其指定的范围是连续的. </p><p>在经常需要排序的列上创建索引, 因为索引已经排序, 这样查询可以利用索引的排序, 加快排序查询时间. </p><p>在经常使用在where子句中的列上面创建索引, 加快条件的判断速度. </p><h2 id="存储过程、触发器"><a href="#存储过程、触发器" class="headerlink" title="存储过程、触发器"></a>存储过程、触发器</h2><h3 id="存储过程"><a href="#存储过程" class="headerlink" title="存储过程"></a>存储过程</h3><p>存储过程是一种数据库对象, 独立存储在数据库内, 它可以接受参数、输出参数、返回单个或多个结果集以及返回值. 是个预编译的SQL语句集合, 优点是可以建立非常复杂的查询, 只需创建一次, 在程序中即可多次调用, 且比执行单纯的SQL语句要快. 可以创建一个命令对象进行调用. </p><h3 id="触发器"><a href="#触发器" class="headerlink" title="触发器"></a>触发器</h3><p>触发器是用户定义在关系表上的一类由数据驱动的一类由事物驱动的特殊过程, 类似于约束, 但是比约束更灵活, 是保证数据库完整性的一种方法. 任何用户对表进行增删改操作都会有数据库服务器自动激活相应的触发器, 对数据库进行相应的检查和操作. </p><h2 id="数据库并发控制"><a href="#数据库并发控制" class="headerlink" title="数据库并发控制"></a>数据库并发控制</h2><h3 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h3><p>事务是并发控制的基本单位, 它是一个操作序列, 这些操作要么都执行, 要么都不执行, 是一个不可分割的工作单位. 事务是数据库维护数据一致性的单位, 在每个事务结束时, 都能保持数据一致性. </p><p><strong>ACID特性:</strong></p><p><strong>A, atomacity</strong>: 原子性. 事务必须是原子工作单元, 对于数据的执行要不是全部执行, 要不都不执行. 如果只执行一个子集, 可能会破坏事务的总体目标. </p><p><strong>C, consistency</strong>: 一致性. 事务将数据库从一种一致状态变为下一种一致状态</p><p><strong>I, isolation</strong>: 隔离性. 由并发事务所作的修改必须与任何其它并发事务所作的修改隔离. 事务查看数据时数据所处的状态, 要么是另一并发事务修改它之前的状态, 要么是另一事务修改它之后的状态, 事务不会查看中间状态的数据. 换句话说, 一个事务的影响在该事务提交前对其他事务都不可见. </p><p><strong>D, durability</strong>: 持久性. 事务完成之后, 它对于系统的影响是永久性的. 该修改即使出现致命的系统故障也将一直保持. </p><p><strong>为什么事务非正常结束时会影响数据库数据的正确性</strong></p><p>事物具有一致性, 事务执行的结果必须是使数据库从一个一致性状态变到另一个一致性状态. 如果数据库系统运行中发生故障, 有些事务尚未完成就被迫中断, 这些未完成事务对数据库所做的修改有一部分已写入物理数据库, 这时数据库就处于一种不正确的状态, 或者说是不一致的状态. </p><h3 id="并发访问不一致性"><a href="#并发访问不一致性" class="headerlink" title="并发访问不一致性"></a>并发访问不一致性</h3><p><strong>在数据库中为什么要并发控制？</strong></p><p>数据库是共享资源, 通常有多个事物在同时执行, 当多个事物并发的存取数据库时就会存在同时读或写统一数据的情况, 如果对并发操作不加控制, 就会存在数据读取或存取错误, 破坏数据库的一致性. </p><p><strong>都有哪几种并发出现的问题？</strong></p><p>后面三个都是两次数据读取不一样. 脏读是因为ROLLBACK, 不重复读是因为修改数据, 幻读是因为插入或删除了数据. 而丢失数据是因为两个事物都对同一条数据进行了修改, 然后撤销时一起撤销了.</p><ol><li><p><strong>丢失数据</strong>:</p><ul><li><p>两个事务都对一个数据进行修改, 一个先修改, 另一个随后修改, 第二个修改覆盖了第一个的修改. </p></li><li><p>两个事务更新相同的数据源, 如果第一个事务被提交, 第二个却被撤销, 那么连同第一个事务做的更新也被撤销. </p></li></ul></li><li><p><strong>读脏数据</strong>: T1对一个数据做了修改, T2读取这一个数据. <strong>若T1执行 ROLLBACK 操作, 则 T2读取的结果和第一次的结果不一样</strong>. 当一个事务读取另一个事务尚未提交的修改时, 产生脏读. </p></li><li><p><strong>不可重复读(非重复读)</strong>: T1读取一个数据, T2对该数据做了修改. 如果 T1再次读取这个数据, 此时读取的结果和第一次读取的结果不同. <strong>一个事务对同一行数据重复读取两次, 但是却得到了不同的结果</strong>. 同一查询在同一事务中多次进行, 由于其他提交事务所做的修改或删除, 每次返回不同的结果集. </p></li><li><p><strong>幻读</strong>: T1读取某个范围的数据, T2在这个范围内插入新的数据, T1再次读取这个范围的数据, 此时读取的结果和和第一次读取的结果不同. 事务在操作过程中进行两次查询, 第二次查询的结果包含了第一次查询中未出现的数据（这里并不要求两次查询的SQL语句相同）. <strong>这是因为在两次查询过程中有另外一个事务插入数据造成的</strong>. </p></li></ol><p><strong>解决方案</strong>:</p><p>在并发环境下, 事务的隔离性很难保证, 因此会出现很多并发一致性问题. 产生并发不一致性问题的主要原因是破坏了事务的隔离性. 解决方法是通过并发控制来保证隔离性, 并发控制可以通过封锁来实现. </p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/DB/并发性.png" style="zoom:50%;" /><h3 id="封锁机制"><a href="#封锁机制" class="headerlink" title="封锁机制"></a>封锁机制</h3><h4 id="锁"><a href="#锁" class="headerlink" title="锁"></a>锁</h4><p>锁是最常用的并发控制机构, 是防止其他事务访问指定资源, 实现并发控制的一种手段. </p><p><strong>共享锁(S锁)</strong>: 用于只读操作(SELECT), 锁定共享的资源. 共享锁不会阻止其他用户读, 但是阻止其他的用户写和修改. </p><p><strong>更新锁(U锁)</strong>: 用于可更新的资源中. 防止当多个会话在读取、锁定以及随后可能进行的资源更新时发生常见形式的死锁. </p><p><strong>独占锁(X锁, 也叫排他锁)</strong>: 一次只能有一个独占锁用在一个资源上, 并且阻止其他所有的锁包括共享缩. 写是独占锁, 可以有效的防止“脏读”. </p><p><strong>死锁</strong>: 系统中有两个或两个以上的事务都处于等待状态, 并且每个事务都在等待其中另一个事务解除封锁, 它才能继续执行下去, 结果造成任何一个事务都无法继续执行, 这种现象称系统进入了“死锁”（dead lock）状态. </p><h4 id="封锁协议"><a href="#封锁协议" class="headerlink" title="封锁协议"></a>封锁协议</h4><p><strong>一级封锁协议</strong>: 事务在修改数据之前加<strong>写锁</strong>, 直到事务结束才释放. 该协议可以防止<strong>丢失修改</strong>;<br><strong>二级封锁协议</strong>: 在一级封锁协议的基础上, 加上了事务在读取数据之前对其加<strong>读锁</strong>, 读完后即可释放读锁. 该协议避免了<strong>读脏数据</strong>;<br><strong>三级封锁协议</strong>: 在一级封锁协议的基础上, 加上了事务在读取数据之前必须<strong>加上读锁</strong>, <strong>直到事务结束才释放</strong>. 该协议解决了<strong>不可重复读</strong>问题. </p><h2 id="数据库的备份与恢复"><a href="#数据库的备份与恢复" class="headerlink" title="数据库的备份与恢复"></a>数据库的备份与恢复</h2><p><strong>数据库故障的种类有哪几种？相应的恢复策略是什么？</strong><br>答: 三种: 事物故障、系统故障、介质故障; </p><p>事物故障是指事物在运行至终点前被中止;<br>事物故障恢复是由系统自动完成, 利用日志文件撤销此事务已对数据库进行的修改; </p><p>系统故障是指造成系统停止运转的任何事件, 使得系统要重新启动;<br>系统故障的恢复是在重新启动后系统自动完成, 为了防止系统故障造成的数据不一致性, 必须撤销故障时未完成的事物, 重做已完成的事物; </p><p>介质故障是指磁盘上的物理数据和日志文件被破坏;<br>介质故障的恢复方法就是重装数据库, 重做已经完成的事物. </p><p><strong>数据库恢复的基本技术有哪些？</strong><br>数据转储和登录日志文件是数据库恢复的基本技术.  <br>数据转储是指DBA定期将数据库复制到其他介质上进行保存, 这些备份的数据叫做后备副本;<br>登记日志文件, 日志文件是用来记录事物对数据库进行更新操作的文件.<br>当系统运行过程中发生故障, 利用转储的数据库后备副本和日志文件就可以将数据库恢复到故障前的某个一致性状态. </p><p><strong>数据库恢复的关键是什么？</strong><br>关键是建立冗余数据.</p><p><strong>登记日志文件时为什么必须先写日志文件, 后写数据库？</strong><br>把对数据的修改写到数据库中和把表示这个修改的日志记录写到日志文件中是两个不同的操作. 有可能在这两个操作之间发生故障, 即这两个写操作只完成了一个.<br>如果先写了数据库修改, 而在运行记录中没有登记这个修改, 则以后就无法恢复这个修改了. 如果先写日志, 但没有修改数据库, 在恢复时并<strong>不会影响数据库的正确性</strong>. 所以一定要先写日志文件, 即首先把日志记录写到日志文件中, 然后写数据库的修改</p><p><strong>备份的几种方式?</strong></p><p><strong>完整备份</strong>: 指备份整个数据库的所有内容, 包括事务日志. </p><p><strong>差异备份</strong>: 对完整备份的补充, 只备份上次完整备份后更改的数据. </p><p><strong>事务日志备份</strong>: 只备份事务日志里的内容, 即数据库的所有变动. </p><p><strong>文件或文件组备份</strong>: 只备份数据库中的某些文件. </p>]]></content>
      
      
      <categories>
          
          <category> 计算机基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 面试 </tag>
            
            <tag> 数据库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jsDelivr和minivaline一起使用</title>
      <link href="/posts/28290.html"/>
      <url>/posts/28290.html</url>
      
        <content type="html"><![CDATA[<h1 id="jsDelivr和minivaline一起使用"><a href="#jsDelivr和minivaline一起使用" class="headerlink" title="jsDelivr和minivaline一起使用"></a>jsDelivr和minivaline一起使用</h1><p>这个问题只有在使用<code>matery1.31</code>及以下版本时会遇到. 作者虽然在主题的<code>_config.yml</code>中添加了<code>minivaline</code>这个js, 但是当同时启用<code>jsDelivr</code>时会发生地址拼接的错误, 导致跟评论相关的所有内容全部显示不出来. 本人是<strong>前端小白</strong>, 没有前端基础, 纯靠自己摸索, 所以本文是站在前端小白的视角去分析问题并解决的. 想看解决方法的可以直接看最后的<strong>修复</strong>部分.</p><h2 id="现象"><a href="#现象" class="headerlink" title="现象"></a>现象</h2><p>直接去看<code>HTML</code>, 发现<code>minivaline</code>的script所用的地址变成了<strong>jsDelivr设置的地址 + <a href="https://cdn.jsdelivr.net/npm/minivaline/dist/MiniValine.min.js" target="_blank" rel="noopener">https://cdn.jsdelivr.net/npm/minivaline/dist/MiniValine.min.js</a></strong>. 直接就出现了地址拼接的错误.</p><h2 id="推测"><a href="#推测" class="headerlink" title="推测"></a>推测</h2><p>首先, 这个问题是出在matery里面的, 因为先在matery的配置文件进行修改, 启用<code>jsDelivr</code>后才出现的bug, 那么问题的范围一定在matery中. 并且<code>minivaline</code>也是matery新加入的插件, 不可能与Hexo框架本身有关.</p><h3 id="文件树分析"><a href="#文件树分析" class="headerlink" title="文件树分析"></a>文件树分析</h3><p>直接看一下<code>blog\themes\hexo-theme-matery</code>下的文件结构:</p><blockquote><p>│  _config.yml<br>│<br>├─languages<br>│<br>├─layout<br>│  │<br>│  ├─_partial<br>│  └─_widget<br>│<br>└─source<br>        ├─css<br>        ├─js<br>        ├─libs<br>        └─medias</p></blockquote><table><thead><tr><th>文件夹 / 文件名</th><th>作用</th></tr></thead><tbody><tr><td>_config.yml</td><td>主题配置文件</td></tr><tr><td>languages</td><td>不同语言文字的配置文件</td></tr><tr><td>layout</td><td>除去文件夹外, 存放的是page的配置</td></tr><tr><td>_partial</td><td>页面细节的各种配置</td></tr><tr><td>_widget</td><td>页面添加的组件的配置</td></tr><tr><td>source</td><td>额外库的源文件</td></tr><tr><td>css</td><td>额外库的css</td></tr><tr><td>libs</td><td>额外库的源码</td></tr><tr><td>medias</td><td>图片和其他媒体流</td></tr></tbody></table><h3 id="缩小范围"><a href="#缩小范围" class="headerlink" title="缩小范围"></a>缩小范围</h3><p><code>languages</code>存放的是不同语言所对应博客文字的翻译, 嫌疑可以排除, <code>layout</code>肯定就是博客的布局, 因为涉及到显示位置, 可以从这里先开刀, 再慢慢找到问题,  <code>source</code>里面有各种前端本地配置, 最后肯定要对它进行修改.</p><p>既然问题出在评论上, 就先从<code>layout\contact.ejs</code>中找关于<code>minivaline</code>的部分. </p><pre class="line-numbers language-html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>%</span> <span class="token attr-name">if</span> <span class="token attr-name">(theme.valine</span> <span class="token attr-name">&amp;&amp;</span> <span class="token attr-name">theme.valine.enable)</span> <span class="token attr-name">{</span> <span class="token attr-name">%</span><span class="token punctuation">></span></span>&lt;%- partial('_partial/valine') %><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>%</span> <span class="token attr-name">}</span> <span class="token attr-name">%</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>%</span> <span class="token attr-name">if</span> <span class="token attr-name">(theme.minivaline</span> <span class="token attr-name">&amp;&amp;</span> <span class="token attr-name">theme.minivaline.enable)</span> <span class="token attr-name">{</span> <span class="token attr-name">%</span><span class="token punctuation">></span></span>&lt;%- partial('_partial/minivaline') %><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>%</span> <span class="token attr-name">}</span> <span class="token attr-name">%</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>果然能够找到关于<code>valine</code>和<code>minivaline</code>的对应代码. 它指向了<code>_partial</code>这个文件夹. 找到<code>_partial\minivaline.ejs</code>, 就能找到插入script的代码.</p><pre class="line-numbers language-html"><code class="language-html">&lt;script src="<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>%-</span> <span class="token attr-name">theme.jsDelivr.url</span> <span class="token attr-name">%</span><span class="token punctuation">></span></span><span class="token script language-javascript"><span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> <span class="token function">url_for</span><span class="token punctuation">(</span>theme<span class="token punctuation">.</span>libs<span class="token punctuation">.</span>js<span class="token punctuation">.</span>minivaline<span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">></span>"<span class="token operator">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>script</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>参照一下<code>-partial\valine.ejs</code>的同样对应的代码.</p><pre class="line-numbers language-html"><code class="language-html">&lt;script src="<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>%-</span> <span class="token attr-name">theme.jsDelivr.url</span> <span class="token attr-name">%</span><span class="token punctuation">></span></span><span class="token script language-javascript"><span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> <span class="token function">url_for</span><span class="token punctuation">(</span>theme<span class="token punctuation">.</span>libs<span class="token punctuation">.</span>js<span class="token punctuation">.</span>valine<span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">></span>"<span class="token operator">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>script</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>证明这部分没有问题. 那一定是里面配置的<code>theme.jsDelivr.url</code>或者后半部分的<code>url_for(theme.libs.js.minivaline)</code>出了问题.</p><h2 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h2><p>按照推测部分提供的线索去找, script里对应的部分一定是一个路径. 在主题下的<code>_config.yml</code>文件中, 找到<code>libs.js</code>端, 里面有对<code>minivaline</code>的引用, 会发现它是一个url而不是一个本地路径.</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">libs</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># ...</span>  <span class="token key atrule">js</span><span class="token punctuation">:</span>     <span class="token comment" spellcheck="true"># ...</span>    <span class="token key atrule">valine</span><span class="token punctuation">:</span> /libs/valine/Valine.min.js    <span class="token key atrule">minivaline</span><span class="token punctuation">:</span> https<span class="token punctuation">:</span>//cdn.jsdelivr.net/npm/minivaline/dist/MiniValine.min.js<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>valine</code>和<code>minivaline</code>不一样.</p><h2 id="修复"><a href="#修复" class="headerlink" title="修复"></a>修复</h2><p>用最笨的办法, 假设你开了<code>jsDelivr</code>, <code>theme.jsDelivr.url</code>的值肯定是你设置的github.io的地址, <code>url_for(theme.libs.js.minivaline)</code>本来应该是上传到github之后的库的地址, 但是现在它的值是一个url, 只需要把前半部分删掉就能保证在开启jsDelivr时不出错了.</p><p>即更改<code>_partial\minivaline.ejs</code>中原来有问题的部分为:</p><pre class="line-numbers language-html"><code class="language-html">&lt;script src="<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>%-</span> <span class="token attr-name">url_for(theme.libs.js.minivaline)</span> <span class="token attr-name">%</span><span class="token punctuation">></span></span><span class="token script language-javascript">"<span class="token operator">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>script</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>但是要考虑复用性, 总不能以后不使用jsDelivr后再把它换回来吧. 所以最好更改在主题下的<code>_config.yml</code>文件为本地文件路径:</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">minivaline</span><span class="token punctuation">:</span> /libs/minivaline/MiniValine.min.js<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在主题下<code>source\libs</code>下是没有<code>minivaline</code>这个库的, 但是却有<code>valine</code>. 所以只要搞一份这个库的js过来, 放到指定目录再部署, jsDelivr就能按照自己github上的文件地址访问到了. 从<code>博客目录\node_modules\minivaline\dist</code>下复制<code>MiniValine.min.js</code>到主题目录下的<code>source\libs\minivaline</code>中, 重新编译一遍, 再<strong>部署到github上</strong>, 就大功告成了.</p>]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Matery </tag>
            
            <tag> jsDelivr </tag>
            
            <tag> Minivaline </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo主题修改</title>
      <link href="/posts/1370.html"/>
      <url>/posts/1370.html</url>
      
        <content type="html"><![CDATA[<h1 id="Hexo主题修改"><a href="#Hexo主题修改" class="headerlink" title="Hexo主题修改"></a>Hexo主题修改</h1><p>Hexo有很多漂亮的主题模板. 在它的<a href="https://hexo.io/themes/" target="_blank" rel="noopener">官网主题页</a>有很多好看的模板. 但是有一部分主题是没有显示在官网上的, 如果想看更多可以直接在Github上搜.</p><h2 id="推荐的主题"><a href="#推荐的主题" class="headerlink" title="推荐的主题"></a>推荐的主题</h2><p>我挑选了几个比较赏心悦目的模板, 在这列出来.</p><h3 id="Ayer"><a href="#Ayer" class="headerlink" title="Ayer"></a>Ayer</h3><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/ayer.png" alt=""></p><p>极简风格, 刚进博客有整个屏, 很棒. 博客预览的背景也很干净.</p><h3 id="Yilia"><a href="#Yilia" class="headerlink" title="Yilia"></a>Yilia</h3><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200724005316.png" alt=""></p><p>看起来很精简的博客, 该有的都有. 麻雀虽小五脏俱全.</p><h3 id="Yilia-plus"><a href="#Yilia-plus" class="headerlink" title="Yilia-plus"></a>Yilia-plus</h3><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200724010601.png" alt=""></p><p>在原来的Yilia上做的升级, 扩展了很多功能, 好像代码羊用的就是这个.</p><h3 id="Yelee"><a href="#Yelee" class="headerlink" title="Yelee"></a>Yelee</h3><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200724012420.png" alt=""></p><p>也是一个Yilia的变种, 看来大家都很喜欢Yilia, 这个版本换上了更多彩的背景.</p><h3 id="Tranquilpeak"><a href="#Tranquilpeak" class="headerlink" title="Tranquilpeak"></a>Tranquilpeak</h3><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200724010150.png" alt=""></p><p>双栏的, 看着贼舒服.</p><h3 id="Icarus"><a href="#Icarus" class="headerlink" title="Icarus"></a>Icarus</h3><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200726012900.png" alt=""></p><p>简洁大气.</p><h3 id="NexT"><a href="#NexT" class="headerlink" title="NexT"></a>NexT</h3><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200724011238.png" alt=""></p><p>简到极致. Github上最火爆的主题.</p><h3 id="Butterfly"><a href="#Butterfly" class="headerlink" title="Butterfly"></a>Butterfly</h3><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/TIM%E6%88%AA%E5%9B%BE20200724011924.jpg" alt=""></p><p>这个主题也很酷, 有一整面的图. 博客浏览也很是赏心悦目.</p><h3 id="Matery"><a href="#Matery" class="headerlink" title="Matery"></a>Matery</h3><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200724012502.png" alt=""></p><p>这款主题具有预加载, 访问速度比较快, 更像是年轻人使用的. 我主要看上了它的文章浏览风格和模板功能的强大. 虽然说有多余的视觉效果, 但是根据个人情况进行调整后肯定会符合自己的需求. 所以我采用Matery作为我现在的博客模板.</p><h3 id="主题汇总"><a href="#主题汇总" class="headerlink" title="主题汇总"></a>主题汇总</h3><table><thead><tr><th>Preview</th><th>Github</th></tr></thead><tbody><tr><td><a href="https://shen-yu.gitee.io/" target="_blank" rel="noopener">Ayer</a></td><td><a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank" rel="noopener">https://github.com/Shen-Yu/hexo-theme-ayer</a></td></tr><tr><td><a href="http://litten.me/" target="_blank" rel="noopener">Yilia</a></td><td><a href="https://github.com/litten/hexo-theme-yilia" target="_blank" rel="noopener">https://github.com/litten/hexo-theme-yilia</a></td></tr><tr><td><a href="https://zhousiwei.gitee.io/yilia-plus-demo/" target="_blank" rel="noopener">Yilia-Plus</a></td><td><a href="https://github.com/JoeyBling/hexo-theme-yilia-plus" target="_blank" rel="noopener">https://github.com/JoeyBling/hexo-theme-yilia-plus</a></td></tr><tr><td><a href="http://moxfive.xyz/" target="_blank" rel="noopener">Yelee</a></td><td><a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" rel="noopener">https://github.com/MOxFIVE/hexo-theme-yelee</a></td></tr><tr><td><a href="https://louisbarranqueiro.github.io/hexo-theme-tranquilpeak/" target="_blank" rel="noopener">Tranquilpeak</a></td><td><a href="https://github.com/LouisBarranqueiro/hexo-theme-tranquilpeak" target="_blank" rel="noopener">https://github.com/LouisBarranqueiro/hexo-theme-tranquilpeak</a></td></tr><tr><td><a href="https://blog.zhangruipeng.me/hexo-theme-icarus/" target="_blank" rel="noopener">Icarus</a></td><td><a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">https://github.com/ppoffice/hexo-theme-icarus</a></td></tr><tr><td><a href="https://www.jiaxi.io/" target="_blank" rel="noopener">NexT</a></td><td><a href="https://github.com/theme-next/hexo-theme-next" target="_blank" rel="noopener">https://github.com/theme-next/hexo-theme-next</a></td></tr><tr><td><a href="https://demo.jerryc.me/" target="_blank" rel="noopener">Butterfly</a></td><td><a href="https://github.com/jerryc127/hexo-theme-butterfly/tree/dev" target="_blank" rel="noopener">https://github.com/jerryc127/hexo-theme-butterfly/tree/dev</a></td></tr><tr><td><a href="http://blinkfox.com/" target="_blank" rel="noopener">Matery</a></td><td><a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank" rel="noopener">https://github.com/blinkfox/hexo-theme-matery</a></td></tr></tbody></table><h2 id="变更模板"><a href="#变更模板" class="headerlink" title="变更模板"></a>变更模板</h2><p>安装任何Hexo的主题路子都是一样的. 在按照之前的操作建立好Hexo基本框架后, 需要更改的内容不多.</p><ol><li>下载好github中的模板, 并放到博客文件夹下的<code>themes</code>中.</li><li>调整博客文件夹根目录下的<code>_config.yml</code>中的<code>theme</code>字段, 在使用默认主题时字段应该是<code>landscape</code>. <code>_config.yml</code>这个文件是全局的Hexo设置, 无论你的主题是什么, 这里都记载一些基本的博客配置, 更详细的博客配置在对应主题文件夹的<code>_config.yml</code>下进行修改.</li><li>调整全局的其他设置, 如<code>per_page</code>等, 需要根据主题的要求而更改.</li></ol><h2 id="Matery定制化"><a href="#Matery定制化" class="headerlink" title="Matery定制化"></a>Matery定制化</h2><p>这部分基本上根据Github上提供的<a href="https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md" target="_blank" rel="noopener">官方文档</a>进行个性化修改. 其实这个主题已经足够满足美观了, 不要再往上加机器人, 即时对话之类的功能了. 背景彩带, 点击爱心之类的视觉效果也很鸡肋, 我也都关闭了.</p><h3 id="视觉效果"><a href="#视觉效果" class="headerlink" title="视觉效果"></a>视觉效果</h3><p>很多内容能在配置文件中找到, 这里只说一些出现的bug或者经常要调整的东西.</p><h4 id="代码显示BUG"><a href="#代码显示BUG" class="headerlink" title="代码显示BUG"></a>代码显示BUG</h4><p>起初, 如果使用官方自带的代码高亮会非常的拉胯, 代码行号和代码段直接裂开.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200724013845.png" style="zoom:50%;" /><p>所以我们必须安装<code>prism_plugin</code>插件.</p><p>在命令行中输入:</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> i -S hexo-prism-plugin<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>安装这个插件, 并在Hexo根目录的<code>_config.yml</code>中修改<code>highlight</code>的值为<code>false</code>, 再添加关于<code>prism</code> 插件相关的配置.</p><pre class="line-numbers language-yml"><code class="language-yml">highlight:  enable: false  # ....prism_plugin:  mode: 'preprocess' # realtime/preprocess  theme: 'tomorrow'  line_number: true # default false<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>prism</code>的<code>mode</code>指的是处理代码显示是实时模式, 还是预处理模式. 一般选预处理.</p><h4 id="头疼的Banner"><a href="#头疼的Banner" class="headerlink" title="头疼的Banner"></a>头疼的Banner</h4><p>博客刚进来的那个背景总是有个滤镜, 其实很多时间是起反作用的. 在<code>博客文件夹\themes\hexo-theme-matery\source\css\matery.css</code>中将下面这段代码直接注释掉, 就不再添加有色滤镜了.</p><pre class="line-numbers language-yaml"><code class="language-yaml">.bg<span class="token punctuation">-</span>cover<span class="token punctuation">:</span>after <span class="token punctuation">{</span>    <span class="token punctuation">-</span><span class="token key atrule">webkit-animation</span><span class="token punctuation">:</span> rainbow 60s infinite;    <span class="token key atrule">animation</span><span class="token punctuation">:</span> rainbow 60s infinite;<span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h4 id="主题配色和文章配色"><a href="#主题配色和文章配色" class="headerlink" title="主题配色和文章配色"></a>主题配色和文章配色</h4><p>主题配色主要是顶部导航栏和右侧浮动按钮的颜色, 文章配色则是文章的字体颜色. 同样在<code>matery.css</code>文件中进行更改:</p><pre class="line-numbers language-yaml"><code class="language-yaml">.bg<span class="token punctuation">-</span>color <span class="token punctuation">{</span>    <span class="token key atrule">background-image</span><span class="token punctuation">:</span> linear<span class="token punctuation">-</span>gradient(to right<span class="token punctuation">,</span> <span class="token comment" spellcheck="true">#16b182 0%, #058044 100%);</span>    <span class="token key atrule">opacity</span><span class="token punctuation">:</span> 0.8; //透明效果 值范围 0~1，看情况自己修改 <span class="token punctuation">}</span><span class="token punctuation">}</span>.text<span class="token punctuation">-</span>color <span class="token punctuation">{</span>    <span class="token key atrule">color</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true">#0f9d58 !important;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>bg-color</code>中对应的两个颜色是渐变, 从0%到100%是什么颜色到什么颜色的变化, <code>text-color</code>则是文字字体颜色. 下方的进度条颜色和宽度需要找<code>.progress-bar</code>修改.</p><h4 id="网站icon和标题icon"><a href="#网站icon和标题icon" class="headerlink" title="网站icon和标题icon"></a>网站icon和标题icon</h4><p>在<code>博客文件夹\themes\hexo-theme-matery\_config.yml</code>中找到<code>favicon</code>和<code>logo</code>, 分别对应的是网站在浏览器标签上的图标和标题中的logo. 进行修改即可. 由于某些未知原因我这里无法访问到<code>source</code>目录下的图像, 所以我一并放到了<code>source\medias</code>中. </p><h4 id="Banner图片和文章图片"><a href="#Banner图片和文章图片" class="headerlink" title="Banner图片和文章图片"></a>Banner图片和文章图片</h4><p>Banner的图片是可以进行轮换的, 默认一天换一次, 可以在配置中关掉. 如果不关掉的话会自动读取<code>themes\hexo-theme-matery\source\medias\banner</code>下的图片根据日期进行更换. 关掉后默认读取<code>0.jpg</code>. 文章图片路径在<code>themes\hexo-theme-matery\source\medias\featureimages</code>下, 这些图片是当文章没有指定图片时进行随机挑选的. 如果加入了新的图片, 需要在配置中加入新图片的路径.</p><h3 id="功能增强"><a href="#功能增强" class="headerlink" title="功能增强"></a>功能增强</h3><h4 id="新建文章头修改"><a href="#新建文章头修改" class="headerlink" title="新建文章头修改"></a>新建文章头修改</h4><p>每次新建文章都需要输入很多额外的文件头, 其实只要更改<code>博客目录\scaffolds\post.md</code>的内容, 之后再新建文章时就不需要重新敲很多东西了. 当然, 每次<code>new page</code>创建的md也可以修改, 方式是一样的, 文件在<code>scaffolds\page.md</code>.</p><pre><code>---title: {{ title }}date: {{ date }}mathjax: falsesummary: keywords: password: top: falseimg: categories: tags:---</code></pre><h4 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h4><p>对应着右上角的搜索栏, 如果不装这个插件就是个摆设.</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> hexo-generator-search --save<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>并在根目录的<code>_config.yml</code>添加:</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">search</span><span class="token punctuation">:</span>  <span class="token key atrule">path</span><span class="token punctuation">:</span> search.xml  <span class="token key atrule">field</span><span class="token punctuation">:</span> post<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h4 id="中文链接转拼音"><a href="#中文链接转拼音" class="headerlink" title="中文链接转拼音"></a>中文链接转拼音</h4><p>如果文章名称是中文的, Hexo 默认生成的永久链接也会有中文, 这样不利于 SEO. 可以用一个插件使在生成文章时生成中文拼音的永久链接.</p><pre class="line-numbers language-yaml"><code class="language-yaml">npm i hexo<span class="token punctuation">-</span>permalink<span class="token punctuation">-</span>pinyin <span class="token punctuation">-</span><span class="token punctuation">-</span>save<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>根目录<code>_config.yml</code>添加:</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">permalink_pinyin</span><span class="token punctuation">:</span>  <span class="token key atrule">enable</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">separator</span><span class="token punctuation">:</span> <span class="token string">'-'</span> <span class="token comment" spellcheck="true"># default: '-'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h4 id="文章字数统计插件"><a href="#文章字数统计插件" class="headerlink" title="文章字数统计插件"></a>文章字数统计插件</h4><p>在文章中显示文章字数, 阅读时长信息.</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> i --save hexo-wordcount<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在<strong>主题</strong>的<code>_config.yml</code>下添加:</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">postInfo</span><span class="token punctuation">:</span>  <span class="token key atrule">date</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">update</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>  <span class="token key atrule">wordCount</span><span class="token punctuation">:</span> <span class="token boolean important">false </span><span class="token comment" spellcheck="true"># 设置文章字数统计为 true.</span>  <span class="token key atrule">totalCount</span><span class="token punctuation">:</span> <span class="token boolean important">false </span><span class="token comment" spellcheck="true"># 设置站点文章总字数统计为 true.</span>  <span class="token key atrule">min2read</span><span class="token punctuation">:</span> <span class="token boolean important">false </span><span class="token comment" spellcheck="true"># 阅读时长.</span>  <span class="token key atrule">readCount</span><span class="token punctuation">:</span> <span class="token boolean important">false </span><span class="token comment" spellcheck="true"># 阅读次数.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="评论"><a href="#评论" class="headerlink" title="评论"></a>评论</h4><p>评论所用的插件有很多, 比如Gitalk, Gitment, Valine 和 Disqus. 最后一种被墙了, 前两种必须有Github账号才能登陆评论, 门槛比较高. 我选择了回复门槛低的<code>Valine</code>. Matery加入的<code>Valine</code>版本是不支持<code>enableqq</code>这个功能的(其实也可以直接更新, 自行替换配置, 但是minivaline比valine好看一些), 所以采用<a href="https://github.com/MiniValine/MiniValine" target="_blank" rel="noopener">minivaline</a>作为代替, 它比<code>valine</code>界面更好看一些, 并且能识别qq邮箱, 自动显示用户qq的头像.</p><p>在国内使用<code>valine</code>主要借助<a href="https://www.leancloud.cn/" target="_blank" rel="noopener">LeanCloud</a>, 要注册一个账号和应用, 并选择免费的<strong>开发版</strong>. 注册过程和创建APP过程就不做演示了. 在控制台中的应用找到对应的key, 并填入主题下的<code>_config.yml</code>的<code>minivaline</code>字段即可, 其余内容都可以自己更改.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200724021143.png" style="zoom: 50%;" /><h4 id="添加额外社交链接"><a href="#添加额外社交链接" class="headerlink" title="添加额外社交链接"></a>添加额外社交链接</h4><p>主页上的社交链接可能不够用, 直接修改插件即可. 比如说我要添加Bilibili的社交链接, 但是主题配置文件中没有, 在<code>themes\hexo-theme-matery\layout\_partial\social-link.ejs</code>中添加入以下代码:</p><pre class="line-numbers language-html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>%</span> <span class="token attr-name">if</span> <span class="token attr-name">(theme.socialLink.bilibili)</span> <span class="token attr-name">{</span> <span class="token attr-name">%</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>&lt;%<span class="token punctuation">=</span> theme.socialLink.bilibili %<span class="token punctuation">></span><span class="token punctuation">"</span></span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>tooltipped<span class="token punctuation">"</span></span> <span class="token attr-name">target</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>_blank<span class="token punctuation">"</span></span> <span class="token attr-name">data-tooltip</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>关注我的Bilibili: &lt;%<span class="token punctuation">=</span> theme.socialLink.bilibili %<span class="token punctuation">></span><span class="token punctuation">"</span></span> <span class="token attr-name">data-position</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>top<span class="token punctuation">"</span></span> <span class="token attr-name">data-delay</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>50<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>i</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>fas fa-bold<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>i</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>%</span> <span class="token attr-name">}</span> <span class="token attr-name">%</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>然后再在主题配置文件中对应的<code>socialink</code>部分加入bilibli网址即可.</p><p>其余的内容可以看<strong>小师弟</strong>的<a href="https://www.liuyao-blog.cn/posts/7410.html#toc-heading-59" target="_blank" rel="noopener">这篇博客</a>, 和<strong>Yafine</strong>的<a href="https://yafine-blog.cn/posts/4ab2.html" target="_blank" rel="noopener">这篇博客</a>写的很详细, 后半部分涉及到加速之类的内容, 强推.</p>]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Matery </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo+Github Page的博客搭建</title>
      <link href="/posts/8332.html"/>
      <url>/posts/8332.html</url>
      
        <content type="html"><![CDATA[<h1 id="Hexo-Github-Page-博客的搭建"><a href="#Hexo-Github-Page-博客的搭建" class="headerlink" title="Hexo+Github Page 博客的搭建"></a>Hexo+Github Page 博客的搭建</h1><p>顺手搭了个博客, 之前一直想整, 但是没整, 感觉还是有个博客好一些, 可以方便自己记载一些东西, 我之前写过一些笔记都可以传上去了, 尤其是存一份云端备份.</p><h2 id="平台和框架选择"><a href="#平台和框架选择" class="headerlink" title="平台和框架选择"></a>平台和框架选择</h2><h3 id="博客框架的选择"><a href="#博客框架的选择" class="headerlink" title="博客框架的选择"></a>博客框架的选择</h3><p>其实搭建博客有很多框架, 比如用<code>PHP</code>实现的Wordpress算比较老牌的博客框架(动态), 还有基于<code>nodejs</code>实现的Hexo, <code>Go</code>实现的Hugo, 现在比较新潮的Typecho都很不错. 它们各有各的优点. 考虑到<strong>轻量级, 易扩展</strong>, 因为自己也没买服务器, 想先利用Github Page先搭建一个博客练练手, 以后再做迁移. 综上考虑, 就选择了<strong>Hexo</strong>, 大多数解决方案都能从网上查到, 用起来也很简单. 纯静态的Hexo加载速度是相当快的. 大家的技术博客都以简洁, 快速为主, <strong>博客内容才是最主要的, 花里胡哨都白扯, 应该集中注意力在内容创作上</strong>. 最关键的是它支持<strong>Markdown和MathJax</strong>, 写作很方便.</p><h3 id="平台的选择"><a href="#平台的选择" class="headerlink" title="平台的选择"></a>平台的选择</h3><p>不收费的平台主要就是Github和Gitee, 但是听说Gitee老崩, 所以还是老老实实选择了Github, 而且现在国内的访问速度也跟上来了. Gitee适合拿来当做图床, 以后会说.</p><h2 id="Hexo本地环境搭建"><a href="#Hexo本地环境搭建" class="headerlink" title="Hexo本地环境搭建"></a>Hexo本地环境搭建</h2><p>搭建可以说是非常简单了, 如果是基于Github个人页可以基本不怎么管服务器的事情. 安装流程主要参考了<a href="https://lixint.github.io/hexo-blog.html" target="_blank" rel="noopener">lixint的BOLG</a>. </p><h3 id="安装NodeJS"><a href="#安装NodeJS" class="headerlink" title="安装NodeJS"></a>安装NodeJS</h3><p>登录<a href="https://nodejs.org/zh-cn/download/" target="_blank" rel="noopener">NodeJS官网下载页</a>, 选择长期支持版的对应自己机子配置的客户端进行下载. 中间会有一个让额外安装扩展的选项不要选就好, 剩下<strong>一路next</strong>即可.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200723225100.png" style="zoom: 67%;" /><p>命令行敲一下<code>node -v</code>看一下有没有安装成功就好.</p><h3 id="安装git"><a href="#安装git" class="headerlink" title="安装git"></a>安装git</h3><p>和安装Nodejs一样, 打开<a href="https://git-scm.com/" target="_blank" rel="noopener">git下载页</a>, 下载客户端, 这个也没啥可说的一路<strong>next</strong>就行. git在后面的作用就是为更新静态博客, 将更换的文件传到github上用的. 在安装完成后桌面右键应该能够看到两个选项<code>Git GUI here</code>和<code>Git Bash here</code>.</p><h3 id="Github账号注册"><a href="#Github账号注册" class="headerlink" title="Github账号注册"></a>Github账号注册</h3><p>如果你之前没有Github账号, 那么需要打开<a href="https://github.com/" target="_blank" rel="noopener">github</a>注册一个. </p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200723230057.png" alt=""></p><p>在这里直接输入自己要注册的账号密码邮箱和右上角的<code>sign up</code>实际上是一样的. 可以取消掉后续的邮件更新通知, 通过一个旋转验证后选择免费的方案.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200723230749.png" style="zoom:50%;" /><p>下一步是个人信息和职业信息的填写, 不想填可以直接划到最下面点complete.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200723231522.png" style="zoom: 33%;" /><p>然后就是校验你的邮箱地址.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200723231552.png" style="zoom:50%;" /><p>校验完成后直接创建一个新的仓库, 点击<code>create a respository</code>.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200723231641.png" alt=""></p><p>然后在仓库名字中填写格式为<code>你的用户名.github.io</code>, 即你的个人主页. Description可写可不写, 权限一定要设为<strong>public</strong>.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200723231815.png" alt=""></p><p>在仓库中随手新建一个<code>index.html</code>来检测一下创建是否成功.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200723232114.png" alt=""></p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200723232234.png" style="zoom: 50%;" /><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200723232254.png" alt=""></p><p>在浏览器中敲入你的个人主页地址应该可以看到刚才输入的内容.</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200723232401.png" style="zoom:50%;" /><p>个人页就配置完成了.</p><h2 id="Hexo初始化"><a href="#Hexo初始化" class="headerlink" title="Hexo初始化"></a>Hexo初始化</h2><p>在安装完Git和NodeJS后, 就可以开始用Hexo初始化了. 如果你熟悉Win自带的CMD, 那么下述操作可以不用git完成. 在电脑某目录新建一个文件夹作为博客网站文件夹, 不要是中文.</p><p>因为国内的原因, npm安装非常慢, 所以一定要对npm进行换源或者直接使用cnpm. 参考<a href="https://www.jianshu.com/p/115594f64b41" target="_blank" rel="noopener">这里</a>.</p><p>直接使用cnpm比较方便:</p><blockquote><p>npm install cnpm -g –registry=<a href="https://registry.npm.taobao.org" target="_blank" rel="noopener">https://registry.npm.taobao.org</a></p></blockquote><p>之后所有含有npm的命令全部用cnpm代替.</p><p>在刚才建立文件夹的目录空白处右键, 选择<code>Git bash here</code>, 会弹出来一个命令行. 依次输入以下命令:</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> -g hexo-clihexo init <span class="token operator">&lt;</span>新建文件夹的名称<span class="token operator">></span><span class="token function">cd</span> <span class="token operator">&lt;</span>新建文件夹的名称<span class="token operator">></span><span class="token function">npm</span> <span class="token function">install</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>当然如果用<code>cnpm</code>要将npm进行相应替换. 这个过程中如果<code>hexo init &lt;新建文件夹名&gt;</code>发生超时错误, 只需要在发生错误后直接输入<code>cnpm install</code>就能安装成功了.</p><p>等初始化完成后, Hexo在本地的框架已经完成, 依次输入:</p><pre class="line-numbers language-bash"><code class="language-bash">hexo ghexo s<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>应该能看到命令行服务器运行在<code>localhost:4000</code>上, 浏览器中进入可以看到Hexo的默认主题landscape.</p><h2 id="关联github"><a href="#关联github" class="headerlink" title="关联github"></a>关联github</h2><p>如果要将本地的内容发布到Github上, 还没有建立你电脑与Github的关联. 这里还需要安装一个插件:</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> hexo-deployer-git --save<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>然后要做的就是将你文件夹与Github关联起来. 命令行用之前注册github时用的邮箱账号生成一个秘钥:</p><pre class="line-numbers language-bash"><code class="language-bash">ssh-keygen -t rsa -C <span class="token string">"邮箱地址"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>去<code>C:\Users\用户名</code>中找到一个隐藏文件夹<code>.ssh</code>, 用记事本或者别的文本工具打开里面的<code>id_rsa.pub</code>, 复制所有内容.</p><p>去github右上角的个人头像点击<code>settings</code>, 在左侧找到<code>SSH and GPG keys</code>, 点击<code>New SSH key</code>, 将复制的内容放进key中, title随便写, 然后点击<code>Add SSH Key</code>.</p><p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200723234258.png" alt=""></p><p>然后检测是否连接成功, 在命令行敲入:</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">ssh</span> -T git@github.com<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>弹出的内容输入yes, 出现<code>Hi &lt;你的github用户名&gt;! You&#39;ve successfully authenticated, but GitHub doesnot provide shell access.</code>表示连接成功.</p><h2 id="修改框架基本配置"><a href="#修改框架基本配置" class="headerlink" title="修改框架基本配置"></a>修改框架基本配置</h2><p>在之前创建的博客文件夹下, 找到<code>_config.yml</code>, 这里储存着Hexo的基本配置. 在文件首部有这么几个内容要修改(图片来自原博主):</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200723235028.png" alt="借用原博主图" style="zoom: 67%;" /><ol><li>title: 博客名</li><li>subtitle: 博客的子标题</li><li>author: 每篇文章的作者</li><li>language: 汉化建议改为zh-CN</li><li>url: 改为<code>&lt;你github的用户名&gt;.github.io</code></li></ol><p>然后去文件尾部的<code>deploy</code>更改如下:</p><img src="https://cdn.jsdelivr.net/gh/ADAning/Image/Other/20200723235441.png" style="zoom:80%;" /><p>直接照原图修改就行了. 注意<code>theme</code>, 这个字段是之后进行Hexo主题美化的关键.</p><h2 id="Hexo常用命令"><a href="#Hexo常用命令" class="headerlink" title="Hexo常用命令"></a>Hexo常用命令</h2><h3 id="本地更新至Github"><a href="#本地更新至Github" class="headerlink" title="本地更新至Github"></a>本地更新至Github</h3><p>完成上述操作后, 命令行输入:</p><pre class="line-numbers language-bash"><code class="language-bash">hexo ghexo d<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><code>hexo g</code>会将当前配置渲染出一套静态页面, <code>hexo d</code>会直接部署到github上. 在自己的github个人主页就能看到更新后的内容了(可能有延迟).</p><h3 id="其他常用命令"><a href="#其他常用命令" class="headerlink" title="其他常用命令"></a>其他常用命令</h3><blockquote><p>hexo init 文件夹名称 # 初始化</p><p>hexo new “postName” #新建文章</p><p>hexo new page “pageName” #新建页</p><p>hexo generate #生成静态页面至public目录</p><p>hexo server #开启预览访问端口（默认端口4000，’ctrl + c’关闭server）</p><p>hexo deploy #将.deploy目录部署到GitHub</p><p>hexo help  # 查看帮助</p><p>hexo version  #查看Hexo的版本</p><p>hexo deploy -g  #生成加部署</p><p>hexo server -g  #生成加预览</p><p>命令的简写</p><p>hexo n == hexo new</p><p>hexo g == hexo generate</p><p>hexo s == hexo server</p><p>hexo d == hexo deploy</p></blockquote><p>一般情况下博客有更新后可以<strong>结合使用</strong>: <code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo s</code>, 这样直接就清除了之前使用的缓存, 重新生成, 然后打开本地服务器, 三个操作自动串行完成.</p>]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MathJax常见问题</title>
      <link href="/posts/33457.html"/>
      <url>/posts/33457.html</url>
      
        <content type="html"><![CDATA[<h1 id="MathJax常见问题"><a href="#MathJax常见问题" class="headerlink" title="MathJax常见问题"></a>MathJax常见问题</h1><p>本文记录了在配置hexo博客和MathJax一起使用的多个问题及解决方法. hexo本身和数学公式的兼容性就不是很好, 所以发生了很多兼容性问题.</p><h2 id="MathJax和英文小括号的冲突"><a href="#MathJax和英文小括号的冲突" class="headerlink" title="MathJax和英文小括号的冲突"></a>MathJax和英文小括号的冲突</h2><p>如果你即使用了hexo也使用了MathJax, 那么有很大几率你会使用小括号来正常书写文章内容. </p><p>譬如:</p><p>This text is (in parenthesis) but it doesn’t work.</p><p>它会显示成:</p><p>This text is $in parenthesis$ but it doesn’t work.</p><p>起初, 我一度认为是MathJax的大括号冲突导致了某个地方的小括号也出了同样的渲染问题, 但后来发现问题完全跑偏了. 这个真的卡了我挺长时间的… 很多地方都没有记载过这个问题. 直到我看到了关于MathJax3.0版本的配置问题, 才找到一段额外管理内联公式的script, 那么2.7是不是也应该有相应的一段呢?</p><p>在<code>blog\themes\hexo-theme-matery\layout\post.ejs</code>中, 有这样一段代码, 是用来调整内联公式的识别方式的. 如果不添加这段js, Mathjax只能识别大的公式, 而不能识别内联公式. 很明显, 这里直接就将转义后的<code>()</code>小括号对也当做公式匹配范围了, 也就不难解释之前错误识别的情况.</p><pre class="line-numbers language-html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>%</span> <span class="token attr-name">if</span> <span class="token attr-name">(theme.mathjax.enable</span> <span class="token attr-name">&amp;&amp;</span> <span class="token attr-name">page.mathjax)</span> <span class="token attr-name">{</span> <span class="token attr-name">%</span><span class="token punctuation">></span></span>&lt;script src="<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>%-</span> <span class="token attr-name">theme.mathjax.cdn</span> <span class="token attr-name">%</span><span class="token punctuation">></span></span><span class="token script language-javascript">"<span class="token operator">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>script</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>script</span><span class="token punctuation">></span></span><span class="token script language-javascript">    MathJax<span class="token punctuation">.</span>Hub<span class="token punctuation">.</span><span class="token function">Config</span><span class="token punctuation">(</span><span class="token punctuation">{</span>        tex2jax<span class="token punctuation">:</span> <span class="token punctuation">{</span>inlineMath<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">'$'</span><span class="token punctuation">,</span> <span class="token string">'$'</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token string">'\('</span><span class="token punctuation">,</span> <span class="token string">'\)'</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>script</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>%</span> <span class="token attr-name">}</span> <span class="token attr-name">%</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我们只要把<code>[&#39;\(&#39;, &#39;\)&#39;]</code>删去, 或者替换成更严格匹配的括号对<code>[&#39;\\(&#39;, &#39;\\)&#39;]</code>, 就能解决在博客markdown不能正确显示小括号内容的问题.</p><h2 id="MathJax和Markdown冲突"><a href="#MathJax和Markdown冲突" class="headerlink" title="MathJax和Markdown冲突"></a>MathJax和Markdown冲突</h2><p>Mathjax和Markdown的下划线语法冲了. 比如:<br>$$<br>w^{(l)}_{ij} = w^{(l)}_{ij} - \eta\frac{\partial E(W, b)}{\partial w^{(l)}_{ij}}<br>$$<br>它大概率会渲染不出来, 变成:</p><blockquote><p>$$</p><p>w^{(l)}<em>{ij} = w^{(l)}</em>{ij} - \eta\frac{\partial E(W, b)}{\partial w^{(l)}_{ij}}</p><p>$$</p></blockquote><p>这是因为Markdown和Mathjax的语法冲了, 这时候必须要修改<code>blogtest\node_modules\marked\lib\marked.js</code>, 找到<code>var inline</code>, 修改<code>escape</code>和<code>em</code>的正则匹配式:</p><pre class="line-numbers language-js"><code class="language-js"><span class="token keyword">var</span> inline <span class="token operator">=</span> <span class="token punctuation">{</span> escape<span class="token punctuation">:</span> <span class="token regex">/^\\([`*\[\]()#$+\-.!_>])/</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">// ...</span> em<span class="token punctuation">:</span> <span class="token regex">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true">// 去除了下划线_的斜体匹配</span> <span class="token comment" spellcheck="true">// ...</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>千万不要去装<code>hexo-renderer-kramed</code>!</strong> 否则如果你安装了<code>hexo-prism-plugin</code>, 也就是代码显示插件, 那这个代码高亮插件就崩了.</p><p>在更换<code>escape</code>的正则后, 一定要避免<code></code>的出现, 最起码要在其中加上一个空格变成<code>{ {</code>和<code>} }</code>, 否则会编译报错. </p><p>我尝试过保留 <code>escape</code>中对<code>\\</code>的转义,  这样4个反斜杠才能换行, <code>Typora</code>里写的时候会额外多空出来一行. 我这里在去掉这个<code>\\</code>后, 在公式换行时仍然会莫名其妙有1个反斜杠被转义, 这样需要3个反斜杠才能换行, 在<code>Typora</code>里会导致公式第二行多一个空格(实际渲染以后没有问题), 但比上面多换一行要强. </p><p><strong>2020.08.11</strong>: 发现3个反斜杠在<code>Typora</code>写表格时使表格无法正确显示.</p><p>还有一个很蛋疼的地方, 有些地方不能在公式中使用带有<code>*</code>的字符, 会导致之后的内容变成斜体. 因为去掉了<code>_</code>在公式中的正则, 那就只能通过<code>*</code>来表示斜体. 虽然斜体我几乎不用, 但是星号是表示斜体唯一的方法了, 考虑到这个就没有把斜体的正则匹配也去掉.</p><p><strong>2020.08.11</strong>: 今天在整理Markdown公式写法的时候, 意外发现了<strong>星号可以用<code>\ast</code>代替</strong>! 于是这个问题也被解决了.</p><h2 id="MiniValine冲突"><a href="#MiniValine冲突" class="headerlink" title="MiniValine冲突"></a>MiniValine冲突</h2><p><strong>2020.8.18</strong>: 当博客的<code>mathjax</code>和<code>minivaline</code>同时使用时, 会变得很卡. 这是因为<code>minivaline</code>里的<code>math</code>和博客公式显示的<code>mathjax</code>有冲突. 把它关掉就行了.</p><pre class="line-numbers language-yml"><code class="language-yml">minivaline:  enable: true  # ...  math: false # Support MathJax.  # ...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="MathJax2迁移到MathJax3"><a href="#MathJax2迁移到MathJax3" class="headerlink" title="MathJax2迁移到MathJax3"></a>MathJax2迁移到MathJax3</h2><p>其实<code>MathJax</code>除了上面那些不太兼容的问题(确实有点恶心)来说, 没什么太大的毛病. 但是一旦公式多了就体现出来一个问题, 慢! 现在<code>MathJax</code>已经更新到3.0及以上版本, 官方团队直接把2的代码重写了, 效率提高了<strong>60% ~ 80%</strong>, 这个速度还是非常诱人的.</p><blockquote><p>There were a number of design goals to the version 3 rewrite. A primary one was to improve the rendering speed of MathJax, and we feel we have accomplished that. Because the two versions operate so differently, it is difficult to make precise comparisons, but in tests that render a complete page with several hundred expressions, we see a reduction in rendering time of between 60 and 80 percent, depending on the browser and type of computer.</p></blockquote><h3 id="更新版本"><a href="#更新版本" class="headerlink" title="更新版本"></a>更新版本</h3><p>因为我用的是<code>Matery</code>, 所以自己稍微改了改. 其他主题的更改方法大致相同.</p><p>先在主题下的<code>_config.yml</code>文件中, 将之前的<code>mathjax</code>连接的网址更换为最新的. 当然我这里直接采用了保留原来2.7的版本. 除了因为需要旧版本可以随时更换回来, 还有一个原因就是3.0的配置方法与2.7版本的配置方式是不同的, 这就意味着在渲染过程中的代码需要重新写一份, 没有必要把旧版本的内容完全删除.</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">mathjax</span><span class="token punctuation">:</span>  <span class="token key atrule">enable2</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>  <span class="token key atrule">enable3</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">cdn2</span><span class="token punctuation">:</span> https<span class="token punctuation">:</span>//cdn.bootcss.com/mathjax/2.7.7/MathJax.js<span class="token punctuation">?</span>config=TeX<span class="token punctuation">-</span>AMS<span class="token punctuation">-</span>MML_HTMLorMML  <span class="token key atrule">cdn3</span><span class="token punctuation">:</span> https<span class="token punctuation">:</span>//cdn.jsdelivr.net/npm/mathjax@3/es5/tex<span class="token punctuation">-</span>mml<span class="token punctuation">-</span>chtml.js<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>然后再去改渲染的脚本<code>blogtest\themes\hexo-theme-matery\layout\post.ejs</code>,  并参考<a href="http://docs.mathjax.org/en/latest/web/configuration.html#configuration" target="_blank" rel="noopener">官方文档配置教程</a>, 在末尾更改:</p><pre class="line-numbers language-javascript"><code class="language-javascript"><span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>theme<span class="token punctuation">.</span>mathjax<span class="token punctuation">.</span>enable2 <span class="token operator">&amp;&amp;</span> page<span class="token punctuation">.</span>mathjax<span class="token punctuation">)</span> <span class="token punctuation">{</span> <span class="token operator">%</span><span class="token operator">></span><span class="token operator">&lt;</span>script src<span class="token operator">=</span><span class="token string">"&lt;%- theme.mathjax.cdn2 %>"</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>script<span class="token operator">></span><span class="token operator">&lt;</span>script<span class="token operator">></span>    MathJax<span class="token punctuation">.</span>Hub<span class="token punctuation">.</span><span class="token function">Config</span><span class="token punctuation">(</span><span class="token punctuation">{</span>        tex2jax<span class="token punctuation">:</span> <span class="token punctuation">{</span> inlineMath<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">'$'</span><span class="token punctuation">,</span> <span class="token string">'$'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">'\\('</span><span class="token punctuation">,</span> <span class="token string">'\\)'</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token punctuation">}</span> <span class="token comment" spellcheck="true">// 行内公式标识</span>    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token operator">&lt;</span><span class="token operator">/</span>script<span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token punctuation">}</span> <span class="token operator">%</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>theme<span class="token punctuation">.</span>mathjax<span class="token punctuation">.</span>enable3 <span class="token operator">&amp;&amp;</span> page<span class="token punctuation">.</span>mathjax<span class="token punctuation">)</span> <span class="token punctuation">{</span> <span class="token operator">%</span><span class="token operator">></span><span class="token operator">&lt;</span>script id<span class="token operator">=</span><span class="token string">"MathJax-script"</span> <span class="token keyword">async</span> src<span class="token operator">=</span><span class="token string">"&lt;%- theme.mathjax.cdn3 %>"</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>script<span class="token operator">></span><span class="token operator">&lt;</span>script<span class="token operator">></span>    MathJax <span class="token operator">=</span> <span class="token punctuation">{</span>        tex<span class="token punctuation">:</span> <span class="token punctuation">{</span>            inlineMath<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">'$'</span><span class="token punctuation">,</span> <span class="token string">'$'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">'\\('</span><span class="token punctuation">,</span> <span class="token string">'\\)'</span><span class="token punctuation">]</span><span class="token punctuation">]</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token operator">&lt;</span><span class="token operator">/</span>script<span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token punctuation">}</span> <span class="token operator">%</span><span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这样在需要旧版本时候直接调整<code>enable</code>就行了. 当然你还可以更改其他的配置, 参考教程即可.</p><h3 id="无法换行"><a href="#无法换行" class="headerlink" title="无法换行"></a>无法换行</h3><p>我换了以后发现一个问题, 多行公式开始变得没法换行了. 我已经排除掉了转义的问题, 确确实实已经有两个反斜杠, 可是公式没法换行.</p><p>测试了一会后, 发现无论多少反斜杠都没有办法换行, 我在<a href="https://github.com/mathjax/MathJax/issues/2312" target="_blank" rel="noopener">官方issue</a>找到了答案(有超多人有这个问题). 官方从旧版本迁移过来还没有实现换行符, 官方提到了两种办法来解决这个问题.</p><ol><li>使用<code>\displaylines</code>.</li></ol><p>$$<br>\displaylines{\lim_{N\to\infty}E(p_{max})=N\frac{1}{N}(1-\frac{1}{N})^{N-1} = \frac{(1-\frac{1}{N})^N}{1-\frac{1}{N}} \\<br>\lim_{N\to \infty}{1-\frac{1}{N}}=1  \qquad\lim_{N\to \infty}({1-\frac{1}{N}})^N=\frac{1}{e} \\<br>\lim_{N\to \infty}{E(p_{max})}=\frac{1}{e}=37\%}<br>$$</p><p>​            我当然比较倾向于用这种, 因为我不习惯下面那种编写方式, 在<code>Typora</code>里实在是太麻烦了.</p><ol start="2"><li>使用<code>{align}</code>或者<code>{aligned}</code>.</li></ol><p>$$<br>\begin{align}<br>x’ = \frac{x - x_{median}}{IQR}  \\<br>IQR = x_{q3} - x_{q1}<br>\end{align}<br>$$<br>​            这种方式当然也有优点, 就是方便公式的<strong>对齐</strong>, 因为对齐时候必须要在多行公式的环境里面.<br>$$<br>\begin{aligned}<br>E(p)&amp;=Np(1-p)^{N-1} \\<br>E’(p)&amp;=N(1-p)^{N-1} -Np(1-p)^{N-2} \\<br>&amp;=N(1-p)^{N-2}((1-p)-p(N-1))\\<br>E’(p)&amp;=0 \Rightarrow p_{max}=\frac{1}{N}<br>\end{aligned}<br>$$<br>还发现了一些意外小惊喜:</p><blockquote><p>WordPress uses <code>\</code> as a special character, and it is often stripped out of WordPress posts (depending on what editor you are using). As an escape character, it can prevent the normal action of the character that follows it, so to get an explicit backslash into your post, you need to double it. That is why you need an extra backslash (because the first two equal a single backslash in the output). You probably should double the second one as well, but apparently that is not required.</p></blockquote><p>官方人员提到了WordPress中的类似问题, 我猜hexo第一个反斜杠被转义也是类似的原因.</p>]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Mathjax </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算机网络-学校</title>
      <link href="/posts/7425.html"/>
      <url>/posts/7425.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>本篇后半部分内容产生格式错乱和渲染错误, 是Ipad导出时所产生的格式混乱所致, 现已无法全部修改, 请转看另一篇计网内容.</p></blockquote><h2 id="Chapter-1-Computer-Networks-and-the-Internet-about-10"><a href="#Chapter-1-Computer-Networks-and-the-Internet-about-10" class="headerlink" title="Chapter 1 Computer Networks and the Internet (about 10%)"></a>Chapter 1 Computer Networks and the Internet (about 10%)</h2><h3 id="1-1-What’s-the-Internet"><a href="#1-1-What’s-the-Internet" class="headerlink" title="1.1 What’s the Internet?"></a>1.1 What’s the Internet?</h3><ul><li>Internet: The Internet is a worldwide computer network, that is a network that inter connects millions of computing devices throughout the world.</li><li>Host(end system): Computers and other devices connected to the Internet run application programs.</li><li>Packet switching: The resources are not reserved; a session’s messages use the resources on demand, and as a consequence, may have to wait for access to a communication link.</li><li>Path/route</li><li>Protocols: controlling the sending and receiving of information within the Internet.<br>A protocol defines the <strong>format</strong> and the <strong>order</strong> of messages exchanged between two or more communicating entities , as well as the <strong>actions</strong> taken on the transmission and/or receipt of a message or other event.</li></ul><h3 id="1-2-The-Network-Edge"><a href="#1-2-The-Network-Edge" class="headerlink" title="1.2 The Network Edge"></a>1.2 The Network Edge</h3><ul><li><p>Client: client host requests, receives service from always-on server</p></li><li><p>Server: Server host provides service</p></li><li><p>Guide and unguided media</p><ul><li>Guide media: signals propagate in solid media: twisted-pair copper, fiber, coaxial cable.</li><li>Unguided media: signals propagate in the atmosphere and in outer space: radio, satellite channel.</li></ul></li><li><p>Three types of access networks</p><ol><li><strong>residential access</strong>: cable modems</li><li><strong>company access</strong>: local area networks</li><li><strong>wireless access</strong> network</li></ol></li><li><p>Differences of circuit switching and packed switching</p><table><thead><tr><th>Difference</th><th>Circuit Switching</th><th>Packet switching</th></tr></thead><tbody><tr><td>Resource</td><td>Pre-allocate resource</td><td>Allocate resource on demand</td></tr><tr><td>Bandwidth</td><td>Dedicated</td><td>Shared</td></tr><tr><td>Forwarding</td><td>Cut-through</td><td>Stored and forward</td></tr></tbody></table></li><li><p>TDM and FDM</p><ul><li>FDM: The <strong>frequency</strong> spectrum of a link is divided up among the connections established across the link.</li><li>TDM: <strong>time</strong> is divided into frames of fixed duration, and each frame is divided into a fixed number of time slots.</li></ul></li><li><p>Store and forward transmission: the switch must receive the <strong>entire packet</strong> before it can begin to transmit the first bit of the packet onto the outbound link.</p></li></ul><h3 id="1-3-Delay-and-Loss-in-Packed-Switched-Networks"><a href="#1-3-Delay-and-Loss-in-Packed-Switched-Networks" class="headerlink" title="1.3 Delay and Loss in Packed-Switched Networks"></a>1.3 Delay and Loss in Packed-Switched Networks</h3><ul><li><p>Types of delay(definition and compute)</p><ul><li>Nodal processing delay: examining the packet’s header and determine where to direct the packet check bit errors.</li><li>Queueing delay: time waiting to be transmitted onto the link depends on congestion level of router.</li><li>Transmission delay: time required to push all of the packet’s bits into the link.<ul><li>$R = link\ bandwidth$</li><li>$L = packet\ length$</li><li>$d_{trans} = L\ /\ R$</li></ul></li><li>Propagation delay: time required to propagate from the beginning of the link to next router (or host).<ul><li>$d = lenght\ of\ physical\ link$</li><li>$s = propagation\ speed\ in\ medium$</li><li>$d_{prop} = d\ /\ s$</li></ul></li><li>$d_{nodel} = d_{proc} + d_{queue} + d_{trans} + d_{prop}$</li><li>end 2 end delay(Ignore queuing delay):<br>$d_{e2e} = N(d_{proc} + d_{trans} + d_{prop})$<br>where N is number of links(N-1)routers</li></ul></li><li><p>Traffic intensity</p><ul><li>$traffic_intensity = La / R$</li></ul><ul><li>$R = link\ bandwidth$</li><li>$L = packet\ length$</li><li>$a = average\ packet\ arrival\ rate$</li><li>Golden rules: design your system so that the traffic intensity is <strong>no greater than 1</strong></li></ul></li></ul><h3 id="1-4-Protocol-Layers-and-their-service-models"><a href="#1-4-Protocol-Layers-and-their-service-models" class="headerlink" title="1.4 Protocol Layers and their service models"></a>1.4 Protocol Layers and their service models</h3><ul><li><p>Layered architecture of computer network</p><ol><li>application</li><li>transport</li><li>network</li><li>link</li><li>physical </li></ol></li><li><p>Encapsulation: add head information that before forwarding</p></li><li><p>Each layer packet name</p><table><thead><tr><th>Layer</th><th>Packet name</th></tr></thead><tbody><tr><td>Application</td><td>message</td></tr><tr><td>Transport</td><td>segment</td></tr><tr><td>Network</td><td>datagram</td></tr><tr><td>Link</td><td>frame</td></tr><tr><td>Physical</td><td>bit</td></tr></tbody></table></li></ul><h3 id="Addtion"><a href="#Addtion" class="headerlink" title="Addtion"></a>Addtion</h3><ul><li>重点：什么是因特网；什么是协议；客户端和服务器端程序；网络接入；物理媒体；电路交换和分组交换；时延分类；排队时延与分组丢失的关系；网络分层中各层次的关系与功能。</li><li>难点：协议的定义；网络接入方法；物理媒体种类；电路交换与分组交换技术及区别；时延的分类与计算；传输时延与传播时延的区别；网络分层的结构以及各层次的关系。</li></ul><h2 id="Chapter-2-Application-Layers-about-20"><a href="#Chapter-2-Application-Layers-about-20" class="headerlink" title="Chapter 2 Application Layers (about 20%)"></a>Chapter 2 Application Layers (about 20%)</h2><h3 id="2-1-Principles-of-Network-Application"><a href="#2-1-Principles-of-Network-Application" class="headerlink" title="2.1 Principles of Network Application"></a>2.1 Principles of Network Application</h3><ul><li>Difference of two types of network application architecture<ol><li>C/S<ul><li>server: always-on host fixed,well-known IP address server farms for scaling.</li><li>client: communicate with server may be intermittently connected may have dynamic IP addresses do not communicate directly with each other.</li><li>In processes view, client is process that <strong>initiates communication</strong>, process that waits to <strong>be contacted</strong>.</li></ul></li><li>P2P: no always-on server arbitrary end systems, directly communicate peers are intermittently connected and change IP addresses</li></ol><ul><li>Socket and port number</li></ul><ul><li>Socket: A socket is the <strong>interface</strong> between the <strong>application layer</strong> and the <strong>transport layer</strong> within a host. Process sends/receives messages to/from its socket.</li><li>Port number: associated with process on host.</li></ul><ul><li>Services which application need</li></ul><ol><li>reliable data transfer</li><li>timing</li><li>security</li><li>bandwidth / throughput</li></ol><ul><li>Services provided by Internet transport protocol</li><li>TCP<ol><li>connetction-oriented</li><li>reliable transport</li><li>flow control</li><li>congestion control</li></ol></li><li>UDP<ol><li>connectionless</li><li>unreliable data transfer</li></ol></li><li><strong>They both can’t provide timing, bandwidth</strong>.</li></ul></li></ul><h3 id="2-2-The-Web-and-HTTP"><a href="#2-2-The-Web-and-HTTP" class="headerlink" title="2.2 The Web and HTTP"></a>2.2 The Web and HTTP</h3><ul><li><p>Web page, object, base HTML file and URL and so on.<br>Web page consist of objects. Objects can be in many types, and each object is addressable by a URL.</p></li><li><p>Difference of non-persistent and persistent connection</p><ul><li>non-persistent: at most <strong>one object</strong> is sent over a TCP connection.</li><li>persistent: <strong>multiple objects</strong> can be sent over single TCP connection.</li></ul></li><li><p>Compute how many RTT used:</p><ol><li>non-persistent without parallel: $2nRTT$</li><li>non-persistent with n parallel: $n\to∞,  (2+2)RTT$</li><li>persistent without pipelining: $(n+1)RTT$</li><li>persistent with pipelining: $2RTT / 3RTT$</li></ol></li><li><p>HTTP’s basic conception</p><ul><li>port number: 80</li><li>stateless</li><li>work in C/S model</li></ul></li><li><p>Format of request and response message</p><ul><li>request:<ol><li>request line</li><li>header line</li><li>blank line</li><li>entity body</li></ol></li><li>response:<ol><li>status line</li><li>header lines</li><li>blank line</li><li>entity body</li></ol></li></ul><ul><li>header line:<ol><li>Date: indicates the time and date when the HTTP response was created and sent by the client.</li><li>Last-Modified: indicates the time and date when the object was created or last modified.</li><li>Content-Length: indicates the number of bytes in the object being sent</li><li>Content-Type: indicates type of the object in the entity body is</li></ol></li></ul></li><li><p>Web caches(proxy server): Web accesses via cache browser sends all HTTP requests to cache.</p></li><li><p>Conditional GET method</p></li></ul><h3 id="2-3-File-Transfer-FTP"><a href="#2-3-File-Transfer-FTP" class="headerlink" title="2.3 File Transfer:FTP"></a>2.3 File Transfer:FTP</h3><ul><li>Basic conception and common commands<ul><li>Conception<ul><li>port number: 21(control) / 20(data)</li><li>control in persistent, data in non-persistent</li><li>C/S</li><li>TCP</li></ul></li></ul><ul><li>Command<ul><li>USER: useranme</li><li>PASS: password</li><li>LIST: return list of file in current directory</li><li>RETR: get file</li><li>STOR: put files onto remote host</li></ul></li></ul></li><li>Difference of FTP and HTTP<ol><li>FTP control connetion out of band, HTTP in band.</li><li>FTP server maintains state, HTTP is stateless</li></ol></li></ul><h3 id="2-4-Electronic-Mail-in-the-Internet"><a href="#2-4-Electronic-Mail-in-the-Internet" class="headerlink" title="2.4 Electronic Mail in the Internet"></a>2.4 Electronic Mail in the Internet</h3><ul><li><p>Three major components of E-mail</p><ol><li>user agents</li><li>mail servers</li><li>SMTP protocol for mail transfer</li></ol></li><li><p>Basic conception of SMTP</p><ul><li>C/S</li><li>TCP</li><li>port number 25</li><li>must be 7-bit ASCII</li></ul></li><li><p>Difference of SMTP and HTTP</p><ul><li>HTTP<ol><li>pull protocol</li><li>encapsulates each object with response message</li><li>has no message restriction</li></ol></li><li>SMTP<ol><li>push protocol</li><li>place all message’s object into one message</li><li>each message must be 7-bit ASCII format</li></ol></li></ul></li><li><p>Mail message formats and MIME</p><ul><li><p>format:</p><blockquote><p>TO:</p><p>From:</p><p>Subject:</p><p>Body:</p></blockquote></li><li><p>MIME: multimedia mail extension additional lines in msg header declare MIME content type.</p></li></ul></li><li><p>Types of mail access protocols</p><ul><li>POP3:port 110</li><li>IMAP: port 143</li><li>HTTP</li></ul></li><li><p>Basic conception of POP3 and IMAP</p><ul><li>POP3<ul><li>session<ol><li>authorization phase</li><li>transaction phase</li><li>update phase</li></ol></li><li>stateless</li><li>have download and delete or download and keep model(download to local host)</li></ul></li><li>IMAP<ul><li>keep all messages in server</li><li>allow user to orgnize message in folders</li><li>keep user state</li></ul></li></ul></li><li><p>C/S sessions of SMTP and POP3</p></li></ul><h3 id="2-5-DNS-The-Internet’s-Direcotry-of-Service"><a href="#2-5-DNS-The-Internet’s-Direcotry-of-Service" class="headerlink" title="2.5 DNS-The Internet’s Direcotry of Service"></a>2.5 DNS-The Internet’s Direcotry of Service</h3><ul><li><p>Services of provided by DNS</p><ol><li>hostname to IP address translation</li><li>host aliasing</li><li>mail server aliasing</li><li>load distribution</li></ol></li><li><p>How DNS works before other applications protocols</p><ul><li><p>port number 53</p></li><li><p>Work in UDP</p></li><li><p><strong>?????</strong></p></li></ul></li><li><p>Hierarchical architecture and local DNS server</p><ul><li>Hierarchical architecture:<ul><li>Root servers: contacts authoritative name server if name mapping not known gets mapping<br>returns mapping to local name server.</li><li>Top-level domain(TLD) servers: responsible for com, org, net, edu, etc, and all top-level country domains uk, fr, ca, jp.</li><li>Authoritative DNS servers: organization’s DNS servers, providing authoritative hostname to IP mappings for organization’s servers (e.g., Web, mail).</li></ul></li><li>local Name servers: when host makes DNS query, query is sent to its local DNS server.</li></ul></li></ul><h3 id="2-6-Peer-to-Peer-Application"><a href="#2-6-Peer-to-Peer-Application" class="headerlink" title="2.6 Peer-to-Peer Application"></a>2.6 Peer-to-Peer Application</h3><ul><li>The basic conception of P2P file sharing<ul><li>based on HTTP</li><li>centralized directory - Napster</li><li>Query flooding - Gnutella<ul><li>fully distributed</li><li>public domain protocol</li></ul></li><li>hierarchical overlay - KaZaA<ul><li>based on TCP</li></ul></li></ul></li></ul><h3 id="Addtion-1"><a href="#Addtion-1" class="headerlink" title="Addtion"></a>Addtion</h3><ul><li>重点：两种类型的网络应用结构（C/S和P2P结构）；持久连接与非持久连接的原理；HTTP请求报文与响应报文格式；HTTP请求报文的方法；HTTP响应报文的状态码；Web缓存的优点；FTP的连接方法与命令；FTP与HTTP的不同点；E-mail的三个组成部分；SMTP的基本概念；SMTP和HTTP的区别；邮件报文格式与MIME；POP3和IMAP协议的基本概念；邮件访问协议种类；DNS的功能；DNS服务器的层次结构；P2P协议基本概念。</li><li>难点：socket和port number的概念；HTTP的基本概念；持久连接与非持久连接的方法；条件GET方法；FTP工作时的两种并行连接；SMTP和HTTP的区别；POP3与IMAP的区别；DNS协议的分层与访问方式。</li></ul><h2 id="Chapter-3-Transport-Layer-about-30"><a href="#Chapter-3-Transport-Layer-about-30" class="headerlink" title="Chapter 3 Transport Layer (about 30%)"></a>Chapter 3 Transport Layer (about 30%)</h2><h3 id="3-1-Introduction-and-Transport-Layer-Service"><a href="#3-1-Introduction-and-Transport-Layer-Service" class="headerlink" title="3.1 Introduction and Transport-Layer Service"></a>3.1 Introduction and Transport-Layer Service</h3><ul><li>Relationship between transport and network layers<ul><li>Transport layer<ol><li>provide logical communication between <strong>processes</strong></li><li>encapsulate message to segment</li></ol></li><li>Network layer<ol><li>provide logical communication between <strong>host</strong></li><li>encapsulate segment to datagram</li></ol></li></ul></li><li>Services provided by TCP and UDP. <ul><li>TCP<ol><li>congestion control</li><li>flow control</li><li>connection setup</li><li>reliable, in-ordered</li></ol></li><li>UDP<ol><li>best effort</li><li>unreliable, unordered delivery</li></ol></li></ul></li></ul><h3 id="3-2-Multiplexing-and-De-multiplexing"><a href="#3-2-Multiplexing-and-De-multiplexing" class="headerlink" title="3.2 Multiplexing and De-multiplexing"></a>3.2 Multiplexing and De-multiplexing</h3><ul><li>Multiplexing and de-multiplexing<ul><li>De-multiplexing: Delivering the data in a segment to the correct socket.</li><li>Multiplexing: gathering data from multiple sockets, encapsulating data with header (later used for demultiplexing) to create segments, and passing to the network layer. </li></ul></li><li>The difference of UDP and TCP Multiplexing/ de-multiplexing<ul><li>demux UDP identified by <strong>2</strong> tuple(dest IP addr, dest port number)</li><li>demux TCP identified by <strong>4</strong> tuple(source IP addr, source port number, dest IP addr, dest port number)</li></ul></li><li>MSS and MTU<ul><li>MSS: maximum segment size, maximum amount of data that can be grabbed and placed in a segment (application-layer data in the segment).</li><li>MTU: maximum transmission unit, maximum amount of data that a link-layer frame can carry.</li></ul></li></ul><h3 id="3-3-Connectionless-Transport-UDP"><a href="#3-3-Connectionless-Transport-UDP" class="headerlink" title="3.3 Connectionless Transport: UDP"></a>3.3 Connectionless Transport: UDP</h3><ul><li><p>Basic conception and format of UDP</p><ul><li>Conception<ul><li>connectionless</li><li>no retransmit</li><li>no congestion control</li><li><strong>small segment header: 8 bytes header</strong></li></ul><ul><li>Format(8 bytes)</li></ul></li></ul><ol><li>source port(16)</li><li>dest port(16)</li><li>length(bytes) (16)</li><li>checksum(16)<ul><li>applications: DNS, SNMP, RIP</li></ul></li></ol></li><li><p>Checksum methods </p><ul><li>carryout</li><li>wraparound</li><li>bits inversion</li></ul></li></ul><h3 id="3-5-Connection-Oriented-Transport-TCP"><a href="#3-5-Connection-Oriented-Transport-TCP" class="headerlink" title="3.5 Connection-Oriented Transport: TCP"></a>3.5 Connection-Oriented Transport: TCP</h3><ul><li><p>Format of TCP segment structure(<strong>20 bytes at least</strong>)</p><ul><li>port number[16 * 2(dest, source)]</li><li>sequence number(32)</li><li>ACK(32)</li><li>checksum(16)</li><li>rcv window(16)</li><li>header length(4)</li><li>flag(6)</li><li>urg data pnter(16)</li></ul></li><li><p>Sequence numbers and ACK numbers</p><ul><li>sequence number: the byte stream “number” of first bytes in segment’s data</li><li>ACK number: The sequence number of next byte expected from other side.</li></ul></li><li><p>Analyze the reliable data transfer of TCP (Simplified TCP, doubling the timeout interval and fast retransmit)</p><ul><li>dobubling timeout interval: Timeout is a kind of congestion event, so next timeout interval = 2× previous timeout interval</li><li>fast retransmit: If sender receives 3 duplicate ACKs for the same data, it supposes that segment after ACKed data was lost.</li></ul></li><li><p>Function of flow control and receive window</p><ul><li>flow control: sender won’t overflow receiver’s buffer by transmitting too much, too fast.</li><li>receive window: app process may be slow at reading from buffer.</li></ul><p>$$<br>RcvWindow = RcvBuffer - [ LastByteRcvd - LastByteRead ]<br>$$</p><p>$$<br>LastByteSent - LastByteAck \le RcvWindow<br>$$</p></li><li><p>How to estimate the RRT and timeout</p><ul><li>RTT</li></ul><p>$$<br>EstimatedRTT_{(n)} = (1 - \alpha) <em> EstimatedRTT_{(n - 1)} + \alpha </em> SampleRTT_{(n - 1)}<br>$$<br>​        Typically, $\alpha=0.125$</p><ul><li>Timeout</li></ul><p>$$<br>DevRTT_{(n)} = (1 - \beta) <em> DevRTT_{(n - 1)} + \beta </em> |SampleRTT_{(n - 1)} - EsimatedRtt_{(n - 1)}|<br>$$</p><p>$$<br>TimeoutInterval = EstimatedRTT + 4 * DevRTT<br>$$</p><p>​        Typically, $\beta = 0.25$</p></li><li><p>Three-way handshaking</p><ol><li>client host sends TCP SYN segment to server The SYN bit, is set to 1 specifies initial sequence number (client_isn), no data</li><li>server host receives SYN, replies with SYNACK segment server allocates buffers<br>specifies server initial sequence number (server_isn), The SYN bit, is set to 1, ACK=client_isn+1</li><li>client receives SYNACK, replies with ACK segment, which may contain data.The client does so by putting the value server_isn+1 in the acknowledgment field of the TCP segment header). The SYN bit is set to 0, since the connection is establishe</li></ol><p>If you feel puzzled, see this table:</p><table><thead><tr><th>Source  host</th><th>Dest  host</th><th>SYN</th><th>Sequence</th><th>ACK</th><th>ACKFLAG</th></tr></thead><tbody><tr><td>A</td><td>B</td><td>1</td><td>client_isn</td><td>none</td><td>0</td></tr><tr><td>B</td><td>A</td><td>1</td><td>server_isn</td><td>client_isn + 1</td><td>1</td></tr><tr><td>A</td><td>B</td><td>0</td><td>client_isn + 1</td><td>server_isn + 1</td><td>1</td></tr></tbody></table></li><li><p>TCP congestion control algorithm</p><ul><li><p>AIMD: </p><ul><li>Approach: increase transmission rate (window size), probing for usable bandwidth, until loss occurs.</li><li>additive increase: increase CongWin by 1 MSS every RTT until loss detected (congestion avoidance).</li><li>multiplicative decrease: cut CongWin in half after loss (CongWin$\geq$1MSS).</li><li>Congwin + 1 / n RTT when ACK.</li></ul></li><li><p>Slow start: When connection begins, increase rate exponentially fast until first loss event</p><ul><li>set CongWin = 0.5 * CongWin</li></ul></li><li><p>Reaction to Timeout Event:</p><ul><li>3 duplicated ACKs:<br>CongWin cut in half and grows linearly.</li><li>timeout event:<br>CongWin = 1 and grows exponetially. To a threshold grows linearly.</li></ul></li></ul></li><li><p>The compute of CongWin and Threshold</p><ul><li>When CongWin is below Threshold, sender in <em>slow- start</em> phase, window grows exponentially.<ul><li>When CongWin is above Threshold, sender is in congestion-avoidance phase, window grows linearly.</li><li>When a triple duplicate ACK occurs, Threshold set to CongWin/2 and CongWin set to Threshold.</li><li>When timeout occurs, Threshold set to CongWin/2 and CongWin is set to 1 MSS.</li></ul></li></ul></li></ul><h3 id="Addtion-2"><a href="#Addtion-2" class="headerlink" title="Addtion"></a>Addtion</h3><ul><li>重点：传输层的功能；TCP与UDP协议所提供的服务；多路复用与多路分解的概念；UDP协议的基本概念与格式；checksum方法；TCP报文段格式；序列号与确认序号；简单TCP传输模型；快速重传方法；流量控制；TCP连接建立的过程与关闭的过程；拥塞窗口计算；TCP拥塞控制算法。</li><li>难点：传输层与网络层的关系与区别；多路复用与多路分解的区别；滑动窗口协议；检查和校验方法；序列号与确认序号的计算方法；flag字段的填写；怎样实现TCP可靠数据传输；简单TCP传输模型实现；加倍超时间隔；快速重传机制；接收窗口的计算；三次握手；拥塞窗口与流量控制窗口的区别；拥塞控制中慢启动、对超时事件的反应处理；CongWin与Threshold的计算方法。</li></ul><h2 id="Chapter-4-Network-Layer-about-30"><a href="#Chapter-4-Network-Layer-about-30" class="headerlink" title="Chapter 4 Network Layer  (about 30%)"></a>Chapter 4 Network Layer  (about 30%)</h2><h3 id="4-1-Introduction"><a href="#4-1-Introduction" class="headerlink" title="4.1 Introduction"></a>4.1 Introduction</h3><ul><li>Definition and difference of forwarding and routing<ul><li>Forwarding: move packets from router’s input to appropriate router output. Like process of getting through single interchange.</li><li>Routing: determine route taken by packets from source to dest. Like process of planning trip from source to dest.</li></ul></li><li>possible services provided by network layer<ul><li>transport segment from sending to receiving host</li><li>on sending side encapsulates segments into datagrams</li><li>on rcving side, delivers segments to transport layer</li></ul></li><li>forwarding table and routing table in the router.<br>Conception is like themselves, forwarding table just care which interface should datagram go next, but routing table care which router should datagram go.</li></ul><h3 id="4-2-Virtual-Circuit-and-Datagram-Networks"><a href="#4-2-Virtual-Circuit-and-Datagram-Networks" class="headerlink" title="4.2 Virtual Circuit and Datagram Networks"></a>4.2 Virtual Circuit and Datagram Networks</h3><ul><li>Difference of virtual circuit and datagram networks<ul><li>Datagram: connectionless</li><li>VC network: connection</li></ul></li><li>Three components of virtual circuit network<ol><li>Path from source to destination</li><li>VC numbers, one number for each link along path</li><li>Entires in forwarding tables in routers along path</li></ol></li><li>VC signaling protocols: <strong>ATM, frame-relay, X.25</strong></li></ul><h3 id="4-3-What’s-inside-a-Router"><a href="#4-3-What’s-inside-a-Router" class="headerlink" title="4.3 What’s inside a Router?"></a>4.3 What’s inside a Router?</h3><ul><li>Functions of four components of the router<ol><li>Input port<ul><li>line termination</li><li>protocol, decapsulation</li><li>lookup forwarding queueing</li></ul></li><li>Switch fabric: conjunction input port and output port</li><li>Router processor: execute routing algorithm</li><li>Output port<ul><li>buffer management</li><li>processing encapsulation</li><li>line termination</li></ul></li></ol></li><li>Three techniques of switching fabrics<ul><li>Memory</li><li>Bus</li><li>Crossbar</li></ul></li><li>Where does queuing occur: Packet queues can form at both the <strong>input ports</strong> and the <strong>output ports</strong>.</li></ul><h3 id="4-4-The-Internet-Protocol"><a href="#4-4-The-Internet-Protocol" class="headerlink" title="4.4 The Internet Protocol"></a>4.4 The Internet Protocol</h3><ul><li><p>Format of IPV4 and IPV6 datagram and the difference</p><ul><li><p>IPV4:</p><ol><li>Version: 0100</li><li>Headlen: length of header</li><li>Type of service(TOS): don’t care message detail</li><li>Datagram Len: header + data]</li><li>Indentifier, flags, fragmentation offset: for fragmentation and reassembly</li><li>TTL: time to live</li><li>Upper layer protocol</li><li>Header checksum: variable(cause of TTL changed)</li><li>Source and dest IP addr 32 bits each</li></ol><ul><li>IPV6:</li></ul></li></ul><ol><li>Version: 0110</li><li>Priority: identify priority among datagrams in flow</li><li>Flow label: identify datagrams in same “flow”</li><li>Next header: identify upper layer protocol for data</li><li>Hop limit: Like TTL</li><li>Source and dest IP addr 128 bits each<ul><li>Difference:</li></ul></li><li>32bit ip addr -&gt; 128bit ip addr</li><li>fixed 40 bytes</li><li>no fragmentation allowed</li><li>no checksum</li><li>no option</li><li>ver from 0100 -&gt; 0110</li></ol></li><li><p>Transition from IPV4 to IPV6</p><ul><li>Dual stack: IPv6 nodes also have a complete IPv4 implementation <strong>as sell</strong>.</li><li>Tunneling: IPv6 carried as payload in IPv4 data-gram among IPv4 routers.</li></ul></li><li><p>IP fragmentation and reassembly</p><ul><li><p>$fragment_num = \lceil {\frac{datagram - 20} {MTU - 20}} \rceil $</p></li><li><p>$ID = x$ for x in all fragments</p></li><li><p>$fragflag = 0$ if it is the last fragment, else 1</p></li><li><p>$offset = \frac{MTU - 20} {8}$</p></li><li><p>$lastLength = datagram - (fragment_num - 1) *  ( MTU - 20 ) $</p></li></ul></li><li><p>IPV4 addressing (compute subnet address, broadcast address, subnet mask and so on, CIDR especially)</p></li><li><p>DHCP: Dynamic Host Configuration Protocol. Allow host to dynamically obtain its IP address from network server when it joins network.</p></li><li><p>Function of ICMP(Internet Control Message Protocol)</p><ol><li>error reporting</li><li>echo request / reply</li><li>ICMP msgs carried in IP datagrams.</li></ol></li></ul><h3 id="4-5-Routing-Algorithms"><a href="#4-5-Routing-Algorithms" class="headerlink" title="4.5 Routing Algorithms"></a>4.5 Routing Algorithms</h3><ul><li>default router, first-hop router, source router, destination router, shortest path and least-cost path<ul><li>default router / first-hop router: the router attached directly to the host.</li><li>source router: source host’s default router.</li><li>destination router: destination host’s default router.</li></ul></li><li>LS algorithm and DV algorithm (compute)<ul><li>Link State(Static): <ul><li>$c(x,y)$: link cost from node x to y; = ∞ if not direct neighbors </li><li>$D(v)$: current value of cost of path from source to dest. v </li><li>$p(v)$: predecessor node along path from source to v</li><li>$N’$: set of nodes whose least cost path definitively known</li></ul></li><li>Distance Vector(Dynamic):<ul><li><strong>Bellman-Ford Equation</strong>:<br>$d_x(y) = \min\limits_{v}^{} \left\{c(x, y) + d_v(y) \right\}$</li><li>$d_x(y)$: estimate of least cost from x to y</li><li>$c(x,v)$: Node x knows cost to each neighbor v</li><li>$D_x = [d_x(y): y є N ]$: Node x maintains distance vector</li><li>$D_v = [D_v(y): y є N ]$: Node x also maintains its neighbors’ distance vectors, For each neighbor v, x maintains</li><li>$S_x = [s_x(y): y є N ]$: Node x maintains successive vector $s_x(y)$ is the successive node from x to y</li></ul></li></ul></li><li>AS(autonomous systems):  aggregate routers into regions, run same routing protocol</li><li>Gateway can interplay between two AS</li></ul><h3 id="4-6-Routing-in-the-Internet"><a href="#4-6-Routing-in-the-Internet" class="headerlink" title="4.6 Routing in the Internet"></a>4.6 Routing in the Internet</h3><ul><li>types of intra-AS and inter-AS routing protocol<ul><li>intra-AS routing also known as IGP(Interior Gateway Protocols)<ul><li>RIP</li><li>OSPF</li></ul></li><li>inter-AS<ul><li>BGP</li></ul></li></ul></li><li>the definition of RIP, OSPF and BGP<ul><li>RIP: routing information protocol, <strong>based on DV</strong>, uses hop count as cost metric</li><li>OSPF: open shortest path first, <strong>based on LS</strong>， broadcast routing info to all other routers in AS</li><li>BGP: border gateway protocol, <strong>based on DV</strong></li></ul></li></ul><h3 id="Addtion-3"><a href="#Addtion-3" class="headerlink" title="Addtion"></a>Addtion</h3><ul><li>重点：转发与路由的概念与区别；虚电路网络与数据报网络的原理；路由器的基本组成；哪里会产生排队；IPV4和IPV6的报文格式及两者之间的区别；IP分段与重组；IPV4寻址；ICMP协议；网络传输中各种不同路由器的概念与区别；LS路由算法；DV路由算法；路由表；ICMP协议的功能；AS的概念；自治系统内部选路算法；RIP和OSPF协议的概念与功能。</li><li>难点：转发与路由的区别；虚电路网络与数据报网络的区别；IP分段与重组；ICMP的功能；IPV4和IPV6的报文格式的区别；LS路由算法；DV路由算法；自治系统内和自治系统间的协议；RIP和OSPF协议的功能与区别。</li></ul><h2 id="Chapter-5-Link-Layer-and-Local-Area-Network-about-10"><a href="#Chapter-5-Link-Layer-and-Local-Area-Network-about-10" class="headerlink" title="Chapter 5 Link Layer and Local Area Network  (about 10%)"></a>Chapter 5 Link Layer and Local Area Network  (about 10%)</h2><h3 id="5-1-Link-Layer-Introduction-and-Services"><a href="#5-1-Link-Layer-Introduction-and-Services" class="headerlink" title="5.1 Link Layer: Introduction and Services"></a>5.1 Link Layer: Introduction and Services</h3><ul><li>Services provided by the link layer<ol><li>Framing: encapsulate datagram into frame, adding header, trailer.</li><li>Link access: Point-to-point link, Broadcast link, MAC protocol specifies the rules by which a frame is transmitted onto the link.</li><li>Reliable delivery between adjacent nodes: move each datagram across the link without error<br>seldom used on low bit-error link (fiber, some twisted pair) wireless links: high error rates.</li><li>Flow control: pacing between adjacent sending and receiving nodes.</li><li>Error detection: errors caused by signal attenuation, noise. receiver detects presence of errors: signals sender for retransmission or drops frame.</li><li>Error correction: receiver identifies and corrects bit error(s) without resorting to retransmission.</li><li>Half-duplex and full-duplex: with half duplex, a node cannot both transmit and receive at the same time, With full-duplex, the node can both transmit and receive at the same time.</li></ol></li><li>Difference of the link layer and transport, network layer: network layer is host to host, link layer is node to node.</li><li>Conception of adapter (NIC), two types of interfaces.<ul><li>NIC(network interface card): implements link, physical layer, attaches into host’s system buses.<ul><li>interfaces: Ethernet card, PCMCI card, 802.11 card.</li></ul></li></ul></li></ul><h3 id="5-2-Error-Detection-and-Correction-Techniques"><a href="#5-2-Error-Detection-and-Correction-Techniques" class="headerlink" title="5.2 Error-Detection and Correction Techniques"></a>5.2 Error-Detection and Correction Techniques</h3><ul><li><strong>Cyclic redundancy checks - CRC</strong> (compute)</li></ul><h3 id="5-3-Multiple-Access-Protocol"><a href="#5-3-Multiple-Access-Protocol" class="headerlink" title="5.3 Multiple Access Protocol"></a>5.3 Multiple Access Protocol</h3><ul><li>Two types of link layer channel<ul><li>point to point: a single sender and a single receiver.</li><li>broadcast: multiple sender and multiple receivers.</li></ul></li><li>Conception and difference of three types of multiple access protocols, random access protocol especially<ul><li>Channel Partitioning: <strong>divide channel</strong> into smaller “pieces” (time slots, frequency, code) allocate piece to node for exclusive use<ol><li>TDMA(time division multiple access)</li><li>FDMA(frequency division multiple access)</li><li>CDMA(code division multiple access)</li></ol></li><li>Random Access: channel not divided, allow collisions <strong>“recover” from collisions</strong><ol><li>Slotted ALOHA</li><li>Pure ALOHA</li><li>CSMA(Carrier Sense Multiple Access)<ul><li>先听后发</li><li>边听边发</li><li>冲突停止</li><li>随机延迟后重发 </li></ul></li></ol></li><li>“Taking turns”: nodes take turns, but nodes with more to send can take longer turns<ol><li>Polling</li><li>Token passing</li></ol></li></ul></li><li>LAN: local area network, a computer network concentrated in a geographical area, such as in a building or on a university campus.</li></ul><h3 id="5-4-Link-Layer-Addressing"><a href="#5-4-Link-Layer-Addressing" class="headerlink" title="5.4 Link-Layer Addressing"></a>5.4 Link-Layer Addressing</h3><ul><li><p>MAC address: function: get frame from one interface to another physically-connected interface (same network), has 48bits long, permanet and unique.</p></li><li><p>Difference of MAC address and IP address, hostname</p><ul><li>MAC flat address: portability, can move LAN card from one LAN to another</li><li>IP hierarchical address not portable, depends on IP subnet to which node is attached</li></ul></li><li><p>Conception and principle of ARP: Translates IP addresses to MAC addresses , only resolves IP addresses only for nodes on the same subnet.</p><ul><li><p>ARP query: </p><ol><li><p>Same LAN, Query packet, A want to find B’s MAC address, it’s a kind of broadcast.</p></li><li><p>Same LAN, Response packet, B sends to A specifically.</p></li><li><p>Different LAN, suppose R is router between two subnets. Frame A to R.</p></li><li><p>Different LAN , Frame R to B.</p><table><thead><tr><th>Condition</th><th>Source IP</th><th>Dest IP</th><th>Source MAC</th><th>Dest MAC</th></tr></thead><tbody><tr><td>1. Query packet</td><td>A’s IP</td><td>B’s IP</td><td>A’s MAC</td><td>FF-FF-FF-FF-FF-FF</td></tr><tr><td>2. Response packet</td><td>B’s IP</td><td>A’s IP</td><td>B’s MAC</td><td>A’s MAC</td></tr><tr><td>3. Frame A to R</td><td>A’s IP</td><td>B’s IP</td><td>A’s MAC</td><td>R’s interface MAC</td></tr><tr><td>4. Frame R to B</td><td>A’s IP</td><td>B’s IP</td><td>R’s interface MAC</td><td>B’s MAC</td></tr></tbody></table></li></ol></li></ul></li></ul><h3 id="5-5-Ethernet"><a href="#5-5-Ethernet" class="headerlink" title="5.5 Ethernet"></a>5.5 Ethernet</h3><ul><li><p>Format of Ethernet frame</p><ol><li>Preamble: used to synchronize receiver, sender clock rates</li><li>Dest addr(MAC) (6 bytes)</li><li>Source addr(MAC) (6 bytes)</li><li>Types(2 bytes): indicates higher layer protocol</li><li>Data</li><li>CRC(4 bytes)</li></ol></li><li><p>Mechanism and principles of CSMA/CD</p><ul><li>Jam Signal: make sure all other transmitters are aware of collision; 48 bits</li><li>Expnential Backoff: adapt retransmission attempts to estimated current load</li></ul></li><li><p>Service provided by Ethernet</p><ul><li>connectionless: No handshaking between sending and receiving NICs</li><li>unreliable: receiving NIC doesn’t send acks or nacks to sending NIC. Stream of datagrams passed to network layer can have gaps (missing datagrams). Gaps will be filled if app is using TCP otherwise, app will see gaps.</li><li>baseband transmission: The adapter sends a digital signal directly into the broadcast channel.</li><li>Ethernet’s MAC protocol: CSMA/CD </li><li>Encoding: Manchester encoding(<strong>up edge is 1, down edge 0</strong>)</li></ul></li></ul><h3 id="5-6-Link-Layer-Switches"><a href="#5-6-Link-Layer-Switches" class="headerlink" title="5.6 Link-Layer Switches"></a>5.6 Link-Layer Switches</h3><ul><li><p>Difference of forwarding and filtering</p><ul><li>Filtering: is the ability of a switch to determine whether a frame should be forwarded to some interface of should just be dropped</li><li>Forwarding: is the ability to determine the interfaces to which a frame should be directed and then directing the frame to those interfaces(use a switch table)</li></ul></li><li><p>Functions of hubs and switches</p><ul><li>hub: physical-layer  (“dumb”) repeaters: acts on individual bit rather than on frames, bits coming in one link go out all other links at same rate, all nodes connected to hub can collide with one another</li><li>switch: smarter than hubs, take active role<br>store, forward Ethernet frames examine incoming frame’s MAC address, selectively forward frame to one-or-more outgoing links when frame is to be forwarded on segment, uses CSMA/CD to access segment</li></ul></li><li><p><strong>Compare hubs and switches to routers</strong></p><table><thead><tr><th>Attribute</th><th>Hubs</th><th>Routers</th><th>Switches</th></tr></thead><tbody><tr><td>traffic isolation</td><td>no</td><td>yes</td><td>yes</td></tr><tr><td>plug &amp; play</td><td>yes</td><td>no</td><td>yes</td></tr><tr><td>optimal routing</td><td>no</td><td>yes</td><td>no</td></tr><tr><td>cut through</td><td>yes</td><td>no</td><td>yes</td></tr><tr><td>layers</td><td>physical</td><td>network</td><td>link</td></tr></tbody></table></li></ul><h3 id="Addtion-4"><a href="#Addtion-4" class="headerlink" title="Addtion"></a>Addtion</h3><ul><li>重点：链路层提供的服务；链路层与网络层和传输层的不同；CRC校验法；多址访问协议；随机访问协议；TDM、FDM、CDMA协议；局域网；MAC寻址及其与IP寻址的区别；ARP、DHCP协议的功能；以太网帧的结构；CSMA/CD协议原理；交换机与集线器的功能；交换机、集线器和路由器的区别。</li><li>难点：CRC校验法；多址访问协议；随机访问协议；MAC寻址和IP寻址的区别；ARP、DHCP协议的功能；CSMA/CD协议的原理；交换机、集线器和路由器的区别。</li></ul><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ol><li><p>Port number and protocols </p><table><thead><tr><th>Protocol</th><th>Port  number</th><th>Transport  layer protocol</th></tr></thead><tbody><tr><td>HTTP</td><td>80</td><td>TCP</td></tr><tr><td>FTP</td><td>21</td><td>TCP</td></tr><tr><td>SMTP</td><td>25</td><td>TCP</td></tr><tr><td>POP3</td><td>110</td><td>TCP</td></tr><tr><td>IMAP</td><td>143</td><td>TCP</td></tr><tr><td>DNS</td><td>53</td><td>UDP</td></tr></tbody></table><ul><li>p2p based on HTTP, it have no self protocol.</li></ul></li><li><p>TCP, UDP server and action</p><table><thead><tr><th>Protocol</th><th>Server</th><th>Action</th></tr></thead><tbody><tr><td>UDP</td><td>error detection</td><td>check sum</td></tr><tr><td>TCP</td><td>connection-orient</td><td>three way handshake</td></tr><tr><td>TCP</td><td>in-order and reliable</td><td>sequence, ACK</td></tr><tr><td>TCP</td><td>flow control</td><td>receive window</td></tr><tr><td>TCP</td><td>congestion control</td><td>AIMD, slow start, reaction to  timeout</td></tr><tr><td>TCP</td><td>error detection</td><td>check sum</td></tr></tbody></table></li><li><p>Layers duty</p><table><thead><tr><th>Layer</th><th>Between</th><th>Implement  method</th></tr></thead><tbody><tr><td>transport</td><td>process</td><td>socket - port number</td></tr><tr><td>network</td><td>host</td><td>router - IP address</td></tr><tr><td>link</td><td>node</td><td>hubs  or switches - MAC address</td></tr></tbody></table></li><li><p>CDMA (code division multiple access) is different from CSMA(Carrier Sense Multiple Access), they are confusable.</p></li><li><p>In fragmentation and reassembly, length means bytes length include headers, but ask you to write bytes, it means payload in the fragment, do not include headers.</p></li></ol><ul><li>Type<ul><li>1.3 Types of delay compute</li><li>2.2 HTTP protocol html</li><li>3.3 Check sum compute</li><li>3.5 TCP congestion control algorithm with timeout and duplicated ACK</li><li>3.5 Receive window</li><li>3.5 TCP connection management(simple condition)</li><li>4.1 Forwarding Table: select link interface – Longest prefix matching</li><li>4.4 IPV4 addressing (compute subnet address, broadcast address, subnet mask and so on, CIDR especially)</li><li>4.4 IP fragmentation and reassembly</li><li>4.5 Routing Algorithm: LS and DV</li><li>5.2 Cyclic redundancy checks - CRC </li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 计算机基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>树莓派入坑指南</title>
      <link href="/posts/12730.html"/>
      <url>/posts/12730.html</url>
      
        <content type="html"><![CDATA[<h1 id="树莓派入坑指南"><a href="#树莓派入坑指南" class="headerlink" title="树莓派入坑指南"></a>树莓派入坑指南</h1><h2 id="配件购买"><a href="#配件购买" class="headerlink" title="配件购买"></a>配件购买</h2><p>最好不要一起套装, 自己组一套可以比店里的套装划算很多.</p><h3 id="主板"><a href="#主板" class="headerlink" title="主板"></a>主板</h3><p>树莓派4B和3B+具有差不多的价格, 但是4B明显性能提升还是挺明显的所以直接买4B</p><h3 id="SD卡-读卡器"><a href="#SD卡-读卡器" class="headerlink" title="SD卡+读卡器"></a>SD卡+读卡器</h3><p>16/32/64GB都可以, 现在情况16和32的价格差不多, 空间翻一倍, 买32G足够. 据说空间更大的可能导致树莓派无法开机.</p><h3 id="风扇-外壳"><a href="#风扇-外壳" class="headerlink" title="风扇+外壳"></a>风扇+外壳</h3><p>没有必要买官方原装或者是金属的外壳, 直接买亚克力板的外壳就行, 最好配备风扇, 夏天这玩意跑起来实在是吃不消.</p><h3 id="充电器"><a href="#充电器" class="headerlink" title="充电器"></a>充电器</h3><p>树莓派4B官方说用5V3A的type-c充电器, 比3B+的要高了0.5A. 充电器最好是买带开关的, 因为树莓派的type-c口拔拔插插的很麻烦, 一个开关可以节省很多事情.</p><h3 id="散热片"><a href="#散热片" class="headerlink" title="散热片"></a>散热片</h3><p>虽然说官方不建议给CPU配备散热片, 但是散热片也没几块钱, 就买了一小套.</p><h3 id="摄像头"><a href="#摄像头" class="headerlink" title="摄像头"></a>摄像头</h3><p>那种配置很高的摄像头没必要, 就买500W像素的就够. 就算你图像质量再高, 到DL模型时候还是需要reshape成更低像素的数据, 只是说图像质量高后提供更多的信息, 如果是跑小项目应该没必要. 我觉得买的24块钱的摄像头就足够.</p><p>摄像头测试指令:</p><pre class="line-numbers language-bash"><code class="language-bash">raspistill -o image.jpg<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="飞鼠"><a href="#飞鼠" class="headerlink" title="飞鼠"></a>飞鼠</h3><p>也就是那种可触控式的鼠标和键盘, 好像挺装逼的, 可以买一个来玩玩, 暂时没啥必要.</p><h2 id="烧录系统"><a href="#烧录系统" class="headerlink" title="烧录系统"></a>烧录系统</h2><ol><li>下载Imager和系统镜像现在树莓派官网有更方便的Raspberry imager了, 也就是更方便的SD卡烧录软件. 去官网可以下载</li></ol><p><a href="https://www.raspberrypi.org/downloads/" target="_blank" rel="noopener">https://www.raspberrypi.org/downloads/</a></p><blockquote><p>系统中建议用Raspbian, NOOBS会占用很多没有必要的空间.</p></blockquote><ol start="2"><li><p>先将读卡器插入SD卡中, 然后把USB读卡器插到电脑上</p></li><li><p>如果SD卡是新的可以不用清除, 若是旧的则先要用Imager进行卡的擦除.</p></li><li><p>下载完镜像<code>.zip</code>后进行解压, 然后打开Imager选择解压完的<code>.img</code>文件和你的SD卡进行系统烧录.</p></li><li><p>安装后打开boot盘(不要看错盘符)</p></li><li><p>在boot下新建一个名为<code>ssh</code>的文件, 没有任何后缀, 这样可以在树莓派没开机的情况下开启SSH.</p></li><li><p>同在boot下新建一个名为<code>wpa_supplicant.conf</code>的文件, 其内容如下</p><pre><code>country=GBctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdevupdate_config=1network={    ssid=&quot;你的wifi名字&quot;    psk=&quot;你的wifi密码&quot;    priority=你想设的优先级大小, 数字越大越优先}</code></pre></li></ol><ol start="8"><li>安全退出SD卡, 并将SD卡怼入树莓派SD卡插槽中, 准备树莓派的开机之旅.</li></ol><h2 id="开机过程"><a href="#开机过程" class="headerlink" title="开机过程"></a>开机过程</h2><h3 id="两种指示灯"><a href="#两种指示灯" class="headerlink" title="两种指示灯"></a>两种指示灯</h3><blockquote><p>树莓派有两个指示灯, 一个是红灯, 一个是绿灯.</p><p>红灯是电源指示灯, 红灯常亮说明电源接通正常, 若红灯闪烁说明电源质量不佳, 可能是功率问题, 也可能是接触不良等.</p><p>绿灯是读写指示灯, 可以通过绿灯闪烁情况观察到储存卡的读写情况.</p></blockquote><p>给树莓派接上电源, 观察到红灯常亮, 绿灯闪烁非常频繁, 这说明树莓派供电良好, 正在初始化系统.正常情况下树莓派4B2G在5分钟以内一定会完成第一次初始化. <strong>当树莓派连入前面设置的wifi后, 初始化就完成了.</strong> 接下来就需要找到树莓派的IP, 用电脑和它进行SSH连接.</p><h3 id="找到树莓派地址"><a href="#找到树莓派地址" class="headerlink" title="找到树莓派地址"></a>找到树莓派地址</h3><p>找到ip地址有很多种花样, 只要会其中一种就行了</p><ol><li><p>通过wifi寻找</p><p>既然树莓派开机默认连上了wifi, 说明树莓派系统初始化已经完成. 直接登录wifi的管理界面就能看到所有设备信息, 能很容易地找到一台刚加入网络的设备, 肯定是树莓派, 记录下它的ip地址. </p><blockquote><p>如果不知道自己家路由器ip, 在cmd里敲ipconfig能看到本机网卡的网关, 那个就是路由器地址.</p></blockquote></li><li><p>ARP查询</p><pre class="line-numbers language-bash"><code class="language-bash">arp -a<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>找到所有的动态IP然后一一排除. 其实与上述办法基本一致.</p></li><li><p>局域网扫描</p><p>这个网上有很多软件一抓一大把. 也是与前两种方法一样.</p></li></ol><h3 id="SSH连接"><a href="#SSH连接" class="headerlink" title="SSH连接"></a>SSH连接</h3><p>用putty可以轻松完成连接功能, 在连接的IP中输入刚才找到的ip, 端口号默认是22不要动, 会弹出一个命令行</p><p><strong>填入用户名pi, 密码raspberry</strong>就能连接到树莓派上了, 若出现命令行变色, 且路径发生变化说明连接成功.</p><h3 id="用Windows远程桌面连接树莓派"><a href="#用Windows远程桌面连接树莓派" class="headerlink" title="用Windows远程桌面连接树莓派"></a>用Windows远程桌面连接树莓派</h3><ul><li><p>安装xrdp和VNC</p><blockquote><p>Xrdp是一个开源工具，允许用户通过Windows RDP访问Linux远程桌面。 除了Windows RDP之外，xrdp工具还接受来自其他RDP客户端的连接，如FreeRDP，rdesktop和NeutrinoRDP。 Xrdp现在支持TLS安全层。</p></blockquote><blockquote><p>VNC(Virtual Network Console)是虚拟网络控制台的缩写。它 是一款优秀的远程控制工具软件，由著名的 AT&amp;T 的欧洲研究实验室开发的。VNC 是在基于 UNIX 和 Linux 操作系统的免费的开源软件，远程控制能力强大，高效实用，其性能可以和 Windows 和 MAC 中的任何远程控制软件媲美。</p></blockquote><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> xrdp<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> tightvncserver<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li><p>将xrdp启动和添加默认项</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">sudo</span> /etc/init.d/xrdp start<span class="token function">sudo</span> update-rc.d xrdp defaults<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li><p>然后用Windows自带的远程连接, 连接树莓派ip即可, 账号密码同前.</p></li></ul><h3 id="用VNCViewer连接树莓派"><a href="#用VNCViewer连接树莓派" class="headerlink" title="用VNCViewer连接树莓派"></a>用VNCViewer连接树莓派</h3><ol><li><p>首先要在电脑中下载VNCviewer</p></li><li><p>在树莓派终端中输入<code>sudo raspi-config</code>, 选择<strong>Interfacing options</strong>, 找到VNC的设置将其打开.</p></li><li><p>用VNCViewer连接树莓派ip, 账号密码同前.</p><blockquote><p>若出现当前桌面无法显示的黑屏情况<code>cannot currently show the desktop</code>, 请在树莓派设置中将<strong>Advanced options</strong>找到分辨率(Resolution)调高, 反正不是默认的就行.</p></blockquote></li></ol><h3 id="用TeamViewer连接树莓派"><a href="#用TeamViewer连接树莓派" class="headerlink" title="用TeamViewer连接树莓派"></a>用TeamViewer连接树莓派</h3><p>坑 待填</p><h3 id="树莓派的关机和重启"><a href="#树莓派的关机和重启" class="headerlink" title="树莓派的关机和重启"></a>树莓派的关机和重启</h3><ul><li><p>树莓派关机</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">shutdown</span> -h now<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>树莓派重启</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">reboot</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul><h2 id="系统配置"><a href="#系统配置" class="headerlink" title="系统配置"></a>系统配置</h2><h3 id="将接口配置打开"><a href="#将接口配置打开" class="headerlink" title="将接口配置打开"></a>将接口配置打开</h3><p>在<code>raspi-config</code>的<code>Interfacing options</code>中, 有很多功能选项都可以打开.</p><blockquote><p>Camera：摄像头</p><p>SSH：ssh远程通信与登陆</p><p>VNC：VNC远程桌面登陆</p><p>Serial：串口控制</p><p>Remote GPIO：远程GPIO引脚控制</p></blockquote><h3 id="给apt-get换源"><a href="#给apt-get换源" class="headerlink" title="给apt-get换源"></a>给apt-get换源</h3><p>在终端中输入:</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">nano</span> /etc/apt/sources.list<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>然后用上下左右控制光标, 将唯一有效的默认网址加<code>#</code>注释掉, 然后在最下面加上:</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># 编辑 `/etc/apt/sources.list` 文件，删除原文件所有内容，用以下内容取代：</span>deb http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ buster main non-free contribdeb-src http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ buster main non-free contrib<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>按<strong>ctrl+o</strong>提交改动, 再按回车保存, 按<strong>ctrl+x</strong>退出nano.</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># 编辑 `/etc/apt/sources.list.d/raspi.list` 文件，删除原文件所有内容，用以下内容取代：</span>deb http://mirrors.tuna.tsinghua.edu.cn/raspberrypi/ buster main ui<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="给pip换源"><a href="#给pip换源" class="headerlink" title="给pip换源"></a>给pip换源</h3><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">mkdir</span> ~/.pip<span class="token function">cd</span> .pip<span class="token function">sudo</span> <span class="token function">nano</span> pip.conf<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>在pip.conf里输入以下内容</p><pre><code>[global]timeout = 10index-url =  http://mirrors.aliyun.com/pypi/simple/extra-index-url= http://pypi.douban.com/simple/[install]trusted-host=    mirrors.aliyun.com    pypi.douban.com</code></pre><p>保存方式同上.</p><h3 id="切换Python版本"><a href="#切换Python版本" class="headerlink" title="切换Python版本"></a>切换Python版本</h3><p>树莓派默认的python版本是2.7, 但是大多数人早在用python3了, 所以应该及时切换到最新版本</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">sudo</span> update-alternatives --install /usr/bin/python python /usr/bin/python2 100<span class="token function">sudo</span> update-alternatives --install /usr/bin/python python /usr/bin/python3 150<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>如果要切换回python2.7可以用如下指令</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">sudo</span> update-alternatives --config python<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="配置中文系统"><a href="#配置中文系统" class="headerlink" title="配置中文系统"></a>配置中文系统</h3><ul><li><p>先装个中文输入法和字体</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> fonts-wqy-zenhei<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> fcitx fcitx-googlepinyin fcitx-module-cloudpinyin fcitx-sunpinyin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>可以使用<code>ctrl</code>+<code>空格</code>切换为中文输入法.</p></li></ul><p>说实话中文linux太捞了, 只建议把时区改回中国, 均为<code>sudo raspi-config</code>的<strong>Localisation Options</strong>中的设置</p><ul><li><strong>Change Locale</strong> 可以更改国家, 等到光标达到zh_CN.UTF-8 UTF-8时按空格, 然后选择默认区域为zh_CN.UTF-8可以改为中文(真的很捞).</li><li><strong>Change Timezone __可以更改时区, 选择</strong>Asia__, <strong>Shanghai</strong>即为中国时间.</li><li>__Change Wi-fi Country __ 选择wifi国家, CN China为中国.</li></ul><h2 id="安装OpenCV-神坑"><a href="#安装OpenCV-神坑" class="headerlink" title="安装OpenCV(神坑)"></a>安装OpenCV(神坑)</h2><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>首先要在<code>raspi-config</code>中的<code>Advanced Options</code>中的第一个选项<code>Expand Filesystem Ensures...</code>将文件系统的空间扩展至整个SD卡. 之后重启, 重启后用<code>df -h</code>查看磁盘是否已经扩展.</p><p>如果空间不够, 可以用删除缓存的方法扩大一些空间</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> purge wolfram-engine<span class="token function">sudo</span> <span class="token function">apt-get</span> purge libreoffice*<span class="token function">sudo</span> <span class="token function">apt-get</span> clean<span class="token function">sudo</span> <span class="token function">apt-get</span> autoremove<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>在这之后要例行更新一下系统.</p><h3 id="安装各种准备库"><a href="#安装各种准备库" class="headerlink" title="安装各种准备库"></a>安装各种准备库</h3><pre class="line-numbers language-bash"><code class="language-bash">//安装Cmake等编译openCV源码的工具<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> build-essential cmake <span class="token function">git</span> pkg-config // 安装常用图像工具包<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> libjpeg-dev libtiff5-dev libjasper-dev libpng12-dev// 安装视频操作的包<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> libavcodec-dev libavformat-dev libswscale-dev libv4l-dev  <span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> libxvidcore-dev libx264-dev// openCV用于图像/GUI展示的功能依赖highgui模块，为了编译它我们需要安装libgtk2.0-dev<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> libgtk2.0-dev// 安装OpenCV数值优化函数包<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> libatlas-base-dev gfortran// 安装构建Python扩展所需要的头文件<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> python2.7-dev python3-dev// 如果缺少其他依赖, 尝试安装下述两个包<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> libhdf5-dev<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> libqt4-test<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="下载Opencv"><a href="#下载Opencv" class="headerlink" title="下载Opencv"></a>下载Opencv</h3><p>一个是Opencv的zip, 还有一个是Opencv_contrib的zip. 经过很多折腾和摸索, 最后发现4.1.0的版本在CMake时编译不会产生错误, 所以最后选择了4.1.0版本.</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">wget</span> -O opencv-4.1.0.zip https://github.com/Itseez/opencv/archive/4.1.0.zip<span class="token function">wget</span> -O opencv_contrib-4.1.0.zip https://github.com/Itseez/opencv_contrib/archive/4.1.0.zip<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="设置交换空间"><a href="#设置交换空间" class="headerlink" title="设置交换空间"></a>设置交换空间</h3><p>这个交换空间是在内存不够用的时候, 将磁盘空间充当内存空间的方法. 分配太多的Swap空间会浪费磁盘, 甚至对SD卡造成损坏且减少寿命, 一般设置在2G一下. 但是因为编译可能会占用很多的内存, 所以短时间增大Swap空间的大小是可以的.</p><p><code>sudo nano /etc/dphys-swapfile</code>将<code>CONF_SWAPSIZE</code>改为2048</p><p><code>sudo /etc/init.d/dphys-swapfile stop</code> </p><p><code>sudo /etc/init.d/dphys-swapfile start</code></p><p>这样才能重启交换服务.</p><p><strong>千万不要忘记在安装完后把它改回来.</strong></p><h3 id="编译过程"><a href="#编译过程" class="headerlink" title="编译过程"></a>编译过程</h3><p>对<code>opencv-4.1.0.zip</code>和<code>opencv_contrib-4.1.0.zip</code>两个文件进行解压</p><pre class="line-numbers language-bash"><code class="language-bash">unzip opencv_contrib-4.1.0.zipunzip opencv-4.1.0.zip<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>然后切换至``opencv-4.1.0<code>这个文件夹, 并在其下面新建编译文件准备存放的目录</code>build`</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">cd</span> ~/opencv-4.1.0/<span class="token function">mkdir</span> build <span class="token operator">&amp;</span> <span class="token function">cd</span> build<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>然后开始进行编译的配置</p><pre class="line-numbers language-bash"><code class="language-bash">cmake -D ENABLE_PRECOMPILED_HEADERS<span class="token operator">=</span>OFF \ -D CMAKE_BUILD_TYPE<span class="token operator">=</span>RELEASE \ -D CMAKE_INSTALL_PREFIX<span class="token operator">=</span>/usr/local \ -D INSTALL_PYTHON_EXAMPLES<span class="token operator">=</span>ON \ -D OPENCV_EXTRA_MODULES_PATH<span class="token operator">=</span>~/opencv_contrib-4.1.0/modules \ -D BUILD_EXAMPLES<span class="token operator">=</span>ON <span class="token punctuation">..</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><blockquote><p>如果遇到BUG<br><code>fatal error: opencv2/xfeatures2d/cuda.hpp: No such file or directoryinclude “opencv2/xfeatures2d/cuda.hpp”</code></p><p><strong>解决办法：</strong><br>在CMakeLists.txt增加以下内容：</p><p><code>INCLUDE_DIRECTORIES(&quot;你的opencv_contrib路径/modules/xfeatures2d/include&quot;)</code></p><p>CMakeLists.txt文件所在路径为：</p><p><code>你的opencv路径/modules/stitching/CMakeLists.txt</code></p></blockquote><p>下面开始编译, 这个过程十分漫长, 可以加上参数<code>-j4</code>让四个线程同时跑, 如果对自己机器没有信心也可以不加, 编译失败的话可以尝试去掉<code>-j4</code>这个参数直接make. __请注意命令中的opencv_contrib-4.1.0文件夹的位置是否和解压的位置相符.</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">make</span> -j4 <span class="token function">make</span> <span class="token function">install</span> <span class="token function">make</span> ldconfig<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>大功告成, 接下来就可以测试你的摄像头好不好使了.</p><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>打开<code>Thoony</code>, 以下是测试摄像头的代码, 如果没有摄像头只需要在终端里面进入python, 然后import cv2看看是否报错即可.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> cv2cap <span class="token operator">=</span> cv2<span class="token punctuation">.</span>VideoCapture<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>cap<span class="token punctuation">.</span>set<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">640</span><span class="token punctuation">)</span>cap<span class="token punctuation">.</span>set<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">480</span><span class="token punctuation">)</span><span class="token keyword">while</span><span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>    ret<span class="token punctuation">,</span> frame <span class="token operator">=</span> cap<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>    cv2<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span><span class="token string">'frame'</span><span class="token punctuation">,</span> frame<span class="token punctuation">)</span>    k <span class="token operator">=</span> cv2<span class="token punctuation">.</span>waitKey<span class="token punctuation">(</span><span class="token number">30</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span> <span class="token number">0xff</span>    <span class="token keyword">if</span> k <span class="token operator">==</span> <span class="token number">27</span><span class="token punctuation">:</span>        <span class="token keyword">break</span>cap<span class="token punctuation">.</span>release<span class="token punctuation">(</span><span class="token punctuation">)</span>cv2<span class="token punctuation">.</span>destroyAllWindows<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 小玩意儿 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 树莓派 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 选购指南 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
